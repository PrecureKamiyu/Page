#+title: Memory and its Performance
#+HTML_HEAD: <style>body {font-size: 200%; margin: 20%;}</style>
* Beginning
  - Note taken on [2023-06-07 Wed 22:37] \\
    This chapter has huge amount of content, but arranged in a shitty way. Becareful.

我们来看一下 Tang 的目录. Gosh, 如果能够居中就好了.

1. 概述
2. Main Memory. 内存也称为 Main 存
3. Cache. Main 存和 CPU (Processor) 之间的
4. 虚拟 Memory. Main 存相关的.
5. 辅助存储器. External 存储器. 比如说硬盘.

* 概述
  - Note taken on [2023-06-07 Wed 22:24] \\
    And you don't have to remember the classification here. You can just learn it, with no pressure.
  - Note taken on [2023-06-07 Wed 22:22] \\
    This section is absolutely shit.

为什么研究存储器非常重要?

- CPU 的运行速度变得高的同时, 存储器的读取速度跟不上这个发展, 于是说计
  算机系统的运行速度很大程度上收到了存储器的制约.
- 另一方面, 当我们 IO 设备的数量不断增多, 若是 IO 设备和存储器之间的信
  息交换都是直接通过 CPU 来实现. 那么其将降低 CPU 的运转效率.

随后我们对存储器进行分类, 因为, 嘛, 反正我们有很多存储器. 随后, 这种分
类让我们看到的那些区别, 正是这些区别让我们有 Memory Heirarchy. 目前我
们有三种分类方法: 

1. 按照存储介质进行分类; 
2. 按照存取方式进行分类; 
3. 按照功能进行分类.

这些分类方法是让我们看见这些存储方式的特点. 总之是有好处的.
** 分类学

- Note taken on [2023-06-16 Fri 12:53] \\
  You should the word "random accessing" which means that you can access to the memory specified by the address you have given.

按照存储介质进行分类:
1. SemiConductor 存储器. 这是一种 volatile 的存储器, 也就是 "断电会丢
   失" 的存储器.
2. 磁表面存储器. Magnetic Disk. 使用了磁介质来存储.
3. 磁芯存储器. ?什么价吧. 
4. 光盘存储器. 在读写过程之中应用了激光.

--------

然后我们还可以按照存储方式分类
1. RAM. 可读可写的存储器. 有 SRAM (Static RAM) 和 DRAM (Dynamic RAM).
2. ROM. 只可读的存储器. 有很多种, 比如说 EPROM, EEPROM, Flash
   Memory. 值得注意的是, Flash Memory 的功能并不是很一样, 虽然其也是
   ROM 的一种, 但是从结果上来看, 功能已经很不一样了.
3. 串行访问存储器. 简单来说, 就是和 Random accessing 相反的一个存在.
   RA 指的是, 能够通过地址来进行访问. 这种就是随机访问, 也就是说, 给定
   了一个地址, 我们能够直接访问到那个地址里面的数据. 串行访问存储器就
   是不能够做到这一点的存储器.

---------

然后我们还可以按照功能进行分类. 这里就不进行分类了, 因为分类是显然的.
这是由他们的物理特性决定的.

** Memory heirarchy

一个金字塔. 越接近 CPU, 存储器就越快, 越贵; 越远离 CPU, 存储器就越慢, 越便宜, 于是相应的就越多. 我们稍微看一下这个层级,简单来说, 我们有

#+begin_example
CPU <-> 缓存 <-> Main 存 <-> 辅存
#+end_example

huancun refers to the cache. Cache is usually not visible to the programmers. Programmers mostly manipulate with the memory, which is here, the main memory. fucun refers to the external memory, for example disk.

Main 存就是我们常说的内存啦. 缓存的存在是为了加速 CPU 和 Main 存之间的交互. 如果说我们知道了 hit rate 的概念, 当这个 hit rate 的数值接近于 1 的时候, 我们就可以说, CPU 能够以 "缓存的速度" 去访问 Main 存, 也就是说, 相当于缓存的大小被扩张为 Main 存的大小.

值得注意的是, Main 存以及 Cache 可以被称为 Internal Memory. 因为其是放
在板上的, 与之相对的是, 辅存被称为 External Memory. 这是说, 存储设备是
和 Chip 分开的. 于是我们能够知道访问 External Memory 的时候, 需要涉及
Bus. 应该.

---------

在 main 存和 CPU 之间还发展出来了虚拟存储. 简单来说, 这是一层抽象.  因
为我们的 Main 存的大小是实际上并没有那么大, 这层抽象让我们可以将一些并
不是 Main 存的地方 "看作" 是 Main 存. 剩下的细节交给系统和操作系统进行
处理.

这里面我们有两种概念: 1. 逻辑 (Logical) 地址; 2. 物理 (Physical) 地址.
前者就是一种逻辑上的地址, 已经被封装过了的地址, 而物理地址就是实际的,
Chip 上面的地址, 程序执行过程之中真正访问的地址.

** Main 存

*Word*: 字长便是 word 的长度. 如果说内部有 4 个 bytes, 这四个字节是可以独立寻
址的. 也就是说, 有一个特定的地址和其对应. 如果说地址长度是 n, word 的
长度是 \(2^m\)  bytes 的话, "寻址范围" 为 \(2^{n-m}\) 以及 \(2^n\).

我们能够有按 word寻址, 也能够按照 bytes 寻址

*寻址范围*: 一般来说, 地址的长度就能够算出 "寻址范围", 虽然说是范围, 但实际上, 我
们可以直接认为是, "能够访问的单元个数的多少". 这里我们可以讨论两种寻址
方式的 "寻址范围". 是很简单的东西.


*技术指标*: 简单来说, 有两个指标: 1. Capacity; 2. Speed. 

- Capacity: 对于前者我们可以计算一个 Main 存之中的 bits 数量, 也可以计算其中 bytes数量. 这很明显是废话. 一般来说, 我们以 bytes 为单位. 当我们说出 1M 的存储容量的时候, 我们能够知道其地址长度为 20. 因为 2^20 约等于 1M, 也即, 我们能够访问 1M 的数量的 bytes.
- Speed: 指的是存储器获取到数据所用的时间. 这个指标可能还可以使用 Latency 来描述
- 带宽: 表示单位时间内, 存储器存取的信息量. 能够知道, 带宽和 bandwidth 和 Speed 之间有强关联.

* SemiConductors Chips
** 半导体存储芯片的基本结构

我们看芯片的接线就行了. *地址线*, 输入地址; *片选器*: 选择芯片内部的 RAM
芯片, 这是说, 一个存储器可以由很多个 RAM 芯片组成, 我们通过这个片选器
来选择具体是哪一个芯片.  *数据线*, 从芯片之中接出来的线, 大小为一个
word. *读写控制器*, 控制读写的使能.

** RAM
*** SRAM 和 DRAM
- Note taken on [2023-06-16 Fri 15:02] \\
  The unit of DRAM can be consist of 4 transistors. But we can simplify it such that it consist of 1 transistor.

SRAM 使用了多个晶体管 (6个, 具体来说), 而 DRAM 使用了 1 个晶体管和一个
电容 (这是指一个存储单元内部用到的元件). 后者的造价便宜, 但是访问速度
比较慢, 并且需要刷新, 这个刷新是很有意思的概念, 这是说, DRAM 之中的数
据会 fade away, 于是说, 经常需要刷新. 其在读取之后, 内部的存储信息也会
失效, 于是需要将原本存储的信息再放回去.

我们先是介绍 DRAM 的结构: 我这里建议读者直接查看书本. 稍微了解到这个晶
体管是如何工作的. 这里说, 当我们读取数据的时候, 电容 (Capacitor) 放电,
于是我们得到了信息, 但是同时电容将电放出, 于是这里需要 recharge.

随后是介绍 SRAM 的结构: 我这里建议读者直接查看书本. 当我们知道了晶体管
是怎么工作的之后, 了解 SRAM 的结构就不是一件难事了. SRAM 里面是一个
Latch. 这是一个简单的 latch. 有两个端: A_1, A_2. 我们有一个 bit 的输入,
记为 B, 那么我们将 B 接入 A_1, bar B接入 A_2. 这就是一个 bit 的写操作.

OK, 去读, 去看图. 参见 Tang 第二版 76 页. 值得注意的是, Tang 写得一坨
答辩. 可以先去 81 页了解一下 DRAM 的工作原理, 因为 Tang 并没有介绍晶体
管是怎么工作的. 也可以参考 Stallings 的书.

*** SRAM 和 DRAM 的结构示意图

草泥马这写的是什么几把. 建议查看 [[https:en.wikipedia.org/wiki/Dynamic_random-access_memory][wikipedia]] for more information. 主要
是接线很多, 但是又不知道接线是用来干什么的. 首先我们要认知到, SRAM 和
DRAM 的基本存储单元是什么? 有多少个接线. 随后我们才能读懂书上的这些图.

DRAM 和 SRAM 的单元是类似的, 都有着: 
- 读选择线, 写选择线
- 读数据线, 写数据线
就是说, 一个单元格具有两个输入和两个输出. 这里有一点不同, 就是 DRAM 有
一个预充电信号. 
总之略, 最好还是看看书. 我这里就不进行抄写了.

One should learn the structure of DRAM and then the structure of SRAM. Because you need to know how the transistor works, which is very important.

OK, 去读, 去看图. 参见 Tang 第二版 76 页. 值得注意的是, Tang 写得一坨
答辩. 可以先去 81 页了解一下 DRAM 的工作原理, 因为 Tang 并没有介绍晶体
管是怎么工作的. 也可以参考 Stallings 的书.

**** The structure of DRAM

我们先是介绍 DRAM 的结构: 我这里建议读者直接查看书本. 稍微了解到这个晶
体管是如何工作的. 这里说, 当我们读取数据的时候, 电容 (Capacitor) 放电,
于是我们得到了信息, 但是同时电容将电放出, 于是这里需要 recharge.

DRAM consists of a capatitor (idk how it spell) and three transistor (and we also have a version with only one transistor). 

The transistor works like a gate. The middle input can be viewed as a handle. If it is on the signal can go through the transistor, (the gate is open).

The capacitor store some 电压; if the gate is open, the current just let go; if the gate is closed, the current remains. The read process is the exact process of let go the electron stored in capacitor. 

Conversely, the process of write is to store some electron into the capacitor.

**** The structure of SRAM

随后是介绍 SRAM 的结构: 我这里建议读者直接查看书本. 当我们知道了晶体管
是怎么工作的之后, 了解 SRAM 的结构就不是一件难事了. SRAM 里面是一个
Latch. 这是一个简单的 latch. 有两个端: A_1, A_2. 我们有一个 bit 的输入,
记为 B, 那么我们将 B 接入 A_1, bar B接入 A_2. 这就是一个 bit 的写操作.

Simple practise: go and check the book. Because I can't draw a picture here. If you think the book is shit, then you can check /Stallings/' book. 

*** SRAM 时序分析

- Note taken on [2023-06-16 Fri 17:53] \\
  we analyse a cycle of read or write operation. The start of the cycle and the end of the cycle are marked by the change of the address line. That is to say, if the address line change, then we are heading to next cycle. What we want to know is what elements are here to compose the minimun of the cycle time.
Although the analysis seems so useless, you should read it as well. 


*Read*: Anyway, we need to figure out the signals first. As you can see, the addresss line is the input, and the there is a signal called pianxuan signal (OK, the input method is down for now, I can just type in English). And the output is the data line. Namely, we have: 

- *A*: address line: usually it is 32-bit long or 64-bit long. It depends on the system, you know. Our device is mainly x64, that is to say the address line is 64-bit long. 
- \(\bf\overline{CS}\): the 片选 signal. CS is 低有效. It is triggerred when the signal become 0
- *D_{out}*: the data we get from the chip.

All this is what we need to analyse the read operation. The process is like 1. The address is ready; 2. CS is ready; 3. Consequently, the Data is ready. The rest of  the text is all blah.

*Write*:  Write is relatively complex. We have the write enable signal, namly, \(\bf \overline{WE}\). And we should note that when the data is not valid, it is at 高态. 

- *A*: the address line
- \(\bf \overline{WE}\): Write enable
- D_{in}: the input Data, that is the data that we want to write.

When the address line is ready, the write enable and cs signal should wait for a moment for the data line ready. After the dataline is (almost) ready, we enable the write function. And then we write the data to the memory. And some how idk why we need to hold on for a sec, and then we shall continue. 

It takes many procedure: 1. A ready; 2. wait; 3. WE and CS is valid; 4. wait for writing done; 5.(we and cs is not valid now) and another wait (this wait is called write restoration time); 6. OK for next cycle. 

what we have here is a lot of phase: 2. wait Data line be ready; 4. wait for writing operation (latency); 5. another wait (write restoration). 

Here is some other thing that you should notice; you can check out the book.

*** TODO The Examples of SRAM and DRAM

This section is like shit. 

What we are going to do is to analyse the pictures in the book which show some examples of SRAM and DRAM chips. 

None of the example is useful. But it is a chance for use to practise some skills of shit-eating.
*** TODO DRAM 时序分析
*** DRAM 的刷新方式

有三种刷新方式, 我们依次介绍其特点. 以一个 128 × 128 的 DRAM 为例子.
刷新 128 行需要 64 μs, 我们每 2ms 就需要刷新.

*集中刷新*: 2ms 之中抽出 64 μs 专门用来刷新. 这段期间并不能进行读写操做.

*分散刷新*: 进行一个读取操作的时候就进行一个行的刷新.

*异步刷新*: 每隔 \(\displaystyle \frac{64\, \mathrm{\mu s}}{128}\) 就刷新一次. 因为刷新操作和读写操作并不是同步的, 于是称为异步的. 也就是说刷新和读写操作并不是一样的. 

*** The comparison between DRAM and SRAM

This one is relatively simple. You should be already very clear about it after you have learned all these shit.

** ROM
*** ROM 的简单分类学

- *基本ROM* (read-only memory 掩模 rom) 介绍过的 (long ago). 在一个节点上面放着一个电容, 导通的时候接入低电压 (因为电容接地了); 如果没有电容, 读出的时候就是高电压. The read operation is a little bit tricky here. Anyway if thre is transistor (MOS), then the corresponding bit is *1*. 

- *PROM* (Programmable ROM) 其内部有一个熔丝, 通过是否熔断这个熔丝来达成
  program. 这种 program 是一次性的. 但是比基本ROM要方便.

- *EPROM* (Erasable) 可擦除的, optically erasable. 结构不介绍了. 几把. 

- *EEPROM* (electrically erasable) 可电擦除的, 不知道用来干嘛. 

- *Flash Memory* 用于手机等, 功能已经和 RAM 差不多了.

*** EPROM 的结构介绍

简单来说, 就是使用了一个特殊的晶体管, 这个晶体管叫什么, 雪崩注入式的晶
体管. 总之是一个很奇怪的名字. 这个晶体管之中有一个名为浮动栅的结构. 当
晶体管上面的一个 D 口接入了电源之后, 这个东西就能开始运作了, 其能够阻
断晶体管内部的电流的流通. 那么当这个电压接入的时候, 其存的就是 0. 没有
接入的话, 存的就是 1.

其实是很简单的东西. 我们稍微看一下就知道是什么了.

** Chips 和 CPU 的链接 (important)

*** How to Deal lianxian

- Note taken on [2023-06-16 Fri 18:42] \\
  The problem here is a completely garbage. As you can see the previous chapter just finished saying things like "the direct link between the CPU  (processor) and the memory will slow down the performance". Shit. Now, what are we doing? Garbage actually.

CPU 和 RAM 或者 ROM 之间的 chip 链接:

 给定了 chips (you can't use other chips) 和一个译码器 (in general case), 要你将 CPU 和 chip 之间连接起来.  片选信号一般连入高位, 地址一般连入低位. 高位的这些信号决定片选信号的产生. 片选信号产生了之后, 链接到 ROM RAM 芯片的 CS 上.

*解题步骤* :
1. 根据地址范围写出相应的二进制地址. 以方便决定如何使用 74138 译码器.
2. 根据地址范围的大小, 决定使用的 chip
3. 分配 CPU 地址线. 一般来说这是简单的.
4. 决定片选信号. 查看第一步的二进制地址. 且, CPU 的 MREQ 信号一般要接
   入译码器的使能端. (MREQ signal is the signal that saying the cpu is going to read / write memory or not. If MREQ is no valid, then the whole memory should not be working).

还需要查看 Tang 99页的例题. 令人无语的题. 大概就考这种程度的东西. 真是
丢人. 令人叹息, 说到底就是喜欢这种垃圾.

*** TODO Example of the lianxian of chips and cpu
- Note taken on [2023-06-16 Fri 18:48] \\
  The problems here are boring. But it may vary, in a shitty way though.

SHIT
** 存储器的校验 Parity

建议阅读 Stallings 一节.

*** 校验的电路结构

参考 Stallings 一节. 我们说我们有一串数据需要传输. 在传输之前, 我们通过函数 f, 生成一个 K bits 的校验码. 传输了之后我们再次进行校验码的生成.

对比两次得到的校验码, 我们知道, 数据是否有损坏. 两次校验码取 XOR 得到数据, 这个 XOR 得到的结果称为 Syndrome Word.

我们假设 N 是 数据的长度. 因为 K bits 的校验码, 其能够做到 2^K 的定位.
那么我们实际上有不等式:

\[
2 ^ K  - 1 \ge  N  + K 
\]

实际上我们还能够确认 K bits 的校验码在传输的过程之中是否有发生错误.  所以说不等号后面加上了一个 K. 还有, 如果说 Syndrome Word 是 0 的话, 其就说明这里并没有错误. 于是说不等号前面有一个  \(-1\), 因为其中有一个值拿去放到别的地方了

*** Hamming Code

Hamming Code 是常见的 single error correction code. 其能够检测出一位数据的错误. n其工作原理就是将某些位取 XOR 得到的结果. 直观理解请看 Stallings 的书.

在这里我们进一步采用一个模式, 这个模式能够让我们比较简单的生成 Hamming
Code. 我们将 Hamming Code 和 数据 bits 放到一排.  对于 2 的次幂的位置,
其上面放的是 Hamming Code 的位. 我们设 C1 C2 C4 为 Code 的位, 设 Dn 
是第 n 个数据位. 编排如下.

#+begin_example
C1C2D1C4D2D3D4
#+end_example

上面是一个 4 位数据的校验码 (D stands for data), 校验码 (C for idk) 是三位. 
我们按照下面方式得出 Cn. 考虑位置码, 也就是位置的二进制码, 比如说, 
第6位就是 110. 我们说 C1 的值为, 位置码个位数是 1 的数据位的 XOR. 类似的 
C2 的值为 "位置码的第二位数为 1 的数据位的 XOR".

| 位置   | 001 | 010 | 011 | 100 | 101 | 110 | 111 |
| 位置   |   1 |   2 |   3 |   4 |   5 |   6 |   7 |
| 数据   |  C1 |  C2 |  D1 |  C4 |  D2 |  D3 |  D4 |

#+begin_example
C1 = D1 ^ D2 ^ D4
#+end_example

** DONE 提高访问速度的方式
CLOSED: [2023-06-17 Sat 02:02]
- State "DONE"       from "TODO"       [2023-06-17 Sat 02:02]
*** 总结

总共分为两个部分: 1. 单字多体和多体并行; 2. 高性能存储芯片. 第二个部分主要抄袭 Stallings 的对应部分. 有兴趣的读者可以选择查看后者.

*第一个部分*: 什么是"体"? 体就是一个模块. 模块就是体. 在这里, 体是半个 RAM 或者别的东西. 其能够独立的工作, 结构上也相对独立, 也就是说, 其有独立的控制单元什么的, 我们用其来实现加速, 比如说利用流水线的思想.

*第二个部分*: 可以参考 Stallings. 其告诉了三种结构: 1. Synchronous DRAM; 2. Rambus DRAM; 3. Cache DRAM. 能够看出为什么这里是抄袭. 因为第二个部分针对的是 DRAM, 这样考虑的话, 这个部分应当放到前面来讲述, 但是 Tang 并没有这么做, 使得编排的逻辑并不是很规整. 并且, Cache 还没有介绍.

- *Synchronous DRAM*, 其思想很简单. 为 RAM 增加一个时钟. 我们利用起这个时钟. 一般来说, 当我们传输数据的时候, 需要和 CPU 同步, 并且应当指定地址. 但是 SDRAM 使用了 Burst Mode. 我输入了一个地址, 指定了传输数据的大小 (有多少个 word), 随后 SDRAM 就能够一直传输, 直到传输的数据达到了所需的大小. 这便是 burst mode. 有兴趣的可以查看 wikipedia. 
- *Rambus DRAM*.  Use bus and modules (and of course a control unit)
- *Cache DRAM*. 可以查看 Patterson 相关部分. 其说明得更多. (introduce more concepts) 简单来说, 就是运用了 Cache 的思想, 使用 SRAM 作为一个 buffer.

*** 单体多字

使用一个 bandwidth 为多个字节的存储器, 设为 \(n\). 根据地址, 一次取出
\(n\) 个字节, 送入选择器之类的东西. 使得每隔 \(T\big/ n\) 就能送入一个字节的数据.

*** 多体并行

- Note taken on [2023-06-16 Fri 19:13] \\
  似乎 /高位编址/ 又称为 /顺序存储/; /低位编址/ 又称为 /交叉存储/. 建议 Tang 同学下次抄教材的时候将别人的东西抄完整来.

一个正常的地址可以看为两个部分: 1. *体编号*; 2. *体内地址*. 前者告诉我们应当在哪一个体内寻找数据, 后者告诉我们在体内的哪里寻找数据.

这样将地址分为两个部分处理称为 *交叉编址*. 常用的有两种编址: 我们可以将 *低* 位地址看为体编号, 或者是相反, 将 *高* 位地址看作是体编号. 前者称为低位交叉编址, 后者称为高位交叉编址. 

多体并行运用了类似于流水线的思想. 当我们要 *交叉地访问不同体* 的时候, 速度是最大的. 如果说 *连续的数据* (地址连续) 都在 *一个体内*, 我们访问连续的数据的时候速度就没有变化. 高位编址就是这种情况. 连续的地址, 其高位不容易改变, 那么, 它们倾向于放在同一个体内. 于是高位编址对于 *访问连续的数据* 来说, 并没有加速作用. 相反地, 低位编址就能够加速.

Please read the note under the current item.
*** 存控

这实际上是排队器. 这点 Patterson 有提及; 也就是将访问的请求进行排队.

*** DONE Synchronous DRAM
CLOSED: [2023-06-16 Fri 20:56]
- State "DONE"       from "TODO"       [2023-06-16 Fri 20:56]
  
- Note taken on [2023-06-16 Fri 20:54] \\
  Things are weird. There is bus. Does the cpu really have to wait for the output of the data without SDRAM?

The idea is the key. SDRAM use a clock. The clock enables an important feature, namly the *burst mode*. 

When we want to retrive data from memory, we inform memory with address; then the data is retrived from memory, we capture it; we inform memory with another address (usually this address is adjacent to the previous one); the same thing happens. The cpu have to *wait* for output data. And specify every address of every blocks.

Why? Maybe it is because that it doesn't have a clock. Anyway SDRAM fixes the problem here: the cpu call the memory to do some read / write operation, and then cpu just goes back to its own business; after the data is available, the cpu capture the data (*without waiting for the output*).

In *burst mode*, we want to get a series of blocks; we inform the memory with the address (the start address); then the memory just keeps outputing data without the need to informing the address again. that is why it is called *burst mode*.

*DDR* is short for *Double Data Rate SDRAM*. Anyway, it is fast. I mean, fast.

For more information, please check [[https:google.com/search?q=burstmode][wikipedia]].

*** TODO Rambus DRAM
- Note taken on [2023-06-16 Fri 21:35] \\
  So Stallings wrote shit too.
- Note taken on [2023-06-16 Fri 21:02] \\
  Gonna drown in all that Tang shit. How can a man just keeping shit around?

Source: Stallings

DRAM use *block-oriented* protocol to deliver address and control information (that is actually to say it use something like bus, but the transfer of the information is asychronous). 

Much of the details are omitted here. Just get it and that will be fine. 

Anyway, you can check out the Stallings for more information.

*** Cache DRAM

One shall read Patterson for this section.

This one is rather simple.

* Cache
** An introduction
- /miss/, /miss penalty/, /hit/, /hit rate/

This section tells the principle of cache and then tells some key concepts like *hit*, *miss* and so on.

Anyway. A cache is memory that lies between processor and main memory. It is used to speed up the accessing time. Some blocks of the data are loaded into cache. (Mind the word "block") And if processor want to access to the data, it will check cache first. And if the data is indeed in the cache, then the processor can just get the data via accessing to cache, none of the main memory's business. 

So it will speed up the accessing time, since the cache is faster (and is more expensive than main memory).

If the data is on the cache, then it is called a *hit*; if not, it is called a *miss*. If a miss occurs, we will have to access to the main memory, and send the data to cache and to processor. The extra time that it takes is called *miss penalty*.

** DONE The Elements of Cache
CLOSED: [2023-06-16 Fri 21:59]

- State "DONE"       from "TODO"       [2023-06-16 Fri 21:59]
Anyway, we are going to talk about the structure of the cache here. And moreover we are going to talk something about the attributes of a cache.

*** Hit and Miss

- Note taken on [2023-06-16 Fri 19:23] \\
  This is called improve stability by *redundancy*.

Anyway. A cache is memory that lies between processor and main memory. It is used to speed up the accessing time. Some blocks of the data are loaded into cache. (Mind the word "block") And if processor want to access to the data, it will check cache first. And if the data is indeed in the cache, then the processor can just get the data via accessing to cache, none of the main memory's business. 

So it will speed up the accessing time, since the cache is faster (and is more expensive than main memory).

If the data is on the cache, then it is called a *hit*; if not, it is called a *miss*. If a miss occurs, we will have to access to the main memory, and send the data to cache and to processor. The extra time that it takes is called *miss penalty*.

** The Structure of the Cache

- Note taken on [2023-06-16 Fri 19:27] \\
  For the name of *line*, Stallings has discussed about it: it is used to distinguish between that in cache and that in memory.
- Note taken on [2023-06-16 Fri 19:24] \\
  For principle of locality, check either Stallings or Patterson.

This part is rather simple, for we have already been familiar with the structure of cache. 

The data transferred between cache and main memory is by *blocks*. A block consists of words.

It uses the principle of locality, to improve the performance. So we know that a block has usually more than one word (it won't use spatial locality if it does). The main memory is divided into blocks. Cache can load blocks of data from memory. The *mapping* between the blocks number in main memory and the block number in cache (that is called *line* number, which is the position that in cache) is a topic in next some section.

A line in a cache is the *basic unit* of a cache. It consists of a block and some *extra* information field including *tag field* and *valid-tag field*. The name of line is used, to note the difference of the blocks that in main memory and that in cache, and to note that there is some other information in the cache line.

*** The REAL Structure of Cache

Cache lies between processor and memory.

There are two more important parts: 1. Mapping; 2. Replacing.

Mapp is the map from *read address* to the address in cache. Replacing is about how we deal with the situation where the cache is full. These two part require at least two modules.

So the structure of cache consists of 1. processor; 2. cache; 3. memory; 4. map module; 5. replace module; 6. bus. 

*** Block Size and its Effect to Hit Rate

Source: Patterson

According to the principle of locality, the bigger block size can improve the hit rate, subsequently improving the performance. 

But if we consider the latency (that is the miss penalty), things get interesting, because if the improvement brought by the increase of hit rate is no greater than the *degeneration* brought by the increase of miss penalty, then the performance is being worse as the block size grows. For more information, you can check /Patterson/ for more information.

Moreover, the miss rate will go up eventually if the block size keeps increasing. Because as the blocksize goes up, total number of the blocks is low.

** DONE Mapping Strategy
CLOSED: [2023-06-17 Sat 00:21]

- State "DONE"       from "TODO"       [2023-06-17 Sat 00:21]
The mapping is from the blocks in the memory to the line in the cache, that is to say when given the position of a block, how do we find the corresponding position in the cache? There are some strategies of mapping.

The simplest one is called *Direct Mapping*. It is simple. There are also other ways called *associative mapping* and *set-associative mapping*. 

*** Direct Mapping

Modula!

The details are omitted here. You can check Patterson and Stallings for more information.

*** (full) Associative Mapping

First, cut the word address into two field: 1. tag, 2. word. Word field is used to locate the data inside of the block. Tag is used to identify.

Anyway, initially, we give an address. We load the block into cache (into the first line). Store the tag into the line. 

Next, afterwards, if we want to retrieve from the memory, we check the cache, to see if there is a line, whose tag is the same as the tag of the given address. If there is, then it is a hit; if not, it is a miss. If there is a miss, then load the block into the second line.

And we have done here.

The procedure is done here.

The feature is that the line number is not affected by the address of the memory. Randomly given an address, the line number could be anyone.

*** Set-Associative Mapping

Set-Associative mapping is to combine the two kinds of methods. 

Let us look at direct mapping. The address is divided into two groups. Line number field is the line number. Yes, indeed.
1. line number field
2. word field

Let us look at associative mapping. The address is divided into two groups. 
1. tag field
2. word field

In the combination of the two method, the address is divided into three groups.
1. tag field
2. set field
3. word field

where the tag field works just like that in associative mapping, and set field works just like that in the direct mapping: set field is the set number.

Anyway, the cache is divided into many sets. The set field determined the set number. JUST LIKE THAT IN DIRECT MAPPING. Commonly, the arrangement is like: 

| field   | tag | set     | word |
| address | xxx | xxxxxxx | xx   |
| length  | 3   | 7       | 2    |

here I specify the length of the field. OK, then we have a 12-bits address. and the block size is four bytes; and there are \(2 ^7\) sets; there are \(2 ^3\) lines in one set; there are \(2 ^{10}\) lines in total.

** DONE Write Policy
CLOSED: [2023-06-17 Sat 00:30]
- State "DONE"       from "TODO"       [2023-06-17 Sat 00:30]

For write policy, one can check the page 137 of Stallings.

*** Why We have to maintain the consistency of the memory heirarchy

In computer science, a consistency model specifies a contract between ther programmer and a system. The system guarantees that if the programmer follows the rules for operations on memory, memory will be consistent and the results of reading, writing, or updating memory will be predictable. This is important because it allows for reliable and predicable behavior of programs that rely on shared memory.

idk.

I just don't know why.

*** Two strategies

There are ways to maintain the consistency. In short, there are two ways: *write-through* and *write-back.*

Let us look at write-through, to check how it maintain the consistency. Write-through is to say, when you want to change some data, if it is on cache, you need to change the content of cache and that of the main memory.

*** Write Through

Write through is to say, when we *change* the data in *cache*, we also change the data that in *memory*. It is a good idea? It slows down the performance, right? The speed of writing data remains *unchanged*. The time it takes to write to cache is the same as that to memory. 

*** Write Back

Write Back is to say, when we *change* the data in *cache*, we write the data *back* when that *block* is about to be subtituted. The speed is guaranteed, but its realization is more sophisticated.

*** TODO The More into Write Policy

** DONE Ways to Improve the Performance of Cache
CLOSED: [2023-06-17 Sat 01:54]

- State "DONE"       from "TODO"       [2023-06-17 Sat 01:54]
One can use multiple level of cache.

*** Multi-level Cache

**** Before Muti-level Cache
We add a level of cache. Let us say L1 and L2.

It can improve the performance. 

But how? 

First we should know that what have constrained the speed of a memory chip. Ok, let me put it straight, the *latency* constrain the speed. 

Next we should know the principle of the cache. Of course, you know the principle of locality, but what we need here is something more essential: 

*The speed of cache memory should be faster than the speed of the main memory.*

That is right. So, the speed of L1 should be above of that of L2. How can we do that? 

At the first place, when we consider one-level cache, the cache use SRAM. SRAM is more expensive and takes more space than DRAM and it is faster than DRAM. So SRAM can be the cache if the main memory is consist of DRAM.

OK, that is very convincing, but does it have to do with multi-level cache? 

Of course, what we need here, is to find a way to improve the speed of SRAM, or specifically, to find a way to distinguish the speed of one kind of SRAM and the other, such that we can build multi-level cache.

**** The Bus

WHAT HAD HAPPENED?

The main point is the position of the SRAM chips. We need the knowledge of the bus here. If we put the SRAM chips on the *processor*
's chip, then the speed should be faster. Let us call those SRAM chips L1.

And we put the L2 chips outside of the processor chip.

The  physical distance between the L1 and the processor is smaller. Consequently, the speed of the L1 should be faster. It is because the communication between L1 and processor does not depend on bus. But the communication between processor chip and L2 depend on bus.

The dependency leads to the performance difference. So, as a result, we can build a multi-level cache. And indeed just like SRAM takes more space, the space on the processor chip is limited. So the size of L1 is limited too.

*** Unified VERSUS SPILT CACHE
** TODO Replacement Algorithm

When the cache is full, and we want to fill some blocks into the cache, we need the replacement algorithm to determine which blcok is about to be replaced. 

We have three kinds of algorithms available.

* TODO Virtual Memory (from /Patterson/)
* TODO External Memory
** RAID from Stallings
Source: Stallings

** Some Other External Memory (not important)

包括但是不仅限于硬盘软盘光碟等等. 需要注意的是, 虽然说 Tang 的书上讲得挺tm头头是道的, 这些内容实际上是没什么讲的必要的. 都是垃圾, 简单来说.

在这里我们就不多介绍了, 实际上有兴趣的话, 可以自行查看wikipedia等等.

相对更重要的是 Stallings 的书上介绍的 RAID.