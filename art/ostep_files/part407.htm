<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Table 38.5: Example: Writes To 4, 13, And Respective Parity Blocks</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part406.htm">&lt; 上一个</a><span> | </span><a href="../ostep.html">内容</a><span> | </span><a href="part408.htm">下一个 &gt;</a></p><p style="padding-top: 5pt;padding-left: 50pt;text-indent: 0pt;text-align: justify;">Table 38.5: <b>Example: Writes To 4, 13, And Respective Parity Blocks</b></p><p style="padding-top: 5pt;padding-left: 41pt;text-indent: 11pt;line-height: 84%;text-align: justify;">Now imagine there were 2 small writes submitted to the RAID-4 at about the same time, to blocks 4 and 13 (marked with <span class="s74">∗</span><span class="s200"> </span>in the diagram). The data for those disks is on disks 0 and 1, and thus the read and write to data could happen in parallel, which is good. The problem that arises is with the parity disk; both the requests have to read the related parity</p><p style="padding-top: 1pt;padding-left: 41pt;text-indent: 0pt;line-height: 89%;text-align: justify;">blocks for 4 and 13, parity blocks 1 and 3 (marked with <span class="s101">+</span>). Hopefully, the issue is now clear: the parity disk is a bottleneck under this type of work- load; we sometimes thus call this the <b>small-write problem </b>for parity- based RAIDs. Thus, even though the data disks could be accessed in parallel, the parity disk prevents any parallelism from materializing; all writes to the system will be serialized because of the parity disk. Because the parity disk has to perform two I/Os (one read, one write) per logical I/O, we can compute the performance of small random writes in RAID-4 by computing the parity disk’s performance on those two I/Os, and thus we achieve (R/2) MB/s. RAID-4 throughput under random small writes is terrible; it does not improve as you add disks to the system.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 68pt;text-indent: 12pt;line-height: 89%;text-align: justify;">We conclude by analyzing I/O latency in RAID-4. As you now know, a single read (assuming no failure) is just mapped to a single disk, and thus its latency is equivalent to the latency of a single disk request. The latency of a single write requires two reads and then two writes; the reads can happen in parallel, as can the writes, and thus total latency is about twice that of a single disk (with some differences because we have to wait for both reads to complete and thus get the worst-case positioning time, but then the updates don’t incur seek cost and thus may be a better-than- average positioning cost).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part406.htm">&lt; 上一个</a><span> | </span><a href="../ostep.html">内容</a><span> | </span><a href="part408.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
