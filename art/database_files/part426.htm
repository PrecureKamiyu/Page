<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>23.9  Summary</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part425.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part427.htm">下一个 &gt;</a></p><p class="s65" style="padding-top: 4pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a name="bookmark493">23.9  </a><span style=" color: #00AEEF;">Summary</span><a name="bookmark538">&zwnj;</a></p><p class="s39" style="padding-top: 9pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">A distributed database system consists of a collection of sites or nodes, each of which maintains a local database system. Each node is able to process local trans- actions: those transactions that access data in only that single node. In addition, a node may participate in the execution of global transactions: those transactions that access data in several nodes. Transaction managers at each node manage access to local data, while the transaction coordinator coordinates execution of global transactions across multiple nodes.</span></p><p class="s39" style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">A distributed system may suﬀer from the same types of failure that can aﬄict a centralized system. There are, however, additional failures with which we need to deal in a distributed environment, including the failure of a node, the failure of a link, loss of a message, and network partition. Each of these problems needs to be considered in the design of a distributed recovery scheme.</span></p><p class="s13" style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><span class="s40">To ensure atomicity, all the nodes in which a transaction </span>T <span class="p">executed must agree on the ﬁnal outcome of the execution. </span>T <span class="p">either commits at all nodes or aborts at all nodes. To ensure this property, the transaction coordinator of </span>T <span class="p">must execute a commit protocol. The most widely used commit protocol is the two-phase commit protocol.</span></p><p class="s39" style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">The two-phase commit protocol may lead to blocking, the situation in which the fate of a transaction cannot be determined until a failed node (the coordinator) recovers. We can use distributed consensus protocols, or the three-phase commit protocol, to reduce the risk of blocking.</span></p><p class="s39" style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Persistent messaging provides an alternative model for handling distributed trans- actions. The model breaks a single transaction into parts that are executed at diﬀer- ent databases. Persistent messages (which are guaranteed to be delivered exactly once, regardless of failures), are sent to remote nodes to request actions to be taken there. While persistent messaging avoids the blocking problem, application developers have to write code to handle various types of failures.</span></p><p class="s39" style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">The various concurrency-control schemes used in a centralized system can be mod- iﬁed for use in a distributed environment. In the case of locking protocols, the only change that needs to be incorporated is in the way that the lock manager is imple- mented. Centralized lock managers are vulnerable to overloading and to failures. Deadlock detection in a distributed-lock-manager environment requires coopera- tion between multiple nodes, since there may be global deadlocks even when there are no local deadlocks.</span></p><p class="s39" style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">The timestamp ordering and validation based protocols can also be extended to work in a distributed setting. Timestamps used to order transactions need to be made globally unique.</span></p><p class="s66" style="padding-top: 3pt;padding-left: 119pt;text-indent: 0pt;text-align: right;">23.9 <span style=" color: #00AEEF;">Summary  </span><span class="s164">1163</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s39" style="padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Protocols for handling replicated data must ensure consistency of data. Lineariz- ability is a key property that ensures that concurrent reads and writes to replicas of a single data item can be serialized.</span></p><p class="s39" style="padding-top: 4pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Protocols for handling replicated data include the primary copy, majority, biased, and quorum consensus protocols. These have diﬀerent trade-oﬀs in terms of cost and ability to work in the presence of failures.</span></p><p class="s39" style="padding-top: 4pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s40">The majority protocol can be extended by using version numbers to permit trans- action processing to proceed even in the presence of failures. While the protocol has a signiﬁcant overhead, it works regardless of the type of failure. Less-expensive protocols are available to deal with node failures, but they assume network parti- tioning does not occur.</span></p><p class="s39" style="padding-top: 3pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s40">To provide high availability, a distributed database must detect failures, reconﬁgure itself so that computation may continue, and recover when a processor or a link is repaired. The task is greatly complicated by the fact that it is hard to distinguish between network partitions and node failures.</span></p><p class="s39" style="padding-top: 4pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Globally consistent and unique timestamps are key to extending multiversion two- phase locking and snapshot isolation to a distributed setting.</span></p><p class="s40" style="padding-top: 4pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span>The <span class="s41">CAP </span>theorem indicates that one cannot have consistency and availability in the face of network partitions. Many systems tradeoﬀ consistency to get higher avail- ability. The goal then becomes eventual consistency, rather than ensuring consis- tency at all times. Detecting inconsistency of replicas can be done by using version vector schemes and Merkle trees.</p><p class="s39" style="padding-top: 3pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Many database systems support asynchronous replication, where updates are prop- agated to replicas outside the scope of the transaction that performed the update. Such facilities must be used with great care, since they may result in nonserializ- able executions.</span></p><p class="s39" style="padding-top: 3pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Some of the distributed algorithms require the use of a coordinator. To provide high availability, the system must maintain a backup copy that is ready to assume responsibility if the coordinator fails. Another approach is to choose the new coor- dinator after the coordinator has failed. The algorithms that determine which node should act as a coordinator are called election algorithms. Distributed coordina- tion services such as ZooKeeper support coordinator selection in a fault-tolerant manner.</span></p><p class="s39" style="padding-top: 3pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Distributed consensus algorithms allow consistent updates of replicas, even in the presence of failures, without requiring the presence of a coordinator. Coordina- tors may still be used for eﬃciency, but failure of a coordinator does not aﬀect correctness of the protocols. Paxos and Raft are widely used consensus protocols. Replicated state machines, which are implemented using consensus algorithms, can be used to build a variety of fault-tolerant services.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part425.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part427.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
