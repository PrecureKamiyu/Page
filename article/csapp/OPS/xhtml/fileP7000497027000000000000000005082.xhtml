<?xml version='1.0' encoding='utf-8'?>
<html xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:svg="http://www.w3.org/2000/svg" xmlns:epub="http://www.idpf.org/2007/ops" xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" epub:prefix="index: http://www.index.com/">
  <head>
    <meta name="dcterms.conformsTo" content="PXE Basic 1.0"/>
    <meta name="generator" content="PXE Tools version 1.39.52"/>
    <!--Created by pxe.pl for standard version PXE Basic 1.0,data-profile-product=standard by PXE Tools 1.39.52, partial=false-->
    <title>5.15 Summary </title>
    <link rel="alternate stylesheet" type="text/css" title="night" href="../css/theme/night.css"/>
    <link rel="alternate stylesheet" type="text/css" title="sepia" href="../css/theme/sepia.css"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body epub:type="bodymatter" class="calibre pcalibre pcalibre1">
<section id="P7000497027000000000000000005082" class="pcalibre halftitlepage pcalibre1"><header class="calibre1 pcalibre pcalibre1"><h1 class="pcalibre title pcalibre1" id="P7000497027000000000000000043835" data-uri="chapter05.xhtml#P7000497027000000000000000043835" epub:type="title"><span class="pcalibre label pcalibre1">5.15 </span><span class="pcalibre label pcalibre1">Summary </span></h1></header>
<p id="P7000497027000000000000000043836" data-uri="chapter05.xhtml#P7000497027000000000000000043836" class="pcalibre1 pcalibre calibre2">Although most presentations on code optimization describe how compilers can generate efficient code, much can be done by an application programmer to assist the compiler in this task. No compiler can replace an inefficient algorithm or data <span class="pcalibre pagebreak pcalibre1" id="P7000497027000000000000000005085" title="569" data-uri="chapter05.xhtml#P7000497027000000000000000005085" epub:type="pagebreak"></span>structure by a good one, and so these aspects of program design should remain a primary concern for programmers. We also have seen that optimization blockers, such as memory aliasing and procedure calls, seriously restrict the ability of compilers to perform extensive optimizations. Again, the programmer must take primary responsibility for eliminating these. These should simply be considered parts of good programming practice, since they serve to eliminate unneeded work.</p>
<p id="P7000497027000000000000000043837" data-uri="chapter05.xhtml#P7000497027000000000000000043837" class="pcalibre1 pcalibre calibre2">Tuning performance beyond a basic level requires some understanding of the processor's microarchitecture, describing the underlying mechanisms by which the processor implements its instruction set architecture. For the case of out-of-order processors, just knowing something about the operations, capabilities, latencies, and issue times of the functional units establishes a baseline for predicting program performance.</p>
<p id="P7000497027000000000000000043838" data-uri="chapter05.xhtml#P7000497027000000000000000043838" class="pcalibre1 pcalibre calibre2">We have studied a series of techniques—including loop unrolling, creating multiple accumulators, and reassociation—that can exploit the instruction-level parallelism provided by modern processors. As we get deeper into the optimization, it becomes important to study the generated assembly code and to try to understand how the computation is being performed by the machine. Much can be gained by identifying the critical paths determined by the data dependencies in the program, especially between the different iterations of a loop. We can also compute a throughput bound for a computation, based on the number of operations that must be computed and the number and issue times of the units that perform those operations.</p>
<p id="P7000497027000000000000000043839" data-uri="chapter05.xhtml#P7000497027000000000000000043839" class="pcalibre1 pcalibre calibre2">Programs that involve conditional branches or complex interactions with the memory system are more difficult to analyze and optimize than the simple loop programs we first considered. The basic strategy is to try to make branches more predictable or make them amenable to implementation using conditional data transfers. We must also watch out for the interactions between store and load operations. Keeping values in local variables, allowing them to be stored in registers, can often be helpful.</p>
<p id="P700049702700000000000000004383A" data-uri="chapter05.xhtml#P700049702700000000000000004383A" class="pcalibre1 pcalibre calibre2">When working with large programs, it becomes important to focus our optimization efforts on the parts that consume the most time. Code profilers and related tools can help us systematically evaluate and improve program performance. We described <span class="smallcaps pcalibre pcalibre1">gprof</span>, a standard Unix profiling tool. More sophisticated profilers are available, such as the <span class="smallcaps pcalibre pcalibre1">vtune </span>program development system from Intel, and <span class="smallcaps pcalibre pcalibre1">valgrind</span>, commonly available on Linux systems. These tools can break down the execution time below the procedure level to estimate the performance of each <i class="calibre5 pcalibre pcalibre1">basic block</i> of the program. (A basic block is a sequence of instructions that has no transfers of control out of its middle, and so the block is always executed in its entirety.)</p>
</section></body></html>
