<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>21.4  Replication</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part386.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part388.htm">下一个 &gt;</a></p><p class="s65" style="padding-top: 7pt;padding-left: 72pt;text-indent: 0pt;text-align: left;">21.4  <span style=" color: #00AEEF;">Replication</span></p><p style="padding-top: 12pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">With a large number of nodes, the probability that at least one node will malfunction in a parallel system is signiﬁcantly greater than in a single-node system. A poorly designed parallel system will stop functioning if any node fails. Assuming that the probability of failure of a single node is small, the probability of failure of the system goes up linearly with the number of nodes. For example, if a single node would fail once every 5 years, a system with 100 nodes would have a failure every 18 days.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Parallel data storage systems must, therefore, be resilient to failure of nodes. Not only should data not be lost in the event of a node failure, but also, the system should continue to be <i>available</i>, that is, continue to function, even during such a failure.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">To ensure tuples are not lost on node failure, tuples are replicated across at least two nodes, and often three nodes. If a node fails, the tuples that it stored can still be accessed from the other nodes where the tuples are replicated.<span class="s76">4</span></p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Tracking the replicas at the level of individual tuples would result in signiﬁcant overhead in terms of storage and query processing. Instead, replication is done at the level of partitions (tablets, nodes, or virtual nodes). That is, each partition is replicated; the locations of the partition replicas are recorded as part of the partition table.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Figure 21.5 shows a partition table with replication of tablets. Each tablet is repli- cated in two nodes.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="96" height="1" alt="image" src="Image_3150.png"/></span></p><p class="s77" style="padding-top: 3pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">4<span class="s78">Caching also results in replication of data, but with the aim of speeding up access. Since data may be evicted from cache at any time, caching does not ensure availability in the event of failure.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:127.144pt" cellspacing="0"><tr style="height:17pt"><td style="width:94pt;border-top-style:solid;border-top-width:1pt;border-top-color:#221E1F;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F" bgcolor="#C6E9FA"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;text-align: left;">Value</p></td><td style="width:90pt;border-top-style:solid;border-top-width:1pt;border-top-color:#221E1F;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F" bgcolor="#C6E9FA"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;text-align: left;">Tablet ID</p></td><td style="width:109pt;border-top-style:solid;border-top-width:1pt;border-top-color:#221E1F;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F" bgcolor="#C6E9FA"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;text-align: left;">Node ID</p></td></tr><tr style="height:16pt"><td style="width:94pt;border-top-style:solid;border-top-width:1pt;border-top-color:#221E1F;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-top: 2pt;padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2012-01-01</p></td><td style="width:90pt;border-top-style:solid;border-top-width:1pt;border-top-color:#221E1F;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-top: 2pt;padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Tablet0</p></td><td style="width:109pt;border-top-style:solid;border-top-width:1pt;border-top-color:#221E1F;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-top: 2pt;padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Node0,Node1</p></td></tr><tr style="height:13pt"><td style="width:94pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2013-01-01</p></td><td style="width:90pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Tablet1</p></td><td style="width:109pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Node0,Node2</p></td></tr><tr style="height:13pt"><td style="width:94pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2014-01-01</p></td><td style="width:90pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Tablet2</p></td><td style="width:109pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Node2,Node0</p></td></tr><tr style="height:13pt"><td style="width:94pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2015-01-01</p></td><td style="width:90pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Tablet3</p></td><td style="width:109pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Node2,Node1</p></td></tr><tr style="height:13pt"><td style="width:94pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2016-01-01</p></td><td style="width:90pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Tablet4</p></td><td style="width:109pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Node0,Node1</p></td></tr><tr style="height:13pt"><td style="width:94pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2017-01-01</p></td><td style="width:90pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Tablet5</p></td><td style="width:109pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Node1,Node0</p></td></tr><tr style="height:13pt"><td style="width:94pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2018-01-01</p></td><td style="width:90pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Tablet6</p></td><td style="width:109pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Node1,Node2</p></td></tr><tr style="height:14pt"><td style="width:94pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">MaxDate</p></td><td style="width:90pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Tablet7</p></td><td style="width:109pt;border-left-style:solid;border-left-width:1pt;border-left-color:#221E1F;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#221E1F;border-right-style:solid;border-right-width:1pt;border-right-color:#221E1F"><p class="s72" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Node1,Node2</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 59pt;text-indent: 0pt;text-align: center;">Figure 21.5 <span class="s74">Partition table of Figure 21.4 with replication.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The database system keeps track of failed nodes; requests for data stored at a failed node are automatically routed to the backup nodes that store a replica of the data. Issues of how to handle the case where one or more replicas are stored at a currently failed node are addressed brieﬂy in Section 21.4.2, and in more detail later, in Section 23.4.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">21.4.1 Location of Replicas</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Replication to two nodes provides protection from data loss/unavailability in the event of single node failure, while replication to three nodes provides protection even in the event of two node failures. If all nodes where a partition is replicated fail, obviously there is no way to prevent data loss/unavailability. Systems that use low-cost commodity machines for data storage typically use three-way replication, while systems that use more reliable machines typically use two-way replication.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">There are multiple possible failure modes in a parallel system. A single node could fail due to some internal fault. Further, it is possible for all the nodes in a rack to fail if there is some problem with the rack such as, for example, failure of power supply to the entire rack, or failure of the network switches in a rack, making all the nodes in the rack inaccessible. Further, there is a possibility of failure of an entire data center, for example, due to ﬁre, ﬂooding, or a large-scale power failure.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The location of the nodes where the replicas of a partition are stored must, there- fore, be chosen carefully, to maximize the probability of at least one copy being acces- sible even during a failure. Such replication can be within a data center or across data centers.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s39" style="padding-left: 107pt;text-indent: -16pt;text-align: left;">• <span class="s63">Replication within a data center</span><span class="p">: Since single node failures are the most common failure mode, partitions are often replicated to another node.</span></p><p style="padding-left: 107pt;text-indent: 0pt;text-align: left;">With the tree-like interconnection topology commonly used in data center net- works (described in Section 20.4.3) network bandwidth within a rack is much</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 139pt;text-indent: 0pt;text-align: justify;">higher than the network bandwidth between racks. As a result, replication to an- other node within the same rack as the ﬁrst node reduces network demand on the network between racks. But to deal with the possibility of a rack failure, partitions are also replicated to a node on a diﬀerent rack.</p><p class="s39" style="padding-top: 3pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s63">Replication across data centers</span><span class="p">: To deal with the possibility of failure of an en- tire data center, partitions may also be replicated at one or more geographically separated data centers. Geographic separation is important to deal with disasters such as earthquakes or storms that may shut down all data centers in a geographic region.</span></p><p style="padding-left: 139pt;text-indent: 15pt;text-align: justify;">For many web applications, round-trip delays across a long-distance network can aﬀect performance signiﬁcantly, a problem that is increasing with the use of Ajax applications that require multiple rounds of communication between the browser and the application. To deal with this problem, users are connected with application servers that are closest to them geographically, and data replication is done in such a way that one of the replicas is in the same data center as (or at least, geographically close to) the application server.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 119pt;text-indent: 17pt;text-align: justify;">Suppose all the partitions at a node <i>N</i><span class="s98">1</span> are replicated at a single node <i>N</i><span class="s98">2</span>, and <i>N</i><span class="s98">1</span> fails. Then, node <i>N</i><span class="s98">2</span> will have to handle all the requests that would originally have gone to <i>N</i><span class="s98">1</span>, as well as requests routed to node <i>N</i><span class="s98">2</span>. Asa result, node <i>N</i><span class="s98">2</span> would have to perform twice as much work as other nodes in the system, resulting in execution skew during failure of node <i>N</i><span class="s98">1</span>.</p><p class="s13" style="padding-left: 119pt;text-indent: 17pt;text-align: justify;"><span class="p">To avoid this problem, the replicas of partitions residing at a node, say </span>N<span class="s98">1</span><span class="p">, are spread across multiple other nodes. For example, consider a system with 10 nodes and two-way replication. Suppose node </span>N<span class="s98">1</span><span class="p"> had one of the replicas of partitions </span>p<span class="s98">1</span><span class="p"> through </span>p<span class="s130">9</span><span class="s94">. Then, the other replica of partitions </span>p<span class="s130">1 </span><span class="s94">could be stored on </span>N<span class="s130">2</span><span class="s94">, of </span>p<span class="s130">2 </span><span class="s94">on </span>N<span class="s130">3</span><span class="s94">, and so on to </span>p<span class="s130">9 </span><span class="s94">on </span>N<span class="s130">10</span><span class="s94">. Then in the event of failure of </span>N<span class="s130">1</span><span class="s94">, nodes </span>N<span class="s130">2 </span><span class="s94">through </span>N<span class="s130">10 </span><span class="s94">would share the extra work equally, instead of burdening a single node with all the extra work.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">21.4.2 Updates and Consistency of Replicas</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Since each partition is replicated, updates made to tuples in a partition must be per- formed on all the replicas of the partition. For data that is never updated after it has been created, reads can be performed at any of the replicas, since all of them will have the same value. If a storage system ensures that all replicas are exclusive-locked and updated atomically (using, for example, the two-phase commit protocol which we will see in Section 23.2.1), reads of a tuple can be performed (after obtaining a shared lock) at any of the replicas and will see the most recent version of the tuple.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">If data are updated, and replicas are not updated atomically, diﬀerent replicas may temporarily have diﬀerent values. Thus, a read may see a diﬀerent value depending on which replica it accesses. Most applications require that read requests for a tuple must</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">receive the most recent version of the tuple; updates that are based on reading an older version could result in a <i>lost update problem</i>.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">One way of ensuring that reads get the latest value is to treat one of the replicas of each partition as a <span class="s63">master replica</span>. All updates are sent to the master replica and are then propagated to other replicas. Reads are also sent to the master replica, so that reads get the latest version of any data item even if updates have not yet been applied to the other replicas.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">If a master replica fails, a new master is assigned for that partition. It is important to ensure that every update operation performed by the old master has also been seen by the new master. Further, the old master may have updated some of the replicas, but not all, before it failed; the new master must complete the task of updating all the replicas. We discuss details in Section 23.6.2.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">It is important to know which node is the (current) master for each partition. This information can be stored along with the partition table. Speciﬁcally, the partition table must record, in addition to the range of key values assigned to that partition, where the replicas of the partition are stored, and further which replica is currently the master.</p><p style="padding-left: 106pt;text-indent: 0pt;text-align: justify;">Three solutions are commonly used to update replicas.</p><p class="s39" style="padding-top: 1pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">The </span><span class="s13">two-phase commit </span><span class="p">(2PC) protocol, which ensures that multiple updates per- formed by a transaction are applied atomically across multiple sites, is described in Section 23.2. This protocol can be used with replicas to ensure that an update is performed atomically on all replicas of a tuple.</span></p><p style="padding-left: 107pt;text-indent: 14pt;text-align: justify;">We assume for now that all replicas are available and can be updated. Issues of how to allow two-phase commit to continue execution in the presence of failures, when some replicas may not be reachable, are discussed in Section 23.4.</p><p class="s39" style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: right;">• <span class="s40">Persistent messaging systems, described in Section 23.2.3, which guarantee that a message is delivered once it is sent. Persistent messaging systems can be used to update replicas as follows: An update to a tuple is registered as a persistent mes- sage, sent to all replicas of the tuple. Once the message is recorded, the persistent messaging system ensures it will be delivered to all replicas. Thus, all replicas will get the update, eventually; the property is known as </span><span class="s63">eventual consistency </span><span class="p">of replicas. However, there may be a delay in message delivery, and during that time some replicas may have applied an update while others have not. To ensure that reads get a consistent version, reads are performed only at a master replica, where updates are made ﬁrst. (If the site with a master replica of a tuple has failed, another replica can take over as the master replica after ensuring all pending persistent messages</span></p><p style="padding-left: 107pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">with updates have been applied.) Details are presented in Section 23.6.2.</p><p class="s39" style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Protocols called </span><span class="s13">consensus protocols</span><span class="p">, that allow updates of replicas to proceed even in the face of failures, when some replicas may not be reachable, can be used to manage update of replicas. Unlike the preceding protocols, consensus protocols can work even without a designated master replica. We study consensus protocols in Section 23.8.</span></p><p class="s66" style="padding-top: 3pt;padding-left: 119pt;text-indent: 0pt;text-align: right;"><a name="bookmark465">21.5 </a><span style=" color: #00AEEF;">Parallel Indexing  </span><span class="s164">1017</span><a name="bookmark511">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part386.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part388.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
