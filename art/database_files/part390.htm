<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>21.7  Parallel Key-Value Stores</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part389.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part391.htm">下一个 &gt;</a></p><p class="s65" style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: left;"><a name="bookmark467">21.7  </a><span style=" color: #00AEEF;">Parallel Key-Value Stores</span><a name="bookmark513">&zwnj;</a></p><p style="padding-top: 11pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Many Web applications need to store very large numbers (many billions) of relatively small records (of size ranging from a few kilobytes to a few megabytes). Storage would have to be distributed across thousands of nodes. Storing such records as ﬁles in a distributed ﬁle system is infeasible, since ﬁle systems are not designed to store such large numbers of small ﬁles. Ideally, a massively parallel relational database should be used to store such data. But the parallel relational databases available in the early 2000s were not designed to work at a massive scale; nor did they support the ability to easily add more nodes to the system without causing signiﬁcant disruption to ongoing activities.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">A number of parallel key-value storage systems were developed to meet the needs of such web applications. A <span class="s63">key-value store </span>provides a way to store or update a data item (value) with an associated key and to retrieve the data item with a given key. Some key-value stores treat the data items as uninterpreted sequences of bytes, while others allow a schema to be associated with the data item. If the system supports deﬁnition of a schema for data items, it is possible for the system to create and maintain secondary indices on speciﬁed attributes of data items.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Key-value stores support two very basic functions on tables: <span class="s49">put(table, key, value)</span>, used to store values, with an associated key, in a table, and <span class="s49">get(table, key)</span>, which re- trieves the stored value associated with the speciﬁed key. In addition, they may support other functions, such as range queries on key values, using <span class="s49">get(table, key1, key2)</span>.</p><p style="padding-left: 137pt;text-indent: 0pt;text-align: justify;">Further, many key-value stores support some form of ﬂexible schema.</p><p class="s39" style="padding-top: 2pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Some allow column names to be speciﬁed as part of a schema deﬁnition, similar to relational data stores.</span></p><p style="padding-top: 4pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><span class="s40">Others allow columns to be added to, or deleted from, individual tuples; such key- value stores are sometimes referred to as </span><span class="s63">wide-column stores</span>. Such key-value stores support functions such as <span class="s49">put(table, key, columname, value)</span>, to store a value in a speciﬁc column of a row identiﬁed by the key (creating the column if it does not already exist), and <span class="s49">get(table, key, columname)</span>, which retrieves the value for a speciﬁc column of a speciﬁc row identiﬁed by the key. Further, <span class="s49">delete(table, key, columname) </span>deletes a speciﬁc column from a row.</p><p class="s40" style="padding-top: 3pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span>Yet other key-value stores allow the value stored with a key to have a complex structure, typically based on <span class="s41">JSON</span>; they are sometimes referred to as <span class="s63">document stores</span><span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">The ability to specify a (partial) schema of the stored value allows the key-value store to evaluate selection predicates at the data store; some stores also use the schema to support secondary indices.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">We use the term <i>key-value store </i>to include all the above types of data stores; how- ever, some people use the term <i>key-value store </i>to refer more speciﬁcally to those that</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">do not support any form of schema and treat the value as an uninterpreted sequence of bytes.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Parallel key-value stores typically support <i>elasticity</i>, whereby the number of nodes can be increased or decreased incrementally, depending on demand. As nodes are added, tablets can be moved to the new nodes. To reduce the number of nodes, tablets can be moved away from some nodes, which can then be removed from the system.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Widely used parallel key-value stores that support ﬂexible columns (also known as wide-column stores) include Bigtable from Google, Apache HBase, Apache Cassan- dra (originally developed at Facebook), and Microsoft Azure Table Storage from Mi- crosoft, among others. Key-value stores that support a schema include Megastore and Spanner from Google, and Sherpa/<span class="s44">PNUTS </span>from Yahoo!. Key-value stores that support semi-structured data (also known as document-stores) include Couchbase, DynamoDB from Amazon, and MongoDB, among others. Redis and Memcached are parallel in- memory key-value stores which are widely used for caching data.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Key-value stores are not full-ﬂedged databases, since they do not provide many of the features that are viewed as standard on database systems today. Features that key- value stores typically do not support include declarative querying (using <span class="s44">SQL </span>or any other declarative query language), support for transactions, and support for eﬃcient retrieval of records based on selections on nonkey attributes (traditional databases support such retrieval using secondary indices). In fact, they typically do not support primary-key constraints for attributes other than the key, and do not support foreign-key constraints.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 7pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">21.7.1 Data Representation</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">As an example of data management needs of web applications, consider the proﬁle of a user, which needs to be accessible to a number of diﬀerent applications that are run by an organization. The proﬁle contains a variety of attributes, and there are frequent additions to the attributes stored in the proﬁle. Some attributes may contain complex data. A simple relational representation is often not suﬃcient for such complex data.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Many key-value stores support the <i>JavaScript Object Notation </i>(<span class="s69">JSON </span>) representa- tion, which has found increasing acceptance for representing complex data (<span class="s44">JSON </span>is covered in Section 8.1.2). The <span class="s44">JSON </span>representation provides ﬂexibility in the set of attributes that a record contains, as well as the types of these attributes. Yet others, such as Bigtable, deﬁne their own data model for complex data, including support for records with a very large number of optional columns.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">In Bigtable, a record is not stored as a single value but is instead split into compo- nent attributes that are stored separately. Thus, the key for an attribute value conceptu- ally consists of (record-identiﬁer, attribute-name). Each attribute value is just a string as far as Bigtable is concerned. To fetch all attributes of a record, a range query, or more precisely a preﬁx-match query consisting of just the record identiﬁer, is used. The <span class="s49">get() </span>function returns the attribute names along with the values. For eﬃcient retrieval</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">of all attributes of a record, the storage system stores entries sorted by the key, so all attribute values of a particular record are clustered together.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">In fact, the record identiﬁer can itself be structured hierarchically, although to Bigtable itself the record identiﬁer is just a string. For example, an application that stores pages retrieved from a web crawl could map a <span class="s44">URL </span>of the form:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: center;"><a href="http://www.cs.yale.edu/people/silberschatz.html" class="s56">www.cs.yale.edu/people/silberschatz.html</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">to the record identiﬁer:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s49" style="padding-left: 38pt;text-indent: 0pt;text-align: center;">edu.yale.cs.www/people/silberschatz.html</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">so that pages are clustered in a useful order.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Data-storage systems often allow multiple versions of data items to be stored. Ver- sions are often identiﬁed by timestamp, but they may be alternatively identiﬁed by an integer value that is incremented whenever a new version of a data item is created. Reads can specify the required version of a data item, or they can pick the version with the highest version number. In Bigtable, for example, a key actually consists of three parts: (record-identiﬁer, attribute-name, timestamp).</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Some key-value stores support <i>columnar storage </i>of rows, with each column of a row stored separately, with the row key and the column value stored for each row. Such a representation allows a scan to eﬃciently retrieve a speciﬁed column of all rows, without having to retrieve other columns from storage. In contrast, if rows are stored in the usual manner, with all column values stored with the row, a sequential scan of the storage would fetch columns that are not required, reducing performance.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Further, some key-value stores support the notion of a <span class="s63">column family</span>, which groups sets of columns into a column family. For a given row, all the columns in a speciﬁc col- umn family are stored together, but columns from other column families are stored separately. If a set of columns are often retrieved together, storing them as a column family may allow more eﬃcient retrieval, as compared to either columnar storage where these are stored and retrieved separately, or a row storage, which could result in retriev- ing unneeded columns from storage.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">21.7.2 Storing and Retrieving Data</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">In this section, we use the term <i>tablet </i>to refer to partitions, as discussed in Section</p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">21.3.3. We also use the term <span class="s63">tablet server </span>to refer to the node that acts as the server for a particular tablet; all requests related to a tablet are sent to the tablet server for that tablet.<span class="s76">5</span>. The tablet server would be one of the nodes that has a replica of the tablet and plays the role of master replica as discussed in Section 21.4.1.<span class="s76">6</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="95" height="1" alt="image" src="Image_3169.png"/></span></p><p class="s111" style="padding-top: 3pt;padding-left: 119pt;text-indent: 0pt;line-height: 10pt;text-align: justify;"><span class="s77">5</span><span class="s78">HBase uses the terms </span>region <span class="s80">and </span>region server <span class="s80">in place of the terms </span>tablet <span class="s80">and </span>tablet server</p><p class="s77" style="padding-left: 119pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">6<span class="s78">In BigTable and HBase, replication is handled by the underlying distributed ﬁle system; tablet data are stored in ﬁles, and one of the nodes containing a replica of the tablet ﬁles is chosen as the tablet server.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 17pt;text-align: justify;">We use the term <span class="s63">master </span>to refer to a site that stores a master copy of the partition information, including, for each tablet, the key ranges for the tablet, the sites storing the replicas of the tablet, and the current tablet server for that tablet.<span class="s76">7</span> The master is also responsible for tracking the health of tablet servers; in case a tablet server node fails, the master assigns one of the other nodes that contains a replica of the tablet to act as the new tablet server for that tablet. The master is also responsible for reassigning tablets to balance the load in the system if some node is overloaded or if a new node is added to the system.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">For each request coming into the system, the tablet corresponding to the key must be identiﬁed, and the request routed to the tablet server. If a single master site were responsible for this task, it would get overloaded. Instead, the routing task is parallelized in one of two ways:</p><p class="s39" style="padding-top: 9pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">By replicating the partition information to the client sites; the key-value store API used by clients looks up the partition information copy stored at the client to de- cide where to route a request. This approach is used in Bigtable and HBase.</span></p><p class="s39" style="padding-top: 4pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">By replicating the partition information to a set of router sites, which route requests to the site with the appropriate tablet. Requests can be sent to any one of the router sites, which forward the request to the correct tablet master. This approach is used, for example, in the PNUTS system.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Since there may be a gap between actually splitting or moving a tablet and updating the partition information at a router (or client), the partition information may be out of date when the routing decision is made. When the request reaches the identiﬁed tablet master node, the node detects that the tablet has been split, or that the site no longer stores a (master) replica of the tablet. In such a case, the request is returned to the router with an indication that the routing was incorrect; the router then retrieves up-to-date tablet mapping information from the master and reroutes the request to the correct destination.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Figure 21.8 depicts the architecture of a cloud data-storage system, based loosely on the <span class="s44">PNUTS </span>architecture. Other systems provide similar functionality, although their architecture may vary. For example, Bigtable/HBase do not have separate routers; the partitioning and tablet-server mapping information is stored in the Google File Sys- tem/HDFS, and clients read the information from the ﬁle system and decide where to send their requests.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">21.7.2.1 Geographically Distributed Storage</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Several key-value stores support replication of data to geographically distributed loca- tions; some of these also support partitioning of data across geographically distributed locations, allowing diﬀerent partitions to be replicated in diﬀerent sets of locations.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="95" height="1" alt="image" src="Image_3170.png"/></span></p><p class="s80" style="padding-top: 3pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;"><span class="s77">7</span><span class="s78">The term </span><i>tablet controller </i>is used by <span class="s161">PNUTS </span>to refer to the master site.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="401" height="269" alt="image" src="Image_3171.png"/></span></p><p class="s230" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Requests</p><p style="text-indent: 0pt;text-align: left;"/><p class="s230" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Requests  Requests</p><p style="text-indent: 0pt;text-align: left;"/><p class="s230" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Master copy of</p><p class="s230" style="text-indent: 0pt;text-align: left;">partition table/ tablet mapping</p><p style="text-indent: 0pt;text-align: left;"/><p class="s230" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Routers</p><p style="text-indent: 0pt;text-align: left;"/><p class="s230" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">Tablet</p><p class="s230" style="text-indent: 0pt;line-height: 9pt;text-align: center;">controller</p><p style="text-indent: 0pt;text-align: left;"/><p class="s230" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Tablets</p><p style="text-indent: 0pt;text-align: left;"/><p class="s230" style="padding-top: 3pt;padding-left: 80pt;text-indent: 0pt;text-align: center;">Tablet servers</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 180pt;text-indent: 0pt;text-align: left;">Figure 21.8 <span class="s74">Architecture of a cloud data storage system.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">One of the key motivations for geographic distribution is fault tolerance, which allows the system to continue functioning even if an entire data center fails due to a disaster such as a ﬁre or an earthquake; in fact, earthquakes could cause all data centers in a region to fail. A second key motivation is to allow a copy of the data to reside at a geographic region close to the user; requiring data to be fetched from across the world could result in latencies of hundreds of milliseconds.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">A key performance issue with geographical replication of data is that the latency across geographical regions is much higher than the latency within a data center. Some key-value stores nevertheless support geographically distributed replication, requiring transactions to wait for conﬁrmation of updates from remote locations. Other key- value stores support asynchronous replication of updates to remote locations, allowing a transaction to commit without waiting for conﬁrmation of updates from a remote location. There is, however, a risk of loss of updates in case of failure before the updates are replicated. Some key-value stores allow the application to choose whether to wait for conﬁrmation from remote locations or to commit as soon as updates are performed locally.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Key-value stores that support geographic replication include Apache Cassandra, Megastore and Spanner from Google, Windows Azure storage from Microsoft, and PNUTS/Sherpa from Yahoo!, among others.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">21.7.2.2 Index Structure</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: right;">The records in each tablet in a key-value store are indexed on the key; range queries can be eﬃciently supported by storing records clustered on the key. A B<span class="s181">+</span>-tree ﬁle orga- nization is a good option, since it supports indexing with clustered storage of records.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The widely used key-value stores BigTable and HBase are built on top of distributed ﬁle systems in which ﬁles are <i>immutable</i>; that is, ﬁles cannot be updated once they are created. Thus B<span class="s181">+</span>-tree indices or ﬁle organization cannot be stored in immutable ﬁles, since B<span class="s181">+</span>-trees require updates, which cannot be done on an immutable ﬁle.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Instead, the BigTable and HBase systems use the <i>stepped-merge </i>variant of the <i>log structured merge tree </i>(<span class="s69">LSM </span><i>tree</i>), which we saw in Section 14.8.1, and is described in more detail in Section 24.2. The <span class="s44">LSM </span>tree does not perform updates on existing trees, but instead creates new trees either using new data or by merging existing trees. Thus, it is an ideal ﬁt for use on top of distributed ﬁle systems that only support immutable ﬁles. As an extra beneﬁt, the <span class="s44">LSM </span>tree supports clustered storage of records, and can support very high insert and update rates, which has been found very useful in many applications of key-value stores. Several key-value stores, such as Apache Cassandra and the WiredTiger storage structure used by MongoDB, use the <span class="s44">LSM </span>tree structure.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">21.7.3 Support for Transactions</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Most key-value stores oﬀer limited support for transactions. For example, key-value stores typically support atomic updates on a single data item and ensure that updates on the data item are serialized, that is, run one after the other. Serializability at the level of individual operations is thus trivially satisﬁed, since the operations are run serially. Note that serializability at the level of transactions is not guaranteed by serial execution of updates on individual data items, since a transaction may access more than one data item.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Some key-value stores, such as Google’s MegaStore and Spanner, provide full sup- port for <span class="s44">ACID </span>transactions across multiple nodes. However, most key-value stores do not support transactions across multiple data items.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Some key-value stores provide a test-and-set operation that can help applications implement limited forms of concurrency control, as we see next.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">21.7.3.1 Concurrency Control</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Some key-value stores, such as the Megastore and Spanner systems from Google, sup- port concurrency control via locking. Issues in distributed concurrency control are discussed in Chapter 23. Spanner also supports versioning and database snapshots based on timestamps. Details of the multiversion concurrency control technique im- plemented in Spanner are discussed in Section 23.5.1.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">However, most of the other key-value stores, such as Bigtable, <span class="s44">PNUTS</span>/Sherpa, and MongoDB, support atomic operations on single data items (which may have multiple columns, or may be <span class="s44">JSON </span>documents in MongoDB).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 17pt;text-align: justify;">Some key-value stores, such as HBase and <span class="s44">PNUTS</span>, provide an atomic test-and-set function, which allows an update to a data item to be conditional on the current version of the data item being the same as a speciﬁed version number; the check (test) and the update (set) are performed atomically. This feature can be used to implement a limited form of validation-based concurrency control, as discussed in Section 23.3.7.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Some data stores support atomic increment operations on data items and atomic execution of stored procedures. For example, HBase supports the <span class="s49">incrementColum- nValue() </span>operation, which atomically reads and increments a column value, and a <span class="s49">checkAndPut() </span>which atomically checks a condition on a data item and updates it only if the check succeeds. HBase also supports atomic execution of stored proce- dures, which are called “coprocessors” in HBase terminology. These procedures run on a single data item and are executed atomically.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">21.7.3.2 Atomic Commit</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">BigTable, HBase, and <span class="s44">PNUTS </span>support atomic commit of multiple updates to a single row; however, none of these systems supports atomic updates across diﬀerent rows.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">As one of the results of the above limitation, none of these systems supports sec- ondary indices; updates to a data item would require updates to the secondary index, which cannot be done atomically.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Some systems, such as <span class="s44">PNUTS</span>, support secondary indices or materialized views with deferred updates; updates to a data item result in updates to the secondary index or materialized view being added to a messaging service to be delivered to the node where the update needs to be applied. These updates are guaranteed to be delivered and applied subsequently; however, until they are applied, the secondary index may be inconsistent with the underlying data. View maintenance is also supported by <span class="s44">PNUTS </span>in the same deferred fashion. There is no transactional guarantee on the updates of such secondary indices or materialized views, and only a best-eﬀort guarantee in terms of when the updates reach their destination. Consistency issues with deferred mainte- nance are discussed in Section 23.6.3.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">In contrast, the Megastore and Spanner systems developed by Google support atomic commit for transactions spanning multiple data items, which can be spread across multiple nodes. These systems use two-phase commit (discussed in Section 23.2) to ensure atomic commit across multiple nodes.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">21.7.3.3 Dealing with Failures</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">If a tablet server node fails, another node that has a copy of the tablet should be assigned the task of serving the tablet. The master node is responsible for detecting node failures and reassigning tablet servers.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">When a new node takes over as tablet server, it must recover the state of the tablet. To ensure that updates to the tablet survive node failures, updates to a tablet are logged, and the log is itself replicated. When a site fails, the tablets at the site are assigned to</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">other sites; the new master site of each tablet is responsible for performing recovery actions using the log to bring its copy of the tablet to an up-to-date state, after which updates and reads can be performed on the tablet.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">In Bigtable, as an example, mapping information is stored in an index structure, and the index, as well as the actual tablet data, are stored in the ﬁle system. Tablet data updates are not ﬂushed immediately, but log data are. The ﬁle system ensures that the ﬁle system data are replicated and will be available even in the face of failure of a few nodes in the cluster. Thus, when a tablet is reassigned, the new server for that tablet has access to up-to-date log data.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Yahoo!’s Sherpa/<span class="s44">PNUTS </span>system, on the other hand, explicitly replicates tablets to multiple nodes in a cluster and uses a persistent messaging system to implement the log. The persistent messaging system replicates log records at multiple sites to ensure availability in the event of a failure. When a new node takes over as the tablet server, it must apply any pending log records that were generated by the earlier tablet server before taking over as the tablet server.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">To ensure availability in the face of failures, data must be replicated. As noted in Section 21.4.2, a key issue with replication is the task of keeping the replicas consis- tent with each other. Diﬀerent systems implement atomic update of replicas in diﬀer- ent fashions. Google BigTable and Apache HBase use replication features provided by an underlying ﬁle system (GFS for BigTable, and HDFS for HBase), instead of im- plementing replication on their own. Interestingly, neither GFS nor HDFS supports atomic updates of all replicas of a ﬁle; instead they support appends to ﬁles, which are copied to all replicas of the ﬁle blocks. An append is successful only when it has been applied to all replicas. System failures can result in appends that are applied to only some replicas; such incomplete appends are detected using sequence numbers and are cleaned up when they are detected.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Some systems such as <span class="s44">PNUTS </span>use a persistent messaging service to log updates; the messaging service guarantees that updates will be delivered to all replicas. Other systems, such as Google’s Megastore and Spanner, use a technique called distributed consensus to implement consistent replication, as we discuss in Section 23.8. Such systems require a majority of replicas to be available to perform an update. Other sys- tems, such as Apache Cassandra and MongoDB, allow the user control over how many replicas must be available to perform an update. Setting the value low could result in conﬂicting updates, which must be resolved later. We discuss these issues in Section 23.6.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 7pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">21.7.4 Managing Without Declarative Queries</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Key-value stores do not provide any query processing facility, such as SQL language support, or even lower-level primitives such as joins. Many applications that use key- value stores can manage without query language support. The primary mode of data access in such applications is to store data with an associated key and to retrieve data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">with that key. In the user proﬁle example, the key for user-proﬁle data would be the user’s identiﬁer.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">There are applications that require joins but implement the joins either in appli- cation code or by a form of view materialization. For example, in a social-networking application, each user should be shown new posts from all her friends, which concep- tually requires a join.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">One approach to computing the join is to implement it in the application code, by ﬁrst ﬁnding the set of friends of a given user, and then querying the data object representing each friend, to ﬁnd their recent posts.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">An alternative approach is as follows: Whenever a user makes a post, for each friend of the user a message is sent to, the data object representing that friend and the data associated with the friend are updated with a summary of the new post. When that user checks for updates, all required data are available in one place and can be retrieved quickly.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Both approaches can be used without any underlying support for joins. There are trade-oﬀs between the two alternatives such as higher cost at query time for the ﬁrst alternative versus higher storage cost and higher cost at the time of writes for the second alternative.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 7pt;padding-left: 119pt;text-indent: 0pt;text-align: left;">21.7.5 Performance Optimizations</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">When using a data storage system, the physical location of data are decided by the stor- age system and hidden from the client. When storing multiple relations that need to be joined, partitioning each independently may be suboptimal in terms of communication cost. For example, if the join of two relations is computed frequently, it may be best if they are partitioned in exactly the same way, on their join attributes. As we will see in Section 22.7.4, doing so would allow the join to be computed in parallel at each storage site, without data transfer.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">To support such scenarios, some data storage systems allow the schema designer to specify that tuples of one relation should be stored in the same partitions as tuples of another relation that they reference, typically using a foreign key. A typical use of this functionality is to store all tuples related to a particular entity together in the same partition; the set of such tuples is called an <span class="s63">entity group</span>.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Further, many data storage systems, such as HBase, support <i>stored functions </i>or <i>stored procedures</i>. Stored functions/procedures allow clients to invoke a function on a tuple (or an entity group) and instead of the tuples being fetched and executed lo- cally, the function is executed at the partition where the tuple is stored. Stored func- tions/procedures are particularly useful if the stored tuples are large, while the func- tion/procedure results are small, reducing data transfer.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Many data storage systems provide features such as support for automatically delet- ing old versions of data items after some period of time, or even deleting data items that are older than some speciﬁed period.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part389.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part391.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
