<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>appendix F</title>
    
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
  <div class="tocheadb">
    <h1 class="tochead" id="heading_id_2"><a id="pgfId-998402"></a><a id="pgfId-998517"></a>appendix F. Classification problems and randomnized algorithm metrics</h1>
  </div>

  <p class="body"><a id="pgfId-998540"></a>To understand <a id="marker-1005633"></a>the performance analysis of data structures such as Treaps (chapter 3) and Bloom filters (chapter 4), we need to take a step back and first introduce a class of algorithms with which not every developer is familiar.</p>

  <p class="body"><a id="pgfId-998624"></a>Most people, when prompted about algorithms, immediately think about deterministic algorithms. It’s easy to mistake this subclass of algorithms for the whole group: our common sense makes us expect an algorithm to be a sequence of instructions that, when provided a specific input, applies the same steps to return a well-defined output.</p>

  <p class="body"><a id="pgfId-998739"></a>That’s indeed the common scenario. It is possible, however, to describe an algorithm as a sequence of well-defined steps that will produce a deterministic result but take an unpredictable (though finite) amount of time. Algorithms that behave this way are called <i class="calibre17">Las Vegas algorithms</i><a id="marker-1003001"></a><i class="calibre17">.</i></p>

  <p class="body"><a id="pgfId-998836"></a>Even less intuitively, it is possible to describe algorithms that might produce a different, unpredictable result for the same input in every execution, or might also not terminate in a finite amount of time. There is a name for this class of algorithms as well: they are called <i class="calibre17">Monte Carlo algorithms</i><a id="marker-1003005"></a>.</p>

  <p class="body"><a id="pgfId-998947"></a>There is some debate about whether the latter should be considered as algorithms or heuristics, but I lean toward including them, because the sequence of steps in Monte Carlo algorithms is deterministic and well-defined. In this book we do present a few Monte Carlo algorithms, so you will have a chance to form an educated opinion.</p>

  <p class="body"><a id="pgfId-999072"></a>But before delving into these classes, we should define a particularly useful class of problems, <i class="calibre17">decision problems</i><a id="marker-1003009"></a>.</p>

  <h2 class="fm-head" id="heading_id_3"><a id="pgfId-999116"></a>F.1  Decision problems</h2>

  <p class="body"><a id="pgfId-999131"></a>When <a id="marker-1003013"></a>talking about algorithms that return either <code class="fm-code-in-text">true</code><a id="marker-1003017"></a> or <code class="fm-code-in-text">false</code><a id="marker-1003021"></a>, we fall into the class of problems called <i class="calibre17">binary decision problems</i><a id="marker-1003025"></a>.</p>

  <p class="body"><a id="pgfId-999186"></a>Binary classification algorithms assign one of two labels to the data. The two labels can actually be anything, but conceptually, this is equivalent to assigning a Boolean label to each point, so we’ll use this notation in the rest of the book.</p>

  <p class="body"><a id="pgfId-999278"></a>Considering only binary classification is not restrictive, since multiclass classifiers can be obtained by combining several binary classifiers.</p>

  <p class="body"><a id="pgfId-999322"></a>One fundamental result in computer science, and in particular in the field of computability and complexity, is that any optimization problem can be expressed as a decision problem and vice versa.</p>

  <p class="body"><a id="pgfId-999397"></a>For instance, if we are looking for the shortest path in a graph, we can define an equivalent decision problem by setting a threshold <code class="fm-code-in-text">T</code> and checking if there exists a path with length at most <code class="fm-code-in-text">T</code>. By solving this problem for different values of <code class="fm-code-in-text">T</code>, and by choosing these values using binary search, we can find a solution to the optimization problem by using an algorithm for the decision <a id="marker-1003029"></a><a id="marker-1003033"></a>problem.<a href="#pgfId-1003234"><sup class="footnotenumber">1</sup></a></p>

  <h2 class="fm-head" id="heading_id_4"><a id="pgfId-999554"></a>F.2  Las Vegas algorithms</h2>

  <p class="body"><a id="pgfId-999572"></a>The <a id="marker-1003037"></a><a id="marker-1003041"></a>class of so-called <i class="calibre17">Las Vegas algorithms</i><a id="marker-1003045"></a> includes all randomized algorithms that</p>

  <ol class="calibre18">
    <li class="fm-list-numbered">
      <p class="list"><a class="calibre14" id="pgfId-999607"></a>Always give as output the correct solution to the problem (or return a failure).</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-999647"></a>Can be run using a finite, but unpredictable (random) amount of resources. The key point is that how much resources are needed can’t be predicted based on input.</p>
    </li>
  </ol>

  <p class="body"><a id="pgfId-999717"></a>The most notable example of a Las Vegas algorithm is probably randomized quicksort:<a href="#pgfId-1003252"><sup class="footnotenumber">2</sup></a> it always produces the correct solution, but the pivot is chosen randomly at every recursive step, and execution time swirls between <code class="fm-code-in-text">O(n*log n)</code> and <code class="fm-code-in-text">O(n<sup class="superscript1">2</sup>)</code>, depending on the choices <a id="marker-1006261"></a><a id="marker-1006262"></a>made.</p>

  <h2 class="fm-head" id="heading_id_5"><a id="pgfId-999817"></a>F.3  Monte Carlo algorithms</h2>

  <p class="body"><a id="pgfId-999834"></a>We <a id="marker-1003057"></a><a id="marker-1003061"></a>classify algorithms as Monte Carlo methods when the output of the algorithm can sometimes be incorrect. The probability of an incorrect answer is usually a trade-off with the resources employed, and practical algorithms manage to keep this probability small while using a reasonable amount of computing power and memory.</p>

  <p class="body"><a id="pgfId-999951"></a>For decision problems, when the answer to a Monte Carlo algorithm is just <code class="fm-code-in-text">true/false</code><a id="marker-1003065"></a>, we have three possible situations:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-999998"></a>The algorithm is always correct when it returns <code class="fm-code-in-text">false</code> (so-called <i class="calibre15">false-biased</i> algorithm<a class="calibre14" id="marker-1003069"></a>).</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1000036"></a>The algorithm is always correct when it returns <code class="fm-code-in-text">true</code> (so-called <i class="calibre15">true-biased</i> algorithm<a class="calibre14" id="marker-1003073"></a>).</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1000074"></a>The algorithm might return the wrong answer indifferently for the two cases.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1000108"></a>Monte Carlo algorithms are deterministic in the amount of resources needed, and they are used often as the dual of a Las Vegas algorithm.</p>

  <p class="body"><a id="pgfId-1000164"></a>Suppose we have an algorithm A that always return the correct solution, but whose resource consumption is not deterministic. If we have a limited amount of resources, we can run A until it outputs a solution or exhausts the allotted resources. For instance, we could stop randomized quicksort after <code class="fm-code-in-text">n*log n</code> swaps.</p>

  <p class="body"><a id="pgfId-1000276"></a>This way, we are trading accuracy (the guarantee of a correct result) for the certainty to have a (sub-optimal) answer within a certain time (or using at most a certain amount of space).</p>

  <p class="body"><a id="pgfId-1000349"></a>It’s also worth noting that as we mentioned, for some randomized algorithms, we don’t even know if they will eventually stop and find a solution (although this is usually not the case). Take, for example, the randomized version of <a id="marker-1003077"></a><a id="marker-1003081"></a>Bogosort.</p>

  <h2 class="fm-head" id="heading_id_6"><a id="pgfId-1000446"></a>F.4  Classification metrics</h2>

  <p class="body"><a id="pgfId-1000461"></a>When <a id="marker-1003085"></a>we analyze data structures such as Bloom filters or Treaps, examining the time and memory requirements of their methods is paramount, but not enough. Besides how fast they run, and how much memory they use, for Monte Carlo algorithms like these we also need to ask one more question: “How well does it work?”</p>

  <p class="body"><a id="pgfId-1000581"></a>To answer this question, we need to introduce <i class="calibre17">metrics</i><a id="marker-1003089"></a>, functions that measure the distance between an approximated solution and the optimal one.</p>

  <p class="body"><a id="pgfId-1000634"></a>For classification algorithms, the quality of an algorithm is a measure of how well each input is assigned to its right class. For binary (that is, <code class="fm-code-in-text">true/false</code>) classification, therefore, it’s about how accurately the algorithm outputs <code class="fm-code-in-text">true</code><a id="marker-1003097"></a> for the input points.</p>

  <h3 class="fm-head2" id="heading_id_7"><a id="pgfId-1000728"></a>F.4.1  Accuracy</h3>

  <p class="body"><a id="pgfId-1000740"></a>One <a id="marker-1003101"></a>way of measuring the quality of a classification algorithm is to assess the rate of correct predictions. Suppose that over N<code class="fm-code-in-text"><sub class="subscript1">P</sub></code> points actually belonging to the <code class="fm-code-in-text">true</code> class<a id="marker-1003105"></a>:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1000809"></a><code class="fm-code-in-text">P<sub class="subscript1">P</sub></code> are predicted as <code class="fm-code-in-text">true.</code></p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1000831"></a><code class="fm-code-in-text">T<sub class="subscript1">P</sub></code><a class="calibre14" id="marker-1003109"></a>, the so-called true positives, are both predicted as <code class="fm-code-in-text">true</code> and actually belong to the <code class="fm-code-in-text">true</code> class.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1000878"></a>Likewise, let <code class="fm-code-in-text">N<sub class="subscript1">N</sub></code> be the number of points belonging to the <code class="fm-code-in-text">false</code> class<a id="marker-1003113"></a>:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1000914"></a><code class="fm-code-in-text">P<sub class="subscript1">N</sub></code> is the total number of points for which our algorithm predicts the class <code class="fm-code-in-text">false.</code></p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1000956"></a><code class="fm-code-in-text">T<sub class="subscript1">N</sub></code><a class="calibre14" id="marker-1003117"></a>, the so-called true negatives, is the number of times both the predicted and the actual class of a point are <code class="fm-code-in-text">false</code>.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1001014"></a>Then we can define <i class="calibre17">accuracy</i><a id="marker-1003121"></a>:</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/appE_F1.png"/></p>

  <p class="body"><a id="pgfId-1001042"></a>When the accuracy is 1, we have an algorithm that is always correct.</p>

  <p class="body"><a id="pgfId-1001075"></a>Unfortunately, except for that ideal case, accuracy is not always a good measure of the quality of our algorithms. Consider an edge case where 99% of the points in a database actually belongs to the <code class="fm-code-in-text">true</code> class<a id="marker-1003125"></a>. Then look at the following three classifiers:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1001170"></a>A classifier correctly labeling 100% of the false points and 98.98% of the true ones</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1001212"></a>A classifier correctly labeling 0.5% of the false points and 99.49% of the true ones</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1001256"></a>A classifier that always returns true as label</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1001281"></a>Astonishingly, the latter has better accuracy than the other two, even if it misses every single point in the false category.</p>

  <p class="fm-callout"><a id="pgfId-1001334"></a><span class="fm-callout-head">Tip</span> If you’d like to double check this, plug in these numbers into the formula for accuracy.</p>

  <p class="body"><a id="pgfId-1001380"></a>In machine learning, if you were to use this metric on a training set skewed in a similar way,<a href="#pgfId-1003269"><sup class="footnotenumber">3</sup></a> you’d get a terrible model, or more precisely, a model that is likely to generalize <a id="marker-1003129"></a>poorly.</p>

  <h3 class="fm-head2" id="heading_id_8"><a id="pgfId-1001469"></a>F.4.2  Precision and recall</h3>

  <p class="body"><a id="pgfId-1001486"></a>To <a id="marker-1003133"></a>improve over bare accuracy, we need to keep track of the info about each category separately. To this end, we can define two new metrics:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1001553"></a><i class="calibre15">Precision</i><a class="calibre14" id="marker-1003137"></a> (also called <i class="calibre15">positive predictive value</i><a class="calibre14" id="marker-1003141"></a>) defined as the ratio of correctly predicted true points (the <i class="calibre15">true positives</i><a class="calibre14" id="marker-1003145"></a>) over the total number of points for which <code class="fm-code-in-text">true</code><a class="calibre14" id="marker-1003149"></a> was predicted by the algorithm:</p>

      <p class="fm-figure"><img alt="" class="calibre23" src="../Images/appE_F2.png"/></p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1001645"></a><i class="calibre15">Recall</i><a class="calibre14" id="marker-1003153"></a> (also known as <i class="calibre15">sensitivity</i><a class="calibre14" id="marker-1003157"></a>) defined as the fraction of <i class="calibre15">true positives</i><a class="calibre14" id="marker-1003161"></a> over the total number of actual positives:</p>

      <p class="fm-figure"><img alt="" class="calibre23" src="../Images/appE_F3.png"/></p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1001707"></a>It is possible to give an alternate definition of precision and recall by introducing the number of <i class="calibre17">false positives</i> <code class="fm-code-in-text">Fp</code><a id="marker-1003165"></a>, that is, points belonging to the <code class="fm-code-in-text">false</code> class<a id="marker-1003169"></a> for which <code class="fm-code-in-text">true</code><a id="marker-1003173"></a> was incorrectly predicted, and the number of false negatives <code class="fm-code-in-text">Fn</code><a id="marker-1003177"></a>:</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/appE_F4.png"/></p>

  <p class="body"><a id="pgfId-1001819"></a>Intuitively, while accuracy only measures how well we do on predictions, precision/recall metrics weigh our successes with our errors for the dual category. As a matter of fact, precision and recall are not independent, and one can be defined in terms of the other, so it’s not possible to improve both indefinitely. In general, if you improve recall, you’ll often get a slightly worse precision and vice versa.</p>

  <p class="body"><a id="pgfId-1001967"></a>For machine learning<a id="marker-1003181"></a> classifiers, every model can be associated with a precision/recall curve (see figure F.1), and the model’s parameters can be tuned during training so that we can trade off the two characteristics.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/appF_F01.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1006811"></a>Figure F.1 Examples of precision-recall curve. The one on the left is a generic curve for a hypothetical algorithm. This is plotted by tuning the algorithm’s parameters and recording the precision and recall obtained. After recording many attempts, we can get a drawing similar to this graphic. In general, when a given choice of the model’s parameters improves precision, there will be a slight worsening of recall, and vice versa. On the right, a degenerate P-R curve for a false-biased algorithm<a id="marker-1006812"></a>, such as Bloom filters; its recall is always equal to 1, for any precision. True-biased<a id="marker-1006813"></a> algorithms have a symmetrical P-R curve.</p>

  <p class="body"><a id="pgfId-1002276"></a>So, for binary categorization, a perfect (100%) precision means that every point for which we output <code class="fm-code-in-text">true</code><a id="marker-1003193"></a> actually belonged to the <code class="fm-code-in-text">true</code> category. Similarly, for recall, when it’s 100%, we never have a false negative.</p>

  <p class="body"><a id="pgfId-1002357"></a>As an exercise, try to compute precision and recall for the examples in the previous sections and see how they give us more information about the quality of a model with respect to just its accuracy.</p>

  <p class="body"><a id="pgfId-1002436"></a>If we take Bloom filters (chapter 4) as an example, we explain in section 4.8 that when the output is <code class="fm-code-in-text">false</code><a id="marker-1003197"></a>, we can be sure it’s right, so Bloom filters have a 100% recall.</p>

  <p class="body"><a id="pgfId-1002515"></a>Their precision, unfortunately, is not as perfect. In section 4.10 we delve into the details of how to compute it. In this case, however, like for all <i class="calibre17">false-biased</i><a id="marker-1003201"></a> algorithms, there is no trade-off with recall, and improving precision can only be done by increasing the resources <a id="marker-1003205"></a>used.</p>

  <h3 class="fm-head2" id="heading_id_9"><a id="pgfId-1002622"></a>F.4.3  Other metrics and recap</h3>

  <p class="body"><a id="pgfId-1002642"></a>There are, of course, other metrics that can be useful for classifiers. One of the most useful is the F-measure,<a href="#pgfId-1003283"><sup class="footnotenumber">4</sup></a> combining precision and recall into one formula. These alternative metrics, however, go beyond our scope.</p>

  <p class="body"><a id="pgfId-1002730"></a>If we continue with our parallel with Bloom filters, we can recap the meaning of the metrics we examined in that context:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1002782"></a><i class="calibre15">Accuracy</i><a class="calibre14" id="marker-1003209"></a> answers the question, “What percentage of calls to <code class="fm-code-in-text">contains</code><a class="calibre14" id="marker-1003213"></a> are correct?”</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1002820"></a><i class="calibre15">Precision</i><a class="calibre14" id="marker-1003217"></a> answers the question, “What percentage of calls to <code class="fm-code-in-text">contains</code> returning <code class="fm-code-in-text">true</code> were correct?”</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1002861"></a><i class="calibre15">Recall</i><a class="calibre14" id="marker-1003221"></a> answers the question, “Among all calls to <code class="fm-code-in-text">contains</code> on elements actually contained in the filter, what percent of them returned <code class="fm-code-in-text">true</code>?” (This one, as we know, will always be 100% for <a class="calibre14" id="marker-1003225"></a>Bloom <a class="calibre14" id="marker-1003229"></a>filters).</p>
    </li>
  </ul>
  <hr class="calibre22"/>

  <p class="fm-footnote"><sup class="footnotenumber">1.</sup> <a id="pgfId-1003234"></a>If the threshold is guaranteed to be an integer or rational number, we can also guarantee that the solution will be in the same computational class as the solution to the decision problem, so if, for example, we have a polynomial solution to the decision problem, solving the optimization problem will also require polynomial time.</p>

  <p class="fm-footnote"><sup class="footnotenumber">2.</sup> <a id="pgfId-1003252"></a>See <i class="calibre17">Grokking Algorithms</i>, by Aditya Bhargava (Manning Publications, 2016), chapter 4, page 51.</p>

  <p class="fm-footnote"><sup class="footnotenumber">3.</sup> <a id="pgfId-1003269"></a>For instance, a dataset where one of the classes is rare or difficult to get data for: this is often the case in medicine with rare diseases.</p>

  <p class="fm-footnote"><sup class="footnotenumber">4.</sup> <a id="pgfId-1003283"></a>See <span class="fm-hyperlink"><a href="https://en.wikipedia.org/wiki/F1_score">https://en.wikipedia.org/wiki/F1_score</a></span>.</p>
</body>
</html>
