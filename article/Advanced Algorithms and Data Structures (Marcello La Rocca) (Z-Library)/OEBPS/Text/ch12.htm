<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>12</title>
    
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
  <div class="tocheadb">
    <h1 class="tochead" id="heading_id_2"><a id="pgfId-998649"></a><a id="pgfId-998661"></a>12 Clustering</h1>
  </div>

  <p class="co-summary-head"><a id="pgfId-1015987"></a>This chapter covers</p>

  <ul class="calibre19">
    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1016021"></a>Classifying different types of clustering</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1016022"></a>Partitioning clustering</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1016023"></a>Understanding and implementing k-means</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1016024"></a>Density-based clustering</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1016025"></a>Understanding and implementing DBSCAN</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1016026"></a>OPTICS: Refining DBSCAN as hierarchical clustering</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1016012"></a>Evaluating clustering results</li>
  </ul>

  <p class="body"><a id="pgfId-998770"></a>In the previous chapters we have described, implemented, and applied three data structures designed to efficiently solve nearest neighbor search. When we moved to their applications, we mentioned that clustering was one of the main areas where an efficient nearest neighbor search could make a difference. We had to delay this discussion, but now it’s finally time to put the icing on the cake and get the most out of our hard work. In this chapter, we will first briefly introduce clustering, explaining what it is and how it relates to machine learning and AI. We’ll see that there are different types of clustering, with radically different approaches, and then we will present and discuss in detail three algorithms that use different approaches. By going through the whole chapter, readers will be exposed to the theoretical foundations for this topic, learn about algorithms that can be implemented or just applied to break down datasets into smaller homogeneous groups, and also, in the process, get a deeper understanding of nearest neighbor search and multidimensional indexing.</p>

  <p class="body"><a id="pgfId-998793"></a>But before we start, let’s quickly introduce an example of a problem that motivates the use of clustering. Throughout the previous chapters in part 2 of this book, we have developed this example of an e-commerce site, starting from the early days when the internet became mainstream. Now it’s time to bring our company into the 2010s and add a data science team. In fact, for sales to thrive, we will need to perform customer segmentation in order to understand our customers’ behavior and categorize customers based on what we know about them:<a href="#pgfId-1009115"><sup class="footnotenumber">1</sup></a> their purchasing habits and financial situation, as well as their demographics, age, level of education, and country or state where they live, which are all factors that influence people’s taste and spending capacity.</p>

  <p class="body"><a id="pgfId-998814"></a>Customer segmentation partitions customers into homogeneous groups sharing similar purchasing power, purchase history, or expected behavior. Clustering is one step of this process, where the groups are formed from raw, unlabeled data. Clustering algorithms don’t output a description of the groups; they just return a partitioning of the whole customer base, and then data scientists need to perform further analysis of the different morphotypes to understand how these groups are composed. Once this knowledge is derived, it can be used by marketing teams to tailor targeted campaigns toward each of these groups (or some of them, if some of the groups are crucial to the company’s wealth). For instance, on a video streaming website (such as Netflix), data scientists might be able to identify a group of users that will likely watch comedies, another group more interested in action movies, and so on.</p>

  <p class="body"><a id="pgfId-998837"></a>In real-world examples, customers have hundreds of features that are considered for marketing segmentation; here, for the sake of visualization and to make explanations easier, we will use a simplified example, with just two features: annual income and average monthly expenses on our e-commerce site. We will pick up this example again later in the chapter, but first, we will give you some more context and the tools to perform clustering analysis.</p>

  <h2 class="fm-head" id="heading_id_3"><a id="pgfId-998848"></a>12.1 Intro to clustering</h2>

  <p class="body"><a id="pgfId-998862"></a>In recent years, especially from the second half of the first decade of this century, a single branch of AI got so much momentum that now it’s often considered in the media and public opinion as synonymous to AI. I’m talking, of course, about <i class="calibre17">machine learning</i><a id="marker-999436"></a> (ML), which in turn has been lately (since ~2015) increasingly identified with <i class="calibre17">deep learning</i><a id="marker-999440"></a> (DL).</p>

  <p class="body"><a id="pgfId-998891"></a>The truth is that deep learning constitutes just a part of machine learning, gathering all those models built with <i class="calibre17">deep</i> (intended as “with many layers”) neural networks, and machine learning, in turn, is just a branch of artificial intelligence.</p>

  <p class="body"><a id="pgfId-998904"></a>In particular, machine learning is the branch that is focused on developing mathematical models that describe a system after learning its characteristics from data.</p>

  <p class="body"><a id="pgfId-998913"></a>ML and deep DL can achieve impressive, eye-catching, and sometimes incredible results (at the time of writing you can think of, for instance, life-like artificially generated faces, landscapes, and even movies created by GANs<a href="#pgfId-1009129"><sup class="footnotenumber">2</sup></a>), but they can’t, and neither do they aim to, build an “intelligent” agent—something closer to the romantic idea of an artificial consciousness that cult movies like <i class="calibre17">Short Circuit</i> or <i class="calibre17">War Games</i> gave us. That is rather the goal of <i class="calibre17">general artificial intelligence</i>.</p>

  <h3 class="fm-head2" id="heading_id_4"><a id="pgfId-998942"></a>12.1.1 Types of learning</h3>

  <p class="body"><a id="pgfId-998956"></a>The <a id="marker-999444"></a>main classification of machine learning models is based on the type of “learning” they perform, in particular focusing on the way we provide feedback to the model during training:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-998967"></a>Supervised learning—These models are trained on labeled data; that is, for each entry in the training set, there is a label (for <i class="calibre15">classification</i><a class="calibre14" id="marker-999448"></a> algorithms) or a value (<i class="calibre15">regression</i> algorithms<a class="calibre14" id="marker-999452"></a>) that is associated to that entry. Training will tune models’ parameters in order to increase the accuracy of the model in associating the correct class or value to new entries. Examples of SL are object detection (classification) or predictive models to estimate goods’ prices (regression).</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-998994"></a>Reinforcement learning—Rather than providing explicit labels associated to the data, the model performs some kind of task, and only at the end does it receive feedback about the outcome (stating either success or failure). This is one of the most interesting areas of research at the time of writing, and some examples include game theory (for instance, an agent learning to play chess or Go) and many areas of robotics (such as teaching a robotic arm to juggle).</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-999014"></a>Unsupervised learning—This category differs from the first two because in this case the algorithms are not presented with any feedback on data, but their goal is rather to make sense of data by extrapolating its inner (and often hidden) structure. Clustering is the main form of unsupervised learning.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-999030"></a>We will obviously focus on unsupervised learning in the rest of the chapter. Clustering algorithms, in fact, take an unlabeled dataset and try to gather as much information as possible about its structure, grouping together similar data points while setting apart dissimilar ones.</p>

  <p class="body"><a id="pgfId-999039"></a>Although at first it might seem less intuitive than supervised or reinforcement learning, clustering<a id="marker-1027135"></a> has several natural applications, and possibly the best way to describe clustering is by exemplifying a few of <a id="marker-1027136"></a>them:</p>

  <ol class="calibre18">
    <li class="fm-list-numbered">
      <p class="list"><a class="calibre14" id="pgfId-999053"></a><i class="calibre17">Market segmentation</i><a class="calibre14" id="marker-999464"></a>—Take purchase data and find groups of similar customers. Since they behave in the same way, it’s likely that a marketing strategy would work (or fail) consistently across a group (if segmentation is done properly).</p>

      <p class="fm-list-body"><a class="calibre14" id="pgfId-1016160"></a>An algorithm won’t output labels for the groups; it won’t tell us if a group is “students under 25” and another “mid-aged book writers, with a passion for comics.” It will only gather similar people together, and then data analysts can further examine the groups to understand more about their composition.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-999084"></a><i class="calibre15">Finding outliers</i><a class="calibre14" id="marker-999468"></a>—Find data that stands out. Depending on the context, it could be noise in a signal, a new species of flowers in a rain forest, or even a new pattern or behavior in customer analytics.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-999105"></a><i class="calibre15">Preprocessing</i>—Find clusters in the data and process each cluster separately (and possibly in parallel). This obviously provides a speedup, but it also has another side-effect. Since it reduces the max amount of space needed at any single time, when a huge dataset is broken up into smaller pieces, each piece can fit in memory or be processed on a single machine, while the whole dataset can’t. Sometimes you can even use a fast clustering algorithm (for instance, <i class="calibre15">canopy clustering</i><a class="calibre14" id="marker-999472"></a>) as a preprocessor step for a slower clustering algorithm.</p>
    </li>
  </ol>

  <h3 class="fm-head2" id="heading_id_5"><a id="pgfId-999146"></a>12.1.2 Types of clustering</h3>

  <p class="body"><a id="pgfId-999160"></a>Clustering <a id="marker-999476"></a>is an <a id="id_Hlk56489913"></a>NP-Hard<a href="#pgfId-1009145"><sup class="footnotenumber">3</sup></a> problem, and as such it is computationally difficult (impossible for today’s real datasets) to solve it exactly. Moreover, it is hard to even define objective metrics that can assess the quality of a solution! Figure 12.1 explains this concept. For some cluster shapes, our intuition tells us that the two rings should be in different clusters, but it’s hard to come up with a metric function that objectively states so (minimum distance, for instance, wouldn’t work). And this becomes even harder for high-dimensional datasets (where our intuition can’t even help us validate these metrics, because it’s hard to represent and interpret anything beyond 3-D spaces).</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F1.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1040187"></a>Figure 12.1 A dataset that’s challenging to cluster. On the left, the ideal clustering that matches our intuition. Next to it, non-optimal results produced by metrics like proximity or affinity propagation.</p>

  <p class="body"><a id="pgfId-999206"></a>For these reasons, all clustering algorithms are heuristics that converge more or less quickly to a locally-optimal solution.</p>

  <p class="body"><a id="pgfId-999217"></a>Under the category labeled “clustering,” we group several data-partitioning algorithms, using approaches completely different from each other. All these approaches can be applied to the problems described in the previous section almost transparently, although, obviously, each approach has strengths and flaws, and we should select the best-fitting algorithms based on our requirements.</p>

  <p class="body"><a id="pgfId-999232"></a>A first relevant distinction is made between hard and soft clustering, as also shown in figure 12.2:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-999241"></a>In hard clustering<a class="calibre14" id="marker-1027234"></a>, to every point the output assigns a single cluster, one and only one (or at most one, if the clustering algorithm can also detect noise).</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-999256"></a>In soft clustering<a class="calibre14" id="marker-1021422"></a>, for each point <code class="fm-code-in-text">P</code> and each group <code class="fm-code-in-text">G</code>, the output provides a probability that <code class="fm-code-in-text">P</code> belongs to <code class="fm-code-in-text">G</code>.</p>
    </li>
  </ul>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F2.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1040240"></a>Figure 12.2 The difference between hard and soft clustering can be explained in terms of the membership function they adopt. Hard clustering’s outputs are either <code class="fm-code-in-text">0</code> or <code class="fm-code-in-text">1</code> for each point and each cluster, with the constraint that it can be <code class="fm-code-in-text">1</code> for only one of the point-cluster combinations. Soft clustering’s membership function outputs a probability between <code class="fm-code-in-text">0</code> and <code class="fm-code-in-text">1</code> (any value in-between) and for each point, it can be non-zero for multiple clusters.</p>

  <p class="body"><a id="pgfId-999312"></a>The other main criterion to classify clustering algorithms differentiates <i class="calibre17">partitioning clustering</i><a id="marker-999488"></a> from <i class="calibre17">Hierarchical clustering</i><a id="marker-999492"></a>:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-999327"></a>Partitioning clustering, aka <i class="calibre15">flat clustering</i><a class="calibre14" id="marker-999496"></a>, outputs a division of the input dataset into partitions, so no cluster is the subset of another, nor does it intersect any other cluster.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-999343"></a>Hierarchical clustering produces a hierarchy of clusters that can be then interpreted and “sliced” at any given point, depending on parameters set for the algorithm.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-999357"></a>Obviously these two criteria are orthogonal; for instance, you can have hard partitioning clustering algorithms (such as k-means<a id="marker-999500"></a>) or soft hierarchical ones (such as OPTICS).</p>

  <p class="body"><a id="pgfId-999379"></a>Other criteria often used to classify these algorithms are centroid-based versus density-based, and randomized versus deterministic.</p>

  <p class="body"><a id="pgfId-999398"></a>In the next section, we’ll first describe an algorithm for partitioning clustering: k-means, the progenitor of all clustering algorithms. Then we’ll move to a different type of flat clustering, DBSCAN, and finally we’ll pick up the discussion on hierarchical clustering and introduce OPTICS, which is also density-based. Table 12.1 summarizes the “identity card” for the three algorithms presented in this chapter.</p>

  <p class="fm-table-caption"><a id="pgfId-1016432"></a>Table 12.1 Summary of the characteristics of clustering algorithms presented in this chapter</p>

  <table border="1" class="contenttable" width="100%">
    <tr class="calibre8">
      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1016440"></a>Categories</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1016442"></a>k-means</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1016444"></a>DBSCAN</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1016446"></a>OPTICS</p>
      </th>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016448"></a><code class="fm-code-in-text2">Membership</code><a id="marker-1016487"></a></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016450"></a><code class="fm-code-in-text2">Hard</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016452"></a><code class="fm-code-in-text2">Hard</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016454"></a><code class="fm-code-in-text2">Soft</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016456"></a><code class="fm-code-in-text2">Structure</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016458"></a><code class="fm-code-in-text2">Flat</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016460"></a><code class="fm-code-in-text2">Flat</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016462"></a><code class="fm-code-in-text2">Hierarchical</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016464"></a><code class="fm-code-in-text2">Strategy</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016466"></a><code class="fm-code-in-text2">Centroid-based</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016468"></a><code class="fm-code-in-text2">Density-based</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016470"></a><code class="fm-code-in-text2">Density-based</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016472"></a><code class="fm-code-in-text2">Determinism</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016474"></a><code class="fm-code-in-text2">Randomized</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016476"></a><code class="fm-code-in-text2">Deterministic</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016478"></a><code class="fm-code-in-text2">Deterministic</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016480"></a><code class="fm-code-in-text2">Outliers detection</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016482"></a><code class="fm-code-in-text2">No</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016484"></a><code class="fm-code-in-text2">Yes</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016486"></a><code class="fm-code-in-text2">Yes</code></p>
      </td>
    </tr>
  </table>

  <p class="body"><a id="pgfId-999503"></a>Don’t worry; in the next sections for each of these properties we will explain in detail what it means and provide examples to make the <a id="marker-1008632"></a>distinction clearer.</p>

  <h2 class="fm-head" id="heading_id_6"><a id="pgfId-999773"></a>12.2 K-means</h2>

  <p class="body"><a id="pgfId-999785"></a>Let’s <a id="marker-1008636"></a>start our discussion with a classic algorithm, one with a history of success going back to the 1950s: k-means, a partitioning algorithm that gathers data in a predetermined number of spherical clusters.</p>

  <p class="body"><a id="pgfId-999796"></a>Figure 12.3 illustrates how k-means works. We can break down the algorithm in three high-level steps:</p>

  <ol class="calibre18">
    <li class="fm-list-numbered">
      <p class="list"><a class="calibre14" id="pgfId-999820"></a><i class="calibre15">Initialization</i>—Create <code class="fm-code-in-text">k</code> random <i class="calibre15">centroids</i><a class="calibre14" id="marker-1008640"></a>, random points that can or cannot belong to the dataset and will be the centers of the spherical clusters.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-999841"></a><i class="calibre15">Classification</i>—For each point in the dataset, compute the distance to each centroid, and assign the aforementioned point to the closest among the centroids.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-999861"></a><i class="calibre15">Recentering</i>—The points assigned to a centroid form a cluster. For each cluster, compute its center of mass (as described in section 10.3), and then move the cluster’s centroid to the center of mass computed.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-999881"></a>Repeat steps <i class="calibre15">classification</i><a class="calibre14" id="marker-1008644"></a> and <i class="calibre15">recentering</i><a class="calibre14" id="marker-1008648"></a> until no point, at step 2, switches to a different cluster, or the maximum number of iterations is reached.</p>
    </li>
  </ol>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F3.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1040286"></a>Figure 12.3 An example of the k-means algorithm with <code class="fm-code-in-text">k==3</code>. Dataset points are initially lightly shaded. Step 1: <code class="fm-code-in-text">k</code> centroids (shaped as pentagons) are randomly created. Each centroid is assigned a different color: RGB. Step 2: For each point, measure the distance to all centroids, and assign the point to the closest one. At the end, each centroid <code class="fm-code-in-text">C</code> will define a cluster, a sphere centered at <code class="fm-code-in-text">C</code> and whose radius is the distance to the furthest point assigned to <code class="fm-code-in-text">C</code>. The clusters are highlighted in the same color as their centroids. Step 3: Update the centroids: for each cluster, compute its center of mass. Step 4: Repeat steps 2 and 3 <code class="fm-code-in-text">j</code> times. Some points will switch to a different cluster.</p>

  <p class="body"><a id="pgfId-999934"></a>Steps 2–4 of this algorithm are a deterministic heuristic that computes exact distances between points and centroids, and then updates the centroids to the center of mass of each cluster. However, the algorithm qualifies as a Monte Carlo randomized algorithm because of its first step, where we use random initialization. Turns out this step is crucial for the algorithm. The final result, in fact, is heavily influenced by the initial choice of the centroids. A bad choice can slow down convergence, which, in turn, given that the maximum number of iterations is bounded, will likely lead to an early stop and a poor result. And because the algorithm will remove centroids to which no points are assigned—a very bad initial choice with several centroids close to each other—this could lead to an unwanted reduction of the number of centroids in the early stages of the heuristic (see figure 12.4).</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F4.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1040328"></a>Figure 12.4 A very unlucky choice of initial centroids (shown as polygons). All centroids are gathered together in one corner of the dataset. The black line shows an approximation of the border between the regions determined by each centroid (on each line, ideally, the distance between the centroids it separates is the same). Since the centroid represented with a hexagon is further away than the other two, no point will be assigned to it, so it will be removed from the list of centroids. The choice appears also unbalanced for the other two centroids, although the next update steps will (slowly) rebalance the situation, moving the square toward the center of the cluster on the right.</p>

  <p class="body"><a id="pgfId-999981"></a>To mitigate this issue, in practice k-means is always used with a random-restart strategy: the algorithm is run several times on a dataset, each time with a different random initialization of the centroids, and then results are compared to choose the best clustering (more about this in the last section of the chapter).</p>

  <p class="body"><a id="pgfId-999990"></a>Listing 12.1 shows the code for the main method performing k-means clustering. We broke down the main steps into separate functions to get cleaner, more easily maintainable code and we’ll delve into each step in the next pages. You can also check out implementations of the method on the book’s <span class="fm-hyperlink">repo</span> on GitHub.<a href="#pgfId-1009177"><sup class="footnotenumber">4</sup></a></p>

  <p class="fm-code-listing-caption"><a id="pgfId-1017063"></a>Listing 12.1 k-means clustering</p>
  <pre class="programlisting"><b class="calibre21">function</b> kmeans(points, numCentroids, maxIter)               <span class="fm-combinumeral">❶</span>
  centroids ← randomCentroidInit(points, numCentroids)       <span class="fm-combinumeral">❷</span>
  clusterIndices[p] ← 0 (<span class="cambria">∀</span> p <span class="cambria">ϵ</span> points)                      <span class="fm-combinumeral">❸</span>
  <b class="calibre21">for</b> iter <b class="calibre21">in</b> {1, .., maxIter} <b class="calibre21">do</b>                            <span class="fm-combinumeral">❹</span>
    newClusterIndices ← classifyPoints(points, centroids)    <span class="fm-combinumeral">❺</span>
    <b class="calibre21">if</b> clusterIndices == newClusterIndices <b class="calibre21">then</b>              <span class="fm-combinumeral">❻</span>
      break
    clusterIndices ← <b class="calibre21">new</b>ClusterIndices                       <span class="fm-combinumeral">❼</span>
    centroids ← updateCentroids(points, clusterIndices)      <span class="fm-combinumeral">❽</span>
  <b class="calibre21">return</b> (centroids, clusterIndices)                         <span class="fm-combinumeral">❾</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1038527"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">kmeans</code><a id="marker-1038531"></a> takes a list of points and an integer, the number of clusterings it should create (remember, one clustering per centroid). We also pass <code class="fm-code-in-text2">maxIter</code>, the maximum number of iterations. This function returns a pair, the list of centroids and the list of centroid indices associated to each point.</p>

  <p class="fm-code-annotation"><a id="pgfId-1038552"></a><span class="fm-combinumeral">❷</span> Initializes the list of centroids. Different strategies can be used for random initialization, and non-random initialization functions are viable (more on this in the next listing).</p>

  <p class="fm-code-annotation"><a id="pgfId-1038569"></a><span class="fm-combinumeral">❸</span> Initializes the list with the cluster index for each point. At first, each point belongs to the same big cluster containing the whole dataset.</p>

  <p class="fm-code-annotation"><a id="pgfId-1038586"></a><span class="fm-combinumeral">❹</span> Repeats the main cycle (at most) <code class="fm-code-in-text2">maxIter</code><a id="marker-1038591"></a> times</p>

  <p class="fm-code-annotation"><a id="pgfId-1038604"></a><span class="fm-combinumeral">❺</span> Updates the assignment of points to clusters. Stores the result in a temporary variable to compare the new classification with the one from the previous iteration.</p>

  <p class="fm-code-annotation"><a id="pgfId-1038621"></a><span class="fm-combinumeral">❻</span> If no point has switched clusters, the algorithm converged and it can exit.</p>

  <p class="fm-code-annotation"><a id="pgfId-1038638"></a><span class="fm-combinumeral">❼</span> Otherwise, copies over the assignments from the temporary variable</p>

  <p class="fm-code-annotation"><a id="pgfId-1038655"></a><span class="fm-combinumeral">❽</span> Updates the centroids based on the new classification</p>

  <p class="fm-code-annotation"><a id="pgfId-1038672"></a><span class="fm-combinumeral">❾</span> Once the algorithm converges, we return both the centroids and the assignments for each cluster.</p>

  <p class="body"><a id="pgfId-1000306"></a>The algorithm can be seen as a search heuristic converging to a (local) optimum, with slightly more complex than normal functions to compute the score and the gradient step. At line #6, we have a stop condition that checks if the algorithm converged. If the classification hasn’t changed in the last step, then the centroids will be the same as in the previous step, so any further iteration will be futile. Since these functions are quite expensive to compute at each step, and convergence is not guaranteed, we add another stop condition by capping the execution to a maximum number of iterations.</p>

  <p class="body"><a id="pgfId-1000331"></a>As mentioned, we abstracted away the logic of the update and score functions in separate methods. But before looking at those, it’s interesting to check out the random initialization step, which is often underestimated. Listing 12.2 shows the implementation we suggest for this function.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1018167"></a>Listing 12.2 <code class="fm-code-in-text">randomCentroidInit</code></p>
  <pre class="programlisting"><b class="calibre21">function</b> randomCentroidInit(points, numCentroids)         <span class="fm-combinumeral">❶</span>
  centroids ← sample(points, numCentroids)                <span class="fm-combinumeral">❷</span>
  <b class="calibre21">for</b> i <b class="calibre21">in</b> {0, .., numCentroids-1} <b class="calibre21">do</b>                     <span class="fm-combinumeral">❸</span>
    <b class="calibre21">for</b> j <b class="calibre21">in</b> {0, .., dim-1} <b class="calibre21">do</b>                            <span class="fm-combinumeral">❹</span>
      centroids[i][j] ← centroids[i][j] + randomNoise()   <span class="fm-combinumeral">❺</span>
  <b class="calibre21">return</b> centroids                                        <span class="fm-combinumeral">❻</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1038134"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">randomCentroidInit</code><a id="marker-1038138"></a> takes the list of points in the dataset and the number of centroids it should create. It returns the list of centroids generated.</p>

  <p class="fm-code-annotation"><a id="pgfId-1038156"></a><span class="fm-combinumeral">❷</span> Initializes the list of centroids by randomly sampling (without replacement) <code class="fm-code-in-text2">numCentroids</code><a id="marker-1038161"></a> points from the dataset</p>

  <p class="fm-code-annotation"><a id="pgfId-1038177"></a><span class="fm-combinumeral">❸</span> Cycles through the list of centroids (their indices)</p>

  <p class="fm-code-annotation"><a id="pgfId-1038194"></a><span class="fm-combinumeral">❹</span> For each centroid, cycles through its coordinates (assuming <code class="fm-code-in-text2">dim</code>, the number of coordinates, is, for instance, a class variable; otherwise, you could use <code class="fm-code-in-text2">|centroids[i]|</code>).</p>

  <p class="fm-code-annotation"><a id="pgfId-1038211"></a><span class="fm-combinumeral">❺</span> Updates the current coordinate by adding some random noise</p>

  <p class="fm-code-annotation"><a id="pgfId-1038228"></a><span class="fm-combinumeral">❻</span> Returns the list of centroids</p>

  <p class="body"><a id="pgfId-1000542"></a>While there are several viable alternatives (for instance, randomly drawing each coordinate from the domain’s boundaries, or using actual dataset’s points), a solution that gets us several advantages is randomly perturbating <code class="fm-code-in-text">k</code> points casually drawn from the dataset:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1000554"></a>First, we don’t have to worry about the domain’s boundaries. If each centroid’s coordinate was generated completely at random, we would have to first scan the dataset to find the acceptable range for each coordinate.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1000570"></a>Even paying attention to the dataset’s boundaries, sampled points could end up in sparse or empty regions, and as such they could later be removed. Instead, by uniformly drawing points from the dataset, the centroids will be close to points in the data and . . .</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1000587"></a>. . . centroids will be drawn with higher probability in areas with higher density of points.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1000604"></a>Randomly perturbating the points, however, helps reduce the risk that all the points will be concentrated in the denser areas.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1000616"></a>The code in listing 12.2 is also intuitive, though there are a couple of interesting points to raise.</p>

  <p class="body"><a id="pgfId-1000625"></a>At line #2 we use a generic sampling function that draws <code class="fm-code-in-text">n</code> elements from a set without repetition; the details of this method are not important here, and you don’t need to worry about them. Most programming languages will provide such a function in their core libraries,<a href="#pgfId-1009193"><sup class="footnotenumber">5</sup></a> so it’s unlikely you will have to implement it.<a href="#pgfId-1009213"><sup class="footnotenumber">6</sup></a></p>

  <p class="body"><a id="pgfId-1000644"></a>We can move to the classification step, described in listing 12.3. Method <code class="fm-code-in-text">classifyPoints</code><a id="marker-1008668"></a> is a brute-force search of all the point-centroid pairs, whose goal is finding the closest centroid to each pair. If you’ve read the book thus far, you should by now feel goose bumps when you hear “brute-force” and, as a conditional reflex, think, can we do any better? We’ll talk about this question a couple of sections down the road.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1030421"></a>Listing 12.3 <code class="fm-code-in-text">classifyPoints</code></p>
  <pre class="programlisting"><b class="calibre21">function</b> classifyPoints(points, centroids)         <span class="fm-combinumeral">❶</span>
  clusters ← []                                    <span class="fm-combinumeral">❷</span>
  <b class="calibre21">for</b> i <b class="calibre21">in</b> {0, .., |points|-1} <b class="calibre21">do</b>                  <span class="fm-combinumeral">❸</span>
    minDistance ← <b class="calibre21">inf</b>                              <span class="fm-combinumeral">❹</span>
    <b class="calibre21">for</b> j <b class="calibre21">in</b> {0, .., |centroids|-1} <b class="calibre21">do</b>             <span class="fm-combinumeral">❺</span>
      d ← <code class="fm-code-in-text2">distance(points[i], centroids[j])</code>   <span class="fm-combinumeral">❻</span>
      <b class="calibre21">if</b> d &lt; minDistance <b class="calibre21">then</b>                      <span class="fm-combinumeral">❼</span>
        minDistance ← d                            <span class="fm-combinumeral">❽</span>
        clusters[i] ← j
  <b class="calibre21">return</b> clusters                                  <span class="fm-combinumeral">❾</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1037561"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">classifyPoints</code><a id="marker-1037565"></a> takes the list of points in the dataset and the current list of centroids. It returns the list of the centroids (technically, their indices) associated with each point in the dataset.</p>

  <p class="fm-code-annotation"><a id="pgfId-1037586"></a><span class="fm-combinumeral">❷</span> Initializes the list of cluster assignments</p>

  <p class="fm-code-annotation"><a id="pgfId-1037603"></a><span class="fm-combinumeral">❸</span> Cycles through the list of points in the dataset (their indices)</p>

  <p class="fm-code-annotation"><a id="pgfId-1037620"></a><span class="fm-combinumeral">❹</span> Initializes the minimum distance (between <code class="fm-code-in-text2">points[i]</code> and any centroid) to the maximum possible value</p>

  <p class="fm-code-annotation"><a id="pgfId-1037637"></a><span class="fm-combinumeral">❺</span> Cycles through the list of centroids (again, their indices)</p>

  <p class="fm-code-annotation"><a id="pgfId-1037654"></a><span class="fm-combinumeral">❻</span> Computes the distance between the current point and the current centroid and stores it in a temporary variable</p>

  <p class="fm-code-annotation"><a id="pgfId-1037671"></a><span class="fm-combinumeral">❼</span> Checks if the distance computed is smaller than the minimum found so far</p>

  <p class="fm-code-annotation"><a id="pgfId-1037688"></a><span class="fm-combinumeral">❽</span> If it is, updates <code class="fm-code-in-text2">minDistance</code><a id="marker-1037692"></a> and assigns the <code class="fm-code-in-text2">i</code>-th point to the <code class="fm-code-in-text2">j</code>-th centroid</p>

  <p class="fm-code-annotation"><a id="pgfId-1037706"></a><span class="fm-combinumeral">❾</span> Returns the list of clusters</p>

  <p class="body"><a id="pgfId-1000933"></a>For the moment, let’s see the last helper method we need to implement to complete the k-means algorithm, the one updating centroids, described in listing 12.4.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1030457"></a>Listing 12.4 <code class="fm-code-in-text">updateCentroids</code></p>
  <pre class="programlisting"><b class="calibre21">function</b> updateCentroids(points, clusterIndices)                    <span class="fm-combinumeral">❶</span>
  centroids ← []                                                    <span class="fm-combinumeral">❷</span>
  <b class="calibre21">for</b> cIndex in uniqueValues(clusterIndices) <b class="calibre21">do</b>                     <span class="fm-combinumeral">❸</span>
    <b class="calibre21">for</b> j <b class="calibre21">in</b> {0, .., dim-1} <b class="calibre21">do</b>                                      <span class="fm-combinumeral">❹</span>
      centroids[cIndex][j] ← 
               mean({points[k][j] | clusterIndices[k] == cIndex})   <span class="fm-combinumeral">❺</span>
  <b class="calibre21">return</b> centroids                                                  <span class="fm-combinumeral">❻</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1037108"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">updateCentroids</code><a id="marker-1037112"></a> takes the list of points in the dataset and the classification of points with regard to current clusters (centroids). It returns the list of the centroids computed for each cluster.</p>

  <p class="fm-code-annotation"><a id="pgfId-1037133"></a><span class="fm-combinumeral">❷</span> Initializes the array of centroids</p>

  <p class="fm-code-annotation"><a id="pgfId-1037150"></a><span class="fm-combinumeral">❸</span> Goes through all (unique) centroids’ indices. Assuming the clusters’ indices go from <code class="fm-code-in-text2">0</code> to a certain value <code class="fm-code-in-text2">m</code>, without any “hole,” this could be expressed as the range between <code class="fm-code-in-text2">0</code> and <code class="fm-code-in-text2">max(clusterIndices)</code>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1037167"></a><span class="fm-combinumeral">❹</span> Cycles through all coordinates (assuming <code class="fm-code-in-text2">dim</code>, the number of coordinates, is, for instance, a class variable; otherwise, you could use <code class="fm-code-in-text2">|centroids[i]|</code>).</p>

  <p class="fm-code-annotation"><a id="pgfId-1037184"></a><span class="fm-combinumeral">❺</span> Each centroid’s coordinate is computed as the mean of the corresponding coordinate of all points assigned to it.</p>

  <p class="fm-code-annotation"><a id="pgfId-1037201"></a><span class="fm-combinumeral">❻</span> Returns the centroids</p>

  <p class="body"><a id="pgfId-1001147"></a>To update centroids, as we’ve mentioned, we will just compute the center of mass for each cluster. This means that we need to group all points by the centroid they are assigned to, and then for each group, compute the mean of the points’ coordinates.</p>

  <p class="body"><a id="pgfId-1001160"></a>This pseudo-code implementation doesn’t apply any correction to remove centroids that have no point assigned to them—they are just ignored. The issue with the centroids array is also not tackled; the array is simply initialized to the empty one, assuming it will be resized dynamically while adding new elements to it. But of course, in a real implementation, both issues should be taken care of, according to what each language allows for initialization and resizing of arrays.</p>

  <p class="body"><a id="pgfId-1001183"></a>If you’d like to take a look at real implementations of k-means, you can find a Python version on the book’s <span class="fm-hyperlink">repo</span> on GitHub. At <span class="fm-hyperlink"><a href="https://github.com/andreaferretti/kmeans">https://github.com/andreaferretti/ kmeans</a></span> you can also find a nice resource (unrelated to the book) that, like a Rosetta Stone, contains implementations of this algorithm in many programming languages (you can find a version for most of the mainstream languages).</p>

  <p class="body"><a id="pgfId-1001204"></a>Now that we have described how k-means is implemented, let’s take another look at how it works.</p>

  <h3 class="fm-head2" id="heading_id_7"><a id="pgfId-1001213"></a>12.2.1 Issues with k-means</h3>

  <p class="body"><a id="pgfId-1001229"></a>Figure 12.5 <a id="marker-1022654"></a><a id="marker-1022655"></a>shows the result of applying k-means to an artificial dataset. The dataset has been carefully crafted to represent the ideal situation where k-means excels. The clusters are easily (and linearly<a href="#pgfId-1009233"><sup class="footnotenumber">7</sup></a>) separable, and they all are approximately spherical.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F5.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1040370"></a>Figure 12.5 A typical clustering produced by k-means. Dataset’s points are plotted as circles, and centroids are shown as thick-edged Xs.</p>

  <p class="body"><a id="pgfId-1001272"></a>First, let’s start with a positive note about k-means. Figure 12.5 shows how it manages to correctly identify clusters with different density; for instance, the two clusters in the bottom-left corner have a larger average distance between points (and hence a lower density) than the cluster in the top-right corner. This might seem like something to take for granted, but not all clustering algorithms will deal so well with heterogeneous distributions. In the next section, we’ll see why this is a problem for DBSCAN.</p>

  <p class="body"><a id="pgfId-1001298"></a>That’s all for the good news: you can also see that there are a few points that are not close to any of the spherical clusters. We added some noise to the dataset as well to show one of k-means’ critical issues: it can’t detect outliers, and in fact, as shown in figure 12.5, outlier points are added to the closest clusters. Since centroids are computed as centers of the mass of the clusters, and the mean function is sensitive to outliers, if we don’t filter out outliers before running k-means, the undesirable consequence is that the centroids of the clusters will be “attracted” by outliers away from the best position they could hold. You can see this phenomenon in several clusters in figure 12.5.</p>

  <p class="body"><a id="pgfId-1001320"></a>The issue with outliers, however, is not the worst problem with k-means. As mentioned, this algorithm can only produce spherical clusters, but unfortunately, in real datasets not all clusters are spherical! Figure 12.6 shows three examples where k-means clustering fails to recognize the best clustering.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F6.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1040423"></a>Figure 12.6 Three examples where the clustering result produced by k-means can never be optimal. (Left) Two concentric rings (two centroids). (Right) A spiral (three centroids: the optimal solution here would be to have a single cluster). (Bottom) Two linear clusters close to each other.</p>

  <p class="body"><a id="pgfId-1001353"></a>Not-linearly separable clusters can’t be approximated with spherical clusters, and as such, k-means can’t separate non-convex clusters like clusters shaped as two concentric rings. Moreover, in all those situations where the clusters’ shape is not spherical and the points can’t be separated correctly using minimum distance from a centroid, k-means will struggle to find good solutions.</p>

  <p class="body"><a id="pgfId-1001367"></a>Another issue with k-means is that the number of clusters is a hyper-parameter, meaning that the algorithm is not able to determine the right number of clusters automatically,<a href="#pgfId-1009266"><sup class="footnotenumber">8</sup></a> and instead it takes the number of centroids as an argument. This means that unless we have some insight deriving from domain knowledge and suggesting to us the right number of categories into which we should cluster the dataset, then to find the right number of clusters for a dataset, we will need to run the algorithm multiple times, trying different values for the number of centroids, and comparing the results using some kind of metric (visual comparison is only possible for 2D and 3D datasets). We’ll talk about this in section 12.5.</p>

  <p class="body"><a id="pgfId-1001386"></a>These issues are certainly limiting, although there are domains in which we can assume or even prove that data can be modeled well with spherical clusters. But even in the best-case scenario or in intermediate situations where a spherical cluster is still a good approximation, there is another issue that can limit the application of this clustering algorithm: the curse of <a id="marker-1008692"></a><a id="marker-1008696"></a>dimensionality.</p>

  <h3 class="fm-head2" id="heading_id_8"><a id="pgfId-1001404"></a>12.2.2 The curse of dimensionality strikes again</h3>

  <p class="body"><a id="pgfId-1001424"></a>We <a id="marker-1008700"></a><a id="marker-1008704"></a>already bumped into the curse of dimensionality when we described k-d trees in chapter 9: a data structure that works well for low-to-medium-dimensional spaces but behaves poorly in high-dimensional spaces.</p>

  <p class="body"><a id="pgfId-1001436"></a>It’s not a coincidence that we find this issue again for k-means. This algorithm is a search heuristic that minimizes the Euclidean distance of points to cluster’s centroid. In high-dimensions, however</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1001455"></a>The ratio between volume and surface grows exponentially.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1001468"></a>Most points of a uniformly distributed dataset are not close to the center of mass, but rather far away toward the surface of the cluster.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1001481"></a>If data is uniformly distributed in a hypercube (a domain where each feature is uniformly distributed in a fixed range), then in high dimensions most points are close to the faces of the hypercube.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1001493"></a>Approximating a hypercube with its subscribed hypersphere<a class="calibre14" id="marker-1008708"></a> will leave out most of the points.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1001509"></a>To include all the points, we need the hypersphere superscribed to the hypercube, and as we saw in section 10.5.1, the amount of volume wasted grows exponentially with the number of dimensions.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1001526"></a>In a <code class="fm-code-in-text">d</code>-dimensional space, with <code class="fm-code-in-text">d &gt;&gt; 10</code>, under certain reasonable assumptions for data distribution, the nearest neighbor problem becomes ill-defined,<a class="calibre14" href="#pgfId-1009280"><sup class="footnotenumber">9</sup></a> because in high dimensions the ratio between the distance from a target to the nearest and farthest neighbor becomes almost 1. For instance, if points are equally spaced (for example, placed on a grid), then the <code class="fm-code-in-text">2<sup class="superscript1">d</sup></code> closest points to any centroid are all at the same distance from it. This means that the nearest neighbor of a point becomes ambiguously defined.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1001561"></a>In simple terms, for higher-dimensional datasets, unless the distribution of clusters is exactly spherical, the spheres needed to include all points are so large that they will likely overlap each other for a significant portion of their volume; moreover, for close-to-uniformly distributed datasets and when many centroids are used, the search for the nearest centroid can be inaccurate. This, in turn, leads to slower convergence, because some points can be assigned back and forth to different, almost equally close centroids.</p>

  <p class="body"><a id="pgfId-1001578"></a>In summary, we need to keep in mind that k-means is a good option only for low-to-medium-dimensional (with at most around 20 dimensions) datasets where we know that clusters can be accurately approximated with <a id="marker-1008712"></a><a id="marker-1008716"></a>hyperspheres.</p>

  <h3 class="fm-head2" id="heading_id_9"><a id="pgfId-1001590"></a>12.2.3 K-means performance analysis</h3>

  <p class="body"><a id="pgfId-1001606"></a>When <a id="marker-1008720"></a><a id="marker-1008724"></a>we do know that a dataset meets the pre-conditions to apply k-means, however, this algorithm is a viable option and produces good results quickly. How quickly? That’s what we are going to ascertain in this section.</p>

  <p class="body"><a id="pgfId-1001620"></a>Let’s assume we have a dataset with <code class="fm-code-in-text">n</code> points, each point belonging to a <code class="fm-code-in-text">d</code>-dimensional space (hence each point can be represented as a tuple of <code class="fm-code-in-text">d</code> real numbers), and that we would like to partition the points into <code class="fm-code-in-text">k</code> different clusters.</p>

  <p class="body"><a id="pgfId-1001637"></a>If we examine each sub-step in listing 12.1, we can conclude that</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1001646"></a>The random initialization step takes <code class="fm-code-in-text">O(n*d)</code> assignments (<code class="fm-code-in-text">d</code> coordinates for each of the <code class="fm-code-in-text">n</code> points).</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1001665"></a>Initializing cluster indices takes <code class="fm-code-in-text">O(n)</code> assignments.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1001680"></a>The main cycle is repeated <code class="fm-code-in-text">m</code> times, where <code class="fm-code-in-text">m</code> is the maximum number of iterations allowed.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1001697"></a>Assigning points to centroids takes <code class="fm-code-in-text">O(k*n*d)</code> operations, since for each point, we need to compute <code class="fm-code-in-text">k d</code>-dimensional (squared) distances, and each requires <code class="fm-code-in-text">O(d)</code> operations to be computed.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1001721"></a>Comparing two points classifications requires <code class="fm-code-in-text">O(n)</code> time, but this can be amortized inside the method doing the partitioning, with a careful implementation.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1001736"></a>Updating the centroids requires computing, for each centroid, <code class="fm-code-in-text">d</code> times (one for each coordinate<a class="calibre14" href="#pgfId-1009294"><sup class="footnotenumber">10</sup></a>) a mean over at most <code class="fm-code-in-text">n</code> points, and so a total of <code class="fm-code-in-text">O(k*n*d)</code> operations. If we can assume that the points are distributed evenly among clusters, each cluster will at most contain <code class="fm-code-in-text">n/k</code> points, and therefore the average worst-case running time becomes <code class="fm-code-in-text">O(k*(n/k)*d) = O(n*d)</code>.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1001771"></a>The running time for the algorithm is therefore <code class="fm-code-in-text">O(m*k*n*d)</code>, with <code class="fm-code-in-text">O(n+k)</code> extra memory needed to store the points classification and the list of <a id="marker-1008728"></a><a id="marker-1008732"></a>centroids.</p>

  <h3 class="fm-head2" id="heading_id_10"><a id="pgfId-1001791"></a>12.2.4 Boosting k-means with k-d trees</h3>

  <p class="body"><a id="pgfId-1001805"></a>When <a id="marker-1008736"></a><a id="marker-1008740"></a><a id="marker-1008744"></a><a id="marker-1008748"></a>we described the code for k-means, we saw that the partitioning step, where we assign each point to exactly one of the centroids, is a brute-force search among all combinations of points and centroids. Now we wonder, can we speed it up in any way?</p>

  <p class="body"><a id="pgfId-1001827"></a>In section 10.5.3 we saw how k-means can help SS<sup class="superscript">+</sup>-trees’ performance by making them more balanced when used in split heuristics. But is it possible that the opposite is also true? You might see where I’m going with this: Is it possible to replace brute-force search with something more efficient?</p>

  <p class="body"><a id="pgfId-1001841"></a>If you think about it, for each point we are looking for its nearest neighbor among the set of centroids, and we already know a data structure or two to speed up this search!</p>

  <p class="body"><a id="pgfId-1001852"></a>But rather than SS<sup class="superscript">+</sup>-trees, in this case the context would suggest we could try k-d trees for three reasons. Before reading them, try to pause for a minute and think about why we could prefer k-d trees. If you can’t think of all three reasons, or if you don’t have a completely clear idea about why these reasons hold, you can look at sections 9.4 and 10.1 that explain these concepts in more detail.</p>

  <p class="body"><a id="pgfId-1001872"></a>The reasons why k-d trees would be better suited than SS<sup class="superscript">+</sup>-trees for the nearest centroid search are</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1001884"></a>The size of the dataset (the centroids) is small: hence there is a very large probability that it will fit into memory. In this case, k-d trees are the best choice if the other two conditions also hold.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1001903"></a>The dimension of the search space goes from low to medium. We know this is the case (if we have done our homework!) because k-means also suffer from the curse of dimensionality, and shouldn’t be applied to high-dimensional datasets.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1001922"></a>K-d trees can offer a theoretical worst-case upper bound that is better than brute-force search: <code class="fm-code-in-text">O(2<sup class="superscript1">d</sup> + d*log(k))</code> for <code class="fm-code-in-text">k d</code>-dimensional centroids. SS<sup class="superscript">+</sup>-trees, however, can’t offer better-than-linear worst-case running time.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1001952"></a>Moreover, the data structure used will have to be recreated from scratch at each iteration of k-means’ main cycle, so we won’t be dealing with a dynamic dataset that would cause a k-d tree to become imbalanced over time.</p>

  <p class="body"><a id="pgfId-1001961"></a>The fact that we have to create a new dataset at each iteration, at the same time, is one of the biggest cons of using this approach, because we will have to pay this extra price. Also, we will need <code class="fm-code-in-text">O(k)</code> extra memory to store it. It won’t change the asymptotic memory print of the algorithm, but in practice it will be relevant, especially if <code class="fm-code-in-text">k</code>, the number of centroids, is high.</p>

  <p class="body"><a id="pgfId-1001984"></a>In most applications, though, it is reasonable to expect to have <code class="fm-code-in-text">k &lt;&lt; n</code>. In other words, the number of centroids will be several orders of magnitude smaller than the number of points.</p>

  <p class="body"><a id="pgfId-1001998"></a>Listing 12.5 shows how we can change the pseudo-code for method <code class="fm-code-in-text">classifyPoints</code><a id="marker-1008752"></a> to use a k-d tree in place of brute-force search.</p>

  <p class="body"><a id="pgfId-1002012"></a>As you can see, the code is much shorter than the version in listing 12.3, because most of the complexity of the search is now encapsulated in the <code class="fm-code-in-text">KdTree</code> class<a id="marker-1008756"></a>.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1018294"></a>Listing 12.5 <code class="fm-code-in-text">classifyPoints</code> using k-d trees</p>
  <pre class="programlisting"><b class="calibre21">function</b> classifyPoints(points, centroids)              <span class="fm-combinumeral">❶</span>
  clusters ← []                                         <span class="fm-combinumeral">❷</span>
  kdTree ← <b class="calibre21">new</b> KdTree(centroids)                        <span class="fm-combinumeral">❸</span>
  <b class="calibre21">for</b> i <b class="calibre21">in</b> {0, .., |points|-1} <b class="calibre21">do</b>                       <span class="fm-combinumeral">❹</span>
    clusters[i] ← kdTree.nearestNeighborIndex(point)    <span class="fm-combinumeral">❺</span>
  <b class="calibre21">return</b> clusters                                       <span class="fm-combinumeral">❻</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1036667"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">classifyPoints</code><a id="marker-1036671"></a> takes the list of points in the dataset and the current list of centroids. It returns the list of the centroids (technically, their indices) associated with each point in the dataset.</p>

  <p class="fm-code-annotation"><a id="pgfId-1036689"></a><span class="fm-combinumeral">❷</span> Initializes the list of cluster assignments to an empty list</p>

  <p class="fm-code-annotation"><a id="pgfId-1036706"></a><span class="fm-combinumeral">❸</span> Creates a KdTree<a id="marker-1036710"></a> instance, initializing it with the list of centroids</p>

  <p class="fm-code-annotation"><a id="pgfId-1036727"></a><span class="fm-combinumeral">❹</span> Cycles through the list of points in the dataset (their indices)</p>

  <p class="fm-code-annotation"><a id="pgfId-1036744"></a><span class="fm-combinumeral">❺</span> Gets the index of the nearest centroid for this point by querying the KdTree</p>

  <p class="fm-code-annotation"><a id="pgfId-1036761"></a><span class="fm-combinumeral">❻</span> Returns the list of clusters</p>

  <p class="body"><a id="pgfId-1002199"></a>Notice that since we would like to find out the index of the centroid closer to each point, we assume that the <code class="fm-code-in-text">KdTree</code> object can keep track of the indices of its points in the initialization array, and that we have a query method that returns the index of the closest point, rather than the point itself. It’s not hard to find a workaround if this is not the case. We can keep a hash table associating centroids to their indices and add an extra step to retrieve the index of the centroid returned by the <code class="fm-code-in-text">KdTree</code>.</p>

  <p class="body"><a id="pgfId-1002216"></a>Performance-wise, since creating a k-d tree for the centroids will require <code class="fm-code-in-text">O(k*log(k))</code> steps, each of which can require up to <code class="fm-code-in-text">O(d)</code> operations (because we are dealing with <code class="fm-code-in-text">d</code>-dimensional points), then the whole classification step will require <code class="fm-code-in-text">O(d*k*log(k) + n*2<sup class="superscript1">d</sup> + n*d*log(k))= O(n*2<sup class="superscript1">d</sup> + d*(n+k)*log(k))</code><a id="id_Hlk21191431"></a> steps, instead of <code class="fm-code-in-text">O(n*k*d)</code> operations.</p>

  <p class="body"><a id="pgfId-1002264"></a>Formally, we can work out the exact conditions for which <code class="fm-code-in-text">O(n*2<sup class="superscript1">d</sup> + d*(n+k)*log(k))</code> &lt; <code class="fm-code-in-text">O(n*k*d)</code>; for our purpose, however, we can just informally use some intuition to see that</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1002289"></a><code class="fm-code-in-text">n*2<sup class="superscript1">d</sup> &lt; n*k*d</code> <span class="cambria">⇔</span> <code class="fm-code-in-text">d &lt;&lt; k</code></p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1002309"></a><code class="fm-code-in-text">d*(n+k)*log(k) &lt; n*k*d</code> <span class="cambria">⇔</span> <code class="fm-code-in-text">(n+k)*log(k) &lt; n*k</code> <span class="cambria">⇔</span> <code class="fm-code-in-text">n &lt; n*k/log(k)–k</code> This can be shown to hold for <code class="fm-code-in-text">n &gt; k</code>, but plotting the difference between the two sides shows that for a fixed <code class="fm-code-in-text">n</code>, the difference grows as <code class="fm-code-in-text">k</code> grows, so the savings are more noticeable when there are many centroids.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1002362"></a>The whole algorithm, assuming that clusters are evenly distributed with approximately <code class="fm-code-in-text">n/k</code> points each, would then run in <code class="fm-code-in-text">O(m*(n*2<sup class="superscript1">d</sup> + d*(n+k)*log(k) + n*d)) = O(m*(n*2<sup class="superscript1">d</sup> + d*(n+k)*log(k)))</code> and so, in theory, and net of the constant multipliers and implementation details, it seems that it could be a good idea to use a data structure such as k-d trees to improve nearest neighbor search.</p>

  <p class="body"><a id="pgfId-1002405"></a>As we have seen, however, if the theoretical margin is small, sometimes a more complex implementation only beats asymptotically worse solutions for large, or at times very large, inputs. To double-check if, in practice, it is worth it to go through the trouble of creating and searching a k-d tree at each iteration, we ran some profiling that you can check out on the book’s <span class="fm-hyperlink">repo.<a href="#pgfId-1009311"><sup class="footnotenumber">11</sup></a></span></p>

  <p class="body"><a id="pgfId-1002429"></a>Once again, we used a Python implementation to do the comparison. By now it should go without saying, but these results are obviously only significant for this language and this particular implementation. Nevertheless, they do prove something.</p>

  <p class="body"><a id="pgfId-1002438"></a>For k-d trees, we used SciPy’s <span class="fm-hyperlink">implementation<a href="#pgfId-1009327"><sup class="footnotenumber">12</sup></a></span> provided in module <code class="fm-code-in-text">scipy.spatial</code><a id="marker-1008768"></a>. If you have read this book’s earlier chapters, you probably remember one of the golden rules we mentioned. Before implementing something yourself from scratch, look to see if there is something trustworthy already available. In this case, SciPy’s implementation is not only likely more reliable and efficient than the version we could write ourselves (since SciPy’s code has been already tested and tuned at length), but it also implements the <code class="fm-code-in-text">query</code> method<a id="marker-1008772"></a>, performing NN search, by returning the index (relative to the order of insertion) of the point returned. That’s exactly what we need for our k-means method, and it will save us from storing extra data to get from points to indices.</p>

  <p class="body"><a id="pgfId-1002465"></a>Figure 12.7 shows the result of profiling for the Python method <code class="fm-code-in-text">cluster_points</code><a id="marker-1008776"></a> implemented with brute-force search, and k-d tree’s nearest neighbor.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F7.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1040469"></a>Figure 12.7 Running time comparison between <code class="fm-code-in-text">classifyPoints</code><a id="marker-1040468"></a> implementations using brute-force search (darker line) and k-d tree’s nearest neighbor (lighter). The running times are shown as functions of <code class="fm-code-in-text">n</code>, the number of points, for fixed values of <code class="fm-code-in-text">k</code>, the number of centroids. (Top left) <code class="fm-code-in-text">k==5</code>, the method using k-d tree, is always slower, and its running time grows faster. (Top Right) <code class="fm-code-in-text">k == 50</code>, the method using a k-d tree, is still slower, but its growth is similar to the brute-force one. (Bottom Left) <code class="fm-code-in-text">k==75</code>, the method using brute force, is initially taking a similar amount of time, but as <code class="fm-code-in-text">n</code> grows, the curves for the two running times diverge and k-d tree implementation grows more slowly. (Bottom Right) <code class="fm-code-in-text">k==200</code>, the k-d tree implementation, takes half the time of the brute-force approach even for small values of <code class="fm-code-in-text">n</code>, and it grows much more slowly.</p>

  <p class="body"><a id="pgfId-1002515"></a>If you look closely at the four charts in figure 12.7, you can see how the line for the k-d tree implementation is more stable through the various values of <code class="fm-code-in-text">k</code>, while the slope of the upper line for the brute-force search algorithm becomes steeper as <code class="fm-code-in-text">k</code> grows.</p>

  <p class="body"><a id="pgfId-1002532"></a>This trend is even more apparent if we look at the data from a different angle, by keeping <code class="fm-code-in-text">n</code> fixed and plotting the running time as a function of <code class="fm-code-in-text">k</code>, as we do in fig-ure 12.8.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F8.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1040513"></a>Figure 12.8 Running time comparison between <code class="fm-code-in-text">classifyPoints</code><a id="marker-1040511"></a> (left) and <code class="fm-code-in-text">k_means</code><a id="marker-1040512"></a> (right) implementations using brute-force search and k-d tree’s nearest neighbor. The running times are shown as functions of <code class="fm-code-in-text">k</code>, the number of centroids, for fixed values of <code class="fm-code-in-text">n</code>, the size of the dataset. The charts are plotted for <code class="fm-code-in-text">n==1500</code>, but they show the same trend for all tested values of <code class="fm-code-in-text">n &gt; 1000</code>. When comparing the two plots, it’s also evident how the <code class="fm-code-in-text">classifyPoints</code> method accounts for most of the running time of this k-means implementation.</p>

  <p class="body"><a id="pgfId-1002582"></a>So, we can say that at least in Python, implementing the points partitioning sub-routine by using k-d trees provides an advantage over the naïve brute-force <a id="marker-1008792"></a><a id="marker-1008796"></a><a id="marker-1008800"></a><a id="marker-1008804"></a>search.</p>

  <h3 class="fm-head2" id="heading_id_11"><a id="pgfId-1002596"></a>12.2.5 Final remarks on k-means</h3>

  <p class="body"><a id="pgfId-1002610"></a>To conclude this section on k-means, let’s summarize our conclusions:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1002619"></a>K-means is a centroid-based hard-clustering method.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1002632"></a>If implemented with an auxiliary k-d tree, the running time for the algorithm is <code class="fm-code-in-text">O(m*(n*2<sup class="superscript1">d</sup> + d*(n+k)*log(k)))</code>.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1002658"></a>K-means works well on low-to-medium-dimensional data, when cluster shapes are spherical and the number of clusters can be estimated <i class="calibre15">a priori</i>, but works well even if the dataset doesn’t have a homogeneous distribution.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1002674"></a>K-means works poorly on high-dimensional data, and when clusters can’t be approximated with hyperspheres.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1002686"></a>Now that we have described the progenitor of all clustering algorithms, we need to fast-forward 40 years to the invention of the next algorithm we are going to <a id="marker-1008808"></a><a id="marker-1008812"></a>present.</p>

  <h2 class="fm-head" id="heading_id_12"><a id="pgfId-1002700"></a>12.3 DBSCAN</h2>

  <p class="body"><a id="pgfId-1002712"></a>The <a id="marker-1008816"></a><a id="marker-1008820"></a>paper introducing <i class="calibre17">DBSCAN</i> was published in 1996, presenting a novel approach to address the problem.<a href="#pgfId-1009343"><sup class="footnotenumber">13</sup></a> DBSCAN is an acronym for “<a id="id_Hlk56496929"></a>Density-based spatial clustering of applications with noise,” and the main difference in the approach with respect to k-means is already clear from its name. While k-means is a centroid-based algorithm, and as such builds clusters as convex sets around points elected as centroids, a density-based algorithm defines clusters as sets of points that are close to each other, close enough that the density of points in any area of a cluster is above a certain threshold. The natural extension of this definition, by the way, introduces the concept of noise (also referred to as <i class="calibre17">outliers</i><a id="marker-1008824"></a>) for those points that are in low-density regions. We will formally define both categories in a few lines, but first, we still have a few high-level considerations on the algorithm.</p>

  <p class="body"><a id="pgfId-1002742"></a>Like k-means, DBSCAN is a flat hard-clustering algorithm, meaning that each point is assigned to (at most) one cluster (or no cluster, for outliers) with 100% confidence, and that all clusters are objects at the same level, no hierarchy of these groups is kept.</p>

  <p class="body"><a id="pgfId-1002753"></a>In k-means, random initialization of the centroids has a major role in the algorithm (with good choices speeding up convergence), so much so that often several random restarts of the algorithm are compared before choosing the best clustering. This isn’t true for DBSCAN, where points are cycled through somewhat randomly. But this has a lower influence, if any, on the final result; therefore, this algorithm can be considered deterministic.<a href="#pgfId-1009361"><sup class="footnotenumber">14</sup></a></p>

  <p class="body"><a id="pgfId-1002778"></a>DBSCAN, finally, extends the concept of <i class="calibre17">single-linkage clustering</i><a id="id_Hlk56497098"></a><a href="#pgfId-1009375"><sup class="footnotenumber">15</sup></a><a id="marker-1008828"></a> (<i class="calibre17">SLC</i>) by introducing a minimum points-density required to consider two points connected to each other. This reduces the <i class="calibre17">single-link chain effect</i><a id="marker-1008832"></a>, the worst side effect of SLC, causing independent clusters connected by a thin line of (noise) points to be mistakenly classified as a single cluster.</p>

  <h3 class="fm-head2" id="heading_id_13"><a id="pgfId-1002803"></a>12.3.1 Directly vs density-reachable</h3>

  <p class="body"><a id="pgfId-1002819"></a>To <a id="marker-1008836"></a><a id="marker-1008840"></a>understand how DBSCAN works, we need to start with a few definitions. Please use figure 12.9 as a reference while going through them to help check your understanding:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1002835"></a>Point <code class="fm-code-in-text">p,</code> in figure 12.9, is said to be a core point because there are at least <code class="fm-code-in-text">minPoints</code> points (including <code class="fm-code-in-text">p</code> itself) within distance <span class="cambria">ϵ</span> from it (where, in the example, <code class="fm-code-in-text">minPoints==3</code>).</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1002861"></a>Point <code class="fm-code-in-text">q</code> is <i class="calibre15">directly reachable</i><a class="calibre14" id="marker-1008844"></a> from <code class="fm-code-in-text">p</code> because point <code class="fm-code-in-text">q</code> is within distance <span class="cambria">ϵ</span> from <code class="fm-code-in-text">p</code>, which is a core point. A point can only be directly reachable from a core point.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1002886"></a>A point <code class="fm-code-in-text">w</code> is <i class="calibre15">reachable</i> (or, equivalently, <i class="calibre15">density-reachable</i>) from a core point (such as <code class="fm-code-in-text">p</code>) through a path of core points <code class="fm-code-in-text">p=w<sub class="subscript1">1</sub>, ..., w<sub class="subscript1">n</sub>=w</code>, if each <code class="fm-code-in-text">w<sub class="subscript1">i</sub>+1</code> is directly reachable from <code class="fm-code-in-text">w<sub class="subscript1">i</sub></code>. From the previous definition of direct-reachability, it follows that all points in the path, except <code class="fm-code-in-text">w</code>, need to be core points.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1002925"></a>Any two points that are density-reachable from each other are, by definition, in the same cluster.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1002937"></a>If there is any point <code class="fm-code-in-text">r</code> that is not reachable from any other point in the dataset, then <code class="fm-code-in-text">r</code> (and all the points like <code class="fm-code-in-text">r</code>) is marked as an outlier (or, equivalently, as noise).</p>
    </li>
  </ul>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F9.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1040562"></a>Figure 12.9 Core points, directly reachable points, and reachable points, given a radius <span class="cambria">ϵ</span> and a threshold <code class="fm-code-in-text">minPoints</code> (the minimum number of points in a core region) equal to 3; hence, core points need to have at least two neighbors within distance <span class="cambria">ϵ</span>.</p>

  <p class="body"><a id="pgfId-1002980"></a>The algorithm is built around the concept of core points<a id="marker-1008848"></a>: each of them has at least a certain number of neighbors within a specific distance. This can be seen from a different angle as well: core points are points in areas with at least a minimum density.</p>

  <p class="body"><a id="pgfId-1002996"></a>Core points (such as <code class="fm-code-in-text">p</code>, <code class="fm-code-in-text">q,</code> and so on in figure 12.9) that are reachable (meaning, adjacent) to each other belong to the same cluster. Why? Because we conjecture that high-density areas (as opposed to the low-density majority of the domain) define clusters. But all points that are within a distance <span class="cambria">ϵ</span> from a core point <code class="fm-code-in-text">p</code> belong to the same cluster as <code class="fm-code-in-text">p</code>’s <a id="marker-1008852"></a><a id="marker-1008856"></a>too.</p>

  <h3 class="fm-head2" id="heading_id_14"><a id="pgfId-1003022"></a>12.3.2 From definitions to an algorithm</h3>

  <p class="body"><a id="pgfId-1003038"></a>Moving <a id="marker-1008860"></a><a id="marker-1008864"></a>from the definitions in the previous section to an algorithm is surprisingly simple.</p>

  <p class="body"><a id="pgfId-1003050"></a>For a given point <code class="fm-code-in-text">p</code>, we need to check how many of its neighbors lie within a radius <span class="cambria">ϵ</span>. If there are more than a certain number <code class="fm-code-in-text">m</code>, then we mark <code class="fm-code-in-text">p</code> as a core point and add its neighbors to the same cluster; otherwise, never mind; we do nothing. Figure 12.10 illustrates this step that is going to be repeated for every point in the dataset, with the help of a set to keep track of the points that we want to process next (in any order) for the current cluster.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F10.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1003101"></a>Figure 12.10 A few steps of the main loop of DBSCAN. When core points are processed (the first three steps in the figure) all their undiscovered neighbors are added to current cluster and to a set (not a queue) of points to be processed. Conversely, when the current point being processed is not a core point, like <code class="fm-code-in-text">v</code> in the last step, then no further action is taken.</p>

  <p class="body"><a id="pgfId-1040668"></a>Now we have to ask ourselves what happens when we process a point <code class="fm-code-in-text">w</code> that is not a core point, but is directly reachable from a core point <code class="fm-code-in-text">p</code>. Is it okay if we don’t take any action while processing <code class="fm-code-in-text">w</code>?</p>

  <p class="body"><a id="pgfId-1003116"></a>As you can see from figures 12.9 and 12.10, if <code class="fm-code-in-text">p</code> is a core point and <code class="fm-code-in-text">w</code> is directly reachable from it, then the distance between <code class="fm-code-in-text">w</code> and <code class="fm-code-in-text">p</code> must be at most <span class="cambria">ϵ</span>; therefore, when we check <code class="fm-code-in-text">p</code>, we will add all of <code class="fm-code-in-text">p</code>’s neighbors within radius <span class="cambria">ϵ</span> to the same cluster as <code class="fm-code-in-text">p</code>, and hence <code class="fm-code-in-text">w</code> will end up in the same cluster as <code class="fm-code-in-text">p</code>.</p>

  <p class="body"><a id="pgfId-1003153"></a>What if there are two core points <code class="fm-code-in-text">p</code> and <code class="fm-code-in-text">q</code> that are density-reachable from each other, and are both reachable from <code class="fm-code-in-text">w</code>? Well, by definition, there will be a chain of core points <code class="fm-code-in-text">w<sub class="subscript1">1</sub>, ..., w<sub class="subscript1">n</sub></code> between <code class="fm-code-in-text">q</code> and <code class="fm-code-in-text">p</code>, so in turn each core point in the path will be added to the same cluster as <code class="fm-code-in-text">q</code>, and finally so will <code class="fm-code-in-text">p</code> as well when it’s <code class="fm-code-in-text">w<sub class="subscript1">n</sub></code>’s turn.</p>

  <p class="body"><a id="pgfId-1003190"></a>What if <code class="fm-code-in-text">p</code> and <code class="fm-code-in-text">q</code> are core points that are not reachable from each other, but are both reachable from a point <code class="fm-code-in-text">w</code>? Can <code class="fm-code-in-text">w</code> be a core point?</p>

  <p class="body"><a id="pgfId-1003207"></a>Let’s reason <i class="calibre17">ad absurdum</i>:<i class="calibre17"><a href="#pgfId-1009389"><sup class="footnotenumber">16</sup></a></i> suppose that <code class="fm-code-in-text">w</code> is reachable from <code class="fm-code-in-text">p</code>, a core point, that it is processed before <code class="fm-code-in-text">w</code>. Hence, there is a chain of core points <code class="fm-code-in-text">p<sub class="subscript1">1</sub>, ..., p<sub class="subscript1">n</sub></code>, each reachable from the previous one, that connects <code class="fm-code-in-text">p</code> to <code class="fm-code-in-text">w</code>.</p>

  <p class="body"><a id="pgfId-1003243"></a>Suppose also that <code class="fm-code-in-text">w</code>, by the time <code class="fm-code-in-text">p<sub class="subscript1">n</sub></code> is processed, has already been added to another cluster, different from <code class="fm-code-in-text">p</code>’s. This means that there is a core point <code class="fm-code-in-text">q</code>, for which <code class="fm-code-in-text">w</code> is reachable from <code class="fm-code-in-text">q</code> (and hence there is a chain of core points <code class="fm-code-in-text">q<sub class="subscript1">1</sub>, ..., q<sub class="subscript1">k</sub>,</code> and so on), but <code class="fm-code-in-text">p</code> is not reachable from <code class="fm-code-in-text">q</code> because they are in different clusters.</p>

  <p class="body"><a id="pgfId-1003287"></a>Now, <code class="fm-code-in-text">w</code> can be a core point, or not a core point.</p>

  <p class="body"><a id="pgfId-1003298"></a>If <code class="fm-code-in-text">w</code> was a core point, then there would be a chain made of the core points <code class="fm-code-in-text">q, q<sub class="subscript1">1</sub>, ..., q<sub class="subscript1">k</sub>, w, p<sub class="subscript1">1</sub>,... , p<sub class="subscript1">n</sub>, p</code>, where all points are reachable from each other; therefore, by definition <code class="fm-code-in-text">p</code> would be reachable from <code class="fm-code-in-text">q</code>, and this goes against our initial hypothesis.</p>

  <p class="body"><a id="pgfId-1003339"></a>It follows that <code class="fm-code-in-text">w</code> can’t be a core point. It must be a non-core point reachable from at least two different core points, a situation illustrated in figure 12.11.</p>

  <p class="body"><a id="pgfId-1003354"></a>In these cases, reachable points can be added to either cluster, but the difference will be just about a single point. It also means that the two clusters are separated by an area with lower-than-threshold density (although not completely empty).</p>

  <p class="body"><a id="pgfId-1003367"></a>The final result won’t be influenced by the order in which points are processed, because sooner or later we will discover that all density-reachable points belong to the same cluster. Nevertheless, there is an efficient way and a bad way to process the points. Depending on the order we follow, we might be forced to use different methods to keep track of the clusters.</p>

  <p class="body"><a id="pgfId-1003380"></a>The bad way is this: If we processed points in a completely random order, then we would need to keep track of the cluster assigned to each point (initially each point being in its own cluster), but we would also need to keep track of which clusters need to be merged (every time we process a core point, we’ll have to try<a href="#pgfId-1009409"><sup class="footnotenumber">17</sup></a> to merge at least <code class="fm-code-in-text">minPoints-1</code> clusters); this becomes complicated to handle and requires an ad hoc data structure, the disjoint set we described in chapter 5.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F11.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1040818"></a>Figure 12.11 An edge point <code class="fm-code-in-text">w</code> is directly reachable from at least two different clusters. In this example, <code class="fm-code-in-text">minPoints</code> is set to <code class="fm-code-in-text">4</code>; the path from <code class="fm-code-in-text">q</code> to <code class="fm-code-in-text">p</code> is highlighted with thicker arrows.</p>

  <p class="body"><a id="pgfId-1003427"></a>If, instead, we process the neighbors of each core point <code class="fm-code-in-text">p</code> right after finishing with <code class="fm-code-in-text">p</code>, as shown in figure 12.10, then we can just build clusters in a sequence, growing each cluster point by point until no further point can be added to it, without any need of merging clusters or keeping track of the history of merges.</p>

  <p class="body"><a id="pgfId-1003442"></a>By following this order, points like <code class="fm-code-in-text">q</code> and <code class="fm-code-in-text">p</code> in figure 12.11 will never be added to the same cluster, and it doesn’t really matter to which of them a (so-called) edge point like <code class="fm-code-in-text">w</code> is merged. As a matter of fact, edge points are the only points for which DBSCAN is not entirely deterministic, because they can be added to any of the clusters from which they are reachable, and the one cluster to which they are eventually added depends on the order used to process the dataset’s points.</p>

  <p class="body"><a id="pgfId-1003466"></a>There is one last question we need to ask: How many times do we need to iterate DBSCAN’s main loop? That will be exactly once per point. This is completely different from k-means, where we had many iterations of a few steps on the whole dataset. While k-means is a search heuristic adjusting some parameters to move to a local minimum,<a href="#pgfId-1009424"><sup class="footnotenumber">18</sup></a> DBSCAN is a one-pass deterministic algorithm computing the best partitioning (and at the same time identifying outliers) based on the points’ density in different <a id="marker-1040802"></a><a id="marker-1040803"></a>areas.</p>

  <h3 class="fm-head2" id="heading_id_15"><a id="pgfId-1003487"></a>12.3.3 And finally, an implementation</h3>

  <p class="body"><a id="pgfId-1003503"></a>Once <a id="marker-1028444"></a><a id="marker-1028445"></a>we have outlined how the algorithm works at a high level, we are ready to write an implementation of DBSCAN, shown in listing 12.6. A Python implementation is also available on the book’s <span class="fm-hyperlink">repo</span> on GitHub.<a href="#pgfId-1009437"><sup class="footnotenumber">19</sup></a></p>

  <p class="fm-code-listing-caption"><a id="pgfId-1018742"></a>Listing 12.6 DBSCAN clustering</p>
  <pre class="programlisting"><b class="calibre21">function</b> dbscan(points, eps, minPoints)                                 <span class="fm-combinumeral">❶</span>
  currentIndex ← 0                                                      <span class="fm-combinumeral">❷</span>
  clusterIndices ← 0 (<span class="cambria">∀</span> p <span class="cambria">ϵ</span> points)                                    <span class="fm-combinumeral">❸</span>
  kd ← <b class="calibre21">new</b> KdTree(points)                                               <span class="fm-combinumeral">❹</span>
  <b class="calibre21">for</b> p in points <b class="calibre21">do</b>                                                    <span class="fm-combinumeral">❺</span>
    <b class="calibre21">if</b> clusterIndices[p] != 0 <b class="calibre21">then</b>                                      <span class="fm-combinumeral">❻</span>
      continue
    toProcess ← {p}                                                     <span class="fm-combinumeral">❼</span>
    clusterIndices[p] ← -1                                              <span class="fm-combinumeral">❽</span>
    currentIndex ← currentIndex + 1                                     <span class="fm-combinumeral">❾</span>
    <b class="calibre21">for</b> q <b class="calibre21">in</b> toProcess <b class="calibre21">do</b>                                               <span class="fm-combinumeral">❿</span>
      neighbors ← kd.pointsInSphere(q, eps)                             <span class="fm-combinumeral">⓫</span>
      <b class="calibre21">if</b> |neighbors| &lt; minPoints <b class="calibre21">then</b>                                   <span class="fm-combinumeral">⓬</span>
        continue
      clusterIndices[q] ← currentIndex                                  <span class="fm-combinumeral">⓭</span>
      toProcess ← toProcess + {w in neighbors | clusterIndices[w] <span class="cambria">≤</span> 0} <span class="fm-combinumeral">⓮</span>
  <b class="calibre21">return</b> clusterIndices                                                 <span class="fm-combinumeral">⓯</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1035219"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">dbscan</code><a id="marker-1035223"></a> takes a list of points, the radius of the dense area defining core points, and the minimum number of points needed in the dense area for a point to be a core point. This function returns an array of the cluster indices associated with the points (or, equivalently, a dictionary associating points to cluster indices).</p>

  <p class="fm-code-annotation"><a id="pgfId-1035244"></a><span class="fm-combinumeral">❷</span> Initializes the current cluster’s index to <code class="fm-code-in-text2">0</code>. We will use the special value <code class="fm-code-in-text2">0</code> to mark a point as unprocessed, while the other special value <code class="fm-code-in-text2">-1</code> will be used to flag outliers. Valid cluster indices start from <code class="fm-code-in-text2">1</code>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1035261"></a><span class="fm-combinumeral">❸</span> Initializes a list with the cluster index for each point. At first, each point is marked as unprocessed.</p>

  <p class="fm-code-annotation"><a id="pgfId-1035278"></a><span class="fm-combinumeral">❹</span> Creates a k-d tree to speed range queries. It initializes it with the full dataset.</p>

  <p class="fm-code-annotation"><a id="pgfId-1035295"></a><span class="fm-combinumeral">❺</span> Cycles through each point in the dataset</p>

  <p class="fm-code-annotation"><a id="pgfId-1035312"></a><span class="fm-combinumeral">❻</span> If <code class="fm-code-in-text2">p</code>’s cluster index is not <code class="fm-code-in-text2">0</code> anymore, it means that the point has already been processed, so it can be skipped here.</p>

  <p class="fm-code-annotation"><a id="pgfId-1035329"></a><span class="fm-combinumeral">❼</span> Initializes the set of points that will have to be processed while constructing the current cluster. Initially, <code class="fm-code-in-text2">p</code> is the only point in the dataset.</p>

  <p class="fm-code-annotation"><a id="pgfId-1035346"></a><span class="fm-combinumeral">❽</span> Marks <code class="fm-code-in-text2">p</code> as processed by initially flagging it as an outlier</p>

  <p class="fm-code-annotation"><a id="pgfId-1035363"></a><span class="fm-combinumeral">❾</span> We are creating a new cluster, so we can increment the index of the current cluster. This implementation doesn’t worry about having all the cluster’s indices as consecutive integers: in other words, whenever we find out that <code class="fm-code-in-text2">p</code> is an outlier, we skip the current index. This can be easily fixed, for instance, by using a Boolean flag.</p>

  <p class="fm-code-annotation"><a id="pgfId-1035380"></a><span class="fm-combinumeral">❿</span> Cycles through each point <code class="fm-code-in-text2">q</code> in the list of points to process</p>

  <p class="fm-code-annotation"><a id="pgfId-1035397"></a><span class="fm-combinumeral">⓫</span> Performs the range query collecting all points in the hyper-sphere with center <code class="fm-code-in-text2">q</code>. In this implementation, we assume the function does include the point <code class="fm-code-in-text2">q</code> itself (as per the implementation in listing 9.11).</p>

  <p class="fm-code-annotation"><a id="pgfId-1035414"></a><span class="fm-combinumeral">⓬</span> If the number of points in <code class="fm-code-in-text2">q</code>’s neighborhood (including <code class="fm-code-in-text2">q</code> itself) is less than <code class="fm-code-in-text2">minPoints</code>, then we don’t need to do anything (<code class="fm-code-in-text2">q</code> at this point is still marked as noise).</p>

  <p class="fm-code-annotation"><a id="pgfId-1035431"></a><span class="fm-combinumeral">⓭</span> Otherwise, adds <code class="fm-code-in-text2">q</code> to current cluster by setting its cluster index</p>

  <p class="fm-code-annotation"><a id="pgfId-1035448"></a><span class="fm-combinumeral">⓮</span> Updates the list of points to process by adding all of <code class="fm-code-in-text2">q</code>’s neighbors that haven’t yet been processed</p>

  <p class="fm-code-annotation"><a id="pgfId-1035465"></a><span class="fm-combinumeral">⓯</span> Returns the classification of points in the dataset (by means of their cluster indices)</p>

  <p class="body"><a id="pgfId-1004000"></a>If you recall, we mentioned in chapter 11 that data structures such as k-d trees and SS-trees are often used in clustering. For k-means and DBSCAN, a multidimensional indexing structure is used to speed up range queries. You might have figured this out from the definition of <i class="calibre17">core points</i><a id="marker-1008888"></a>, given that to decide whether a point <code class="fm-code-in-text">p</code> is a core point, we need to check how many dataset points there are in its neighborhood within a certain radius from <code class="fm-code-in-text">p</code>.</p>

  <p class="body"><a id="pgfId-1004034"></a>And while for k-means we need to perform nearest neighbors queries, for DBSCAN we will instead run range queries, looking for all the points within a hypersphere.</p>

  <p class="body"><a id="pgfId-1004045"></a>You can also find a Python implementation on the book’s repo,<code class="fm-code-in-text"><a class="calibre14" href="#pgfId-1009453"><sup class="footnotenumber4">20</sup></a></code> as well as a Jupyter Notebook<code class="fm-code-in-text"><a class="calibre14" href="#pgfId-1009467"><sup class="footnotenumber4">21</sup></a></code> to experiment with the algorithm.</p>

  <p class="body"><a id="pgfId-1004071"></a>Obviously, for DBSCAN as well as for k-means, it is possible to use brute-force linear search to find each point’s neighborhood. For k-means, however, the speedup is nice but not vital (usually k-means implementations don’t bother about this), for DBSCAN the performance gain would be dramatic. Can you guess why? Before going further and reading the explanation, try to think about it for a minute.</p>

  <p class="body"><a id="pgfId-1004088"></a>The difference with DBSCAN is that the search extends on the whole dataset, while for k-means we only look for the closest among <code class="fm-code-in-text">k</code> centroids (and usually <code class="fm-code-in-text">k &lt;&lt; n</code>).</p>

  <p class="body"><a id="pgfId-1004101"></a>If we have <code class="fm-code-in-text">n</code> points in the input dataset, then the difference, given the running time of the whole algorithm, is between <code class="fm-code-in-text">O(n2)</code> and <code class="fm-code-in-text">O(n*log(n)),</code> assuming that the range queries take each <code class="fm-code-in-text">O(log(n))</code>.<code class="fm-code-in-text"><a class="calibre14" href="#pgfId-1009483"><sup class="footnotenumber4">22</sup></a></code> Just as a reminder, with one million points in our dataset, this means going down from ~<code class="fm-code-in-text">10<sup class="superscript1">12</sup></code> (a trillion) operations to ~<code class="fm-code-in-text">6*10<sup class="superscript1">6</sup></code> <a id="marker-1008892"></a><a id="marker-1008896"></a>(six million).</p>

  <h3 class="fm-head2" id="heading_id_16"><a id="pgfId-1004138"></a>12.3.4 Pros and cons of DBSCAN</h3>

  <p class="body"><a id="pgfId-1004156"></a>We <a id="marker-1008900"></a><a id="marker-1008904"></a>have already mentioned a few characteristics peculiar to DBSCAN that help us overcome some limits of other clustering algorithms like k-means or single-linkage clustering. Let’s quickly review them here:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1004172"></a>DBSCAN is able to determine the number of clusters (given the hyper-parameters with which it is called), while k-means needs this number to be provided as a parameter.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1004185"></a>It only takes two parameters, that can also be derived by the domain (possibly through a preliminary scan to collect statistics on the dataset).</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1004198"></a>DBSCAN can handle noise in the datasets by identifying outliers.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1004211"></a>It can find arbitrarily shaped clusters and can partition non-linearly separable clusters (see figure 12.12 and compare it to figure 12.6 for k-means).</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1004224"></a>By tuning the <code class="fm-code-in-text">minPoints</code> parameter, it is possible to reduce the single-link effect.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1004241"></a>The algorithm is almost entirely deterministic and the order in which points are processed is mostly irrelevant. A different order can only change the assignment of points on the edge of clusters when they are equally close to more than one cluster (as we have seen in figure 12.11).</p>
    </li>
  </ul>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F12.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1040866"></a>Figure 12.12 The three example datasets in figure 12.6, where k-means was failing: processing them with DBSCAN, and with the right choice of parameters, we can obtain a proper clustering for each of them.</p>

  <p class="body"><a id="pgfId-1004278"></a>So much for the good news. As you can imagine, every rose has its thorns, and DBSCAN has some shortcomings as well:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1004289"></a>DBSCAN is <i class="calibre15">almost</i> entirely deterministic, but not completely. For some applications, it might be a problem if points at the border of two or more clusters are aleatorily assigned to one or another.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1004307"></a>DBSCAN also suffers from the curse of dimensionality. If the metric used is the Euclidean distance, then as we saw in section 12.2.2, in high-dimensional spaces all neighbors of a point are at the same distance, and the distance function becomes basically useless. Luckily, other metrics can also be used with DBSCAN.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1004328"></a>If a dataset has areas with different densities, it becomes challenging, or sometimes impossible, to choose parameters <code class="fm-code-in-text"><span class="cambria">ε</span></code> and <code class="fm-code-in-text">minPoints</code> such that all clusters will be partitioned correctly. Figure 12.13 shows an example illustrating how the result produced by DBSCAN is sensitive to the choice of parameters, and figure 12.14 shows another example with areas of different density that make it impossible to choose a value for epsilon that will cluster both areas properly.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1004350"></a>Related to this aspect, one of the problems with successfully running this algorithm is that it can be challenging to find the best values for its parameters when there is no previous knowledge of the dataset.</p>
    </li>
  </ul>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F13.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1040911"></a>Figure 12.13 Using the first example dataset in figure 12.12, we can see how sensitive to the choice of parameters (in particular of <span class="cambria">ϵ</span>) the result of DBSCAN is. All the examples ran DBSCAN on the same dataset with parameter <code class="fm-code-in-text">minPoints</code> set to 3 (the dimension of the domain plus 1). (Left) Using a value too small for <span class="cambria">ϵ</span> causes the dense area to be too small, so data is partitioned into too many small clusters. A few points, plotted with X-shaped markers, are even marked as outliers. (Center) When the right value for the domain is chosen, the result is that DBSCAN partitions data perfectly into two concentric rings. (Right) When the radius <span class="cambria">ϵ</span> is set to a value so big that the dense area of inner points extends to points of the outer ring, then the whole dataset is mistakenly assigned to the same cluster.</p>

  <p class="body"><a id="pgfId-1004401"></a>Hyperparameter<a href="#pgfId-1009506"><sup class="footnotenumber">23</sup></a> tuning, as it often happens in machine learning, is crucial and not always easy. Setting <code class="fm-code-in-text">minPoints</code> is usually straightforward. As a rule of thumb, for <code class="fm-code-in-text">d</code>-dimensional datasets, you choose <code class="fm-code-in-text">minPoints &gt; d</code>, and values around <code class="fm-code-in-text">2*d</code> are often ideal. For particularly noisy datasets it is also recommended to use larger values of this parameter to strengthen the noise filtering. Conversely, determining the right value for <span class="cambria">ϵ</span> is often challenging and requires either deep domain knowledge, or extensive tuning. Figure 12.13 shows, by keeping fixed <code class="fm-code-in-text">minPoints</code>, how values of <span class="cambria">ϵ</span> that are too small (for a given dataset) cause an excessive fragmentation of the dataset into small clusters, while values that are too large have the opposite effect, reducing the ability of the algorithm to spot different clusters.</p>

  <p class="body"><a id="pgfId-1004461"></a>Clearly, the need for such a tuning would bring us back to a situation similar to using k-means, where we needed to know in advance how many clusters we wanted. And while in this two-dimensional example it might seem easy to find what the “right” number of clusters should be, when we move to higher-dimensional spaces, we lose the possibility of using our intuition, and determining the right values of the algorithm hyperparameters based on the number of clusters becomes impossible.</p>

  <p class="body"><a id="pgfId-1004476"></a>In the next couple of sections, we will examine two different ways to cope with hyperparameters and address <a id="marker-1008908"></a><a id="marker-1008912"></a>their <a id="marker-1008916"></a><a id="marker-1008920"></a>issues.</p>

  <h2 class="fm-head" id="heading_id_17"><a id="pgfId-1004491"></a>12.4 OPTICS</h2>

  <p class="body"><a id="pgfId-1004503"></a>As <a id="marker-1008924"></a><a id="marker-1008928"></a>we saw in the last section, DBSCAN is a powerful algorithm that is able to identify non-linearly separable clusters of any shape; however, it has a weak spot connected to the parameters regulating the density thresholds. In particular, it is hard to find the best value for epsilon, the radius of the core-region determining what points are reachable from each other. When a dataset has areas with different densities, then it becomes even more impossible to find a value that works equally well for the whole dataset (as shown in figure 12.14).</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F14.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1040968"></a>Figure 12.14 An example of a dataset with areas of heterogeneous density for which it’s not possible to find a single value of <span class="cambria">ϵ</span> that fits all areas. The dataset has a low-density cluster in the left area and two high-density clusters, close to each other, in the top-right corner. (Left) With a low value of <span class="cambria">ϵ</span>, the low-density cluster is broken into many small clusters surrounded by noise. (Right) With higher values, the two clusters on the right are merged. There is no value for which these clusters are correctly separated and at the same time the one on the left is recognized as a single cluster.</p>

  <p class="body"><a id="pgfId-1004527"></a>It doesn’t matter if we try to search for this value manually or semi-automatically (see section 12.5). If we stick with the DBSCAN algorithm alone, the algorithm is not enough to handle non-uniform datasets.</p>

  <p class="body"><a id="pgfId-1004540"></a>It didn’t take too long until computer scientists came up with a new idea that could help in these situations: the key step missing was “<a id="id_Hlk56498425"></a>Ordering Points To Identify the Clustering Structure,” which the authors turned into an acronym, <i class="calibre17">OPTICS</i>,<i class="calibre17"><a href="#pgfId-1009518"><sup class="footnotenumber">24</sup></a></i> to name the algorithm they had invented.</p>

  <p class="body"><a id="pgfId-1004565"></a>When we were discussing DBSCAN, we talked about the order in which points are processed. For DBSCAN, the only thing that matters is that points in the same cluster (that is, core points reachable from each other) are processed together; however, as we have seen, this is more a matter of optimization so that we don’t have to keep a disjoint set and merge clusters when we find a pair of directly reachable points, because the order of processing may only influence the assignment of non-core points on the edge of clusters.</p>

  <p class="body"><a id="pgfId-1004582"></a>The idea behind OPTICS is that, instead, this order does matter, and in particular it can make sense to keep expanding a “frontier” for the current cluster by adding the unprocessed point that is closest to the cluster (if it is reachable from the cluster).</p>

  <p class="body"><a id="pgfId-1004591"></a>Figure 12.15 illustrates this concept in a simplified scenario. In order to choose the right point, it’s obviously necessary to keep track of the distances of all undiscovered neighbors of a cluster,<a href="#pgfId-1009534"><sup class="footnotenumber">25</sup></a> and for this purpose, the authors of the paper introduced two definitions: to each point, they associated a core distance and a <i class="calibre17">reachability distance</i>.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F15.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041016"></a>Figure 12.15 Considering the distance of undiscovered points (<code class="fm-code-in-text">P<sub class="subscript1">1</sub></code> to <code class="fm-code-in-text">P<sub class="subscript1">4</sub></code>) from (points within) a certain cluster. The key idea in OPTICS is “discovering” these points from the closest to the furthest; in the example, <code class="fm-code-in-text">P3</code> would be the next one to be processed.</p>

  <h3 class="fm-head2" id="heading_id_18"><a id="pgfId-1004635"></a>12.4.1 Definitions</h3>

  <p class="body"><a id="pgfId-1004647"></a>The <a id="marker-1008932"></a><a id="marker-1008936"></a>core distance of a point <code class="fm-code-in-text">p</code> is the minimum distance for that point to be considered a core point. Since the definition of a core point, given in section 12.3 for DBSCAN, depends on the parameters <span class="cambria">ϵ</span>, the radius of dense regions, and <code class="fm-code-in-text">minPoints</code> (<code class="fm-code-in-text">M</code> for short, in the following formulas), the minimum number of points needed to define a dense region, these definitions also depend on those hyperparameters:</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F_EQ1.png"/></p>

  <p class="body"><a id="pgfId-1004683"></a>Naturally, if a point is not a core point (and so less than <code class="fm-code-in-text">M</code> points are in its <span class="cambria">ϵ</span>-neighborhood), its core distance is undefined. Conversely, its core distance will be the distance of its <code class="fm-code-in-text">M</code>-th nearest neighbor:<a href="#pgfId-1009548"><sup class="footnotenumber">26</sup></a> if this happens, core distance can be interpreted as the minimum value that we can assign to the parameter <span class="cambria">ϵ</span> for <code class="fm-code-in-text">p</code> to be a core point (given a fixed value for <code class="fm-code-in-text">minPoints</code>).</p>

  <p class="body"><a id="pgfId-1004720"></a>Once we have defined the core distance, we can use it to define a new kind of metric, the reachability distance between two points:</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F_EQ2.png"/></p>

  <p class="body"><a id="pgfId-1004738"></a>Reachability distance is not symmetric. Given a point <code class="fm-code-in-text">q</code>, the reachability distance of <code class="fm-code-in-text">p</code> from <code class="fm-code-in-text">q</code> can be interpreted as the minimum value of <span class="cambria">ϵ</span> for which <code class="fm-code-in-text">p</code> is density-reachable from <code class="fm-code-in-text">q</code>; therefore, it must be at least <code class="fm-code-in-text">p</code>’s core distance (for the closest <code class="fm-code-in-text">minPoints</code> to <code class="fm-code-in-text">p</code>), or the actual distance between <code class="fm-code-in-text">p</code> and <code class="fm-code-in-text">q</code>. Moreover, if <code class="fm-code-in-text">p</code> and <code class="fm-code-in-text">q</code> are nearest neighbors (in particular if there is no point closer to <code class="fm-code-in-text">p</code> than <code class="fm-code-in-text">q</code>), then this value is the smallest value we can assign to <span class="cambria">ϵ</span> in order for <code class="fm-code-in-text">p</code> and <code class="fm-code-in-text">q</code> to belong to the same <a id="marker-1008940"></a><a id="marker-1008944"></a>cluster.</p>

  <h3 class="fm-head2" id="heading_id_19"><a id="pgfId-1004793"></a>12.4.2 OPTICS algorithm</h3>

  <p class="body"><a id="pgfId-1004807"></a>Once <a id="marker-1008948"></a><a id="marker-1008952"></a>given these two definitions, we can easily describe the algorithm. Its core idea is similar to DBSCAN, adding points to the current cluster only if they are reachable from points already in the cluster. However, OPTICS is also completely different. For starters, it doesn’t produce just a flat partitioning of the points, but instead it builds a hierarchical clustering. We’ll get back to this in a minute.</p>

  <p class="body"><a id="pgfId-1004833"></a>OPTICS takes the same parameters as DBSCAN, but <span class="cambria">ϵ</span>, in this case, has a different meaning. It’s the largest radius that will be considered to determine the core and reachability distance. For this reason (and for disambiguation, as we’ll see), in the rest of the chapter we will refer to this parameter for OPTICS as <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">MAX</sub></code>. By using this approach, the algorithm simultaneously constructs density-based clusterings for an infinite number of values of <span class="cambria">ϵ</span> (all values between <code class="fm-code-in-text">0</code> and <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">MAX</sub></code>), and hence for an infinite number of densities.</p>

  <p class="body"><a id="pgfId-1004860"></a>In the formulas in the previous sub-section we have seen that these distances are set to <code class="fm-code-in-text">undefined</code> if the <span class="cambria">ϵ</span>-neighborhood of a point doesn’t have enough neighbors. This also means that the reachability distance from a point <code class="fm-code-in-text">q</code> (to any other core point) will be <code class="fm-code-in-text">undefined</code> if it’s not a core point, with <code class="fm-code-in-text">M-1</code> neighbors within a radius <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">MAX</sub></code>.</p>

  <p class="body"><a id="pgfId-1004889"></a>Therefore, if we set <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">MAX</sub></code> to a large value, then we will have fewer points marked as noise and, in a way, we will leave more open options because we will allow larger values for the core distance, and we will have more points with a defined value for reachability distance.</p>

  <p class="body"><a id="pgfId-1004901"></a>However, a larger radius for the core density areas means that these areas will contain more points, and this makes the algorithm slower.</p>

  <p class="body"><a id="pgfId-1004912"></a>In the main section of the algorithm, in fact, for each point processed we need to update the reachability distance of all undiscovered points in its <span class="cambria">ϵ</span>-neighborhood. The more points it contains, the slower the algorithm will be.</p>

  <p class="body"><a id="pgfId-1004927"></a>As we briefly mentioned, OPTICS’ main cycle grows the current cluster<a href="#pgfId-1009578"><sup class="footnotenumber">27</sup></a> <code class="fm-code-in-text">C</code> by adding the closest point to the cluster’s frontier, which is the point with the smallest reachability distance from any point already in <code class="fm-code-in-text">C</code>. The cluster is formed in a way similar to DBSCAN’s because all points reachable from the cluster’s seed (the first point processed for a new cluster) are added to it. However, forming these clusters is not the direct objective of OPTICS.</p>

  <p class="body"><a id="pgfId-1004946"></a>When a new point is “discovered” and processed, its reachability distance from the cluster is set in stone. This cluster-to-point distance is not to be confused with the reachability distance between two points. For a point <code class="fm-code-in-text">q</code> and a cluster <code class="fm-code-in-text">C</code>, we define the reachability distance of <code class="fm-code-in-text">q</code> from <code class="fm-code-in-text">C</code> as the minimum of the reachability distance from <code class="fm-code-in-text">p</code> to <code class="fm-code-in-text">q</code>, for all points <code class="fm-code-in-text">p</code> in <code class="fm-code-in-text">C</code>:</p>

  <p class="fm-equation">reachability-distance<sub class="subscript"><span class="cambria">ϵ</span>, <i class="calibre15">M</i></sub> (<i class="calibre17">q,C</i>) = <i class="calibre17">max</i>{reachability-distance<sub class="subscript"><span class="cambria">ϵ</span>, <i class="calibre15">M</i></sub> (<i class="calibre17">q, p</i>)<span class="cambria">∀</span><i class="calibre17">p</i>|<i class="calibre17">p<span class="cambria">ϵ</span>C</i>}</p>

  <p class="body"><a id="pgfId-1004980"></a>If when we process a point <code class="fm-code-in-text">p</code>, we update the reachability distances of all points in its <code class="fm-code-in-text"><span class="cambria">ε</span></code>-neighborhood, and we keep a priority queue with all the undiscovered points in the neighborhoods of any point in <code class="fm-code-in-text">C</code>, then we can be sure that</p>

  <ol class="calibre18">
    <li class="fm-list-numbered">
      <p class="list"><a class="calibre14" id="pgfId-1004995"></a>The reachability distance of all the points at the frontier of the cluster (all the points that are neighbors to at least one point in the cluster) is correctly stored.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1005008"></a>The value stored is the smallest of the reachability distances from any of the points already processed to <code class="fm-code-in-text">q</code> (although we only care about the current cluster’s).</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1005023"></a>The top of the queue holds the closest point to cluster <code class="fm-code-in-text">C:</code></p>

      <ol class="calibre35">
        <li class="fm-list-numbered1">
          <p class="list"><a class="calibre14" id="pgfId-1005038"></a>Technically, at the top of the queue we have the point <code class="fm-code-in-text">q</code> with an associated value <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">q</sub></code> (the smallest value for <code class="fm-code-in-text"><span class="cambria">ε</span></code> for which <code class="fm-code-in-text">q</code> would still be reachable from <code class="fm-code-in-text">C)</code> such that, considering any other points <code class="fm-code-in-text">w</code> in <code class="fm-code-in-text">C</code>’s frontier, <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">q</sub></code> <span class="cambria">≤</span> <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">w</sub></code>.</p>
        </li>
      </ol>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1005075"></a>Therefore, any of the points still to be processed will have at least the same reachability distance from <code class="fm-code-in-text">C</code>, or higher.</p>
    </li>
  </ol>

  <p class="body"><a id="pgfId-1005089"></a>Listings 12.7 and 12.8 describe the main OPTICS algorithm, while on the book’s <span class="fm-hyperlink">repo</span> on GitHub,<a href="#pgfId-1009597"><sup class="footnotenumber">28</sup></a> you can find a Python implementation of the algorithm and this Jupyter <span class="fm-hyperlink">Notebook</span> to experiment with it.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1019446"></a>Listing 12.7 OPTICS Clustering</p>
  <pre class="programlisting"><b class="calibre21">function</b> optics(points, epsMax, minPoints)                <span class="fm-combinumeral">❶</span>
  reachabilityDistances ← <b class="calibre21">null</b> (<span class="cambria">∀</span> p <span class="cambria">ϵ</span> points)            <span class="fm-combinumeral">❷</span>
  <b class="calibre21">or</b>dering ← []                                           <span class="fm-combinumeral">❸</span>
  kdTree ← <b class="calibre21">new</b> KdTree(points)                             <span class="fm-combinumeral">❹</span>
  <b class="calibre21">for</b> p <b class="calibre21">in</b> points <b class="calibre21">do</b>                                      <span class="fm-combinumeral">❺</span>
    <b class="calibre21">if</b> p <b class="calibre21">in</b> ordering <b class="calibre21">then</b>                                 <span class="fm-combinumeral">❻</span>
      continue
    <b class="calibre21">or</b>dering.insert(p)                                    <span class="fm-combinumeral">❼</span>
    neighbors ← kdTree.pointsInSphere(p, epsMax)          <span class="fm-combinumeral">❽</span>
    <b class="calibre21">if</b> |neighbors| &gt;= minPoints <b class="calibre21">then</b>                      <span class="fm-combinumeral">❾</span>
      toProcess ← <b class="calibre21">new</b> PriorityQueue()                     <span class="fm-combinumeral">❿</span>
      toProcess, reachabilityDistances ← updateQueue(
          p, neighbors, toProcess, 
          reachabilityDistances, epsMax, minPoints)       <span class="fm-combinumeral">⓫</span>
      <b class="calibre21">while</b>  not toProcess.isEmpty() <b class="calibre21">do</b>                   <span class="fm-combinumeral">⓬</span>
        q ← toProcess.top()                               <span class="fm-combinumeral">⓭</span>
        <b class="calibre21">or</b>dering.insert(q)                                <span class="fm-combinumeral">⓮</span>
        toProcess, reachabilityDistances ← updateQueue(
          q, kdTree, toProcess, ordering,
          reachabilityDistances, epsMax, minPoints)       <span class="fm-combinumeral">⓯</span>
  <b class="calibre21">return</b> ordering, reachabilityDistances                  <span class="fm-combinumeral">⓰</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1034136"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">optics</code><a id="marker-1034140"></a> takes a list of points, the (maximum) radius of the dense area defining core points, and the minimum number of points needed in the dense area for a point to be a core point. This function returns a tuple: an array for the order in which points were processed (an array of indices), and a second array with the reachability distance for each point.</p>

  <p class="fm-code-annotation"><a id="pgfId-1034161"></a><span class="fm-combinumeral">❷</span> Initializes the reachability distance of each point to <code class="fm-code-in-text2">null</code></p>

  <p class="fm-code-annotation"><a id="pgfId-1034178"></a><span class="fm-combinumeral">❸</span> Initializes the ordering for the points. Initially an empty array; we’ll add the elements as they are processed.</p>

  <p class="fm-code-annotation"><a id="pgfId-1034195"></a><span class="fm-combinumeral">❹</span> Creates a k-d tree to speed range queries. It initializes it with the full dataset.</p>

  <p class="fm-code-annotation"><a id="pgfId-1034212"></a><span class="fm-combinumeral">❺</span> Cycles through each point in the dataset</p>

  <p class="fm-code-annotation"><a id="pgfId-1034246"></a><span class="fm-combinumeral">❻</span> Checks if point <code class="fm-code-in-text2">p</code> hasn’t yet been processed; if it was processed, just skips it</p>

  <p class="fm-code-annotation"><a id="pgfId-1034229"></a><span class="fm-combinumeral">❼</span> Otherwise, adds <code class="fm-code-in-text2">p</code> as the next point in the ordering</p>

  <p class="fm-code-annotation"><a id="pgfId-1034263"></a><span class="fm-combinumeral">❽</span> Performs the range query collecting all points in the hyper-sphere with center <code class="fm-code-in-text2">p</code>. In this implementation we assume the function does include the point p itself (as per the implementation in listing 9.11).</p>

  <p class="fm-code-annotation"><a id="pgfId-1034280"></a><span class="fm-combinumeral">❾</span> Checks if <code class="fm-code-in-text2">p</code> is a core point; hence, if the query at line #9 returns at least <code class="fm-code-in-text2">minPoints</code> points</p>

  <p class="fm-code-annotation"><a id="pgfId-1034297"></a><span class="fm-combinumeral">❿</span> Creates a new priority queue for the points reachable from <code class="fm-code-in-text2">p</code>, which will be processed next</p>

  <p class="fm-code-annotation"><a id="pgfId-1034314"></a><span class="fm-combinumeral">⓫</span> Updates the queue (by adding all points directly reachable from <code class="fm-code-in-text2">p</code>); also updates the reachability distance for <code class="fm-code-in-text2">p</code>’s neighbors</p>

  <p class="fm-code-annotation"><a id="pgfId-1034331"></a><span class="fm-combinumeral">⓬</span> Cycles through all points in the queue, ordered by their reachability distance</p>

  <p class="fm-code-annotation"><a id="pgfId-1034348"></a><span class="fm-combinumeral">⓭</span> Extracts from the queue the point <code class="fm-code-in-text2">q</code> with the smallest reachability distance (from the set of points already processed)</p>

  <p class="fm-code-annotation"><a id="pgfId-1034365"></a><span class="fm-combinumeral">⓮</span> Adds <code class="fm-code-in-text2">q</code> as the next point in the ordering</p>

  <p class="fm-code-annotation"><a id="pgfId-1034382"></a><span class="fm-combinumeral">⓯</span> Updates the queue (by adding all points directly reachable from <code class="fm-code-in-text2">q</code>) and the reachability distance for <code class="fm-code-in-text2">q</code>’s neighbors</p>

  <p class="fm-code-annotation"><a id="pgfId-1034399"></a><span class="fm-combinumeral">⓰</span> Returns the ordering for dataset’s points and their reachability distances</p>

  <p class="body"><a id="pgfId-1005614"></a>The algorithm’s core (also illustrated in figure 12.16 on a simplified example) consists of a cycle through each point in the dataset. Entries are processed in chunks of contiguous, reachable points (the starting point is chosen either randomly or according to its position in the dataset), and non-core points are “skipped” (similar to what happened in DBSCAN).</p>

  <p class="body"><a id="pgfId-1005627"></a>The points that are reachable from the ones already processed are kept in a priority queue and extracted according to their reachability distance (the smallest reachability distance from any of the processed points is on top of the queue, as we have already seen).</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F16.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041058"></a>Figure 12.16 An example of how OPTICS constructs the ordering and list of reachability distances. Notice that the choice of the first point is completely arbitrary; it can even be random. Here we chose to start from <code class="fm-code-in-text">P<sub class="subscript1">1</sub></code> just for convenience, to present a cleaner example. Likewise, at step (E) we could have chosen <code class="fm-code-in-text">P<sub class="subscript1">6</sub></code> instead of <code class="fm-code-in-text">P<sub class="subscript1">5</sub></code>: if that happened, we wouldn’t have been able to compute its reachability distance from <code class="fm-code-in-text">P<sub class="subscript1">5</sub></code> (or vice versa). Choosing a larger value for <span class="cambria">ϵ</span><code class="fm-code-in-text"><sub class="subscript1">MAX</sub></code> would help avoid these situations.</p>

  <p class="body"><a id="pgfId-1005680"></a>There is a key piece of code still missing, the <code class="fm-code-in-text">updateQueue</code> method<a id="marker-1008960"></a>. That’s where the reachability distances are actually updated (together with the priority queue). Listing 12.8 bridges this gap. The main purpose of this method is to go through all points <code class="fm-code-in-text">q</code> in the <span class="cambria">ϵ</span>-neighborhood of a point <code class="fm-code-in-text">p</code> (the point that is currently being processed) and update <code class="fm-code-in-text">q</code>’s reachability distances by checking to see if <code class="fm-code-in-text">p</code> is closer to it than any of the other points previously processed.</p>

  <p class="body"><a id="pgfId-1005710"></a>As you can see from figure 12.16, we use the priority queue to keep track of the (intermediate) reachability distances for points and set these distances in stone only when a point is processed. Moreover, we can see that the reachability distance of the first point to be processed for each of the clusters will certainly be either undefined or larger than <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">MAX</sub></code>.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1019504"></a>Listing 12.8 OPTICS <code class="fm-code-in-text">updateQueue</code></p>
  <pre class="programlisting"><b class="calibre21">function</b> updateQueue(p, kdTree, queue, processed, rDists, eps, minPts)  <span class="fm-combinumeral">❶</span>
  neighbors ← kdTree.pointsInSphere(p, eps)                             <span class="fm-combinumeral">❷</span>
  <b class="calibre21">if</b> |neighbors| &lt; minPoints <b class="calibre21">then</b>                                       <span class="fm-combinumeral">❸</span>
    <b class="calibre21">return</b> queue, rDists
  <b class="calibre21">for</b> q <b class="calibre21">in</b> neighbors <b class="calibre21">do</b>                                                 <span class="fm-combinumeral">❹</span>
    <b class="calibre21">if</b> q <b class="calibre21">in</b> processed <b class="calibre21">then</b>                                              <span class="fm-combinumeral">❹</span>
      continue
    newRDist = max(coreDistance(p, eps, minPts), distance(p, q))        <span class="fm-combinumeral">❺</span>
    <b class="calibre21">if</b> rDists[q] == <b class="calibre21">null then</b>                                           <span class="fm-combinumeral">❻</span>
      rDists[q] ← <b class="calibre21">new</b>RDist                                              <span class="fm-combinumeral">❼</span>
      queue.insert(q, newRDist)                                         <span class="fm-combinumeral">❼</span>
    <b class="calibre21">elsif</b> newRDist &lt; rDist[q] <b class="calibre21">then</b>                                      <span class="fm-combinumeral">❽</span>
      rDist[q] ← <b class="calibre21">new</b>RDist                                               <span class="fm-combinumeral">❾</span>
      queue.update(q, newRDist)                                         <span class="fm-combinumeral">❾</span>
  <b class="calibre21">return</b> queue, rDists                                                  <span class="fm-combinumeral">❿</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1033166"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">updateQueue</code><a id="marker-1033177"></a> takes a point <code class="fm-code-in-text2">p</code>, a k-d tree with all the points in the dataset, the queue of points reachable from the current cluster, an array with the points that have already been processed, an array with the dataset’s reachability distances, and the parameters epsilon and min points. It returns a tuple with the queue and the array of reachability distances (possibly updated during this call).</p>

  <p class="fm-code-annotation"><a id="pgfId-1033198"></a><span class="fm-combinumeral">❷</span> Performs the range query collecting all points in the hyper-sphere with center <code class="fm-code-in-text2">q</code></p>

  <p class="fm-code-annotation"><a id="pgfId-1033215"></a><span class="fm-combinumeral">❸</span> Checks if <code class="fm-code-in-text2">p</code> is a core point (if and only if the query at line #2 returns at least <code class="fm-code-in-text2">minPts</code> points). If it isn’t, returns without performing any action.</p>

  <p class="fm-code-annotation"><a id="pgfId-1033232"></a><span class="fm-combinumeral">❹</span> Cycles through all unprocessed points in <code class="fm-code-in-text2">p</code>’s <span class="cambria">ϵ</span>-neighborhood</p>

  <p class="fm-code-annotation"><a id="pgfId-1033249"></a><span class="fm-combinumeral">❺</span> Computes the reachability distance of <code class="fm-code-in-text2">q</code> from <code class="fm-code-in-text2">p</code></p>

  <p class="fm-code-annotation"><a id="pgfId-1033266"></a><span class="fm-combinumeral">❻</span> Checks if the reachability distance of <code class="fm-code-in-text2">q</code> from the previously processed points is <code class="fm-code-in-text2">null</code></p>

  <p class="fm-code-annotation"><a id="pgfId-1033283"></a><span class="fm-combinumeral">❼</span> If it is, <code class="fm-code-in-text2">q</code> hasn’t been added to the queue yet, so insert it now, and set the reachability distance for <code class="fm-code-in-text2">q</code>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1033300"></a><span class="fm-combinumeral">❽</span> Otherwise, checks if the reachability distance of <code class="fm-code-in-text2">q</code> from <code class="fm-code-in-text2">p</code> is smaller than from the previously processed points (meaning, if <code class="fm-code-in-text2">p</code> is closer to <code class="fm-code-in-text2">q</code> than those points)</p>

  <p class="fm-code-annotation"><a id="pgfId-1033317"></a><span class="fm-combinumeral">❾</span> If that’s the case, updates <code class="fm-code-in-text2">q</code>’s reachability distance and its priority in the queue</p>

  <p class="fm-code-annotation"><a id="pgfId-1033334"></a><span class="fm-combinumeral">❿</span> Returns the updated queue and array of reachability distances</p>

  <p class="body"><a id="pgfId-1006108"></a>I realize that the discussion so far has been quite abstract. To understand the real purpose of computing these values for the reachability distance and the ordering of processing for the dataset, and how they can be used to build a hierarchical clustering, we need to get ahead of ourselves and take a look at figure 12.17, which shows a reachability plot<a id="marker-1025464"></a> that can be considered the output of OPTICS (and also requires a further step after the main algorithm, as we’ll see in the next <a id="marker-1025465"></a><a id="marker-1025466"></a>section).</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F17.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041100"></a>Figure 12.17 A reachability plot. The top chart shows the clustered dataset, the bottom half the reachability distances of points in the dataset, in the same order as points are processed by OPTICS. Outliers are shown in black (with an X marker in the top plot); clusters are assigned the same shade (or color, if you are reading the digital version) in both charts (dashed lines link clusters across the charts). The reachability distances are computed by OPTICS given parameters <code class="fm-code-in-text">minPoints</code> and <span class="cambria">ϵ</span><code class="fm-code-in-text"><sub class="subscript1">MAX</sub></code>. Further, there is a parameter <span class="cambria">ϵ</span> that determines the cutoff point for reachability distances and therefore the actual partitioning into clusters.</p>

  <h3 class="fm-head2" id="heading_id_20"><a id="pgfId-1006166"></a>12.4.3 From reachability distance to clustering</h3>

  <p class="body"><a id="pgfId-1006184"></a>The <a id="marker-1008980"></a><a id="marker-1008984"></a>first thing you should notice in figure 12.17 is that the plot is determined by three parameters: <code class="fm-code-in-text">minPoints</code> and <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">MAX</sub></code> are the arguments passed to OPTICS, but there is also another value <code class="fm-code-in-text"><span class="cambria">ε</span></code> that is used as a threshold for the reachability distance. Obviously, as you can imagine, <span class="cambria">ϵ</span> <span class="cambria">≤</span> <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">MAX</sub></code>; this threshold is used in a step that is performed separately on OPTICS’ results (we’ll describe it in a few lines).</p>

  <p class="body"><a id="pgfId-1006212"></a>Setting a threshold <span class="cambria">ϵ</span> for the reachability distance means deciding that the radius of the core region around points is equal to <span class="cambria">ϵ</span>. This is equivalent to running DBSCAN with that particular value as radius.</p>

  <p class="body"><a id="pgfId-1006228"></a><a id="Stopped_At"></a>The reachability plot shown in figure 12.17 is a special plot composed of two interconnected charts. The top one simply shows the final (flat) clustering that we obtain by setting the threshold to <span class="cambria">ϵ</span>, while the bottom one shows the ordered sequence of reachability distances and explains how the clustering was derived. Clusters and reachability distances are filled with matching colors, so it’s easier to see what points in the top half match the sections in the bottom half (outliers are marked using black, and for the sake of convenience and to properly plot all values, we assign <code class="fm-code-in-text"><span class="cambria">ε</span>MAX</code> instead of <code class="fm-code-in-text">undefined</code> to the reachability distance of outliers). By doing so we are, in practice, loosening the requirements for the reachability criterion for these points, but since it’s already the largest possible value that we can assign to the threshold <span class="cambria">ϵ</span>, it won’t change anything in the following steps.</p>

  <p class="body"><a id="pgfId-1006266"></a>So, how do we form these clusters, given <span class="cambria">ϵ</span><span class="cambria">≤</span><code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">MAX</sub></code>? Figure 12.18 illustrates the idea behind this algorithm (at a high level, combining it with elements of OPTICS). We start looking at reachability distances, following the ordering in which points were processed by OPTICS, so that we know that the reachability distance of the next point is the smallest possible among all undiscovered points (keep in mind that the reachability distances stored are points-to-cluster distances, and that’s also why ordering is important!).</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F18.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041142"></a>Figure 12.18 An example of the algorithm to derive a flat clustering from OPTICS’ reachability distances. Notice that while the reachability distances are built on the chart left-to-right, points are examined in order of their reachability distance, starting from a randomly chosen entry (so this particular order for the points was chosen, in this example, just for convenience).</p>

  <p class="body"><a id="pgfId-1006304"></a>We start from the first point processed—call it <code class="fm-code-in-text">P<sub class="subscript1">1</sub></code>—and create a new cluster <code class="fm-code-in-text">C<sub class="subscript1">1</sub></code>; being the first point of a new cluster, <code class="fm-code-in-text">P<sub class="subscript1">1</sub></code>’s reachability distance is undefined, so we still don’t know if <code class="fm-code-in-text">P<sub class="subscript1">1</sub></code> is an outlier or part of a cluster. We can only tell after the next step, shown in part (A) of figure 12.18, by checking the reachability distance (from <code class="fm-code-in-text">C<sub class="subscript1">1</sub></code>) of the next point <code class="fm-code-in-text">P<sub class="subscript1">2</sub></code>.</p>

  <p class="body"><a id="pgfId-1006334"></a>Since this value is smaller than <span class="cambria">ϵ</span> (in the example, 0.8), then we add <code class="fm-code-in-text">P<sub class="subscript1">2</sub></code> to <code class="fm-code-in-text">C<sub class="subscript1">1</sub></code> and move to the next point <code class="fm-code-in-text">P3</code>, also added to <code class="fm-code-in-text">C<sub class="subscript1">1</sub></code> as shown in (B) and (C). From a visual point of view, in the reachability chart you can see that the reachability distance for <code class="fm-code-in-text">P<sub class="subscript1">2</sub></code> is below the threshold line (parallel to the horizontal axis) for <code class="fm-code-in-text"><span class="cambria">ϵ</span>=0.8</code>.</p>

  <p class="body"><a id="pgfId-1006371"></a>When we get to <code class="fm-code-in-text">P<sub class="subscript1">4</sub></code>, we realize that its reachability distance from <code class="fm-code-in-text">C<sub class="subscript1">1</sub></code> (or technically from any of the points processed before <code class="fm-code-in-text">P<sub class="subscript1">4</sub></code>) is larger than <span class="cambria">ϵ</span>, and hence <code class="fm-code-in-text">P<sub class="subscript1">4</sub></code> is not reachable from <code class="fm-code-in-text">C<sub class="subscript1">1</sub></code>. Since the points are processed in order of reachability distance from the current cluster, this means that there is no point still to be processed that can be reachable from points in <code class="fm-code-in-text">C<sub class="subscript1">1</sub></code> if the radius of core regions is equal to <span class="cambria">ϵ</span>; hence, we “close” the current cluster and start a new one.</p>

  <p class="body"><a id="pgfId-1006409"></a>We start a new cluster for <code class="fm-code-in-text">P<sub class="subscript1">4</sub></code>, but we don’t know how to classify it yet. When we check <code class="fm-code-in-text">P5</code>’s reachability distance (D), we discover that it is also larger than <span class="cambria">ϵ</span>, and this means that the algorithm has detected that <code class="fm-code-in-text">P<sub class="subscript1">4</sub></code> is an outlier (because it’s not reachable by any point in the dataset, given parameters <code class="fm-code-in-text">minPoints</code> and <span class="cambria">ϵ</span>). Notice that the radius that matters here is <span class="cambria">ϵ</span>. By using a higher value for <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">MAX</sub></code>, we have simply instructed OPTICS to filter out as noise only those points with a reachability distance larger than <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">MAX</sub></code>, while for points with a reachability distance at most <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">MAX</sub></code>, we simply defer the decision to this second step. In practice, this allows us to compute reachability distances only once, and try several values for <span class="cambria">ϵ</span> (up to <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">MAX</sub></code> ) in this second step with minimal computational effort.</p>

  <p class="body"><a id="pgfId-1006466"></a>We repeat the process one more time (E) for <code class="fm-code-in-text">P<sub class="subscript1">6</sub></code>, and since its reachability distance (from <code class="fm-code-in-text">P<sub class="subscript1">5</sub></code>) is smaller than <span class="cambria">ϵ</span>, we know that it’s reachable from <code class="fm-code-in-text">P<sub class="subscript1">5</sub></code> and we can add both to the same cluster (F). If there were more points to process, we would have continued in the same way.</p>

  <p class="body"><a id="pgfId-1006490"></a>One final remark before moving to the implementation, shown in listing 12.9. Notice that if we run OPTICS twice starting with the same point, the dataset will be processed in the same order, and the results will be identical (net of possible ties on reachability distance). Therefore, OPTICS can be considered as completely deterministic.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1019859"></a>Listing 12.9 OPTICS <code class="fm-code-in-text">opticsCluster</code></p>
  <pre class="programlisting"><b class="calibre21">function</b> opticsCluster(ordering, reachabilityDistances, eps)    <span class="fm-combinumeral">❶</span>
  currentClusterIndex ← 0                                      <span class="fm-combinumeral">❷</span>
  incrementCurrentIndex ← <b class="calibre21">false</b>                                <span class="fm-combinumeral">❸</span>
  clusterIndices ← -1 (<span class="cambria">∀</span> p <span class="cambria">ϵ</span> points)                           <span class="fm-combinumeral">❹</span>
  <b class="calibre21">for</b> i <b class="calibre21">in</b> {0, .. |ordering|} <b class="calibre21">do</b>                                <span class="fm-combinumeral">❺</span>
    <b class="calibre21">if</b> reachabilityDistances[ordering[i]] == null or
         reachabilityDistances[ordering[i]] &gt; eps <b class="calibre21">then</b>         <span class="fm-combinumeral">❻</span>
      incrementCurrentIndex ← <b class="calibre21">true</b>                             <span class="fm-combinumeral">❼</span>
    <b class="calibre21">else</b>
      <b class="calibre21">if</b> incrementClusterIndex <b class="calibre21">then</b>                             <span class="fm-combinumeral">❽</span>
        currentClusterIndex ← currentClusterIndex + 1          <span class="fm-combinumeral">❾</span>
        clusterIndices[ordering[i-1]] ← currentClusterIndex    <span class="fm-combinumeral">❿</span>
      clusterIndices[ordering[i]] ← currentClusterIndex        <span class="fm-combinumeral">⓫</span>
      incrementCurrentIndex ← <b class="calibre21">false</b>                            <span class="fm-combinumeral">⓬</span>
  <b class="calibre21">return</b> clusterIndices                                         <span class="fm-combinumeral">⓭</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1032189"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">opticsCluster</code><a id="marker-1032193"></a> takes the ordering and reachability distances produced by <code class="fm-code-in-text2">optics</code>, and a parameter <code class="fm-code-in-text2">eps&lt;=epsMax</code>, used to extract a flat clustering from the (infinitely many) possible clusterings computed by OPTICS. It returns an array of the cluster indices associated with the points (or, equivalently, a dictionary associating points to cluster indices).</p>

  <p class="fm-code-annotation"><a id="pgfId-1032214"></a><span class="fm-combinumeral">❷</span> Initializes the index of the current cluster</p>

  <p class="fm-code-annotation"><a id="pgfId-1032231"></a><span class="fm-combinumeral">❸</span> Initializes a flag that is used to keep track of when a previous cluster ended</p>

  <p class="fm-code-annotation"><a id="pgfId-1032248"></a><span class="fm-combinumeral">❹</span> Initializes the indices of the cluster assignments. Initially all points are marked as outliers.</p>

  <p class="fm-code-annotation"><a id="pgfId-1032265"></a><span class="fm-combinumeral">❺</span> Cycles through all points in the order they were processed</p>

  <p class="fm-code-annotation"><a id="pgfId-1032282"></a><span class="fm-combinumeral">❻</span> Checks if the reachability distance of a point is undefined or larger than the core radius epsilon</p>

  <p class="fm-code-annotation"><a id="pgfId-1032306"></a><span class="fm-combinumeral">❼</span> If so, then we need to close the current cluster.</p>

  <p class="fm-code-annotation"><a id="pgfId-1032323"></a><span class="fm-combinumeral">❽</span> Otherwise, checks if it needs to start a new cluster (by incrementing current index)</p>

  <p class="fm-code-annotation"><a id="pgfId-1032340"></a><span class="fm-combinumeral">❾</span> Actually performs the increment</p>

  <p class="fm-code-annotation"><a id="pgfId-1032357"></a><span class="fm-combinumeral">❿</span> If <code class="fm-code-in-text2">incrementClusterIndex</code><a id="marker-1032361"></a> is true<a id="marker-1032363"></a>, it means that we have found the first point in the new cluster with a non-null reachability distance; this means that the current point is reachable from the previous point in the ordering (which, because of the way OPTICS work, will certainly have a reachability distance that’s either <code class="fm-code-in-text2">null</code> or too large). So, we need to include the previous point as well in the current cluster.</p>

  <p class="fm-code-annotation"><a id="pgfId-1032376"></a><span class="fm-combinumeral">⓫</span> Adds the <code class="fm-code-in-text2">i</code>-th point in the ordering to current cluster</p>

  <p class="fm-code-annotation"><a id="pgfId-1032393"></a><span class="fm-combinumeral">⓬</span> Since we added the current point to a cluster, we know that we need to check if the next points are also reachable from it.</p>

  <p class="fm-code-annotation"><a id="pgfId-1032410"></a><span class="fm-combinumeral">⓭</span> Returns a flat clustering, an array with the cluster index of each point</p>

  <p class="body"><a id="pgfId-1006910"></a>One note on the method <code class="fm-code-in-text">opticsCluster</code><a id="marker-1029307"></a>: In the original paper, the authors presented a slightly different method using points’ core distance to decide if a new cluster should be started. The version presented here stems from the consideration that if the reachability of a point <code class="fm-code-in-text">q</code> is undefined or above the threshold <span class="cambria">ϵ</span>, while point <code class="fm-code-in-text">p</code>, the successor to <code class="fm-code-in-text">q</code> in the processing order, has a reachability distance that’s below <span class="cambria">ϵ</span>, then <code class="fm-code-in-text">q</code> must be a core point. <code class="fm-code-in-text">p</code>’s reachability distance from the set of points processed before <code class="fm-code-in-text">q</code>, in fact, must also be undefined or greater than <code class="fm-code-in-text">q</code>’s; otherwise <code class="fm-code-in-text">p</code> would have been processed before <code class="fm-code-in-text">q</code>. Consequently, <code class="fm-code-in-text">p</code>’s reachability distance in the plot is the reachability distance between <code class="fm-code-in-text">p</code> and <code class="fm-code-in-text">q</code>, and according to section 12.4.1, that’s only defined if <code class="fm-code-in-text">q</code> is a core <a id="marker-1029309"></a><a id="marker-1029310"></a>point.</p>

  <h3 class="fm-head2" id="heading_id_21"><a id="pgfId-1006967"></a>12.4.4 Hierarchical clustering</h3>

  <p class="body"><a id="pgfId-1006979"></a>Now <a id="marker-1009012"></a><a id="marker-1009016"></a>that we have learned how to produce a flat clustering from OPTICS results, we are in good shape to proficiently use the algorithm. We had mentioned, though, that OPTICS is a hierarchical clustering algorithm. What does that mean and how does it work with OPTICS?</p>

  <p class="body"><a id="pgfId-1006996"></a>Hierarchical clustering algorithms produce a multi-layer partitioning of a dataset. This result is often represented with a dendrogram, a tree-like structure that holds a stratified structure. I like to think of exploring dendrograms as analogous to coring: we can take a section of the dendrogram and see what the flat clustering associated with that section looks like.</p>

  <p class="body"><a id="pgfId-1007012"></a>But enough with the abstract analogies; let’s delve into an example to clarify how this works!</p>

  <p class="body"><a id="pgfId-1007021"></a>Figure 12.19 shows a reachability plot obtained from the same reachability distances and ordering as the one in figure 12.17, but with a different value for <span class="cambria">ϵ</span>. Using a larger value for this radius, obviously the reachability areas around points are larger and fewer clusters are formed.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F19.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041189"></a>Figure 12.19 A reachability plot similar to the one in figure 12.17, but with a different choice for <span class="cambria">ϵ</span>. With a larger value, fewer clusters are formed.</p>

  <p class="body"><a id="pgfId-1007059"></a>Does this clustering make more sense than the one shown in figure 12.17? It’s hard to say just by looking at it; answering the question would probably require some domain knowledge or the tools that we will present in the next section. But since there is now just a single noise point surrounded by three clusters, we might think it makes sense that the points on the left half of the dataset all form a single cluster. To obtain that, we can try a larger value for <span class="cambria">ϵ</span>, as shown in figure 12.20.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F20.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041236"></a>Figure 12.20 Another reachability plot from the same results for OPTICS with a larger value for <span class="cambria">ϵ</span>: 0.7. Unfortunately, the value is too large, and clusters <code class="fm-code-in-text">C3</code> and <code class="fm-code-in-text">C4</code>, that we would like to keep distinct, are merged.</p>

  <p class="body"><a id="pgfId-1007106"></a>With <code class="fm-code-in-text"><span class="cambria">ϵ</span>==0.7</code> the goal is reached, but there is a catch: the two clusters on the right are also merged into a single one! This happens because the two peaks in the reachability distance plot, highlighted in figure 12.20, that mark the edges of clusters C1 and C3 in the same figure, have values smaller than 0.7, so these two small clusters are reachable from the biggest ones next to them.</p>

  <p class="body"><a id="pgfId-1007117"></a>Is there a way to keep <code class="fm-code-in-text">C</code><sub class="subscript">3</sub> separated from <code class="fm-code-in-text">C</code><sub class="subscript">4</sub>, but <code class="fm-code-in-text">C</code><sub class="subscript">1</sub> merged to <code class="fm-code-in-text">C</code><sub class="subscript">2</sub>? For DBSCAN, we have seen in figure 12.14 that this is not possible. For OPTICS, a flat clustering separating one but not the other would only be possible if there was a value of <span class="cambria">ϵ</span> smaller than <code class="fm-code-in-text">C</code><sub class="subscript">3</sub>’s threshold but larger than <code class="fm-code-in-text">C</code><sub class="subscript">1</sub>’s. As figure 12.21 illustrates, this is not the case.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F21.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041281"></a>Figure 12.21 Yet another reachability plot with <span class="cambria">ϵ</span><code class="fm-code-in-text">==0.695</code>. This value is smaller than the reachability distance between clusters <code class="fm-code-in-text">C<sub class="subscript1">1</sub></code> and <code class="fm-code-in-text">C<sub class="subscript1">2</sub></code>, but it’s not small enough to keep clusters <code class="fm-code-in-text">C3</code> and <code class="fm-code-in-text">C4</code> separated.</p>

  <p class="body"><a id="pgfId-1007180"></a>We seem to be back to the old issue with DBSCAN: it is not possible to find a single value of <span class="cambria">ϵ</span> that works for the whole dataset, since there is one “half” of it (where <code class="fm-code-in-text">x&lt;0</code>) that has a sensibly lower density than the rest of the dataset. (You can also see this from the reachability distances: the area, originally in green, on the right, [starting at index 50 on the lower graph] has a dramatically lower average for the reachability distance.) This is where the hierarchical clustering’s added value comes into play; when we run the main OPTICS algorithm, we produce an ordering and a reachability plot. These don’t provide a clustering immediately, but they are (or rather imply) a set of flat clusterings that we can obtain by “cutting” the reachability plot with a specific value of <span class="cambria">ϵ</span> taken from <code class="fm-code-in-text">[0, <span class="cambria">ϵ</span><sub class="subscript1">MAX</sub>]</code>.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F22.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041323"></a>Figure 12.22 The dendrogram built from the result produced by OPTICS, on our example dataset, with parameters <span class="cambria">ϵ</span><code class="fm-code-in-text"><sub class="subscript1">MAX</sub>=2.0</code> and <code class="fm-code-in-text">minPoints=3</code>. For the sake of clarity, we omit the bottom portion of the dendrogram for <span class="cambria">ϵ</span><code class="fm-code-in-text">&lt;0.5</code>. The clusters formed for <span class="cambria">ϵ</span><code class="fm-code-in-text">==0.5</code> are named from <code class="fm-code-in-text">C<sub class="subscript1">1</sub></code> to <code class="fm-code-in-text">C<sub class="subscript1">7</sub></code>, and are considered the basic units in this plot (normally we would start from the single points). Super-clusters merged from them are named <code class="fm-code-in-text">C<sub class="subscript1">A</sub></code> to <code class="fm-code-in-text">C<sub class="subscript1">F</sub></code>.</p>

  <p class="body"><a id="pgfId-1007217"></a>If we try all possible values of epsilon and keep track of the resulting clustering, we can analyze how the partitioning evolves. The best way to perform this analysis is through a dendrogram, a tree-like structure shown in figure 12.22, where—for the sake of clarity—the <code class="fm-code-in-text">x</code> axis only shows as lowest-level entries the eight clusters (plus some noise) in figure 12.17, while it would normally have one entry per point in the dataset. Notice how the clusters and noise points are ordered along the <code class="fm-code-in-text">x</code> axis of the dendrogram: all points in this plot must follow the same order as in the reachability plot (which, in turn, is the same order points are processed by OPTICS).</p>

  <p class="body"><a id="pgfId-1007287"></a>Now, looking at a dendrogram you can see why this is called hierarchical clustering: it keeps track of a hierarchy of clusters going (top to bottom) from a single cluster containing the whole dataset, to <code class="fm-code-in-text">N</code> singletons, which are proto-clusters with a single dataset point in them. Moving from the top of the dendrogram to its bottom means virtually exploring all the possible values for <span class="cambria">ϵ</span>, and the flat clusterings associated with those values: when, in figures 12.17, 12.19, and so on, we chose <code class="fm-code-in-text"><span class="cambria">ϵ</span>=0.5</code> or <code class="fm-code-in-text"><span class="cambria">ϵ</span>=0.6</code>. We were figuratively cutting a section of the dendrogram and taking a peak at the clusters formed at that level. As we have seen, though, cutting such a section with a line perpendicular to the <span class="cambria">ϵ</span> axis (meaning a line where <span class="cambria">ϵ</span> is constant through the whole dataset) doesn’t work for our non-uniform dataset.</p>

  <p class="body"><a id="pgfId-1007321"></a>The great thing about having a hierarchy of clusters, though, is that we don’t have to cut this dendrogram at the same height for the whole dataset! In other words, we can use different values of <span class="cambria">ϵ</span> in different branches of the dendrogram. How do we decide which branches and what values? Well, with 2-D datasets, we can be guided by our intuition, but in higher dimensions, it can stem from domain knowledge or be derived by some metric. For instance, in our example, we can consider the partitioning after the first split in the dendrogram, <code class="fm-code-in-text">C<sub class="subscript1">E</sub></code> and <code class="fm-code-in-text">C<sub class="subscript1">F</sub></code>, and compare their average density. Since density is clearly very different between the two branches, we can come up with two different values for <span class="cambria">ϵ</span> in each of them, based on the statistics of each subset: higher where density is lower (or, equivalently, where the average reachability distance is higher).</p>

  <p class="body"><a id="pgfId-1007351"></a>If the result is not yet satisfactory, we can keep traversing each branch of the dendrogram repeating this step, until one of the following happens:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1007362"></a>We reach a point were both branches have similar characteristics.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1007378"></a>We get the desired number of clusters (if we have an idea from domain knowledge).</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1007394"></a>We are satisfied by the result of some metric we have defined (see the next section).</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1007412"></a>Or we chose a threshold for the max depth we can traverse the tree, and we reach it.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1007425"></a>Figure 12.23 shows how this would work for our example, assuming we are satisfied with traversing the dendrogram only up to the first split. You can see that now, instead of a segment, we have a step function<a id="marker-1009020"></a> cutting through the reachability plot and the dendrogram. In figure 12.23, while we only retain three clusters, <code class="fm-code-in-text">C<sub class="subscript1">F</sub></code>, <code class="fm-code-in-text">C<sub class="subscript1">6</sub></code>, and <code class="fm-code-in-text">C<sub class="subscript1">7</sub></code>, we have left the border of all <code class="fm-code-in-text">C<sub class="subscript1">F</sub></code>’s sub-clusters visible in the <a id="marker-1009024"></a><a id="marker-1009028"></a>chart.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F23.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041371"></a>Figure 12.23 The dendrogram built from the result produced by OPTICS, on our example dataset, with parameters <span class="cambria">ϵ</span><code class="fm-code-in-text"><sub class="subscript1">MAX</sub>=2.0</code> and <code class="fm-code-in-text">minPoints=3</code>. For the sake of clarity, we omit the bottom portion of the dendrogram for <span class="cambria">ϵ</span><code class="fm-code-in-text">&lt;0.5</code>. Here we apply two different thresholds in two branches of the dendrogram.</p>

  <h3 class="fm-head2" id="heading_id_22"><a id="pgfId-1007504"></a>12.4.5 Performance analysis and final considerations</h3>

  <p class="body"><a id="pgfId-1007522"></a>Hierarchical <a id="marker-1009032"></a><a id="marker-1009036"></a>clustering is powerful, but also resource-consuming, compared to flat clustering. While it is estimated in the original paper that the core OPTICS algorithm runs approximately 1.6 times slower than DBSCAN (on the same datasets), keeping the hierarchical clustering and building and exploring the dendrogram obviously also require extra memory and computation.</p>

  <p class="body"><a id="pgfId-1007534"></a>For the core algorithm, a quick glance at listing 12.6 and 12.7 shows us that the code processes each point exactly once (because each point is added to and removed from the priority queue once), and runs one region-query for each point processed; however, entries in the priority queue can be updated multiple times, potentially each time a point is processed. The size of the priority queue depends on the size of the dense regions, and the larger <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">MAX</sub></code>, the more points will be in the queue. Potentially, the queue could contain all the points starting from iteration 1 (if <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">MAX</sub></code> ≥ max pairwise distance), and all points in the queue could be updated each time a new point is processed. Likewise, the time needed for nearest neighbor searches, even if using worst-case bounded structures such as k-d trees, depends on <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">MAX</sub></code>, and if the radius of the region searched is large enough, these queries become linear scans of the dataset.</p>

  <p class="body"><a id="pgfId-1007572"></a>For these reasons, the worst-case running time of the algorithm is quadratic! Nevertheless, it can be shown that with an appropriate choice for <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">MAX</sub></code>, the nearest neighbor search can be performed in amortized logarithmic time, and similarly the size of the priority queue can be bound.<a href="#pgfId-1009613"><sup class="footnotenumber">29</sup></a> Therefore, with a wise choice of <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">MAX</sub></code>, the average running time can be as low as <code class="fm-code-in-text">O(n*log(n))</code> for a dataset with <code class="fm-code-in-text">n</code> points.</p>

  <p class="body"><a id="pgfId-1007595"></a>The original paper contains a more formal description of the algorithm and the theory behind it, and an automated procedure to build a hierarchical clustering structure from the reachability plot. It’s a good starting point if you’d like to deepen your understanding of this algorithm.</p>

  <p class="body"><a id="pgfId-1007608"></a>For more interesting reading, see the paper on <i class="calibre17">DeLiClu</i>,<a href="#pgfId-1009632"><sup class="footnotenumber">30</sup></a> an advanced algorithm that extends OPTICS with ideas from single-linkage clustering, allowing us to avoid the parameter <code class="fm-code-in-text"><span class="cambria">ϵ</span><sub class="subscript1">MAX</sub></code> altogether, and at the same time optimizing the algorithm and improving its <a id="marker-1009040"></a><a id="marker-1009044"></a>running <a id="marker-1009048"></a><a id="marker-1009052"></a>time.</p>

  <h2 class="fm-head" id="heading_id_23"><a id="pgfId-1007629"></a>12.5 Evaluating clustering results: Evaluation metrics</h2>

  <p class="body"><a id="pgfId-1007647"></a>At <a id="marker-1009056"></a>this point in the chapter, we have learned about three different (and increasingly complex and effective) clustering algorithms, their strengths, and their weaknesses. Despite being so different from one another, there is one thing that all of these algorithms have in common: they require a human to set one or more hyper-parameters in order to achieve the best result.</p>

  <p class="body"><a id="pgfId-1007666"></a>Setting these parameters manually can be beyond challenging: while with 2-D datasets it’s possible to take a look at the resulting clustering and decide if it looks good or not, with higher-dimensional datasets we don’t have the luxury of using our intuition.</p>

  <p class="body"><a id="pgfId-1007677"></a>It is time to define a more formal way to assess the quality of a clustering. It’s finally time to talk about evaluation metrics.</p>

  <p class="body"><a id="pgfId-1007690"></a>To best develop this discussion, we’ll revive our initial example: customer segmentation for our e-commerce site, based on two features, annual income and average monthly bill on the platform. Figure 12.24 shows a synthetic dataset with a realistic distribution (based on data from a real website). The dataset has been preprocessed, normalizing the points. Data massaging is a standard step in data science; it helps make sure that features with larger values have the same weight on the final decision. For instance, in our example, we can reasonably assume that the annual salary is in the range $100K–200K, while the average monthly expenses on the website are somewhere between $500–1,000. If that was the case, one feature would have values three orders of magnitude larger than the other, and the dataset would be completely skewed. In other words, if we used Euclidean distance to measure how close two points are, the difference in annual salary would contribute overwhelmingly more than the difference in monthly expenses to the final distance between two customers, making the second feature irrelevant.</p>

  <p class="body"><a id="pgfId-1007715"></a>To avoid this effect, we perform a normalization step by subtracting the mean of each feature to every single point, and then dividing by the feature’s standard deviation.<a href="#pgfId-1009646"><sup class="footnotenumber">31</sup></a></p>

  <p class="body"><a id="pgfId-1007726"></a>On the left side of figure 12.24, we show a possible clustering for the dataset. You don’t need to be a domain expert to see that this appears to be a bad choice for the number of clusters.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F24.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041417"></a>Figure 12.24 A synthetic dataset showing customers of a hypothetic e-commerce website after feature scaling. On the right, a possible (bad) clustering for the dataset.</p>

  <p class="body"><a id="pgfId-1007759"></a>Can we write a method or a mathematical formula to express the (poor) quality of this choice?</p>

  <p class="body"><a id="pgfId-1007768"></a>Let’s start from a consideration: What are the consequences of a bad clustering? Looking at the picture, we can see that points that should belong to different clusters (for example the four clusters in the top-right quadrant) are instead grouped together. As a result, points that are far away from each other are all in the same cluster. And if we think about the very definition of k-means, its target is minimizing the squared Euclidean distance of the points to their centroids. When too few centroids are created (as in the example), this will force many points to be assigned to a single centroid, even if far away. Therefore, the average distance of points in each cluster from their centroid will be higher than if we chose the right number of centroids!</p>

  <p class="body"><a id="pgfId-1007790"></a>If we run the algorithm several times, with different values for the number of centroids, we can compute the average (or median) distance of points from their centroids, and choose the lower value. Is that good enough?</p>

  <p class="body"><a id="pgfId-1007799"></a>Well, we are close, but not quite there. If you think about it, whatever the value of <code class="fm-code-in-text">k</code> (the number of centroids) that we tested last, if we choose <code class="fm-code-in-text">k+1</code>, with one more centroid the average distance will drop a little further because each centroid will have fewer and closer points in its cluster. If we take this to an extreme, by choosing <code class="fm-code-in-text">k=n</code> (one centroid per point in the dataset) we will obtain an average distance-to-centroid equal to zero (because each point will have its own centroid).</p>

  <p class="body"><a id="pgfId-1007826"></a>How can we counter-balance this? The truth is that we can’t really balance it easily, but there is an easy empiric method that helps us choose the best value. We’ll see that in a minute.</p>

  <p class="body"><a id="pgfId-1007839"></a>First, let’s make another important point clear: the distance of points from centroids is only one of many possible metrics. Incidentally, this metric only works for centroid-based clustering algorithms; it wouldn’t be applicable to OPTICS or DBSCAN. A similar metric that would also work for those algorithms is the intra-cluster distance: the average pair-wise distance between points in the same cluster. Another interesting measure, useful when dealing with categorical features, is the total cohesion (similar to the distance to cluster center, but using the cosine distance instead of the Euclidean distance).</p>

  <p class="body"><a id="pgfId-1007854"></a>In the rest of this section, we’ll adopt the intra-cluster average distance as our metric. Now, however, it’s time to reveal the <i class="calibre17">elbow method</i>, an empirical tool that’s used in clustering as well as across machine learning to spot the best value for hyper-parameters.</p>

  <p class="body"><a id="pgfId-1007865"></a>Figure 12.25 shows an example of applying the elbow method to our customers dataset to determine the best number of clusters based on intra-cluster average distance. But it would also be possible to use the same method to decide on the best value for <span class="cambria">ϵ</span> or <code class="fm-code-in-text">minPoints</code> if we were using DBSCAN instead.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F25.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041459"></a>Figure 12.25 The elbow method used to determine the best value for the number of clusters into which the dataset in figure 12.24 should be split. You can guess where the name comes from.</p>

  <p class="body"><a id="pgfId-1007902"></a>From figure 12.25 you can guess where the name of this method comes from: the plot looks like a bent arm, and we want to choose the value corresponding to the elbow, the point where the growth of the function changes drastically.</p>

  <p class="body"><a id="pgfId-1007911"></a>For our example, that value is <code class="fm-code-in-text">k=6</code>. After that, the metric value improves very little. For real datasets, the transition can be less neat (in this case, clusters are very compact and far from each other, so once we reach the optimal number of clusters, the improvement with adding another centroid is almost null), but there is often a point that’s like a watershed: on its left, the slope of the curve is closer to (or larger than) -45°; on its right, it’s closer to 0°.</p>

  <p class="body"><a id="pgfId-1007927"></a>There are, of course, a few details to take into account to successfully implement this method. First, since k-means is a randomized method, it’s important to run it several times per value of k. Then you can pick the best value among all the runs (and also store the clustering produced, as we do in the Notebook on the book’s repo), or the average or median value for the metric, depending on your goal.<a href="#pgfId-1009663"><sup class="footnotenumber">32</sup></a> Moreover, you need to carefully choose the best metric for your problem. You might want to minimize the distance of points inside one cluster to make sure clusters are as homogeneous as possible, or, for instance, maximize the distance between different clusters to make sure you have a neat separation between different groups.</p>

  <p class="body"><a id="pgfId-1007953"></a>Listing 12.10 summarizes the steps that should be performed to successfully apply the elbow method (up to the plotting, not included for obvious reasons). As mentioned, you can check it out in the Notebook on the book’s repo.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020351"></a>Listing 12.10 The elbow method</p>
  <pre class="programlisting"><b class="calibre21">function</b> kmeansElbow(points, ksToTest, maxIter, runsPerK)        <span class="fm-combinumeral">❶</span>
  results ← {}                                                   <span class="fm-combinumeral">❷</span>
  <b class="calibre21">for</b> k <b class="calibre21">in</b> ksToTest <b class="calibre21">do</b>                                           <span class="fm-combinumeral">❸</span>
    M ← []                                                       <span class="fm-combinumeral">❹</span>
    <b class="calibre21">for</b> i <b class="calibre21">in</b> {1,..,runsPerK} <b class="calibre21">do</b>                                  <span class="fm-combinumeral">❺</span>
      (centroids, clusters) ← kmeans(points, k, maxIter)         <span class="fm-combinumeral">❻</span>
      M[i] ← intraClusterDistance(points, centroids, clusters)   <span class="fm-combinumeral">❼</span>
    results[k] ← min(M)                                          <span class="fm-combinumeral">❽</span>
  <b class="calibre21">return</b> results                                                 <span class="fm-combinumeral">❾</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1031493"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">kmeansElbow</code><a id="marker-1031497"></a> takes a list of points, a list of values to test for <code class="fm-code-in-text2">k</code> (aka <code class="fm-code-in-text2">numCentroids</code><a id="marker-1031499"></a>), the number of clusterings it should create, <code class="fm-code-in-text2">maxIter</code><a id="marker-1031500"></a>, the maximum number of iterations for k-means, and how many runs of the algorithm are needed for each value of <code class="fm-code-in-text2">k</code> tested. This function returns an associative array with the best value for the intra-cluster distance metric obtained for each number of centroids tested.</p>

  <p class="fm-code-annotation"><a id="pgfId-1031516"></a><span class="fm-combinumeral">❷</span> Initializes the associative array with the results</p>

  <p class="fm-code-annotation"><a id="pgfId-1031533"></a><span class="fm-combinumeral">❸</span> Cycles through all the values for <code class="fm-code-in-text2">k</code> to test</p>

  <p class="fm-code-annotation"><a id="pgfId-1031550"></a><span class="fm-combinumeral">❹</span> Initializes an array to keep track of the results (because we are running the algorithm several times for each value of <code class="fm-code-in-text2">k</code>)</p>

  <p class="fm-code-annotation"><a id="pgfId-1031567"></a><span class="fm-combinumeral">❺</span> Repeats <code class="fm-code-in-text2">runsPerK</code><a id="marker-1031571"></a> times</p>

  <p class="fm-code-annotation"><a id="pgfId-1031585"></a><span class="fm-combinumeral">❻</span> Runs k-means with the given input and hyper-parameters</p>

  <p class="fm-code-annotation"><a id="pgfId-1031602"></a><span class="fm-combinumeral">❼</span> Computes the intra-cluster distance metric on the output of the algorithm</p>

  <p class="fm-code-annotation"><a id="pgfId-1031619"></a><span class="fm-combinumeral">❽</span> Chooses the best result of the intra-cluster distance metric among all runs with the current value for the number of centroids</p>

  <p class="fm-code-annotation"><a id="pgfId-1031636"></a><span class="fm-combinumeral">❾</span> Returns the results (optionally you could save the clustering corresponding to the best metric value for each value of <code class="fm-code-in-text2">k</code> and return the list of clusterings as well)</p>

  <h3 class="fm-head2" id="heading_id_24"><a id="pgfId-1008234"></a>12.5.1 Interpreting the results</h3>

  <p class="body"><a id="pgfId-1008248"></a>To check whether the elbow method works, we have stored the results producing the best metric values for each value of <code class="fm-code-in-text">k</code> tested (the ones that are plotted in figure 12.25), and we can take a look at some of them in figure 12.26 to verify that our choice makes sense. And indeed, with <code class="fm-code-in-text">k=5</code> we would have too few centroids, and two clusters would be assigned to the same centroid (left chart, bottom-left corner), while with <code class="fm-code-in-text">k=7</code>, a single “natural” cluster gets split by two centroids (right chart, middle).</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch12_F26.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041501"></a>Figure 12.26 Best k-means clustering for our example dataset with <code class="fm-code-in-text">k</code> equals 5, 6, and 7. You can notice that 6 is indeed the ideal value, and the smallest value that has enough centroids for the dataset natural clusters.</p>

  <p class="body"><a id="pgfId-1008284"></a>Once we have established that six clusters are the best choice for our dataset, we can try to interpret the result of the clustering. We can see the top-right corner, for instance, that is made of customers with high annual income and who spend generously on the website. A bit to the right of these clusters, there is another interesting group: people who, despite a lower income, spend almost as much in monthly purchases as the wealthiest cluster. In the lower left corner, we can see two clusters of people with low incomes that also don’t spend a lot of money on e-commerce. These two groups can either be marketed to together, or further analysis can be performed to understand the differences and target products more appropriately to each group. Considering that they are bringing in limited income, though, the marketing section could rather ask the data science team to focus on the two clusters toward the center of the chart: middle-class customers that could be encouraged with targeted campaigns, and that can be asked to fill out surveys that will help the marketing team improve customers’ satisfaction.</p>

  <p class="body"><a id="pgfId-1008307"></a>We have seen just one example in action of how clustering can help your company thrive, but there are many, many more. Now it’s your turn to apply these powerful techniques to your <a id="marker-1009076"></a>data!</p>

  <h2 class="fm-head" id="heading_id_25"><a id="pgfId-1008322"></a>Summary</h2>

  <ul class="calibre19">
    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1008334"></a>Clustering<a class="calibre14" id="marker-1009080"></a> is the main application of unsupervised learning, used to make sense of unlabeled data, discovering patterns in raw data.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1008352"></a>Some applications of clustering are marketing segmentation, noise detection (for instance, in signals), and data preprocessing.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1008366"></a>The oldest clustering algorithm, k-means, is the easiest to implement, but also has limitations in the shape of the clusters. It can only spot convex clusters and can’t handle non-linearly separable data.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1008386"></a>A different approach to clustering, DBSCAN is about identifying groups based on the density of the points. It can handle any shape and non-linearly separable data, but it doesn’t work well with datasets with areas of heterogenous density. Moreover, it’s hard and unintuitive to choose the best values for the algorithm hyper-parameters.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1008406"></a>OPTICS, a newer method based on DBSCAN, builds a hierarchical clustering and allows us to handle datasets with varying density. It also makes it easier to choose the values of the parameters.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1008422"></a>To assess the quality of a clustering for a dataset, we can use evaluation metrics such as intra-cluster distance, inter-cluster distance, or total cohesion.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1008438"></a>The elbow method<a class="calibre14" id="marker-1009084"></a> is a tool that provides graphical feedback to choose the best values for the algorithms’ hyper-parameters.</p>
    </li>
  </ul>
  <hr class="calibre22"/>

  <p class="fm-footnote"><sup class="footnotenumber">1.</sup> <a id="pgfId-1009115"></a>That’s also the reason why every company, online or offline, tries to discover as much information about you as possible: data is paramount in this era.</p>

  <p class="fm-footnote"><sup class="footnotenumber">2.</sup> <a id="pgfId-1009129"></a><a id="id_Hlk56489334"></a>Generative Adversarial Networks are a particular type of deep neural networks, where two competing models are trained to generate artificial content (based on a training set) and to discriminate artificial from real content. Their co-evolution increases the lifelikeness of the content generated.</p>

  <p class="fm-footnote"><sup class="footnotenumber">3.</sup> <a id="pgfId-1009145"></a>The <code class="fm-code-in-text1">NP-Hard</code> class of problems includes those problems that are at least as hard as the hardest problems in <code class="fm-code-in-text1">NP</code>; as we have already seen in chapter 2, the <code class="fm-code-in-text1">NP</code> class<a id="marker-1020598"></a> contains those problems that can be solved in polynomial time on a non-deterministic machine, and in particular, the problems that are in <code class="fm-code-in-text1">NP</code> but not in class <code class="fm-code-in-text1">P</code> can’t be solved in polynomial time on a deterministic machine. To date, determining if there is any problem in <code class="fm-code-in-text1">NP-P</code> is one of greatest challenges in computer science.</p>

  <p class="fm-footnote"><sup class="footnotenumber">4.</sup> <a id="pgfId-1009177"></a>See <span class="fm-hyperlink"><a href="https://github.com/mlarocca/AlgorithmsAndDataStructuresInAction#k-means">https://github.com/mlarocca/AlgorithmsAndDataStructuresInAction#k-means</a></span>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">5.</sup> <a id="pgfId-1009193"></a>For instance, in Python you can import <code class="fm-code-in-text1">sample</code> from module <code class="fm-code-in-text1">random</code>, or in JavaScript you can use underscore library’s <code class="fm-code-in-text1">sample</code> method.</p>

  <p class="fm-footnote"><sup class="footnotenumber">6.</sup> <a id="pgfId-1009213"></a>But if you do, then you should check out chapter 11 of <i class="calibre17">Practical Probabilistic Programming</i> by Stuart Russell (Manning Publications, 2016), where it is neatly explained.</p>

  <p class="fm-footnote"><sup class="footnotenumber">7.</sup> <a id="pgfId-1009233"></a>In a <code class="fm-code-in-text1">d</code>-dimensional space, two sets <code class="fm-code-in-text1">S<sub class="subscript2">1</sub></code> and <code class="fm-code-in-text1">S<sub class="subscript2">2</sub></code> are linearly separable if there exists at least one <code class="fm-code-in-text1">(d-1)</code>-dimensional hyperplane that divides the space such that all points of <code class="fm-code-in-text1">S<sub class="subscript2">1</sub></code> are on one side of the hyperplane, and all points of <code class="fm-code-in-text1">S<sub class="subscript2">2</sub></code> are on the other side of it.</p>

  <p class="fm-footnote"><sup class="footnotenumber">8.</sup> <a id="pgfId-1009266"></a>As we have seen, k-means can discard some centroids, but this effect is very limited and unpredictable.</p>

  <p class="fm-footnote"><sup class="footnotenumber">9.</sup> <a id="pgfId-1009280"></a>Beyer, Kevin, et al. “When is “nearest neighbor” meaningful?” International conference on database theory. Springer, Berlin, Heidelberg, 1999.</p>

  <p class="fm-footnote"><sup class="footnotenumber">10.</sup> <a id="pgfId-1009294"></a>On GPUs and processors designed for vector computing, operations can be executed simultaneously on all coordinates of the <code class="fm-code-in-text1">d</code>-dimensional tuples with higher efficiency.</p>

  <p class="fm-footnote"><sup class="footnotenumber">11.</sup> <a id="pgfId-1009311"></a>See <span class="fm-hyperlink"><a href="http://mng.bz/A0x7">http://mng.bz/A0x7</a></span>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">12.</sup> <a id="pgfId-1009327"></a>See <span class="fm-hyperlink"><a href="http://mng.bz/KM5g">http://mng.bz/KM5g</a></span>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">13.</sup> <a id="pgfId-1009343"></a>Ester, Martin, et al. “A density-based algorithm for discovering clusters in large spatial databases with noise.” Kdd. Vol. 96. No. 34. 1996. The approach used is closely related to another paper from 1972: Ling, Robert F. “On the theory and construction of k-clusters.” <i class="calibre17">The Computer Journal</i> 15.4 (1972): 326-332.</p>

  <p class="fm-footnote"><sup class="footnotenumber">14.</sup> <a id="pgfId-1009361"></a>Technically, if two runs of DBSCAN cycle through the points in the same order, then the final result will be exactly the same. For k-means to be considered deterministic, we should instead replace the random initialization with a deterministic one.</p>

  <p class="fm-footnote"><sup class="footnotenumber">15.</sup> <a id="pgfId-1009375"></a>Single-linkage clustering (SLC) is a class of bottom-up hierarchical clustering algorithms where, at each step, the pair of clusters at minimum distance is merged (initially every point is in its own cluster).</p>

  <p class="fm-footnote"><sup class="footnotenumber">16.</sup> <a id="pgfId-1009389"></a><i class="calibre17">Reductio ad absurdum</i>, from Latin “reduction to absurdity” is a logical argument that proves a statement by showing that, if false, it would lead to an impossible result.</p>

  <p class="fm-footnote"><sup class="footnotenumber">17.</sup> <a id="pgfId-1009409"></a>Some of the neighbors of the core point could have already been merged.</p>

  <p class="fm-footnote"><sup class="footnotenumber">18.</sup> <a id="pgfId-1009424"></a>As we have seen in the previous sections, the cost function that k-means minimizes is the Euclidean distance to centroids (and, indirectly, the in-cluster Euclidean distance).</p>

  <p class="fm-footnote"><sup class="footnotenumber">19.</sup> <a id="pgfId-1009437"></a>See <span class="fm-hyperlink"><a href="https://github.com/mlarocca/AlgorithmsAndDataStructuresInAction#dbscan">https://github.com/mlarocca/AlgorithmsAndDataStructuresInAction#dbscan</a></span>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">20.</sup> <a id="pgfId-1009453"></a>See <span class="fm-hyperlink"><a href="http://mng.bz/ZPza">http://mng.bz/ZPza</a></span>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">21.</sup> <a id="pgfId-1009467"></a>See <span class="fm-hyperlink"><a href="http://mng.bz/RXEO">http://mng.bz/RXEO</a></span>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">22.</sup> <a id="pgfId-1009483"></a>The running time of these queries, as we saw in chapter 9, can’t be upper-bounded by anything better than <code class="fm-code-in-text1">O(n)</code> in the worst case. However, the running time of range queries on hyper-spheres depends on the radius of the sphere, and under certain assumptions on the value of <span class="cambria">ϵ</span>, <code class="fm-code-in-text1">O(log(n))</code> can be an accurate estimate for the average running time.</p>

  <p class="fm-footnote"><sup class="footnotenumber">23.</sup> <a id="pgfId-1009506"></a>In machine learning, the parameters passed to an algorithm, and fixed before learning starts, are often called hyperparameters to distinguish them from the parameters of the model (for instance, the weights of a neural network) produced by the algorithm.</p>

  <p class="fm-footnote"><sup class="footnotenumber">24.</sup> <a id="pgfId-1009518"></a>Ankerst, Mihael, et al. “OPTICS: ordering points to identify the clustering structure.” ACM Sigmod record. Vol. 28. No. 2. ACM, 1999.</p>

  <p class="fm-footnote"><sup class="footnotenumber">25.</sup> <a id="pgfId-1009534"></a>Here we adopt the definition of “neighbor of a cluster” meaning a point that is a neighbor (and hence reachable) of any point currently in the cluster.</p>

  <p class="fm-footnote"><sup class="footnotenumber">26.</sup> <a id="pgfId-1009548"></a>Assuming we consider <code class="fm-code-in-text1">p</code> as its own neighbor . . . technically, since we agreed that for <code class="fm-code-in-text1">p</code> to be a core point there needs to be <code class="fm-code-in-text1">M</code> points within a radius <span class="cambria">ϵ</span> from <code class="fm-code-in-text1">p</code>, to compute the core distance we take the distance of its <code class="fm-code-in-text1">(M-1)-th</code> neighbor.</p>

  <p class="fm-footnote"><sup class="footnotenumber">27.</sup> <a id="pgfId-1009578"></a>Technically, as we have noted, it simultaneously grows an infinite number of clusters for all the possible values of density allowed, but for the sake of simplicity, we will focus on a single cluster in the description, the one that would be obtained with <span class="cambria">ϵ</span>=<code class="fm-code-in-text1"><span class="cambria">ϵ</span><sub class="subscript2">MAX</sub></code>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">28.</sup> <a id="pgfId-1009597"></a>See <span class="fm-hyperlink"><a href="http://mng.bz/j4V8">http://mng.bz/j4V8</a></span>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">29.</sup> <a id="pgfId-1009613"></a>Also, by using a Fibonacci heap, the “update priority” method, lowering priorities, would only require an amortized <code class="fm-code-in-text1">O(1)</code> running time, so even a linearithmic number of calls would at most require <code class="fm-code-in-text1">O(n*log(n))</code> total time.</p>

  <p class="fm-footnote"><sup class="footnotenumber">30.</sup> <a id="pgfId-1009632"></a>Achtert, Elke, Christian Böhm, and Peer Kröger. “DeLi-Clu: boosting robustness, completeness, usability, and efficiency of hierarchical clustering by a closest pair ranking.” Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, Berlin, Heidelberg, 2006.</p>

  <p class="fm-footnote"><sup class="footnotenumber">31.</sup> <a id="pgfId-1009646"></a>This is called z-score normalization and has the advantage of producing data with zero-mean and unit-variance. There are other ways to perform feature scaling; to learn more take a look, for instance, at chapter 2 of <i class="calibre17">Machine Learning in Action</i>, by Peter Harrington (Manning Publications, 2012).</p>

  <p class="fm-footnote"><sup class="footnotenumber">32.</sup> <a id="pgfId-1009663"></a>If you are optimizing the choice for the current dataset, picking the best result makes sense. If you’ll have to run the algorithm on several datasets with similar characteristics, then you might need to find a value that can generalize well, and using the median could be your best bet.</p>
</body>
</html>
