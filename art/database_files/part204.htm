<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>10.2  Big Data Storage Systems</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part203.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part205.htm">下一个 &gt;</a></p><p class="s65" style="padding-left: 40pt;text-indent: 0pt;text-align: left;">10.2  <span style=" color: #00AEEF;">Big Data Storage Systems</span></p><p style="padding-top: 12pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Applications on Big Data have extremely high scalability requirements. Popular appli- cations have hundreds of millions of users, and many applications have seen their load increase many-fold within a single year, or even within a few months. To handle the data management needs of such applications, data must be stored partitioned across thousands of computing and storage nodes.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">A number of systems for Big Data storage have been developed and deployed over the past two decades to address the data management requirements of such applica- tions. These include the following:</p><p class="s39" style="padding-top: 8pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s46">Distributed File Systems</span><span class="p">. These allow ﬁles to be stored across a number of ma- chines, while allowing access to ﬁles using a traditional ﬁle-system interface. Dis- tributed ﬁle systems are used to store large ﬁles, such as log ﬁles. They are also used as a storage layer for systems that support storage of records.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s39" style="padding-top: 4pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s46">Sharding across multiple databases. </span><span class="s13">Sharding </span><span class="p">refers to the process of partition- ing of records across multiple systems; in other words, the records are divided up among the systems. A typical use case for sharding is to partition records corre- sponding to diﬀerent users across a collection of databases. Each database is a traditional centralized database, which may not have any information about the other databases. It is the job of client software to keep track of how records are partitioned, and to send each query to the appropriate database.</span></p><p style="padding-top: 3pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><b>Key-Value Storage Systems</b>. These allow records to be stored and retrieved based on a key, and may additionally provide limited query facilities. However, they are not full-ﬂedged database systems; they are sometimes called No<span class="s44">SQL </span>systems, since such storage systems typically do not support the <span class="s44">SQL </span>language.</p><p class="s39" style="padding-top: 3pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s46">Parallel and Distributed Databases</span><span class="p">. These provide a traditional database interface but store data across multiple machines, and they perform query processing in parallel across multiple machines.</span></p><p style="padding-top: 2pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Parallel and distributed database storage systems, including distributed ﬁle systems and key-value stores, are described in detail in Chapter 21. We provide a user-level overview of these Big Data storage systems in this section.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">10.2.1 Distributed File Systems</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">A <span class="s63">distributed ﬁle system </span>stores ﬁles across a large collection of machines while giving a single-ﬁle-system view to clients. As with any ﬁle system, there is a system of ﬁle names and directories, which clients can use to identify and access ﬁles. Clients do not need to bother about where the ﬁles are stored. Such distributed ﬁle systems can store very large amounts of data, and support very large numbers of concurrent clients. Such systems are ideal for storing unstructured data, such as web pages, web server logs, images, and so on, that are stored as large ﬁles.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">A landmark system in this context was the Google File System (GFS), developed in the early 2000s, which saw widespread use within Google. The open-source Hadoop File System (HDFS) is based on the GFS architecture and is now very widely used.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Distributed ﬁle systems are designed for eﬃcient storage of large ﬁles, whose sizes range from tens of megabytes to hundreds of gigabytes or more.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The data in a distributed ﬁle system is stored across a number of machines. Files are broken up into multiple blocks. The blocks of a single ﬁle can be partitioned across mul- tiple machines. Further, each ﬁle block is replicated across multiple (typically three) machines, so that a machine failure does not result in the ﬁle becoming inaccessible.</p><p style="padding-left: 137pt;text-indent: 0pt;text-align: justify;">File systems, whether centralized or distributed, typically support the following:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s39" style="padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s40">A directory system, which allows a hierarchical organization of ﬁles into directo- ries and subdirectories.</span></p><p class="s39" style="padding-top: 4pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s40">A mapping from a ﬁle name to the sequence of identiﬁers of blocks that store the actual data in each ﬁle.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s39" style="padding-top: 4pt;padding-left: 88pt;text-indent: 3pt;text-align: left;">• <span class="s40">The ability to store and retrieve data to/from a block with a speciﬁed identiﬁer. In the case of a centralized ﬁle system, the block identiﬁers help locate blocks in a storage device such as a disk. In the case of a distributed ﬁle system, in addition to pro- viding a block identiﬁer, the ﬁle system must provide the location (machine identiﬁer) where the block is stored; in fact, due to replication, the ﬁle system provides a set of machine identiﬁers along with each block identiﬁer.</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="4" alt="image" src="Image_2315.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="4" alt="image" src="Image_2316.png"/></span></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: left;">Figure 10.1 shows the architecture of the Hadoop File System (<span class="s44">HDFS</span>), which is derived from the architecture of the Google File System (<span class="s44">GFS</span>). The core of <span class="s44">HDFS </span>is</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="422" height="493" alt="image" src="Image_2317.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="4" alt="image" src="Image_2318.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="4" alt="image" src="Image_2319.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="4" alt="image" src="Image_2320.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="4" alt="image" src="Image_2321.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="61" height="51" alt="image" src="Image_2322.png"/></span></p><p class="s33" style="text-indent: 0pt;line-height: 10pt;text-align: left;">Metadata Ops</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="text-indent: 0pt;line-height: 10pt;text-align: left;">Client</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="text-indent: 0pt;line-height: 10pt;text-align: left;">Block Read</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="text-indent: 0pt;line-height: 10pt;text-align: left;">DataNodes</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="text-indent: 0pt;line-height: 10pt;text-align: left;">Blocks</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="text-indent: 0pt;line-height: 10pt;text-align: left;">Client</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="text-indent: 0pt;line-height: 10pt;text-align: left;">Block Write</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="text-indent: 0pt;line-height: 10pt;text-align: left;">Replication</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;text-align: center;">BackupNode</p><p class="s119" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: center;">Metadata (name, replicas, ..)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s266" style="text-indent: 0pt;line-height: 2pt;text-align: left;"><span><img width="3" height="3" alt="image" src="Image_2323.png"/></span>	<span><img width="3" height="3" alt="image" src="Image_2324.png"/></span></p><p class="s33" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: center;">NameNode</p><p class="s119" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: center;">Metadata (name, replicas, ..)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="padding-top: 2pt;padding-left: 225pt;text-indent: 0pt;text-align: left;">Rack 1               Rack 2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 59pt;text-indent: 0pt;text-align: center;">Figure 10.1 <span class="s74">Hadoop Distributed File System (HDFS) architecture.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">a server running a machine referred to as the <span class="s63">NameNode</span>. All ﬁle system requests are sent to the NameNode. A ﬁle system client program that wants to read an existing ﬁle sends the ﬁle name (which can be a path, such as <span class="s49">/home/avi/book/ch10</span>) to the NameNode. The NameNode stores a list of block identiﬁers of the blocks in each ﬁle; for each block identiﬁer, the NameNode also stores the identiﬁers of machines that store copies of that block. The machines that store data blocks in HDFS are called <span class="s63">DataNodes</span>.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">For a ﬁle read request, the <span class="s44">HDFS </span>server sends back a list of block identiﬁers of the blocks in the ﬁle and the identiﬁers of the machines that contain each block. Each block is then fetched from one of the machines that store a copy of the block.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">For a ﬁle write, the <span class="s44">HDFS </span>server creates new block identiﬁers and assigns each block identiﬁer to several (usually three) machines, and returns the block identiﬁers and machine assignment to the client. The client then sends the block identiﬁers and block data to the assigned machines, which store the data.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Files can be accessed by programs by using <span class="s44">HDFS </span>ﬁle system <span class="s44">API</span>s that are available in multiple languages, such as Java and Python; the <span class="s44">API</span>s allow a program to connect to the <span class="s44">HDFS </span>server and access data.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">An <span class="s44">HDFS </span>distributed ﬁle system can also be connected to the local ﬁle system of a machine in such a way that ﬁles in <span class="s44">HDFS </span>can be accessed as though they are stored locally. This requires providing the address of the NameNode machine, and the port on which the <span class="s44">HDFS </span>server listens for requests, to the local ﬁle system. The local ﬁle system recognizes which ﬁle accesses are to ﬁles in <span class="s44">HDFS </span>based on the ﬁle path, and sends appropriate requests to the <span class="s44">HDFS </span>server.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">More details about distributed ﬁle system implementation may be found in Section 21.6.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">10.2.2 Sharding</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">A single database system typically has suﬃcient storage and performance to handle all the transaction processing needs of an enterprise. However, using a single database is not suﬃcient for applications with millions or even billions of users, including social- media or similar web-scale applications, but also the user-facing applications of very large organizations such as large banks.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Suppose an organization has built an application with a centralized database, but needs to scale to handle more users, and the centralized database is not capable of handling the storage or processing speed requirements. A commonly used way to deal with such a situation is to partition the data across multiple databases, with a subset of users assigned to each of the databases. The term <span class="s63">sharding </span>refers to the partitioning of data across multiple databases or machines.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Partitioning is usually done on one or more attributes, referred to as <i>partitioning attributes</i>, <i>partitioning keys</i>, or <i>shard keys</i>. User or account identiﬁers are commonly used as partitioning keys. Partitioning can be done by deﬁning a range of keys that each of the databases handles; for example, keys from 1 to 100,000 may be assigned to the</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">ﬁrst database, keys from 100,001 to 200,000 to the second database, and so on. Such partitioning is called <i>range partitioning</i>. Partitioning may also be done by computing a hash function that maps a key value to a partition number; such partitioning is called <i>hash partitioning</i>. We study partitioning of data in detail in Chapter 21.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">When sharding is done in application code, the application must keep track of which keys are stored on which database, and must route queries to the appropriate database. Queries that read or update data from multiple databases cannot be pro- cessed in a simple manner, since it is not possible to submit a single query that gets executed across all the databases. Instead, the application would need to read data from multiple databases and compute the ﬁnal query result. Updates across databases cause further issues, which we discuss in Section 10.2.5.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">While sharding performed by modifying application code provided a simple way to scale applications, the limitations of the approach soon became apparent. First, the application code has to track how data was partitioned and route queries appropriately. If a database becomes overloaded, parts of the data in that database have to be oﬄoaded to a new database, or to one of the other existing databases; managing this process is a non-trivial task. As more databases are added, there is a greater chance of failure leading to loss of access to data. Replication is needed to ensure data is accessible despite failures, but managing the replicas, and ensuring they are consistent, poses further challenges. Key-value stores, which we study next, address some of these issues. Challenges related to consistency and availability are discussed later, in Section 10.2.5.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">10.2.3 Key-Value Storage Systems</p><p style="padding-top: 6pt;padding-left: 89pt;text-indent: 0pt;text-align: right;">Many web applications need to store very large numbers (many billions or in extreme cases, trillions) of relatively small records (of size ranging from a few kilobytes to a few megabytes). Storing each record as a separate ﬁle is infeasible, since ﬁle systems, including distributed ﬁle systems, are not designed to store such large numbers of ﬁles. Ideally, a massively parallel relational database should be used to store such data.</p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">However, it is not easy to build relational database systems that can run in parallel across a large number of machines while also supporting standard database features such as foreign-key constraints and transactions.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">A number of storage systems have been developed that can scale to the needs of web applications and store large amounts of data, scaling to thousands to tens of thousands of machines, but typically oﬀering only a simple key-value storage interface. A <span class="s63">key-value storage system </span>(or <span class="s63">key-value store</span>) is a system that provides a way to store or update a record (value) with an associated key and to retrieve the record with a given key.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Parallel key-value stores partition keys across multiple machines, and route updates and lookups to the correct machine. They also support replication, and ensure that replicas are kept consistent. Further, they provide the ability to add more machines to a system when required, and ensure that the load is automatically balanced across the machines in a system In contrast to systems that implement sharding in the application</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">code, systems that use a parallel key-value store do not need to worry about any of the above issues. Parallel key-value stores are therefore more widely used than sharding today.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Widely used parallel key-value stores include Bigtable from Google, Apache HBase, Dynamo from Amazon, Cassandra from Facebook, MongoDB, Azure cloud storage from Microsoft, and Sherpa/<span class="s44">PNUTS </span>from Yahoo!, among many others.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">While several key-value data stores view the values stored in the data store as an un- interpreted sequence of bytes, and do not look at their content, other data stores allow some form of structure or schema to be associated with each record. Several such key- value storage systems require the stored data to follow a speciﬁed data representation, allowing the data store to interpret the stored values and execute simple queries based on stored values. Such data stores are called <span class="s63">document stores</span>. MongoDB is a widely used data store that accepts values in the <span class="s44">JSON </span>format.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Key-value storage systems are, at their core, based on two primitive functions, <span class="s49">put(key, value)</span>, used to store values with an associated key, and <span class="s49">get(key)</span>, used to re- trieve the stored value associated with the speciﬁed key. Some systems, such as Bigtable, additionally provide range queries on key values. Document stores additionally support limited forms of querying on the data values.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">An important motivation for the use of key-value stores is their ability to handle very large amounts of data as well as queries, by distributing the work across a <i>cluster </i>consisting of a large number of machines. Records are partitioned (divided up) among the machines in the cluster, with each machine storing a subset of the records and processing lookups and updates on those records.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Note that key-value stores are not full-ﬂedged databases, since they do not provide many of the features that are viewed as standard on database systems today. Key-value stores typically do not support declarative querying (using <span class="s44">SQL </span>or any other declarative query language) and do not support transactions (which, as we shall see in Chapter 17, allow multiple updates to be committed atomically to ensure that the database state remains consistent despite failures, and control concurrent access to data to ensure that problems do not arise due to concurrent access by multiple transactions). Key- value stores also typically do not support retrieval of records based on selections on non-key attributes, although some document stores do support such retrieval.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">An important reason for not supporting such features is that some of them are not easy to support on very large clusters; thus, most systems sacriﬁce these features in order to achieve scalability. Applications that need scalability may be willing to sacriﬁce these features in exchange for scalability.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Key-value stores are also called <span class="s63">No</span><span class="s82">SQL </span>systems, to emphasize that they do not support <span class="s44">SQL</span>, and the lack of support for <span class="s44">SQL </span>was initially viewed as something positive, rather than a limitation. However, it soon became clear that lack of database features such as transaction support and support for <span class="s44">SQL</span>, make application development more complicated. Thus, many key-value stores have evolved to support features, such as the <span class="s44">SQL </span>language and transactions.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="494" height="1" alt="image" src="Image_2325.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s151" style="padding-top: 4pt;padding-left: 115pt;text-indent: 0pt;text-align: left;">show dbs // Shows available databases</p><p class="s151" style="padding-left: 115pt;text-indent: 0pt;text-align: left;">use sampledb // Use database sampledb, creating it if it does not exist db.createCollection(&quot;student&quot;) // Create a collection db.createCollection(&quot;instructor&quot;)</p><p class="s151" style="padding-left: 115pt;text-indent: 0pt;line-height: 10pt;text-align: left;">show collections // Shows all collections in the database</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s151" style="padding-left: 115pt;text-indent: 0pt;text-align: left;">db.student.insert({ &quot;id&quot; : &quot;00128&quot;, &quot;name&quot; : &quot;Zhang&quot;,</p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="1" alt="image" src="Image_2326.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="1" alt="image" src="Image_2327.png"/></span></p><p class="s151" style="padding-left: 134pt;text-indent: 0pt;text-align: left;">&quot;dept name&quot; : &quot;Comp. Sci.&quot;, &quot;tot cred&quot; : 102, &quot;advisors&quot; : [&quot;45565&quot;] })</p><p class="s151" style="padding-left: 115pt;text-indent: 0pt;text-align: left;">db.student.insert({ &quot;id&quot; : &quot;12345&quot;, &quot;name&quot; : &quot;Shankar&quot;,</p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="1" alt="image" src="Image_2328.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="1" alt="image" src="Image_2329.png"/></span></p><p class="s151" style="padding-left: 134pt;text-indent: 0pt;text-align: left;">&quot;dept name&quot; : &quot;Comp. Sci.&quot;, &quot;tot cred&quot; : 32, &quot;advisors&quot; : [&quot;45565&quot;] })</p><p class="s151" style="padding-left: 115pt;text-indent: 0pt;text-align: left;">db.student.insert({ &quot;id&quot; : &quot;19991&quot;, &quot;name&quot; : &quot;Brandt&quot;,</p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="1" alt="image" src="Image_2330.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="1" alt="image" src="Image_2331.png"/></span></p><p class="s151" style="padding-left: 134pt;text-indent: 0pt;text-align: left;">&quot;dept name&quot; : &quot;History&quot;, &quot;tot cred&quot;: 80, &quot;advisors&quot;: [] })</p><p class="s151" style="padding-left: 115pt;text-indent: 0pt;text-align: left;">db.instructor.insert({ &quot;id&quot; : &quot;45565&quot;, &quot;name&quot; : &quot;Katz&quot;,</p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="1" alt="image" src="Image_2332.png"/></span></p><p class="s151" style="padding-left: 134pt;text-indent: 0pt;text-align: left;">&quot;dept name&quot; : &quot;Comp. Sci.&quot;, &quot;salary&quot; : 75000,</p><p class="s151" style="padding-left: 134pt;text-indent: 0pt;text-align: left;">&quot;advisees&quot; : [&quot;00128&quot;,&quot;12345&quot;] })</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s151" style="padding-left: 115pt;text-indent: 0pt;text-align: left;">db.student.find() // Fetch all students in JSON format db.student.findOne({&quot;ID&quot;: &quot;00128&quot;}) // Find one matching student</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="1" alt="image" src="Image_2333.png"/></span></p><p class="s151" style="padding-left: 115pt;text-indent: 0pt;text-align: left;">db.student.remove({&quot;dept name&quot;: &quot;Comp. Sci.&quot;}) // Delete matching students db.student.drop() // Drops the entire collection</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="494" height="1" alt="image" src="Image_2334.png"/></span></p><p class="s73" style="padding-top: 8pt;padding-left: 59pt;text-indent: 0pt;text-align: center;">Figure 10.2 <span class="s74">MongoDB shell commands.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The <span class="s44">API</span>s provided by these systems to store and access data are widely used. While the basic <span class="s49">get() </span>and <span class="s49">put() </span>functions mentioned earlier are straightforward, most systems support further features. As an example of such <span class="s44">API</span>s, we provide a brief overview of the MongoDB <span class="s44">API</span>.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Figure 10.2 illustrates access to the MongoDB document store through a JavaScript shell interface. Such a shell can be opened by executing the mongo com- mand on a system that has MongoDB installed and conﬁgured. MongoDB also pro- vides equivalent <span class="s44">API </span>functions in a variety of languages, including Java and Python. The <span class="s49">use </span>command shown in the ﬁgure opens the speciﬁed database, creating it if it does not already exist. The <span class="s49">db.createCollection() </span>command is used to create collec- tions, which store <i>documents</i>; a document in MongoDB is basically a <span class="s44">JSON </span>object. The code in the ﬁgure creates two collections, <span class="s49">student </span>and <span class="s49">instructor</span>, and inserts <span class="s44">JSON </span>objects representing students and instructors into the two collections.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="1" alt="image" src="Image_2335.png"/></span></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">MongoDB automatically creates identiﬁers for the inserted objects, which can be used as keys to retrieve the objects. The key associated with an object can be fetched using the <span class="s49">id </span>attribute, and an index on this attribute is created by default.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">MongoDB also supports queries based on the stored values. The <span class="s49">db.student.find() </span>function returns a collection of all objects in the <span class="s49">student </span>collection, while the <span class="s49">find- One() </span>function returns one object from the collection. Both functions can take as argu- ment a <span class="s44">JSON </span>object that speciﬁes a selection on desired attributes. In our example, the</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">student with <span class="s44">ID </span>00128 is retrieved. Similarly, all objects matching such a selection can be deleted by the <span class="s49">remove() </span>function shown in the ﬁgure. The <span class="s49">drop() </span>function shown in the ﬁgure drops an entire collection.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">MongoDB supports a variety of other features such as creation of indices on spec- iﬁed attributes of the stored <span class="s44">JSON </span>objects, such as the <span class="s49">ID </span>and <span class="s49">name </span>attributes.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: right;">Since a key goal of MongoDB is to enable scaling to very large data sizes and query/update loads, MongoDB allows multiple machines to be part of a single Mon- goDB cluster. Data are then sharded (partitioned) across these machines. We study partitioning of data across machines in detail in Chapter 21, and we study parallel pro- cessing of queries in detail in Chapter 22. However we outline key ideas in this section.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="1" alt="image" src="Image_2336.png"/></span></p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">In MongoDB (as in many other databases), partitioning is done based on the value of a speciﬁed attribute, called the <span class="s63">partitioning attribute </span>or <span class="s63">shard key</span>. For example, if we specify that the <span class="s49">student </span>collection should be partitioned on the <span class="s49">dept name </span>attribute, all objects of a particular department are stored on one machine, but objects of diﬀerent departments may be stored on diﬀerent machines. To ensure data can be accessed even if a machine has failed, each partition is replicated on multiple machines. This way, even if one machine fails, the data in that partition can be fetched from another machine.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Requests from a MongoDB client are sent to a router, which then forwards requests to the appropriate partitions in a cluster.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Bigtable is another key-value store that requires data values to follow a format that allows the storage system access to individual parts of a stored value. In Bigtable, data values (records) can have multiple attributes; the set of attribute names is not prede- termined and can vary across diﬀerent records. Thus, the key for an attribute value conceptually consists of (record-identiﬁer, attribute-name). Each attribute value is just a string as far as Bigtable is concerned. To fetch all attributes of a record, a range query, or more precisely a preﬁx-match query consisting of just the record identiﬁer, is used. The <span class="s49">get() </span>function returns the attribute names along with the values. For eﬃcient re- trieval of all attributes of a record, the storage system stores entries sorted by the key, so all attribute values of a particular record are clustered together.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">In fact, the record identiﬁer can itself be structured hierarchically, although to Bigtable itself the record identiﬁer is just a string. For example, an application that stores pages retrieved from a web crawl could map a <span class="s44">URL </span>of the form:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 43pt;text-indent: 0pt;text-align: center;"><a href="http://www.cs.yale.edu/people/silberschatz.html" class="s56">www.cs.yale.edu/people/silberschatz.html</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">to the record identiﬁer:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s49" style="padding-left: 38pt;text-indent: 0pt;text-align: center;">edu.yale.cs.www/people/silberschatz.html</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">With this representation, all <span class="s44">URL</span>s of <span class="s49">cs.yale.edu </span>can be retried by a query that fetches all keys with the preﬁx <span class="s49">edu.yale.cs</span>, which would be stored in a consecutive range of key values in the sorted key order. Similarly, all <span class="s44">URL</span>s of <span class="s49">yale.edu </span>would have a preﬁx of <span class="s49">edu.yale </span>and would be stored in a consecutive range of key values.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 17pt;text-align: left;">Although Bigtable does not support <span class="s44">JSON </span>natively, <span class="s44">JSON </span>data can be mapped to the data model of Bigtable. For example, consider the following <span class="s44">JSON </span>data:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s49" style="padding-left: 141pt;text-indent: 0pt;text-align: left;">{ &quot;ID&quot;: &quot;22222&quot;,</p><p class="s49" style="padding-left: 156pt;text-indent: 0pt;text-align: left;">&quot;name&quot;: { &quot;firstname: &quot;Albert&quot;, &quot;lastname: &quot;Einstein&quot; }, &quot;deptname&quot;: &quot;Physics&quot;,</p><p class="s49" style="padding-left: 156pt;text-indent: 0pt;line-height: 12pt;text-align: left;">&quot;children&quot;: [</p><p class="s49" style="padding-left: 171pt;text-indent: 0pt;text-align: left;">{&quot;firstname&quot;: &quot;Hans&quot;, &quot;lastname&quot;: &quot;Einstein&quot; },</p><p class="s49" style="padding-left: 171pt;text-indent: 0pt;text-align: left;">{&quot;firstname&quot;: &quot;Eduard&quot;, &quot;lastname&quot;: &quot;Einstein&quot; } ]</p><p class="s49" style="padding-left: 141pt;text-indent: 0pt;text-align: left;">}</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">The above data can be represented by a Bigtable record with identiﬁer “22222”, with multiple attribute names such as “name.ﬁrstname”, “deptname”, “chil- dren[1].ﬁrstname” or “children[2].lastname”.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Further, a single instance of Bigtable can store data for multiple applications, with multiple tables per application, by simply preﬁxing the application name and table name to the record identiﬁer.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Many data-storage systems allow multiple versions of data items to be stored. Ver- sions are often identiﬁed by timestamp, but they may be alternatively identiﬁed by an integer value that is incremented whenever a new version of a data item is created. Lookups can specify the required version of a data item or can pick the version with the highest version number. In Bigtable, for example, a key actually consists of three parts: (record-identiﬁer, attribute-name, timestamp). Bigtable can be accessed as a ser- vice from Google. The open-source version of Bigtable, HBase, is widely used.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">10.2.4 Parallel and Distributed Databases</p><p class="s63" style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Parallel databases <span class="p">are databases that run on multiple machines (together referred to as a cluster) and are designed to store data across multiple machines and to process large queries using multiple machines. Parallel databases were initially developed in the 1980s, and thus they predate the modern generation of Big Data systems. From a programmer viewpoint, parallel databases can be used just like databases running on a single machine.</span></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Early generation parallel databases designed for transaction processing supported only a few machines in a cluster, while those designed to process large analytical queries were designed to support tens to hundreds of machines. Data are replicated across multiple machines in a cluster, to ensure that data are not lost, and they continue to be accessible, even if a machine in a cluster fails. Although failures do occur and need to be dealt with, failures during the processing of a query are not common in systems with tens to hundreds of machines. If a query was being processed on a node that failed, the query is simply restarted, using replicas of data that are on other nodes.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">If such database systems are run on clusters with thousands of machines, the prob- ability of failure during execution of a query increases signiﬁcantly for queries that pro- cess a large amount of data and consequently run for a long time. Restarting a query in</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 121pt;text-indent: 0pt;text-align: right;">the event of a failure is no longer an option, since there is a fairly high probability that a failure will happen yet again while the query is executing. Techniques to avoid complete restart, allowing only computation on the failed machines to be redone, were developed in the context of <i>map-reduce </i>systems, which we study in Section 10.3. However, these techniques introduce signiﬁcant overhead; given the fact that computation spanning thousands to tens of thousands of nodes is needed only by some exceptionally large applications, even today most parallel relational database systems target applications that run on tens to hundreds of machines and just restart queries in the event of failure. Query processing in such parallel and distributed databases is covered in detail in</p><p style="padding-left: 87pt;text-indent: 0pt;text-align: right;">Chapter 22, while transaction processing in such databases is covered in Chapter 23.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 7pt;padding-left: 119pt;text-indent: 0pt;text-align: left;">10.2.5 Replication and Consistency</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Replication is key to ensuring availability of data, ensuring a data item can be accessed despite failure of some of the machines storing the data item. Any update to a data item must be applied to all replicas of the data item. As long as all the machines containing the replicas are up and connected to each other, applying the update to all replicas is straightforward.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">However, since machines do fail, there are two key problems. The ﬁrst is how to ensure atomic execution of a transaction that updates data at more than one machine: the transaction execution is said to be atomic if despite failures, either all the data items updated by the transaction are successfully updated, or all the data items are reverted back to their original values. The second problem is, how to perform updates on a data item that has been replicated, when some of the replicas of the data item are on a machine that has failed. A key requirement here is <i>consistency</i>, that is, all live replicas of a data item have the same value, and each read sees the latest version of the data item. There are several possible solutions, which oﬀer diﬀerent degrees of resilience to failures. We study solutions to the both these problems in Chapter 23.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">We note that the solutions to the second problem typically require that a majority of the replicas are available for reading and update. If we had 3 replicas, this would require not more than 1 fail, but if we had 5 replicas, even if two machines fail we would still have a majority of replicas available. Under these assumptions, writes will not get blocked, and reads will see the latest value for any data item.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">While the probability of multiple machines failing is relatively low, network link failures can cause further problems. In particular, a <i>network partition </i>is said to occur if two live machines in a network are unable to communicate with each other.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">It has been shown that no protocol can ensure <i>availability</i>, that is, the ability to read and write data, while also guaranteeing consistency, in the presence of network partitions. Thus, distributed systems need to make tradeoﬀs: if they want high availabil- ity, they need to sacriﬁce consistency, for example by allowing reads to see old values of data items, or to allow diﬀerent replicas to have diﬀerent values. In the latter case, how to bring the replicas to a common value by merging the updates is a task that the</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="493" height="616" alt="image" src="Image_2337.png"/></span></p><p class="s73" style="padding-left: 174pt;text-indent: 0pt;text-align: justify;">Note 10.1 <span class="s146">Building Scalable Database Applications</span></p><p style="padding-top: 2pt;padding-left: 97pt;text-indent: 1pt;text-align: justify;">When faced with the task of creating a database application that can scale to a very large number of users, application developers typically have to choose between a database system that runs on a single server, and a key-value store that can scale by running on a large number of servers. A database that supports <span class="s44">SQL </span>and atomic transactions, and at the same time is highly scalable, would be ideal; as of 2018, Google Cloud Spanner, which is only available on the cloud, and the recently developed open source database CockroachDB are the only such databases.</p><p style="padding-left: 97pt;text-indent: 17pt;text-align: justify;">Simple applications can be written using only key-value stores, but more com- plex applications beneﬁt greatly from having <span class="s44">SQL </span>support. Application developers therefore typically use a combination of parallel key-value stores and databases.</p><p style="padding-left: 97pt;text-indent: 17pt;text-align: justify;">Some relations, such as those that store user account and user proﬁle data are queried frequently, but with simple select queries on a key, typically on the user identiﬁer. Such relations are stored in a parallel key-value store. In case select queries on other attributes are required, key-value stores that support indexing on attributes other than the primary key, such as MongoDB, could still be used.</p><p style="padding-left: 97pt;text-indent: 17pt;text-align: justify;">Other relations that are used in more complex queries are stored in a relational database that runs on a single server. Databases running on a single server do exploit the availability of multiple cores to execute transactions in parallel, but are limited by the number of cores that can be supported in a single machine.</p><p style="padding-left: 97pt;text-indent: 17pt;text-align: justify;">Most relational databases support a form of replication where update transac- tions run on only one database (the primary), but the updates are propagated to replicas of the database running on other servers. Applications can execute read- only queries on these replicas, but with the understanding that they may see data that is a few seconds behind in time, as compared to the primary database. Of- ﬂoading read-only queries from the primary database allows the system to handle a load larger than what a single database server can handle.</p><p style="padding-left: 97pt;text-indent: 17pt;text-align: justify;">In-memory caching systems, such as memcached or Redis, are also used to get scalable read-only access to relations stored in a database. Applications may store some relations, or some parts of some relations, in such an in-memory cache, which may be replicated or partitioned across multiple machines. Thereby, appli- cations can get fast and scalable read-only access to the cached data. Updates must however be performed on the database, and the application is responsible for up- dating the cache whenever the data is updated on the database.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">application has to deal with. Some applications, or some parts of an application, may choose to prioritize availability over consistency. But other applications, or some parts of an application, may choose to prioritize consistency even at the cost of potential non-availability of the system in the event of failures. The above issues are discussed in more detail in Chapter 23.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part203.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part205.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
