<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>22.6  Query Processing on Shared-Memory Architectures</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part404.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part406.htm">下一个 &gt;</a></p><p class="s65" style="padding-top: 7pt;padding-left: 72pt;text-indent: 0pt;text-align: left;">22.6  <span style=" color: #00AEEF;">Query Processing on Shared-Memory Architectures</span></p><p style="padding-top: 12pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Parallel algorithms designed for shared-nothing architectures can be used in shared- memory architectures. Each processor can be treated as having its own partition of memory, and we can ignore the fact that the processors have a common shared-</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="95" height="1" alt="image" src="Image_3212.png"/></span></p><p class="s184" style="padding-top: 3pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;"><span class="s77">4</span><span class="s78">Once a </span>map <span class="s80">node ﬁnishes its tasks, redistribution of results from that node to the </span>reduce <span class="s80">nodes can start even if other </span>map <span class="s80">nodes are still active; but the actual computation at the </span>reduce <span class="s80">node cannot start until all </span>map <span class="s80">tasks have completed and all </span>map <span class="s80">results redistributed.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">memory. However, execution can be optimized signiﬁcantly by exploiting the fast access to shared-memory from any of the processors.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Before we study optimizations that exploit shared-memory, we note that while many large-scale systems can execute on a single shared-memory system, the largest- scale systems today are typically implemented using a hierarchical architecture, with a shared-nothing architecture at the outer level, but with each node having a shared- memory architecture locally, as discussed in Section 20.4.8. The techniques we have studied so far for storing, indexing, and querying data in shared-nothing architectures are used to divide up storage, indexing, and query processing tasks among the diﬀerent nodes in the system. Each node is a shared-memory parallel system, which uses parallel query processing techniques to execute the query processing tasks assigned to it. The optimizations we describe in this section can thus be used locally, at each node.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Parallel processing in a shared memory system is typically done by using threads, rather than separate processes. A <span class="s63">thread </span>is an execution stream that shares its entire memory<span class="s76">5</span> with other threads. Multiple threads can be started up, and the operating system schedules threads on available processors.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">We list below some optimizations that can be applied when parallel algorithms that we saw earlier are executed in a shared memory system.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 113pt;text-indent: -16pt;text-align: justify;">1. <span class="p">If we use asymmetric fragment-and-replicate join, the smaller relation need not be replicated to each processor. Instead, only one copy needs to be stored in shared memory, which can be accessed by all the processors. This optimization is particularly useful if there are a large number of processors in the shared-memory system.</span></p><p class="s63" style="padding-top: 6pt;padding-left: 113pt;text-indent: -17pt;text-align: justify;">2. <span class="p">Skew is a signiﬁcant problem in parallel systems, and it becomes worse as the number of processors grows. Handing oﬀ work from an overloaded node to an underloaded node is expensive in a shared-nothing system since it involves net- work traﬃc. In contrast, in a shared memory system, data assigned to a processor can be easily accessed from another processor.</span></p><p style="padding-left: 113pt;text-indent: 14pt;text-align: justify;">To address skew in a shared-memory system, a good option is to use virtual- node partitioning, which allows work to be redistributed in order to balance load. Such redistribution could be done when a processor is found to be overloaded. Alternatively, whenever a processor ﬁnds that it has ﬁnished processing all the virtual nodes assigned to it, it can ﬁnd other processors that still have virtual nodes left to be processed, and take over some of those tasks; as mentioned in Section 22.3.3, this approach is called <i>work stealing</i>. Note that such an approach to avoiding skew would be much more expensive in a shared-nothing environment since a signiﬁcant amount of data movement would be involved, unlike in the shared-memory case.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="95" height="1" alt="image" src="Image_3213.png"/></span></p><p class="s77" style="padding-top: 3pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">5<span class="s78">Technically, in operating-system terminology, its address space.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-top: 4pt;padding-left: 128pt;text-indent: 0pt;text-align: left;">3. <span class="p">Hash join can be executed in two distinct ways.</span></p><p style="padding-top: 6pt;padding-left: 171pt;text-indent: -15pt;text-align: justify;">a. The ﬁrst option is to partition both relations to each processor and then compute the joins of the partitions, in a manner similar to shared-nothing hash join. Each partition must be small enough that the hash index on a build-relation partition ﬁts in the part of shared memory allocated to each processor.</p><p style="padding-top: 6pt;padding-left: 171pt;text-indent: -16pt;text-align: justify;">b. The second option is to partition the relations into fewer pieces, such that the hash index on a build-relation partition ﬁts into common shared mem- ory, rather than a fraction of the shared memory. The construction of the in-memory index, as well as probing of the index, must now be done in parallel by all the processors.</p><p style="padding-left: 171pt;text-indent: 14pt;text-align: justify;">Parallelizing the probe phase is relatively easy, since each processor can work on some partition of the probe relation. In fact it makes sense to use the virtual node approach and partition the probe relation into many small pieces (sometimes called “morsels”), and have processors process a morsel at a time. When a processor is done with a morsel, it ﬁnds an unprocessed morsel and works on it, until there are no morsels left to be processed.</p><p style="padding-left: 171pt;text-indent: 15pt;text-align: justify;">Parallelizing the construction of the shared hash index is more com- plicated, since multiple processors may attempt to update the same part of the hash index. Using locks is an option, but there are overheads due to locking. Techniques based on lock-free data structures can be used to construct the hash index in parallel.</p><p style="padding-top: 7pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">References to more details on how to parallelize join implementations in shared mem- ory may be found in the Further Reading section at the end of the chapter.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Algorithms designed for shared-memory systems must take into account the fact that in today’s processors, memory is divided into multiple memory banks, with each bank directly linked to some processor. The cost of accessing memory from a given processor is less if the memory is directly linked to the processor, and is more if it is linked to a diﬀerent processor. Such memory systems are said to have a <i>Non-Uniform Memory Access </i>or <i>NUMA </i>architecture.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">To get the best performance, algorithms must be <i>NUMA-aware</i>; that is, they must be designed to ensure that data accessed by a thread running on a particular processor is, as far as possible, stored in memory local to that processor. Operating systems support this task in two ways:</p><p class="s63" style="padding-top: 7pt;padding-left: 145pt;text-indent: -16pt;text-align: left;">1. <span class="p">Each thread is scheduled, as far as possible, on the same processor core, every time it is executed.</span></p><p class="s63" style="padding-top: 6pt;padding-left: 145pt;text-indent: -17pt;text-align: left;">2. <span class="p">When a thread requests memory from the operating system memory manager, the operating system allocates memory that is local to that processor core.</span></p><p style="padding-top: 7pt;padding-left: 119pt;text-indent: 17pt;text-align: justify;">Note that the techniques for making the best use of shared memory are comple- mentary to techniques that make the best use of processor caches, including cache-</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;"><a name="bookmark478">conscious index structures (which we saw in Section 14.4.7) and cache-conscious algo- rithms for processing relational operators.</a><a name="bookmark523">&zwnj;</a></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">But in addition, since each processor core has its own cache, it is possible for a cache to have an old value that was subsequently updated on another processor core. Thus, query processing algorithms that update shared data structures should be care- ful to ensure that there are no bugs due to the use of outdated values, and due to race conditions on updating the same memory location from two processor cores. Lock- ing and fence instructions to ensure cache consistency (Section 20.4.5) are used in combination to implement updates to shared data structures.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The form of parallelism we have studied so far allows each processor to execute its own code independently of other processors. However, some parallel systems support a diﬀerent form of parallelism, called <span class="s63">Single Instruction Multiple Data </span>(<span class="s64">SIMD</span>). With <span class="s44">SIMD </span>parallelism, the same instruction is executed on each of multiple data items, which are typically elements of an array. <span class="s44">SIMD </span>architectures became widely used in graphics processing units (<span class="s44">GPU</span>s), which were initially used for speeding up processing of computer graphics tasks. However, more recently, GPU chips have been used for parallelizing a variety of other tasks, one of which is parallel processing of relational operations using the <span class="s44">SIMD </span>support provided by GPUs. Intel’s Xeon Phi coprocessor supports not only multiple cores in a single chip, but also several <span class="s44">SIMD </span>instructions that can operate in parallel on multiple words. There has been a good deal of research on how to process relational operations in parallel on such <span class="s44">SIMD </span>architectures; references to more information on this topic may be found in the bibliographic notes for this chapter, available online.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part404.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part406.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
