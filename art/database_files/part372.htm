<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>(e) tree-like topology</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part371.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part373.htm">下一个 &gt;</a></p><p class="s462" style="padding-top: 4pt;padding-left: 275pt;text-indent: 0pt;text-align: left;">(e) tree-like topology</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 219pt;text-indent: 0pt;text-align: left;">Figure 20.4 <span class="s74">Interconnection networks.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">20.4.3 Interconnection Networks</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Parallel systems consist of a set of components (processors, memory, and disks) that can communicate with each other via an <span class="s63">interconnection network</span>. Figure 20.4 shows several commonly used types of interconnection networks:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s39" style="padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s63">Bus</span><span class="p">. All the system components can send data on and receive data from a single communication bus. This type of interconnection is shown in Figure 20.4a. Bus interconnects were used in earlier days to connect multiple nodes in a network, but they are no longer used for this task. However, bus interconnections are still used for connecting multiple CPUs and memory units within a single node, and they work well for small numbers of processors. However, they do not scale well</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 107pt;text-indent: 0pt;text-align: justify;">with increasing parallelism, since the bus can handle communication from only one component at a time; with increasing numbers of CPUs and memory banks in a node, other interconnection mechanisms such as ring or mesh interconnections are now used even within a single node.</p><p style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><span class="s63">Ring</span>. The components are nodes arranged in a ring (circle), and each node is connected to its two adjacent nodes in the ring, as shown in Figure 20.4b. Unlike a bus, each link can transmit data concurrently with other links in the ring, leading to better scalability. However, to transmit data from one node to another node on the ring may require a large number of hops; speciﬁcally, up to <i>n</i><span class="s15">∕</span>2 hops may be needed on a ring with <i>n </i>nodes, assuming communication can be done in either direction on the ring. Furthermore, the transmission delay increases if the number of nodes in the ring is increased.</p><p class="s39" style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s63">Mesh</span><span class="p">. The components are nodes in a grid, and each component connects to all its adjacent components in the grid. In a two-dimensional mesh, each node connects to (up to) four adjacent nodes, while in a three-dimensional mesh, each node con- nects to (up to) six adjacent nodes. Figure 20.4c shows a two-dimensional mesh. Nodes that are not directly connected can communicate with one another by rout- ing messages via a sequence of intermediate nodes that are directly connected to one another. The number of communication links grows as the number of com- ponents grows, and the communication capacity of a mesh therefore scales better with increasing parallelism.</span></p><p style="padding-left: 107pt;text-indent: 14pt;text-align: justify;">Mesh interconnects are used to connect multiple cores in a processor, or pro- cessors in a single server, to each other; each processor core has direct access to a bank of memory connected to the processor core, but the system transparently fetches data from other memory banks by sending messages over the mesh inter- connects.</p><p style="padding-left: 107pt;text-indent: 15pt;text-align: justify;">However, mesh interconnects are no longer used for interconnecting nodes, since the number of hops required to transmit data increases signiﬁcantly with the number of nodes (the number of hops required to transmit data from one node to another node in a mesh is proportional in the worst case to the square root of the number of nodes). Parallel systems today have very large numbers of nodes, and mesh interconnects would thus be impractically slow.</p><p class="s15" style="text-indent: 0pt;text-align: left;">√                                 √</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><span class="s63">Hypercube</span>. The components are numbered in binary, and a component is con- nected to another if the binary representations of their numbers diﬀer in exactly one bit. Thus, each of the <i>n </i>components is connected to log(<i>n</i>) other components. Figure 20.4d shows a hypercube with eight nodes. In a hypercube interconnection, a message from a component can reach any other component by going through at m<u>o</u>st log(<i>n</i>) links. In contrast, in a mesh architecture a compo<u>n</u>ent may be 2(  <i>n </i><span class="s15">− </span>1) links away from some of the other components (or  <i>n </i>links away, if the mesh interconnection wraps around at the edges of the grid). Thus commu- nication delays in a hypercube are signiﬁcantly lower than in a mesh.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 139pt;text-indent: 15pt;text-align: justify;">Hypercubes have been used to interconnect nodes in massively parallel com- puters in earlier days, but they are no longer commonly used.</p><p class="s39" style="padding-top: 3pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s63">Tree-like</span><span class="p">. Server systems in a data center are typically mounted in racks, with each rack holding up to about 40 nodes. Multiple racks are used to build systems with larger numbers of nodes. A key issue is how to interconnect such nodes.</span></p><p style="padding-left: 139pt;text-indent: 15pt;text-align: justify;">To connect nodes within a rack, there is typically a network switch mounted at the top of the rack; 48 port switches are commonly used, so a single switch can be used to connect all the servers in a rack. Current-generation network switches typically support a bandwidth of 1 to 10 gigabits per second (Gbps) simultaneously from/to each of the servers connected to the switch, although more expensive network interconnects with 40 to 100 Gbps bandwidths are available.</p><p style="padding-left: 139pt;text-indent: 14pt;text-align: justify;">Multiple top-of-rack switches (also referred to as <span class="s63">edge switches</span>) can in turn be connected to another switch, called an <span class="s63">aggregation switch</span>, allowing interconnec- tion between racks. If there are a large number of racks, the racks may be divided into groups, with one aggregation switch connecting a group of racks, and all the aggregation switches in turn connected to a <span class="s63">core switch</span>. Such an architecture is a <span class="s63">tree topology </span>with three tiers. The core switch at the top of the tree also provides connectivity to outside networks.</p><p style="padding-left: 139pt;text-indent: 14pt;text-align: justify;">A problem with this basic tree structure, which is frequently used in local-area networks within organizations, is that the available bandwidth between racks is often not suﬃcient if multiple machines in a rack try to communicate signiﬁcant amounts of data with machines from other racks. Typically, the interconnects of the aggregation switches support higher bandwidths of 10 to 40 Gbps, although interconnects of 100 Gbps are available. Interconnects of even higher capacity can be created by using multiple interconnects in parallel. However, even such high- speed links can be saturated if a large enough number of servers in a rack attempt to communicate at their full connection bandwidth to servers in other racks.</p><p style="padding-left: 139pt;text-indent: 15pt;text-align: justify;">To avoid the bandwidth bottleneck of a tree structure, data centers typically connect each top-of-rack (edge) switch to multiple aggregation switches. Each ag- gregation switch in turn is linked to a number of core switches at the next layer. Such an interconnection topology is called a <span class="s63">tree-like topology</span>; Figure 20.4e shows a tree-like topology with three tiers. The tree-like topology is also referred to as a <span class="s63">fat-tree topology</span>, although originally the fat-tree topology referred to a tree topol- ogy where edges higher in the tree have a higher bandwidth than edges lower in the tree.</p><p style="padding-left: 139pt;text-indent: 14pt;text-align: justify;">The beneﬁt of the tree-like architecture is that each top-of-rack switch can route its messages through any of the aggregation switches that it is connected to, in- creasing the inter-rack bandwidth greatly as compared to the tree topology. Simi- larly, each aggregation switch can communicate with another aggregation switch via any of the core switches that it is connected to, increasing the bandwidth avail- able between the aggregation switches. Further, even if an aggregation or edge switch fails, there are alternative paths through other switches. With appropriate</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 107pt;text-indent: 0pt;text-align: justify;">routing algorithms, the network can continue functioning even if a switch fails, making the network <i>fault-tolerant</i>, at least to failures of one or a few switches.</p><p style="padding-left: 107pt;text-indent: 15pt;text-align: justify;">A tree-like architecture with three tiers can handle a <span class="s63">cluster </span>of tens of thou- sands of machines. Although a tree-like topology improves the inter-rack band- width greatly compared to a tree topology, parallel processing applications, in- cluding parallel storage and parallel database systems, perform best if they are designed in a way that reduces inter-rack traﬃc.</p><p style="padding-left: 107pt;text-indent: 14pt;text-align: justify;">The tree-like topology and variants of it are widely used in data centers today. The complex interconnection networks in a data center are referred to as a <span class="s63">data center fabric</span>.</p><p style="padding-top: 7pt;padding-left: 88pt;text-indent: 17pt;text-align: justify;">While network topologies are very important for scalability, a key to network per- formance is network technology used for individual links. The popular technologies include:</p><p class="s39" style="padding-top: 5pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s63">Ethernet</span><span class="p">: The dominant technology for network connections today is the Ether- net technology. Ethernet standards have evolved over time, and the predominant versions used today are 1-gigabit Ethernet and 10-gigabit Ethernet, which support bandwidths of 1 and 10 gigabits per second respectively. Forty-gigabit Ethernet and 100-gigabit Ethernet technologies are also available at a higher cost and are seeing increasing usage. Ethernet protocols can be used over cheaper copper cables for short distances, and over optical ﬁber for longer distances.</span></p><p class="s39" style="padding-top: 2pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s63">Fiber channel</span><span class="p">: The Fiber Channel Protocol standard was designed for high-speed interconnection between storage systems and computers, and it is predominantly used to implement storage area networks (described in Section 20.4.6). The diﬀer- ent versions of the standard have supported increasing bandwidth over the years, with 16 gigabits per second available as of 2011, and 32 and 128 gigabits per second supported from 2016.</span></p><p class="s39" style="padding-top: 2pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s63">Infiniband</span><span class="p">: The Inﬁniband standard was designed for interconnections with a data center; it was speciﬁcally designed for high-performance computing applications which need not just very high bandwidth, but also very low latency. The Inﬁniband standard has evolved, with link speeds of 8-gigabits per second available by 2007 and 24-gigabits per second available by 2014. Multiple links can be aggregated to give a bandwidth of 120 to 290-gigabits per second.</span></p><p style="padding-left: 107pt;text-indent: 14pt;text-align: justify;">The latency associated with message delivery is as important as bandwidth for many applications. A key beneﬁt of Inﬁniband is that it supports latencies as low as 0.7 to 0.5 microseconds. In contrast, Ethernet latencies can be up to hundreds of microseconds in an unoptimized local-area network, while latency-optimized Ethernet implementations still have latencies of several microseconds.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">One of the important techniques used to reduce latency is to allow applications to send and receive messages by directly interfacing with the hardware, bypassing the oper- ating system. With the standard implementations of the networking stack, applications</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s465" style="padding-left: 137pt;text-indent: 0pt;text-align: left;">	<span><img width="82" height="120" alt="image" src="Image_3113.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="87" height="122" alt="image" src="Image_3114.png"/></span></p><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">M</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;text-align: center;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: center;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;text-align: center;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;text-align: center;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;text-align: center;">P</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="25" height="13" alt="image" src="Image_3115.png"/></span></p><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">M</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;text-align: center;">P</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="25" height="13" alt="image" src="Image_3116.png"/></span></p><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">M</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;text-align: center;">P</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="25" height="13" alt="image" src="Image_3117.png"/></span></p><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">M</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;text-align: center;">P</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="25" height="13" alt="image" src="Image_3118.png"/></span></p><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">M</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: center;">P</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="25" height="13" alt="image" src="Image_3119.png"/></span></p><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">M</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;text-align: center;">P</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 131pt;text-indent: 0pt;text-align: left;">(a) shared memory              (b) shared disk</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s465" style="padding-left: 120pt;text-indent: 0pt;text-align: left;">	</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="326" height="103" alt="image" src="Image_3120.png"/></span></p><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">P</p><p class="s85" style="padding-top: 4pt;text-indent: 0pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">M</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">P</p><p class="s85" style="padding-top: 4pt;text-indent: 0pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">M</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">P</p><p class="s85" style="padding-top: 4pt;text-indent: 0pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">M</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 6pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="padding-left: 3pt;text-indent: 0pt;line-height: 6pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="padding-left: 3pt;text-indent: 0pt;line-height: 6pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="22" height="24" alt="image" src="Image_3121.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="22" height="24" alt="image" src="Image_3122.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="66" height="129" alt="image" src="Image_3123.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="25" height="13" alt="image" src="Image_3124.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="25" height="13" alt="image" src="Image_3125.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="25" height="13" alt="image" src="Image_3126.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="25" height="13" alt="image" src="Image_3127.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="25" height="13" alt="image" src="Image_3128.png"/></span></p><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">M</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">M</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">M</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">M</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;line-height: 10pt;text-align: left;">M</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;text-align: center;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;text-align: center;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;text-align: center;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="text-indent: 0pt;text-align: center;">P</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 132pt;text-indent: 0pt;text-align: left;">(c) shared nothing              (d) hierarchical</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 207pt;text-indent: 0pt;text-align: left;">Figure 20.5 <span class="s74">Parallel database architectures.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">send messages to the operating system, which in turn interfaces with the hardware, which in turn delivers the message to the other computer, where again the hardware interfaces with the operating system, which then interfaces with the application to de- liver the message. Support for direct access to the network interface, bypassing the operating system, reduces the communication latency signiﬁcantly.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Another approach to reducing latency is to use <span class="s63">remote direct memory access </span>(<span class="s64">RDMA</span>), a technology which allows a process on one node to directly read or write to memory on another node, without explicit message passing. Hardware support en- sures that <span class="s44">RDMA </span>can transfer data at very high rates with very low latency. <span class="s44">RDMA </span>implementations can use Inﬁniband, Ethernet, or other networking technologies for physical communication between nodes.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">20.4.4 Parallel Database Architectures</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">There are several architectural models for parallel machines. Among the most promi- nent ones are those in Figure 20.5 (in the ﬁgure, M denotes memory, P denotes a processor, and disks are shown as cylinders):</p><p class="s39" style="padding-top: 5pt;padding-left: 123pt;text-indent: 0pt;text-align: left;">• <span class="s63">Shared memory</span><span class="p">. All the processors share a common memory (Figure 20.5a).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-top: 4pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span>Shared disk<span class="p">. A set of nodes that share a common set of disks; each node has its own processor and memory (Figure 20.5b). Shared-disk systems are sometimes called </span>clusters<span class="p">.</span></p><p class="s39" style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s63">Shared nothing</span><span class="p">. A set of nodes that share neither a common memory nor common disk (Figure 20.5c).</span></p><p class="s39" style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s63">Hierarchical</span><span class="p">. This model is a hybrid of the preceding three architectures (Figure 20.5d). This model is the most widely used model today.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">In Section 20.4.5 through Section 20.4.8, we elaborate on each of these models.</p><p style="padding-left: 106pt;text-indent: 0pt;text-align: justify;">Note that the interconnection networks are shown in an abstract manner in Figure</p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">20.5. Do not interpret the interconnection networks shown in the ﬁgures as necessarily being a bus; in fact other interconnection networks are used in practice. For example, mesh networks are used within a processor, and tree-like networks are often used to interconnect nodes.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">20.4.5 Shared Memory</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">In a <span class="s63">shared-memory </span>architecture, the processors have access to a common memory, typically through an interconnection network. Disks are also shared by the processors. The beneﬁt of shared memory is extremely eﬃcient communication between processes</p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">— data in shared memory can be accessed by any process without being moved with software. A process can send messages to other processes much faster by using memory writes (which usually take less than a microsecond) than by sending a message through a communication mechanism.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Multicore processors with 4 to 8 cores are now common not just in desktop com- puters, but even in mobile phones. High-end processing systems such as Intel’s Xeon processor have up to 28 cores per <span class="s44">CPU</span>, with up to8 <span class="s44">CPU</span>s on a board, while the Xeon Phi coprocessor systems contain around 72 cores, as of 2018, and these numbers have been increasing steadily. The reason for the increasing number of cores is that the sizes of features such as logic gates in integrated circuits has been decreasing steadily, allow- ing more gates to be packed in a single chip. The number of transistors that can be accommodated on a given area of silicon has been doubling approximately every 1 1/2 to 2 years.<span class="s76">4</span></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Since the number of gates required for a processor core has not increased corre- spondingly, it makes sense to have multiple processors on a single chip. To maintain a distinction between on-chip multiprocessors and traditional processors, the term <span class="s63">core </span>is used for an on-chip processor. Thus, we say that a machine has a multicore processor.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="95" height="1" alt="image" src="Image_3129.png"/></span></p><p class="s80" style="padding-top: 3pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;"><span class="s77">4</span><span class="s78">Gordon Moore, cofounder of Intel, predicted such an exponential growth in the number of transistors back in the 1960s; his prediction is popularly known as </span><span class="s162">Moore’s law</span>, even though, technically, it is not a <i>law</i>, but rather an observa- tion and a prediction. In earlier decades, processor speeds also increased along with the decrease in the feature sizes, but that trend ended in the mid-2000s since processor clock frequencies beyond a few gigahertz could not be attained without unreasonable increase in power consumption and heat generation. Moores’s law is sometimes erroneously interpreted to have predicted exponential increases in processor speeds.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 17pt;text-align: justify;">All the cores on a single processor typically access a shared memory. Further, a system can have multiple processors which can share memory. Another eﬀect of the increasing number of gates has been the steady increase in the size of main memory as well as a decrease in cost, per-byte, of main memory.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Given the availability of multicore processors at a low cost, as well as the concur- rent availability of very large amounts of memory at a low cost, shared-memory parallel processing has become increasingly important in recent years.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">20.4.5.1 Shared-Memory Architectures</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">In earlier generation architectures, processors were connected to memory via a bus, with all processor cores and memory banks sharing a single bus. A downside of shared- memory accessed via a common bus is that the bus or the interconnection network becomes a bottleneck, since it is shared by all processors. Adding more processors does not help after a point, since the processors will spend most of their time waiting for their turn on the bus to access memory.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">As a result, modern shared-memory architectures associate memory directly with processors; each processor has locally connected memory, which can be accessed very quickly; however, each processor can also access memory associated with other proces- sors; a fast interprocessor communication network ensures that data are fetched with relatively low overhead. Since there is a diﬀerence in memory access speed depend- ing on which part of memory is accessed, such an architecture is often referred to as <span class="s63">non-uniform memory architecture </span>(<span class="s64">NUMA</span>).</p><p class="s340" style="padding-left: 1pt;text-indent: 2pt;text-align: left;">Memory Controller</p><p style="text-indent: 0pt;text-align: left;"/><p class="s340" style="padding-left: 1pt;text-indent: 2pt;text-align: left;">Memory Controller</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Figure 20.6 shows a conceptual architecture of a modern shared-memory system with multiple processors; note that each processor has a bank of memory directly con- nected to it, and the processors are linked by a fast interconnect system; processors are also connected to <span class="s44">I/O </span>controllers which interface with external storage.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="384" height="122" alt="image" src="Image_3130.png"/></span></p><p class="s119" style="text-indent: 0pt;line-height: 8pt;text-align: left;">CPU</p><p style="text-indent: 0pt;text-align: left;"/><p class="s119" style="text-indent: 0pt;line-height: 8pt;text-align: left;">CPU</p><p style="text-indent: 0pt;text-align: left;"/><p class="s312" style="text-indent: 0pt;line-height: 7pt;text-align: left;">Memory</p><p style="text-indent: 0pt;text-align: left;"/><p class="s312" style="text-indent: 0pt;line-height: 7pt;text-align: left;">Memory</p><p style="text-indent: 0pt;text-align: left;"/><p class="s119" style="text-indent: 0pt;line-height: 8pt;text-align: left;">CPU</p><p style="text-indent: 0pt;text-align: left;"/><p class="s119" style="text-indent: 0pt;line-height: 8pt;text-align: left;">CPU</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s119" style="padding-left: 8pt;text-indent: 10pt;text-align: left;">I/O Controller</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s119" style="padding-left: 8pt;text-indent: 10pt;text-align: left;">I/O Controller</p><p style="text-indent: 0pt;text-align: left;"/><p class="s340" style="padding-left: 1pt;text-indent: 2pt;text-align: left;">Memory Controller</p><p style="text-indent: 0pt;text-align: left;"/><p class="s340" style="padding-left: 1pt;text-indent: 2pt;text-align: left;">Memory Controller</p><p style="text-indent: 0pt;text-align: left;"/><p class="s73" style="padding-top: 4pt;padding-left: 168pt;text-indent: 0pt;text-align: left;">Figure 20.6 <span class="s74">Architecture of a modern shared-memory system.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 17pt;text-align: justify;">Because shared-memory architectures require specialized high-speed interconnects between cores and between processors, the number of cores/processors that can be interconnected in a shared-memory system is relatively small. As a result, the scalability of shared-memory parallelism is limited to at most a few hundred cores.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Processor architectures include cache memory, since access to cache memory is much faster than access to main memory (cache can be accessed in a few nanoseconds compared to nearly a hundred nanoseconds for main memory). Large cache memory is particularly important in shared-memory architectures, since a large cache can help minimize the number of accesses to shared memory.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">If an instruction needs to access a data item that is not in cache, it must be fetched from main memory. Because main memory is much slower than processors, a signiﬁ- cant amount of potential processing speed may be lost while a core waits for data from main memory. These waits are referred to as <span class="s63">cache misses</span>.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Many processor architectures support a feature called <span class="s63">hyper-threading</span>, or <span class="s63">hardware threads</span>, where a single physical core appears as two or more logical cores or threads. Diﬀerent processes could be mapped to diﬀerent logical cores. Only one of the logical cores corresponding to a single physical core can actually execute at any time. But the motivation for logical cores is that if the code running on one logical core blocks on a cache miss, waiting for data to be fetched from memory, the hardware of the physical core can start execution of one of the other logical cores instead of idling while waiting for data to be fetched from memory.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">A typical multicore processor has multiple levels of cache, with the L1 cache being fastest to access, but also the smallest; lower cache levels such as L2 and L3 are slower (although still much faster than main memory) but considerably larger than the L1 cache. Lower cache levels are usually shared between multiple cores on a single pro- cessor. In the cache architecture shown in Figure 20.7, the L1 and L2 caches are local to each of the 4 cores, while the L3 cache is shared by all cores of the processor. data are read into, or written from, cache in units of a <span class="s63">cache line</span>, which typically consists of 64 consecutive bytes.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:185.391pt" cellspacing="0"><tr style="height:42pt"><td style="width:44pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#6DCFF6"><p class="s466" style="padding-top: 12pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">Core 0</p></td><td style="width:44pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#6DCFF6"><p class="s466" style="padding-top: 12pt;text-indent: 0pt;text-align: center;">Core 1</p></td><td style="width:46pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#6DCFF6"><p class="s466" style="padding-top: 12pt;padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">Core 2</p></td><td style="width:42pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#6DCFF6"><p class="s466" style="padding-top: 12pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">Core 3</p></td></tr><tr style="height:12pt"><td style="width:44pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#9DDCF9"><p class="s267" style="padding-right: 2pt;text-indent: 0pt;line-height: 10pt;text-align: right;">L1 Cache</p></td><td style="width:44pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#9DDCF9"><p class="s267" style="text-indent: 0pt;line-height: 10pt;text-align: center;">L1 Cache</p></td><td style="width:46pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#9DDCF9"><p class="s267" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 10pt;text-align: center;">L1 Cache</p></td><td style="width:42pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#9DDCF9"><p class="s267" style="padding-left: 2pt;text-indent: 0pt;line-height: 10pt;text-align: left;">L1 Cache</p></td></tr><tr style="height:12pt"><td style="width:44pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#B9E5FA"><p class="s267" style="padding-right: 1pt;text-indent: 0pt;line-height: 10pt;text-align: right;">L2 Cache</p></td><td style="width:44pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#B9E5FA"><p class="s267" style="text-indent: 0pt;line-height: 10pt;text-align: center;">L2 Cache</p></td><td style="width:46pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#B9E5FA"><p class="s267" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 10pt;text-align: center;">L2 Cache</p></td><td style="width:42pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#B9E5FA"><p class="s267" style="padding-left: 1pt;text-indent: 0pt;line-height: 10pt;text-align: left;">L2 Cache</p></td></tr><tr style="height:25pt"><td style="width:176pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" colspan="4" bgcolor="#D4EFFC"><p class="s267" style="padding-top: 6pt;padding-left: 52pt;text-indent: 0pt;text-align: left;">Shared L3 Cache</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 59pt;text-indent: 0pt;text-align: center;">Figure 20.7 <span class="s74">Multilevel cache system.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">20.4.5.2 Cache Coherency</p><p class="s63" style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Cache coherency <span class="p">is an issue whenever there are multiple cores or processors, each with its own cache. An update done on one core may not be seen by another core, if the local cache on the second core contains an old value of the aﬀected memory location. Thus, whenever an update occurs to a memory location, copies of the content of that memory location that are cached on other caches must be invalidated.</span></p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Such invalidation is done lazily in many processor architectures; that is, there may be some time lag between a write to a cache and the dispatch of invalidation mes- sages to other caches; in addition there may be a further lag in processing invalidation messages that are received at a cache. (Requiring immediate invalidation to be done always can cause a signiﬁcant performance penalty, and thus it is not done in current- generation systems.) Thus, it is quite possible for a write to happen on one processor, and a subsequent read on another processor may not see the updated value.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Such a lack of cache coherency can cause problems if a process expects to see an updated memory location but does not. Modern processors therefore support <span class="s63">memory barrier </span>instructions, which ensure certain orderings between load/store operations be- fore the barrier and those after the barrier. For example, the <span class="s63">store barrier </span>instruction (<span class="s49">sfence</span>) on the x86 architecture forces the processor to wait until invalidation mes- sages are sent to all caches for all updates done prior to the instruction, before any fur- ther load/store operations are issued. Similarly, the <span class="s63">load barrier </span>instruction (<span class="s49">lfence</span>) en- sures all received invalidation messages have been applied before any further load/store operations are issued. The <span class="s49">mfence </span>instruction does both of these tasks.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Memory barrier instructions must be used with interprocess synchronization pro- tocols to ensure that the protocols execute correctly. Without the use of memory bar- riers, if the caches are not “strongly” coherent, the following scenario can happen. Consider a situation where a process <i>P</i>1 updates memory location <i>A </i>ﬁrst, then loca- tion <i>B</i>; a concurrent process <i>P</i>2 running on a diﬀerent core or processor reads <i>B </i>ﬁrst and then reads <i>A</i>. With a coherent cache, if <i>P</i>2 sees the updated value of <i>B</i>, it must also see the updated value of <i>A</i>. However, in the absence of cache coherence, the writes may be propagated out of order, and <i>P</i>2 may thus see the updated value of <i>B </i>but the old value of <i>A</i>. While many architectures disallow out-of-order propagation of writes, there are other subtle errors that can occur due to lack of cache coherency. However, executing <span class="s49">sfence </span>instructions after each of these writes and <span class="s49">lfence </span>before each of the reads will always ensure that reads see a cache coherent state. As a result, in the above example, the updated value of <i>B </i>will be seen only if the updated value of <i>A </i>is seen.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">It is worth noting that programs do not need to include any extra code to deal with cache coherency, as long as they acquire locks before accessing data, and release locks only after performing updates, since lock acquire and release functions typically include the required memory barrier instructions. Speciﬁcally, an <span class="s49">sfence </span>instruction is executed as part of the lock release code, before the data item is actually unlocked. Sim- ilarly an <span class="s49">lfence </span>is executed right after locking a data item, as part of the lock acquisition</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">function, and is thus executed before the item is read. Thus, the reader is guaranteed to see the most recent value written to the data item.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Synchronization primitives supported in a variety of languages also internally ex- ecute memory barrier instructions; as a result, programmers who use these primitives need not be concerned about lack of cache coherency.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">It is also interesting to note that many processor architectures use a form of hardware-level shared and exclusive locking of memory locations to ensure cache co- herency. A widely used protocol, called the <span class="s44">MESI </span>protocol, can be understood as fol- lows: Locking is done at the level of cache lines, containing multiple memory locations, instead of supporting locks on individual memory locations, since cache lines are the units of cache access. Locking is implemented in the hardware, rather than in software, to provide the required high performance.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The <span class="s44">MESI </span>protocol keeps track of the state of each cache line, which can be Modi- ﬁed (updated after exclusive locking), Exclusive locked (locked but not yet modiﬁed, or already written back to memory), Share locked, or Invalid. A read of a memory location automatically acquires a shared lock on the cache line containing that location, while a memory write gets an exclusive lock on the cache line before performing the write. In contrast to database locks, memory lock requests do not wait; instead they immedi- ately revoke conﬂicting locks. Thus, an exclusive lock request automatically invalidates all cached copies of the cache line and revokes all shared locks on the cache line. Sym- metrically, a shared lock request causes any existing exclusive lock to be revoked and then fetches the latest copy of the memory location into cache.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">In principle, it is possible to ensure “strong” cache coherency with such a locking- based cache coherence protocol, making memory barrier instructions redundant. How- ever, many implementations include some optimizations that speed up processing, such as allowing delayed delivery of invalidation messages, at the cost of not guaranteeing cache coherence. As a result, memory barrier instructions are required on many pro- cessor architectures to ensure cache coherency.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 7pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">20.4.6 Shared Disk</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">In the <span class="s63">shared-disk </span>model, each node has its own processors and memory, but all nodes can access all disks directly via an interconnection network. There are two advantages of this architecture over a shared-memory architecture. First, a shared-disk system can scale to a larger number of processors than a shared-memory system. Second, it oﬀers a cheap way to provide a degree of <span class="s63">fault tolerance</span>: If a node fails, the other nodes can take over its tasks, since the database is resident on disks that are accessible from all nodes.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">We can make the disk subsystem itself fault tolerant by using a <span class="s44">RAID </span>architecture, as described in Chapter 12, allowing the system to function even if individual disks fail. The presence ofa large number of storage devices ina <span class="s44">RAID </span>system also provides some degree of <span class="s44">I/O </span>parallelism.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="250" height="241" alt="image" src="Image_3131.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s196" style="padding-left: 71pt;text-indent: 0pt;text-align: center;">storage area network</p><p style="text-indent: 0pt;text-align: left;"/><p class="s196" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">node</p><p style="text-indent: 0pt;text-align: left;"/><p class="s196" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">node</p><p style="text-indent: 0pt;text-align: left;"/><p class="s196" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">node</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s196" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">storage array</p><p style="text-indent: 0pt;text-align: left;"/><p class="s196" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">node</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s196" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">storage array</p><p style="text-indent: 0pt;text-align: left;"/><p class="s196" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">node</p><p style="text-indent: 0pt;text-align: left;"/><p class="s73" style="padding-top: 4pt;padding-left: 84pt;text-indent: 0pt;text-align: center;">Figure 20.8 <span class="s74">Storage-area network.</span></p><p style="padding-top: 9pt;padding-left: 119pt;text-indent: 17pt;text-align: justify;">A <span class="s63">storage-area network </span>(<span class="s64">SAN</span>) is a high-speed local-area network designed to con- nect large banks of storage devices (disks) to nodes that use the data (see Figure 20.8). The storage devices physically consist of an array of multiple disks but provide a view of a logical disk, or set of disks, that hides the details of the underlying disks. For ex- ample, a logical disk may be much larger than any of the physical disks, and a logical disk’s size can be increased by adding more physical disks. The processing nodes can access disks as if they are local disks, even though they are physically separate.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Storage-area networks are usually built with redundancy, such as multiple paths between nodes, so if a component such as a link or a connection to the network fails, the network continues to function.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Storage-area networks are well suited for building shared-disk systems. The shared- disk architecture with storage-area networks has found acceptance in applications that do not need a very high degree of parallelism but do require high availability.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Compared to shared-memory systems, shared-disk systems can scale to a larger number of processors, but communication across nodes is slower (up to a few millisec- onds in the absence of special-purpose hardware for communication), since it has to go through a communication network.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">One limitation of shared-disk systems is that the bandwidth of the network connec- tion to storage in a shared-disk system is usually less than the bandwidth available to access local storage. Thus, storage access can become a bottleneck, limiting scalability.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">20.4.7 Shared Nothing</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: center;">In a <span class="s63">shared-nothing </span>system, each node consists of a processor, memory, and one or more disks. The nodes communicate by a high-speed interconnection network. A node</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;"><a name="bookmark454">functions as the server for the data on the disk or disks that the node owns. Since local disk references are serviced by local disks at each node, the shared-nothing model overcomes the disadvantage of requiring all </a><span class="s44">I/O </span>to go through a single interconnection network.<a name="bookmark501">&zwnj;</a></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Moreover, the interconnection networks for shared-nothing systems, such as the tree-like interconnection network, are usually designed to be scalable, so their trans- mission capacity increases as more nodes are added. Consequently, shared-nothing architectures are more scalable and can easily support a very large number of nodes.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The main drawbacks of shared-nothing systems are the costs of communication and of nonlocal disk access, which are higher than in a shared-memory or shared-disk architecture since sending data involves software interaction at both ends.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Due to their high scalability, shared-nothing architectures are widely used to deal with very large data volumes, supporting scalability to thousands of nodes, or in ex- treme cases, even to tens of thousands of nodes.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 8pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">20.4.8 Hierarchical</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">The <span class="s63">hierarchical architecture </span>combines the characteristics of shared-memory, shared- disk, and shared-nothing architectures. At the top level, the system consists of nodes that are connected by an interconnection network and do not share disks or memory with one another. Thus, the top level is a shared-nothing architecture. Each node of the system could actually be a shared-memory system with a few processors. Alternatively, each node could be a shared-disk system, and each of the systems sharing a set of disks could be a shared-memory system. Thus, a system could be built as a hierarchy, with shared-memory architecture with a few processors at the base, and a shared-nothing architecture at the top, with possibly a shared-disk architecture in the middle. Figure 20.5d illustrates a hierarchical architecture with shared-memory nodes connected to- gether in a shared-nothing architecture.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Parallel database systems today typically run on a hierarchical architecture, where each node supports shared-memory parallelism, with multiple nodes interconnected in a shared-nothing manner.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part371.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part373.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
