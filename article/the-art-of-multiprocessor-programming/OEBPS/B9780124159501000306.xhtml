<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML" lang="EN" xml:lang="en"><head><meta content="text/html; charset=utf-8" http-equiv="default-style"/><title>The Art of Multiprocessor Programming</title><link href="Elsevier_eBook.css" rel="stylesheet" type="text/css"/><link href="math.css" rel="stylesheet" type="text/css"/><link href="media.css" media="only screen" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4f1c4a5b-a3e2-48ff-98f3-ff17812cd57a" name="Adept.expected.resource"/></head><body><section epub:type="chapter" role="doc-chapter"><div aria-label="Page 467" epub:type="pagebreak" id="page_467" role="doc-pagebreak"/><div id="CN"><a id="c0010tit1"/></div><header><hgroup><h1 class="chaptitle" id="c0010tit">Chapter 20: Transactional programming</h1></hgroup><section epub:type="preamble"><div class="abstract"><h2 class="h1hd" id="ab0010"><a id="st0010"/>Abstract</h2><p class="abspara">This chapter introduces transactional programming. Transactional programming raises the level of abstraction, letting a programmer focus on which regions of code must be atomic, and not how to make those regions atomic. By shifting responsibility for achieving atomicity to a run-time library, transactional programming overcomes many limitations and pathologies associated with locks and nonblocking techniques. We consider two categories of transactional programming: transactional lock elision and transactional memory. We also discuss run-time support for transactions, both hardware and software, and show how to build a scalable transactional memory run-time system.</p></div></section><section id="ks0010"><h3 class="h2hd" id="st0015">Keywords</h3><p class="keywords">transactional memory; cache coherence; lock elision; speculation; contention; composition</p></section></header><p class="textfl" id="p0010">Although C++ affords the programmer more control than Java and other high-level languages, the need to explicitly manage memory creates significant challenges, particularly with speculative execution. To ensure that speculative operations do not access reclaimed memory, reclamation is often delayed, which can lead to large amounts of unused memory being allocated for long periods of time.</p><p class="text" id="p0015">In addition, some seemingly correct programs are classified as racy according to the C++ memory model, so their behavior is undefined. Eliminating these races while maintaining good performance is nontrivial, and can lead to code that is difficult to extend and maintain. More generally, the complexity of a program's synchronization mechanisms increases greatly with the complexity of the program, requiring more sophisticated, and difficult, techniques to achieve good performance. A data structure with a large set of operations (especially range queries and other multi-item operations) requires more careful concurrency than one with few operations; a program that uses many threads is likely to need a finer granularity for its locks than one with few threads.</p><p class="text" id="p0020"><i>Transactional programming</i> addresses this complexity by raising the level of abstraction: a programmer focuses on identifying <i>which</i> regions require atomicity, rather than <i>how</i> to make code regions appear to be atomic. Determining how to ensure atomicity without sacrificing performance is left to run-time systems and specialized hardware.</p><section><h2 class="h1hd" id="s0010"><a id="st0020"/>20.1 Challenges in concurrent programming</h2><p class="textfl" id="p0025">We begin with a review of techniques discussed in this book and challenges in applying them, especially in the context of unmanaged languages such as C++.</p><section><h3 class="h2hd" id="s0015"><a id="st0025"/>20.1.1 Problems with locking</h3><p class="textfl" id="p0030">Locking, as a synchronization discipline, has many pitfalls for the inexperienced programmer. <i>Priority inversion</i> occurs when a lower-priority thread is preempted while holding a lock needed by higher-priority threads. <i>Convoying</i> is a form of congestion that is easiest to understand in the context of the hand-over-hand locking pattern: If threads acquire and release locks in a fixed order, then the order in which threads acquire <span aria-label="Page 468" epub:type="pagebreak" id="page_468" role="doc-pagebreak"/>the first lock in the sequence dictates the order in which threads progress through the data structure; if one thread delays, other threads cannot bypass it. <i>Deadlock</i> can occur if threads attempt to lock the same set of objects in different orders. Deadlock avoidance can be awkward if threads must lock many objects, particularly if the set of objects is not known in advance. Furthermore, if the operating system suspends a thread while it holds a lock, then the entire program can grind to a halt.</p><p class="text" id="p0035">A major obstacle to writing good lock-based code is that the association between locks and data is established mostly by convention. It exists in the mind of the programmer, and may be documented only in comments. Consider the following typical comment from a Linux header file<sup><a epub:type="noteref" href="#fn001" id="cf0015" role="doc-noteref">1</a></sup> describing the conventions governing the use of a particular kind of buffer:</p><div class="pageavoid"><figure class="fig" id="f0010"><img alt="Image" class="img" height="92" src="images/B9780124159501000306/fx002.jpg" width="396"/></figure></div><p class="textfl"> Over time, interpreting and observing many such conventions spelled out in this way complicates code maintenance.</p><p class="text" id="p0040">Another challenge with locking is determining the appropriate lock granularity. Consider a nonresizable hash table implemented as a fixed-size array of linked lists. Should one lock protect the entire table? Or should there be one lock per array entry, or even one for each node in the linked lists? If there are few threads, and each rarely accesses the hash table, then a single lock protecting the entire table should suffice. If many threads frequently access the hash table, then fine-grained locking may be required to prevent the hash table from becoming a scalability bottleneck. Although it improves scalability, finer granularity increases complexity and overhead. If the table is frequently read but rarely written, we might consider readers–writer locks.</p><p class="text" id="p0045">A hash table implementation is likely to be written as a generic data structure with a specific contention scenario in mind. If that scenario never manifests during program execution, then the hard-coded strategy may perform poorly. If the number of threads and the way they use the table change over time, then different strategies could be optimal at different points in the program execution. Furthermore, each option could be pessimal in some cases.<span aria-label="Page 469" epub:type="pagebreak" id="page_469" role="doc-pagebreak"/></p></section><section><h3 class="h2hd" id="s0020"><a id="st0030"/>20.1.2 Problems with explicit speculation</h3><p class="textfl" id="p0050">We have seen that the problems with locking can sometimes be mitigated by optimistic synchronization (see, for example, Section <a href="B9780124159501000197.xhtml">9.6</a>). For example, executing read-only critical sections speculatively can greatly reduce the impact of the problems described above for data structures that are read often and written infrequently. A useful tool for implementing such data structures is the <i>sequence lock</i>.</p><p class="text" id="p0055">The core idea behind sequence locks is to use a <img alt="Image" height="9" src="images/B9780124159501000306/fx003.jpg" width="106"/> in place of a mutual exclusion lock or spin-lock. The integer starts with value 0, and is incremented whenever the lock is acquired or released. Thus, the lock is free whenever its value is even and held by some thread whenever its value is odd. The value of a sequence lock serves as a kind of version number for the data structure it protects, and while it is even, the data structure does not change.</p><p class="text" id="p0060">This observation might lead us to think we can execute a read-only critical section speculatively, without writes or atomic operations, as shown in <a href="#f0015" id="cf0020">Fig. 20.1</a>. A reading thread reads the lock, reads the protected data, and then rereads the lock. If the lock value is even, and the same before and after reading the protected data, then no other thread wrote the data in that interval, so the reads of the protected data are valid.</p><div class="pageavoid"><figure class="fig" id="f0015"><img alt="Image" height="355" src="images/B9780124159501000306/gr001.jpg" width="406"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 20.1</span> Incorrect use of a sequence lock: The lock release is not guaranteed to be ordered after the data access.</div></figcaption></figure></div><p class="text" id="p0065">However, this code is incorrect because the program has a data race: A reading thread can execute line 7 at the same time as a writing thread executes line 17. It does not matter that the reader will not <i>use</i> the value that it read: The read is still a race, and programs with races have undefined behavior in C++.</p><p class="text" id="p0070">There are many ways to fix this code. The simplest is to change the type of <img alt="Image" height="13" src="images/B9780124159501000306/fx004.jpg" width="92"/> to <img alt="Image" height="9" src="images/B9780124159501000306/fx003.jpg" width="106"/>. However, this change would impose significant overhead because <span aria-label="Page 470" epub:type="pagebreak" id="page_470" role="doc-pagebreak"/>every access to the data would be a synchronization operation. Furthermore, the default strong ordering on these operations would impose more ordering than necessary among accesses to different variables within a critical section. To avoid this excessive ordering, programmers would need to exploit advanced features of <img alt="Image" height="9" src="images/B9780124159501000306/fx005.jpg" width="86"/>, especially <img alt="Image" height="13" src="images/B9780124159501000306/fx006.jpg" width="166"/>. Lastly, making a variable atomic appears to prevent code reuse. This third challenge can be overcome by using <img alt="Image" height="13" src="images/B9780124159501000306/fx007.jpg" width="113"/>, a new feature in C++20 that allows a variable to be treated as atomic temporarily.</p></section><section><h3 class="h2hd" id="s0025"><a id="st0035"/>20.1.3 Problems with nonblocking algorithms</h3><p class="textfl" id="p0075">One way to avoid the problems of locking is to devise nonblocking algorithms using atomic primitives such as compare-and-swap (available in C++ as the <img alt="Image" height="14" src="images/B9780124159501000306/fx008.jpg" width="165"/> method of <img alt="Image" height="9" src="images/B9780124159501000306/fx005.jpg" width="86"/>). Such nonblocking methods are subtle, and may have high single-thread latency. The principal difficulty is that nearly all synchronization primitives, whether reading, writing, or applying an atomic compare-and-swap, operate only on a single word. This restriction often forces a complex and unnatural structure on algorithms.</p><p class="text" id="p0080">Let us review the lock-free queue of Section <a href="B9780124159501000203.xhtml">10.5</a> (translated to C++ in <a href="#f0020" id="cf0025">Fig. 20.2</a>) with an eye toward the underlying synchronization primitives. On lines 13–14, <span aria-label="Page 471" epub:type="pagebreak" id="page_471" role="doc-pagebreak"/>the <img alt="Image" height="9" src="images/B9780124159501000306/fx010.jpg" width="19"/>() method calls <img alt="Image" height="14" src="images/B9780124159501000306/fx008.jpg" width="165"/> twice to change both the <img alt="Image" height="9" src="images/B9780124159501000306/fx011.jpg" width="23"/> node's <img alt="Image" height="8" src="images/B9780124159501000306/fx012.jpg" width="25"/> field and the <img alt="Image" height="9" src="images/B9780124159501000306/fx011.jpg" width="23"/> field itself to point to the new node. Because these updates occur one-at-a-time, <img alt="Image" height="9" src="images/B9780124159501000306/fx010.jpg" width="19"/>() and <img alt="Image" height="11" src="images/B9780124159501000306/fx013.jpg" width="19"/>() methods must be prepared to encounter a half-finished <img alt="Image" height="9" src="images/B9780124159501000306/fx010.jpg" width="19"/>() (line 13). These methods could be much simpler if we could update both fields together.</p><div class="pageavoid"><figure class="fig" id="f0020"><img alt="Image" height="371" src="images/B9780124159501000306/gr002.jpg" width="396"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 20.2</span> The <img alt="Image" height="11" src="images/B9780124159501000306/fx009.jpg" width="85"/> class: the <img alt="Image" height="9" src="images/B9780124159501000306/fx010.jpg" width="19"/>() method.</div></figcaption></figure></div><p class="text" id="p0085">For example, suppose we had the <img alt="Image" height="11" src="images/B9780124159501000306/fx014.jpg" width="120"/>() primitive shown in <a href="#f0025" id="cf0030">Fig. 20.3</a>, which takes an array of <img alt="Image" height="9" src="images/B9780124159501000306/fx015.jpg" width="99"/> objects, an array of expected <img alt="Image" height="9" src="images/B9780124159501000306/fx016.jpg" width="5"/> values, and an array of <img alt="Image" height="9" src="images/B9780124159501000306/fx016.jpg" width="5"/>-values used for updates, and updates all the array elements if they all have the expected values. (No element is updated if any element does not have the expected value.) Unfortunately, there is no easy way to implement <img alt="Image" height="11" src="images/B9780124159501000306/fx014.jpg" width="120"/>() on conventional architectures. If there were, we could replace the complex logic of lines 12–18 in <a href="#f0020" id="cf0035">Fig. 20.2</a> with a single <img alt="Image" height="11" src="images/B9780124159501000306/fx014.jpg" width="120"/>() call (see <a href="#f0030" id="cf0040">Fig. 20.4</a>).</p><div class="pageavoid"><figure class="fig" id="f0025"><img alt="Image" height="240" src="images/B9780124159501000306/gr003.jpg" width="310"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 20.3</span> Pseudocode for <img alt="Image" height="11" src="images/B9780124159501000306/fx014.jpg" width="120"/>(). This code should execute atomically.</div></figcaption></figure></div><div class="pageavoid"><figure class="fig" id="f0030"><img alt="Image" height="208" src="images/B9780124159501000306/gr004.jpg" width="396"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 20.4</span> The <img alt="Image" height="11" src="images/B9780124159501000306/fx009.jpg" width="85"/> class: simplified <img alt="Image" height="9" src="images/B9780124159501000306/fx010.jpg" width="19"/>() method with <img alt="Image" height="11" src="images/B9780124159501000306/fx014.jpg" width="120"/>().</div></figcaption></figure></div></section><section><h3 class="h2hd" id="s0030"><a id="st0040"/>20.1.4 Problems with compositionality</h3><p class="textfl" id="p0090">One drawback common to all the synchronization mechanisms we have discussed so far is that they cannot easily be <i>composed</i>. For example, suppose we want to dequeue an item <i>x</i> from a queue <img alt="Image" height="11" src="images/B9780124159501000306/fx017.jpg" width="12"/> and enqueue it on another queue <img alt="Image" height="11" src="images/B9780124159501000306/fx018.jpg" width="11"/>. The transfer must be atomic: Concurrent threads must not observe that <i>x</i> has vanished, nor that it is present in both queues. In <img alt="Image" height="11" src="images/B9780124159501000306/fx019.jpg" width="32"/> implementations based on monitors, each method acquires the lock internally, so we cannot combine two method calls in this way.</p><p class="text" id="p0095">There are, of course, <i>ad hoc</i> solutions: We could introduce a lock to be acquired by any thread attempting an atomic modification to both <img alt="Image" height="11" src="images/B9780124159501000306/fx017.jpg" width="12"/> and <img alt="Image" height="11" src="images/B9780124159501000306/fx018.jpg" width="11"/> (in addition to the individual locks for <img alt="Image" height="11" src="images/B9780124159501000306/fx017.jpg" width="12"/> and <img alt="Image" height="11" src="images/B9780124159501000306/fx018.jpg" width="11"/>). Such a lock requires knowing in advance the identities <span aria-label="Page 472" epub:type="pagebreak" id="page_472" role="doc-pagebreak"/>of the two queues, and it could be a bottleneck (no concurrent transfers). Alternatively, the queues could export their synchronization state (say, via <img alt="Image" height="9" src="images/B9780124159501000306/fx020.jpg" width="25"/>() and <img alt="Image" height="9" src="images/B9780124159501000306/fx021.jpg" width="39"/>() methods), and rely on the caller to manage multiobject synchronization. Exposing synchronization state in this way would have a devastating effect on modularity, complicating interfaces and relying on callers to follow complicated conventions. Also, this approach does not work for nonblocking implementations.</p></section><section><h3 class="h2hd" id="s0035"><a id="st0045"/>20.1.5 Summary</h3><p class="textfl" id="p0100">It seems we have a rather dire state of affairs:</p><div><ul><li class="bulllist" id="u0010">•  Locks are hard to manage effectively, especially in large systems.</li><li class="bulllist" id="u0015">•  Atomic primitives, such as compare-and-swap, operate on only one word at a time, resulting in complex algorithms.</li><li class="bulllist" id="u0020">•  The possibility of races necessitates costly synchronization at all times, even when races are extremely rare.</li><li class="bulllist" id="u0025">•  It is difficult to compose multiple calls to multiple objects into atomic units.</li></ul></div><p class="textfl"/><p class="text" id="p0125">In the face of these challenges, transactional programming offers an appealing alternative.</p></section></section><section><h2 class="h1hd" id="s0040"><a id="st0050"/>20.2 Transactional programming</h2><p class="textfl" id="p0130">In transactional programming, a programmer identifies which regions of code cannot interleave with each other, and marks them as transactions. Then, a run-time system, ideally with the assistance of specialized hardware, takes responsibility for finding a way to execute as many transactions concurrently as possible, while ensuring that transactions still appear to execute atomically.</p><p class="text" id="p0135"><span aria-label="Page 473" epub:type="pagebreak" id="page_473" role="doc-pagebreak"/>Transactional programming requires programmers to give up some control: the programmer no longer crafts the low-level synchronization protocol, and has limited means to influence how transactions are scheduled and managed. In return, multiple small transactions are automatically composed into larger transactions; transactions appear to atomically modify multiple locations at once; the run-time system can provide optimizations for read-only transactions, where pessimistic locking would incur high costs; and transactions eliminate the need to think about locks, <img alt="Image" height="9" src="images/B9780124159501000306/fx005.jpg" width="86"/> variables, or other low-level synchronization mechanisms.</p><p class="text" id="p0140">A transactional run-time system must ensure that intermediate effects of a transaction are not visible to other transactions: any values a transaction writes must be hidden from other transactions, becoming visible only when the transaction commits. The system must also ensure that a transaction's behavior is consistent with a serial execution, that is, one in which no transactions run concurrently. As an example, suppose that in some program, variables <i>x</i> and <i>y</i> must always be equal. If transaction <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si1.png" style="vertical-align:middle" width="16"/></span> reads variable <i>x</i>, and then transaction <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si2.png" style="vertical-align:middle" width="17"/></span> increments both <i>x</i> and <i>y</i> and commits, then if <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si1.png" style="vertical-align:middle" width="16"/></span> attempts to read <i>y</i>, it should not be allowed to continue executing: It would see a value that does not equal <i>x</i>, which could lead to erroneous behaviors.</p><p class="text" id="p0145">Transactional run-time systems often employ speculative execution and fine-grained access tracking. In our example, tracking the individual accesses of <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si1.png" style="vertical-align:middle" width="16"/></span> and <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si2.png" style="vertical-align:middle" width="17"/></span> makes it possible to detect that <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si1.png" style="vertical-align:middle" width="16"/></span> read <i>x</i> before <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si2.png" style="vertical-align:middle" width="17"/></span> wrote <i>x</i>, but that <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si1.png" style="vertical-align:middle" width="16"/></span> attempted to read <i>y</i> after <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si2.png" style="vertical-align:middle" width="17"/></span> wrote it. Speculative execution requires that the run-time system somehow transform the execution of <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si1.png" style="vertical-align:middle" width="16"/></span>, so that upon detecting a conflict on <i>y</i>, it can roll back <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si1.png" style="vertical-align:middle" width="16"/></span> and let it try again.</p><section><h3 class="h2hd" id="s0045"><a id="st0055"/>20.2.1 An example of transactional programming</h3><p class="textfl" id="p0150">To see the benefits of transactional programming, consider the code in <a href="#f0035" id="cf0045">Fig. 20.5</a>. When a thread calls this function, it iterates through the indices in <span class="inlinecode">which</span> and, for each index, checks whether the corresponding position in <span class="inlinecode">counters</span> is positive. If <span aria-label="Page 474" epub:type="pagebreak" id="page_474" role="doc-pagebreak"/>so, it increments that counter. To avoid races, the thread locks the entire array of counters for the duration of the operation.</p><div class="pageavoid"><figure class="fig" id="f0035"><img alt="Image" height="158" src="images/B9780124159501000306/gr005.jpg" width="370"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 20.5</span> A lock-based algorithm for conditionally incrementing counters.</div></figcaption></figure></div><p class="text" id="p0155">Suppose two threads call this function simultaneously, with the first thread's <span class="inlinecode">which</span> array consisting only of the value <span class="inlinecode">0</span>, the second thread's <span class="inlinecode">which</span> array consisting only of the value <span class="inlinecode">1023</span>, and all positions in the <span class="inlinecode">counters</span> array set to <span class="inlinecode">1</span>. In that case, acquiring the lock would not be necessary because the threads would not access the same location in the <span class="inlinecode">counters</span> array. Consequently, the program missed the opportunity for greater parallelism. On the other hand, if the second thread's <span class="inlinecode">which</span> array also held the value <span class="inlinecode">0</span>, then acquiring the lock would be necessary, or the two threads' accesses to <span class="inlinecode">counter[0]</span> would race.</p><p class="text" id="p0160">We could enable greater parallelism by replacing <span class="inlinecode">counter_lock</span> with an array of locks. Threads could then use a two-phase locking strategy, in which they acquire each location's lock before accessing the corresponding position in <span class="inlinecode">counters</span>, and release all of the locks at the end of the function. To know which locks to release, and also because an index may appear more than once in <span class="inlinecode">which</span>, the thread must track the locks it has acquired. To avoid deadlock, all threads must also acquire the locks in the same predetermined order. (They can do this by sorting <span class="inlinecode">which</span> first.) Although this fine-grained strategy is more scalable, it may actually be slower than the coarse-grained strategy because it must acquire more locks.</p><p class="text" id="p0165">With transactional programming, we can dispense with thinking about locks at all. We simply execute the entire operation as a single transaction, and rely on the transactional run-time system to avoid races while exploiting parallelism as much as possible. The code might resemble that in <a href="#f0040" id="cf0050">Fig. 20.6</a>. The transactional system would watch what other threads were doing. If a thread's speculative execution would race with another thread, the system would stop the thread before the race manifested, undo its operations, and try again.<span aria-label="Page 475" epub:type="pagebreak" id="page_475" role="doc-pagebreak"/></p><div class="pageavoid"><figure class="fig" id="f0040"><img alt="Image" height="158" src="images/B9780124159501000306/gr006.jpg" width="370"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 20.6</span> A transactional version of <a href="#f0035" id="cf0010">Fig. 20.5</a>.</div></figcaption></figure></div></section></section><section><h2 class="h1hd" id="s0050"><a id="st0060"/>20.3 Hardware support for transactional programming</h2><p class="textfl" id="p0170">Speculation and access tracking have the lowest overhead when implemented in hardware. We now give an overview of how to provide hardware support for transactional programming. Some modern microprocessors already include such support.</p><section><h3 class="h2hd" id="s0055"><a id="st0065"/>20.3.1 Hardware speculation</h3><p class="textfl" id="p0175">Modern microprocessors execute hundreds of instructions simultaneously, even within a single core. Three features make this level of parallelism possible. First, many microprocessors can fetch multiple instructions in a single cycle, and schedule them on parallel arithmetic/logic units. Second, modern microprocessors are pipelined: different instructions can be in different stages of their execution (using different circuits) at the same time. Finally, to keep their pipelines and execution units busy, a modern microprocessor does not stall when it encounters a branch. Instead, it predicts which direction the branch will take, and executes the corresponding instruction stream speculatively. If the microprocessor subsequently determines that an instruction should not have been executed, it undoes the instruction's effect and the effects of instructions that depended on it. If the instructions would overwrite memory, the processor buffers the writes until it is known that the instruction was supposed to execute (e.g., all branches were predicted correctly); the buffered writes are discarded if the prediction was wrong.</p><p class="text" id="p0180">Since processors can already speculate, and undo the effects of any speculative instructions that fail, to support transactions, we only need to allow the programmer to specify a granularity for speculation that extends beyond the pipeline: In addition to aborting any yet-to-complete instructions from a transaction that aborts, the effects of completed instructions by that transaction also need to be undone. To support undoing changes to registers, the instruction that starts a transaction stores their original state, so that the registers can be reset if the transaction aborts. To support undoing changes to memory, the processor must be able to roll back values in the cache that correspond to the writes that the failed transaction made. Invalidation is the simplest mechanism for rolling back a transaction's writes.</p></section><section><h3 class="h2hd" id="s0060"><a id="st0070"/>20.3.2 Basic cache coherence</h3><p class="textfl" id="p0185">Hardware-supported transactional programming relies on the cache coherence protocol for fine-grained access tracking and for detecting conflicting memory accesses by concurrent transactions. Before discussing the details, we briefly review cache coherence. Readers unfamiliar with cache coherence protocols may consult Appendix <a href="B978012415950100032X.xhtml">B</a> for more background.</p><p class="text" id="p0190">In modern multiprocessors each processor has an attached <i>cache</i>, a small high-speed memory used to avoid communicating with large and slow main memory. Each cache entry holds a group of neighboring words called a <i>line</i>, and has some way of mapping addresses to lines. Consider a simple architecture in which processors and memory communicate over a <span aria-label="Page 476" epub:type="pagebreak" id="page_476" role="doc-pagebreak"/>shared broadcast medium called a <i>bus</i>. Each cache line has a <i>tag</i>, which encodes state information. In the standard <i>MESI</i> protocol, each cache line is in one of the following states:</p><div><ul><li class="bulllist" id="u0030">•  <i>Modified</i>: The line in the cache has been modified, and must eventually be written back to memory. No other processor has this line cached.</li><li class="bulllist" id="u0035">•  <i>Exclusive</i>: The line has not been modified, but no other processor has this line cached. (A line is typically loaded in exclusive mode before being modified.)</li><li class="bulllist" id="u0040">•  <i>Shared</i>: The line has not been modified, and other processors may have this line cached.</li><li class="bulllist" id="u0045">•  <i>Invalid</i>: The line does not contain meaningful data.</li></ul></div><p class="textfl"> The <i>cache coherence</i> protocol detects synchronization conflicts among individual loads and stores, and ensures that different processors agree on the state of the shared memory. When a processor loads or stores a memory address <i>a</i>, it broadcasts the request on the bus, and the other processors and memory listen in (sometimes called <i>snooping</i>).</p><p class="text" id="p0215">A full description of a cache coherence protocol can be complex, but here are the principal transitions of interest to us.</p><div><ul><li class="bulllist" id="u0050">•  When a processor requests to load a line in modified mode, the other processors invalidate any copies of that line. A processor with a modified copy of that line must write the line back to memory before the load can be fulfilled.</li><li class="bulllist" id="u0055">•  When a processor requests to load a line into its cache in shared mode, any processor with an exclusive or modified copy must change its state to shared. A processor with a modified copy must also write that line back to memory before the load can be fulfilled.</li><li class="bulllist" id="u0060">•  If the cache becomes full, it may be necessary to <i>evict</i> a line. If the line is shared or exclusive, it can simply be discarded, but if it is modified, it must be written back to memory.</li></ul></div><p class="textfl"/><p class="text" id="p0235">Note that modern cache coherence protocols detect and resolve synchronization conflicts between writers and between readers and writers, and they already allow changes to memory to stay in the cache for a while, instead of immediately and directly updating memory.</p></section><section><h3 class="h2hd" id="s0065"><a id="st0075"/>20.3.3 Transactional cache coherence</h3><p class="textfl" id="p0240">Many of the state transitions in the MESI protocol are asynchronous: They occur in one processor on account of a memory operation performed by another processor. While we customarily think of a data race as something that manifests at a much higher level of abstraction, there is a close relationship between the programming language concept of a race and the state transitions in the MESI protocol.</p><p class="text" id="p0245">Consider the case of two threads attempting to increment a counter at the same time. The statement <span class="inlinecode">counter++</span> translates to three assembly instructions: one to fetch the value of <span class="inlinecode">counter</span> to a register, one to increment the value in that register, and one to update the value of <span class="inlinecode">counter</span> by writing the register's value to memory. <span aria-label="Page 477" epub:type="pagebreak" id="page_477" role="doc-pagebreak"/>A race occurs if there is <i>any</i> interleaving between the three instructions issued by one thread, and the three instructions issued by the other. If we examined every possible interleaving, and looked at the MESI state transitions that occur, we would find that whenever there is a data race, then at some time while the three instructions are being executed, either some line is invalidated or some line in the Modified state is downgraded to Shared or Exclusive. This observation holds for any section of code that accesses shared memory: If a race occurs with any of its accesses to shared memory, then the line containing the accessed data is either invalidated or downgraded from Modified during the execution of that section of code.</p><p class="text" id="p0250">We can use this observation to execute a transaction speculatively, and abort the transaction if a problematic transitions occurs in the cache while the transaction is executing. (If no such transition occurs, then there were no data races with the transaction, so the speculation succeeds.) Assume that each processor has a private L1 cache and executes only one thread at a time. To detect problematic cache line transitions, we add <img alt="Image" height="13" src="images/B9780124159501000306/fx022.jpg" width="52"/> and <img alt="Image" height="13" src="images/B9780124159501000306/fx023.jpg" width="39"/> instructions that delimit a transaction, a flag that indicates whether a transaction is active, and a bit to each line of the cache that indicates whether the line has been accessed by an active transaction. The <img alt="Image" height="13" src="images/B9780124159501000306/fx022.jpg" width="52"/> instruction saves a private copy of the current values of the processor's registers (a <i>checkpoint</i>), raises the flag, and returns <i>true</i>, indicating that the transaction is executing speculatively. While the flag is raised, any access to the cache will set the corresponding bit. The <img alt="Image" height="13" src="images/B9780124159501000306/fx023.jpg" width="39"/> instruction lowers the flag, clears any bits that may have been set, and discards the checkpoint. Thus, if <img alt="Image" height="13" src="images/B9780124159501000306/fx023.jpg" width="39"/> is executed, the transaction does not abort, and the result is the same as if the code were executed without the transaction (i.e., the speculation succeeded).</p><p class="text" id="p0255">With the above mechanics in place, it is straightforward to detect problematic transitions: If a line whose bit is set is about to be evicted or downgraded from Modified, the cache first notifies the processor to abort the speculation.</p><p class="text" id="p0260">When a transaction aborts, any cache lines that it modified are invalidated; their values are <i>not</i> written back or provided to any other processor. In addition, the flag is lowered, all bits indicating transactional access are cleared, and the checkpoint is used to reset the thread to its state at the beginning of the transaction. Then the <img alt="Image" height="13" src="images/B9780124159501000306/fx022.jpg" width="52"/> instruction returns <i>false</i>, indicating that the speculation has failed. Thus, the thread can determine whether it is executing on account of a successful <img alt="Image" height="13" src="images/B9780124159501000306/fx022.jpg" width="52"/>, or in response to an abort.</p></section><section><h3 class="h2hd" id="s0070"><a id="st0080"/>20.3.4 Limitations of hardware support</h3><p class="textfl" id="p0265">Because data in a cache line that has been accessed by a hardware transaction cannot leave the cache without aborting the transaction, the cache size and associativity impose strict limits on the amount of data that a transaction can access. For example, some L1 caches are <i>direct-mapped</i>, which means that each address is mapped to a specific line in the cache; its contents must be cached at that line, and must therefore evict any data that were there before. With such a cache, if a transaction accesses two addresses that map to the same cache line, it can never successfully commit.</p><p class="text" id="p0270"><span aria-label="Page 478" epub:type="pagebreak" id="page_478" role="doc-pagebreak"/>In addition, on many microprocessors, various events may cause significant delays while a transaction is executing, during which time a cache line accessed by the transaction could be evicted. These events may be induced by the transaction (e.g., by making a system call), or they may be unrelated (e.g., the thread executing the transaction gets swapped out).</p><p class="text" id="p0275">Because it is often difficult or impossible to predict when a transaction might not be able to commit, programmers are advised to think of hardware transactional support as being <i>best effort</i>, rather than something that can be relied on. Therefore, when using hardware transactions, they should also provide a <i>fallback</i> mechanism, in case the transaction cannot be committed.</p><p class="text" id="p0280">Requiring programmers to provide a fallback mechanism reduces the burden on computer architects: Transactions are not required to succeed for correctness, only for quality of implementation, so architects are free to exert their best effort.</p></section></section><section><h2 class="h1hd" id="s0075"><a id="st0085"/>20.4 Transactional lock elision</h2><p class="textfl" id="p0285">The most straightforward way to employ transactional programming in existing lock-based software is through a technique known as <i>transactional lock elision</i> (TLE). The core idea in TLE is to modify the critical sections of a program so that they attempt to run as a transactional speculation. If a speculation fails too many times (e.g., due to conflicts with other threads), the execution <i>falls back</i> to the original lock.</p><p class="text" id="p0290">With hardware support, TLE can be implemented as a special lock, whose <span class="inlinecode">acquire</span> and <span class="inlinecode">release</span> methods attempt to use the <img alt="Image" height="13" src="images/B9780124159501000306/fx022.jpg" width="52"/> and <img alt="Image" height="13" src="images/B9780124159501000306/fx023.jpg" width="39"/> instructions, respectively. This makes TLE exceptionally easy to use. However, TLE can only <i>try</i> to extract more concurrency out of an existing lock-based program, it cannot <i>guarantee</i> anything about progress or freedom from pathologies. In particular, the problems we enumerated earlier (e.g., convoying, priority inversion, and deadlock) remain possible: If a speculation fails and falls back to locking, the transactional mechanism is not used, and its benefits on progress and throughput cannot be achieved.</p><p class="text" id="p0295">Since it is always correct for a TLE execution to fall back to using the original locks in the program, TLE can be used to accelerate existing critical sections. Typically, critical sections are small: They touch few memory locations, and they do not last for many cycles. If a transaction executing a small critical section fails, it is often worthwhile to retry it a few times before falling back to locking. We could even augment the return value of <img alt="Image" height="13" src="images/B9780124159501000306/fx022.jpg" width="52"/> to provide more detail about why a speculation failed, which the code can use to decide whether to retry the critical section speculatively, or to fall back to locking. <a href="#f0045" id="cf0055">Fig. 20.7</a> presents a complete implementation of TLE that uses a spin-lock as the fallback path.</p><div class="pageavoid"><figure class="fig" id="f0045"><img alt="Image" height="471" src="images/B9780124159501000306/gr007.jpg" width="345"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 20.7</span> A complete implementation of TLE, using a spin-lock as the fallback path.</div></figcaption></figure></div><p class="text" id="p0300"><a href="#f0045" id="cf0060">Fig. 20.7</a> adds a fair bit of complexity around the calls to <img alt="Image" height="13" src="images/B9780124159501000306/fx022.jpg" width="52"/> (line 5) and <img alt="Image" height="13" src="images/B9780124159501000306/fx023.jpg" width="39"/> (line 24). Of particular importance, we must remember that line 8 indicates that the critical section will run speculatively, with TLE. If the speculation fails, then control flow will return to line 5. That is, it may appear that <img alt="Image" height="13" src="images/B9780124159501000306/fx022.jpg" width="52"/> executes multiple times, with different return values.</p><p class="text" id="p0305"><span aria-label="Page 479" epub:type="pagebreak" id="page_479" role="doc-pagebreak"/>Recall that all modifications to memory that are performed between lines 5 and 24 are undone if the speculation fails. Thus, if we wish to prevent livelock and starvation, it is necessary for us to count the number of attempts <i>outside</i> of the transaction. This is accomplished by the <span class="inlinecode">attempts</span> variable, which is incremented on each iteration of the loop. Each time the speculation fails, control jumps from line 5 to line 16, where <span class="inlinecode">attempts</span> is checked. If the value becomes too large, then the thread stops using TLE, acquires the lock, and then returns. When the thread reaches the end of the critical section, it can observe that the lock is held, conclude that it must be the lock holder, and release the lock to complete the critical section. In a similar fashion, when a speculation fails, and conflict with another thread is not the cause, then line 16 reports a value in <span class="inlinecode">status</span> that indicates the critical section must be run while holding the lock.</p><p class="text" id="p0310"><span aria-label="Page 480" epub:type="pagebreak" id="page_480" role="doc-pagebreak"/>Note that line 7 follows every successful call to <img alt="Image" height="13" src="images/B9780124159501000306/fx022.jpg" width="52"/>. This code serves two purposes. The first is to identify situations in which one thread attempts to run a critical section via TLE while another thread is actively running a critical section using the lock. By checking the lock after calling <img alt="Image" height="13" src="images/B9780124159501000306/fx022.jpg" width="52"/>, the thread can discover cases where the lock is held. When the lock is held, the thread quietly completes its TLE region without doing any meaningful work. Starting on line 11, the thread makes it look like it never even tried to execute speculatively, by decreasing its <span class="inlinecode">attempts</span> and then waiting for the lock to be released.</p><p class="text" id="p0315">The second purpose of this call is more subtle. Suppose that a thread reaches line 8, and has begun to execute its critical section. Let another thread subsequently reach line 17. At this point, the second thread is unaware of the first thread, as the first thread is executing speculatively. Since the second thread is not using TLE, its writes are immediately visible in memory. If the first and second threads access the same memory locations, but in different orders, there is a danger that the speculative thread might see inconsistent state. Suppose there is a program invariant that variables <span class="inlinecode">x</span> and <span class="inlinecode">y</span> are equal, and initially <span class="inlinecode">x == y == 0</span>. Let the first thread read <span class="inlinecode">x == 0</span>, and let then the second thread execute the first line of the sequence <span class="inlinecode">y++; x++</span>. Since the second thread is not using TLE, its write to <span class="inlinecode">y</span> immediately is visible in memory. Since the first thread did not access <span class="inlinecode">y</span> yet, it has no reason to abort. However, if the second thread were to delay and the first thread were to read <span class="inlinecode">y</span>, it would see <span class="inlinecode">y == 1</span>, and thus <span class="inlinecode">y != x</span>.</p><p class="text" id="p0320">The presence of line 7 makes the above situation impossible. Note that the first thread read the lock <i>while it was using TLE</i>. Thus the lock must be in the thread's cache, with the <i>transactional</i> bit set. Consequently, any subsequent change to the lock by another thread, be it nonspeculative or speculative, will cause the cache line holding the lock to move to that thread's cache, in the Modified state. Coherence ensures that the line must first be evicted from the first thread's cache. This will cause the first thread to abort.</p><section><h3 class="h2hd" id="s0080"><a id="st0090"/>20.4.1 Discussion</h3><p class="textfl" id="p0325">TLE is a powerful tool for increasing the concurrency of programs whose critical sections rarely conflict. However, we must be careful about critical sections that try to perform I/O. Note that TLE can be employed in user programs, as well as the operating system kernel. What should happen if a TLE critical section in the kernel attempts to interact with a hardware device? If the critical section subsequently aborted, could the device behave erroneously? For that matter, does it make sense for a TLE critical section in a user program to make system calls?</p><p class="text" id="p0330">Additionally, the TLE mechanism we described thus far has no way to guarantee progress. Livelock and starvation are both possible. Even in our simple example with a single shared counter, it is possible that one thread is always executing the first line of code between the times when the other thread is executing the third line and when it calls <img alt="Image" height="13" src="images/B9780124159501000306/fx023.jpg" width="39"/>.</p><p class="text" id="p0335">Given these constraints, TLE is best thought of as an optimization, not as a fundamentally new programming model. When critical sections rarely <span aria-label="Page 481" epub:type="pagebreak" id="page_481" role="doc-pagebreak"/>conflict, but threads still find themselves spending time waiting for locks, then using TLE to run the corresponding critical sections is likely to improve performance. Note that TLE does affect how a programmer constructs synchronization code: In programs where TLE is effective, it is often the case that the programmer can get by with a few coarse-grained instead of many fine-grained locks.</p></section></section><section><h2 class="h1hd" id="s0085"><a id="st0095"/>20.5 Transactional memory</h2><p class="textfl" id="p0340">We have already seen how TLE can optimize the performance of existing lock-based programs. Can transactional programming also simplify the creation of new concurrent programs? If so, how might <i>de novo</i> programs be designed differently, if transactions were part of the concurrency toolbox from the get-go?</p><p class="text" id="p0345"><i>Transactional memory</i> (TM) refers, broadly, to the programming model that arises when programmers think in terms of transactions instead of locks. The differences between TM and TLE are subtle, but significant:</p><div><ul><li class="bulllist" id="u0065">•  Programmers do not think about <i>how</i> to implement concurrency. Instead, they mark the regions of code that need to run in isolation from each other, and they leave it up to a run-time system to find an optimal concurrency mechanism.</li><li class="bulllist" id="u0070">•  Since programmers think in terms of regions requiring isolation, nesting of transactions is natural, if not encouraged.</li><li class="bulllist" id="u0075">•  Since there is no guarantee of a lock-based fallback, the programming language may need to ensure that transactions do not attempt operations that cannot be undone (e.g., I/O).</li><li class="bulllist" id="u0080">•  Since everything inside of a transaction can be undone, it is natural, if not beneficial, to expose speculation to the programmer, in the form of explicit self-abort instructions.</li><li class="bulllist" id="u0085">•  Since there are no locks in the programming model, traditional problems with locking (deadlock, convoying, priority inversion) are not possible.</li></ul></div><p class="textfl"/><p class="text" id="p0375">To illustrate the difference between TLE and TM, consider the code in <a href="#f0050" id="cf0065">Fig. 20.8</a>. We expect both functions to be able to complete using transactional speculation, since they each update only two locations. However, the TLE code is significantly more complex. The convention in the TLE program is that every access to either integer passed to the function must be performed while the corresponding lock is held. Thus the program must acquire both locks. In the common case, the acquisitions will use TLE, and will be elided. However, the worst case requires the programmer to produce a consistent lock order to avoid deadlock (in this case, we order based on the addresses of the integers). In addition, the programmer must check that the two integers are not protected by the same lock. In contrast, the designer of the TM code knows that every access to either integer, in any other place in the program, will also use TM. Consequently, it suffices to begin a TM region, increment the counters, and then end the region. If the region conflicts with other threads' accesses, the run-time system will determine an order in which the threads will execute.</p><div class="pageavoid"><figure class="fig" id="f0050"><img alt="Image" height="279" src="images/B9780124159501000306/gr008.jpg" width="449"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 20.8</span> Code to atomically increment two counters with TLE (top) and TM (bottom).</div></figcaption></figure></div><section><h3 class="h2hd" id="s0090"><a id="st0100"/>20.5.1 Run-time scheduling</h3><p class="textfl" id="p0380"><span aria-label="Page 482" epub:type="pagebreak" id="page_482" role="doc-pagebreak"/>Since TM does not have a lock-based fallback, it requires some other mechanism to ensure progress. Historically, this mechanism has been called “contention management,” though it may be more appropriately thought of as a scheduler. In the common case, the contention manager does nothing: Threads begin and end transactions, and the transactions should usually succeed. When a thread finds itself repeatedly failing to commit, due to conflicts with other threads, then it may choose to (1) delay itself before trying again, in the hope that the concurrent transactions with which it is conflicting will commit, or (2) employ some mechanism to reduce the number of transactions that run concurrently with it, to decrease the likelihood of a concurrent transaction causing a conflict.</p><p class="text" id="p0385">In the first case, a simple and effective strategy, which we saw in Section <a href="B9780124159501000173.xhtml">7.4</a>, is to use randomized exponential back-off. That is, after <i>n</i> consecutive aborts, the thread will choose a random number between <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si3.png" style="vertical-align:middle" width="31"/></span> and <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>1</mml:mn></mml:math></span><span><img alt="Image" height="11" src="images/B9780124159501000306/si4.png" style="vertical-align:middle" width="43"/></span> and wait for that many CPU cycles before retrying. Usually randomized exponential back-off will place a hard limit on <i>n</i>, so that in high-conflict situations, threads will not wait for minutes.</p><p class="text" id="p0390">In the second case, decreasing the number of running transactions is a heuristic, not a hard-and-fast rule. A simple solution is to use a global Boolean flag. When a thread tries to start a transaction, it first checks the flag. If the flag is true, the thread waits. Once the flag is false, the thread may continue. If a transaction aborts repeatedly, it tries to change the flag from false to true, via a compare-and-swap. If it succeeds, it attempts its transaction until the transaction commits. Otherwise, it waits. When the distressed transaction commits, it clears the flag. In <span aria-label="Page 483" epub:type="pagebreak" id="page_483" role="doc-pagebreak"/><a href="#enun0045" id="cf0070">Exercise 20.8</a>, we explore the performance impact of this approach versus falling back to acquiring the lock in TLE.</p></section><section><h3 class="h2hd" id="s0095"><a id="st0105"/>20.5.2 Explicit self-abort</h3><p class="textfl" id="p0395">Since TM regions always run speculatively, the run-time system is allowed to abort a transaction at any time, and for any reason. Of course, every time a transaction aborts, that transaction's previous work is wasted, so the run-time system should avoid causing unnecessary aborts. But since the potential is there, it is worthwhile to consider letting the programmer request self-abort.</p><p class="text" id="p0400">Indeed, self-abort appears to be the cornerstone of creating truly compositional programs based on transactions. Consider a program in which a thread receives a list of tuples, where each tuple consists of a <i>source account</i>, a <i>destination account</i>, and a <i>transfer amount</i>. Since a single account can appear multiple times as a source and as a destination, and since the account balances cannot be read without using some kind of synchronization, it is not trivial to determine if the list of operations is valid. The challenge is especially great if each account is protected by a lock that is private to the account object. However, with TM and explicit self-abort, we can encapsulate each account's synchronization entirely within its implementation, and still write correct code. Inspired by the conventions in the C++ TM Technical Specification, we say that if an integer exception escapes a transaction, it causes the transaction to abort, but the exception is retained. With such a definition, <a href="#f0055" id="cf0075">Fig. 20.9</a> shows how TM and self-abort together elegantly implement a solution.</p><div class="pageavoid"><figure class="fig" id="f0055"><img alt="Image" height="487" src="images/B9780124159501000306/gr009.jpg" width="376"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 20.9</span> Code to atomically perform multiple transfers between accounts, using exception-based self-abort.</div></figcaption></figure></div></section></section><section><h2 class="h1hd" id="s0100"><a id="st0110"/>20.6 Software transactions</h2><p class="textfl" id="p0405">Thus far, we have assumed hardware support for transactional programming. While there exist microprocessors with such support, it is also possible to implement transactions entirely in software. In addition to offering a pathway to transactional programming on legacy hardware, software transactions also provide a scalable fallback path when hardware transactions fail. In this section, we describe two software implementations that support transactional programming.</p><p class="text" id="p0410">To implement transactions in software, we provide a library satisfying the interface in <a href="#f0060" id="cf0080">Fig. 20.10</a>, which provides functions to call when beginning, committing, or aborting a transaction, and when reading and writing memory within a transaction.</p><div class="pageavoid"><figure class="fig" id="f0060"><img alt="Image" height="76" src="images/B9780124159501000306/gr010.jpg" width="277"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 20.10</span> Interface for software transactions.</div></figcaption></figure></div><p class="text" id="p0415">It would be tedious and error-prone if programmers had to call these functions directly, so we assume programmers can write structured transactional code, and that a compiler inserts the appropriate library calls: Calls to <img alt="Image" height="11" src="images/B9780124159501000306/fx024.jpg" width="46"/> and <img alt="Image" height="9" src="images/B9780124159501000306/fx025.jpg" width="52"/> would be inserted at the beginning and end, respectively, of every transactional region, and every load and store within a transaction would be replaced by the appropriate function call. (The <img alt="Image" height="9" src="images/B9780124159501000306/fx026.jpg" width="46"/> function is used for explicit self-abort.) For example, <img alt="Image" height="11" src="images/B9780124159501000306/fx027.jpg" width="65"/> would become <img alt="Image" height="12" src="images/B9780124159501000306/fx028.jpg" width="97"/>, and <img alt="Image" height="13" src="images/B9780124159501000306/fx029.jpg" width="79"/> would become <img alt="Image" height="14" src="images/B9780124159501000306/fx030.jpg" width="125"/>. <span aria-label="Page 484" epub:type="pagebreak" id="page_484" role="doc-pagebreak"/><span aria-label="Page 485" epub:type="pagebreak" id="page_485" role="doc-pagebreak"/></p><p class="text" id="p0420">As a transaction performs reads, it must track every location it has read, so it can later determine if that location was changed by a concurrent transaction. When it performs writes, it must do so in a manner that can be undone if the transaction ultimately aborts. Thus, a software transaction library would also define a <i>transaction descriptor</i>, a per-thread object to keep track of the state of an in-progress transaction, and some global synchronization data through which the threads can coordinate their accesses to shared memory. We must also be able to checkpoint a thread's state when it begins a transaction, so we can reset the thread if its transaction aborts. In C++, the <span class="inlinecode">setjmp</span> and <span class="inlinecode">longjmp</span> instructions suffice for this purpose. For the sake of simplicity, we omit checkpointing in the following discussion.</p><section><h3 class="h2hd" id="s0105"><a id="st0115"/>20.6.1 Transactions with ownership records</h3><p class="textfl" id="p0425">One of the key challenges for software transactions is to detect conflicts between concurrent transactions. The <i>ownership record</i>, or orec, is a data structure designed for this purpose. An orec superimposes a lock, a version number, and a thread's unique identifier into a single memory word. In its simplest implementation, the lowest bit of an orec serves two roles: It is a lock bit, and it also indicates the meaning of the remaining bits of the orec.</p><p class="text" id="p0430">In more detail, we say that when the orec's low bit is zero, it is unlocked, and the remaining bits can be interpreted as a monotonically increasing integer (the version number). When the low bit is one, the orec is locked, and the remaining bits can be interpreted as the unique ID of the thread that holds the lock. In a sense, orecs enhance sequence locks (Section <a href="#s0020" id="cf0085">20.1.2</a>) by adding information about the lock owner.</p><p class="text" id="p0435">If we built our software transactions using a single orec, it would not afford much concurrency. Instead, we will use an array of orecs. <a href="#f0065" id="cf0090">Fig. 20.11</a>, line 3 declares the table of orecs as an array of <span class="inlinecode">NUM_LOCKS</span> atomic integers. Line 6 implements a many-to-one mapping of memory regions to entries in the table. If we assume that every memory word (<span class="inlinecode">uintptr_t</span>) is aligned on an 8-byte boundary, then as long as <span class="inlinecode">GRAIN</span> is no less than 3, every memory word will map to exactly one entry in <span class="inlinecode">lock_table</span>.</p><div class="pageavoid"><figure class="fig" id="f0065"><img alt="Image" height="563" src="images/B9780124159501000306/gr011.jpg" width="471"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 20.11</span> Software transactions with ownership records (1/2).</div></figcaption></figure></div><p class="text" id="p0440">Our implementation will be able to detect conflicts on location <i>L</i> by any pair of transactions by watching how those transactions interact with the entry in <span class="inlinecode">lock_table</span> that corresponds to <i>L</i>. False conflicts are possible, since there are many more memory locations than table entries. However, if the table size is large enough, false conflicts should be unlikely.</p><p class="text" id="p0445">Before discussing the rest of the implementation, let us consider a strawman implementation of transactions that uses orecs in a manner more reminiscent of traditional locks. Given our <span class="inlinecode">lock_table</span>, we could run a transaction as follows: Any time the transaction tries to read a location, we could check the corresponding orec. If the orec is locked by the current transaction, then we could read the location directly; if the orec is unlocked, we could lock the orec and then read the location; and if the orec is locked by another transaction, we could abort the transaction, invoke the run-time transaction scheduler (to help avoid livelock), and then try again. Writes would run almost identically, except that they could not simply update a location; if they subsequently aborted, we would need some mechanism <span aria-label="Page 486" epub:type="pagebreak" id="page_486" role="doc-pagebreak"/>to undo that write. The easiest approach would be to maintain an <i>undo log</i>, into which the old value could be saved <i>before</i> the location was updated. If the transaction aborted, it would need to use the log to restore the original values in memory. At commit time, a thread would release its locks and discard its undo log.</p><p class="text" id="p0450">The above strategy would allow nonconflicting transactions to run concurrently, without the programmer needing to think about fine-grained locks. Since memory would only be accessed when the thread held the appropriate lock, <span aria-label="Page 487" epub:type="pagebreak" id="page_487" role="doc-pagebreak"/>there would be no races. However, execution would be pessimistic: Any time any transaction accessed any location, that location would be inaccessible to all concurrent transactions. Especially when reads abound, this approach would sacrifice concurrency.</p><p class="text" id="p0455">While we could try to craft a solution based on readers–writer locks, so that multiple threads could have read permission on a location simultaneously, doing so would incur overhead, since nonconflicting threads would conflict when acquiring an orec in read mode. Instead, we will use optimistic reads. That is, when a transaction wishes to read a location <i>L</i>, it will first read the value of the orec that corresponds to <i>L</i>. If the orec is locked, then the code continues or aborts according to the same rules as in the strawman algorithm. However, if the orec is unlocked, we will not lock it. Instead we will record the version number stored in the orec. If that version number never changes before the transaction commits, or if it changes only on account of the same transaction subsequently acquiring the orec as a writer, then the transaction knows that its read remained valid.</p><p class="text" id="p0460">The second change we will make to the strawman algorithm is to employ commit-time locking. With commit-time locking, a transaction writing to <i>L</i> does not acquire the orec for <i>L</i> until it is ready to commit. Consequently, it must log its writes in a private <i>redo log</i> instead of updating <i>L</i> directly.</p><p class="text" id="p0465">The above two changes introduce a subtle but significant question: If a transaction reads <i>L</i>, how frequently must it check the orec that corresponds to <i>L</i>? As we will see in the chapter exercises, the transaction could have an incorrect execution if it subsequently read some other location <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si5.png" style="vertical-align:middle" width="16"/></span> and did not then check <i>L</i>'s orec. Unfortunately, if a transaction performs <i>n</i> reads, it would incur <span class="hiddenClass"><mml:math><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="17" src="images/B9780124159501000306/si6.png" style="vertical-align:middle" width="41"/></span> overhead to <i>validate</i> the consistency of all of its reads.</p><p class="text" id="p0470">To reduce the validation overhead in the common case, we introduce a monotonically increasing counter, which we call the global <span class="inlinecode">clock</span>. This clock will increment every time a writing transaction attempts to commit, and its value will be used to establish the start and end times of transactions. When a transaction commits, it increments <span class="inlinecode">clock</span>, and then uses the new value of the clock as the version of every orec that it releases.</p><p class="text" id="p0475">While the clock becomes a potential bottleneck for writing transactions, its impact on read validation is dramatic. Suppose that a transaction sees the value <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si7.png" style="vertical-align:middle" width="15"/></span> in the clock when it begins. If, before reading a location <i>L</i>, the transaction sees that <i>L</i>'s orec has a value <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>⩽</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si8.png" style="vertical-align:middle" width="50"/></span>, and after reading <i>L</i>, the transaction sees that the orec's value is still <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si9.png" style="vertical-align:middle" width="17"/></span>, then the transaction knows that <i>L</i> could not have been modified after it started. If the same property holds for every orec encountered by the transaction, then it never needs to validate during its execution: It only reads locations that have not been modified since it started, and it conservatively aborts if it attempts to read any location whose orec was modified after it started.</p><p class="text" id="p0480">Our implementation in <a href="#f0065" id="cs0010">Figs. 20.11</a> and <a href="#f0070">20.12</a> illustrates the algorithm in full, for software transactions that operate on word-sized memory locations. Our implementation uses the C <span class="inlinecode">setjmp</span> and <span class="inlinecode">longjmp</span> instructions to capture the state of the registers immediately before the transaction attempts, and to jump back to that point any time the transaction aborts. It also addresses the <span aria-label="Page 488" epub:type="pagebreak" id="page_488" role="doc-pagebreak"/>requirements of the C++ memory model by using <img alt="Image" height="13" src="images/B9780124159501000306/fx007.jpg" width="113"/>, a feature of C++20, so that accesses to program memory by transactions will not form a race. This is a preferable alternative to casting pointers to <img alt="Image" height="9" src="images/B9780124159501000306/fx005.jpg" width="86"/>.</p><div class="pageavoid"><figure class="fig" id="f0070"><img alt="Image" height="592" src="images/B9780124159501000306/gr012.jpg" width="470"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 20.12</span> Software transactions with ownership records (2/2).</div></figcaption></figure></div><p class="text" id="p0485">Every transaction uses a <span class="inlinecode">Descriptor</span> object to store its state during execution (line 8). The <span class="inlinecode">Descriptor</span> tracks three sets: one with the addresses of the orecs it has read, one with the addresses of the orecs it has locked, and one <span aria-label="Page 489" epub:type="pagebreak" id="page_489" role="doc-pagebreak"/>with pairs representing the locations it intends to update, and the values it intends to write to those locations. It also stores its start time and its <span class="inlinecode">setjmp</span> buffer. When a thread creates its <span class="inlinecode">Descriptor</span>, it increments the global <span class="inlinecode">id_gen</span> counter to get a unique integer, and it uses that to craft a value it can store in the orecs it acquires.</p><p class="text" id="p0490">When a transaction begins, it reads the clock (line 20) in order to determine its starting time. To write value <i>V</i> to location <i>L</i>, the transaction stores the pair <span class="hiddenClass"><mml:math><mml:mo stretchy="false">〈</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">〉</mml:mo></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000306/si10.png" style="vertical-align:middle" width="41"/></span> into its write set (line 23). To read a location <i>L</i>, the transaction starts by checking if <i>L</i> is in its write set, in which case it must return the value it intends to write (line 26). If <i>L</i> is not found, the transaction computes the address of the orec for <i>L</i>. It then reads the value of the orec (line 31), reads the value at <i>L</i> (line 32), and then rereads the value of the orec (line 33). This pattern is necessary: A concurrent transaction that is committing might be concurrently updating the location, and that transaction may have incremented <span class="inlinecode">clock</span> and begun its commit sequence before this transaction begins. For the algorithm we present, checking the orec before and after reading <i>L</i> is necessary. Since we expect conflicts to be rare, we optimize <span class="inlinecode">read</span> to be as short as possible. If line 34 detects any discrepancy in the two reads of the orec, the transaction aborts and tries again.</p><p class="text" id="p0495">The most complex part of our algorithm is when a transaction tries to commit. If the transaction is read-only, then we know that it has not performed any writes, and as of its last read, it would have determined that all of its reads returned values that were written before the transaction started. Thus a read-only transaction can commit without any more effort (line 42). Otherwise, the transaction must acquire the orecs that protect the locations in its write set. This process extends from line 46 to line 57. For each address in the write set, the algorithm reads the current value of the orec. If the orec is already owned by the transaction, no work is needed. If the orec is locked by another transaction, then this transaction must abort. There is one more consideration: If the orec is unlocked, but its value is greater than the transaction's start time, then the transaction aborts. This is a conservative step. Suppose that the transaction has also read a location protected by this orec: Once the transaction acquires the lock, it will be unable to see the old value of the orec, to recognize that its read was invalidated by another transaction's commit. To simplify the subsequent checks, our transactions abort in this case.</p><p class="text" id="p0500">Once the locks are acquired, the transaction gets its commit time by incrementing the clock (line 58). If the clock had not changed since the transaction started, then the transaction knows that all of its reads must be valid, and thus it need not check them individually. Otherwise, it must check each entry in its read set (line 59), to make sure the orec has not changed in such a way as to suggest that the corresponding read became invalid.</p><p class="text" id="p0505">Once the transaction has validated its reads, it can replay its writes (line 66) and release its locks (line 68). Then it can clear its lists.</p><p class="text" id="p0510">Lastly, if a transaction aborts, then it must clear its lists. Since the transaction may abort during the commit operation, it is possible that it has acquired some locks. If it has, it must release them during the abort operation (line 75). Note that in this case, the lock versions can be reset to the values they had before they <span aria-label="Page 490" epub:type="pagebreak" id="page_490" role="doc-pagebreak"/>were acquired: A concurrent read that “misses” the locking of the orec will not read invalid values, because the values are only updated after aborting becomes impossible.</p><p class="text" id="p0515">Our implementation of transactions with orecs has a number of desirable properties. Even though it uses locks internally, its locks are not visible to the programmer, and deadlock is not possible: The necessary condition of “hold and wait” is broken, since a transaction that cannot acquire a lock releases all of its locks and tries again. Furthermore, the use of commit-time locking decreases the likelihood of livelock: Symmetric conflicts among transactions can only manifest if those transactions reach their commit points simultaneously.<span aria-label="Page 491" epub:type="pagebreak" id="page_491" role="doc-pagebreak"/><span aria-label="Page 492" epub:type="pagebreak" id="page_492" role="doc-pagebreak"/></p></section><section><h3 class="h2hd" id="s0110"><a id="st0120"/>20.6.2 Transactions with value-based validation</h3><p class="textfl" id="p0520">The orec approach to transactions is not without its drawbacks. Chief among them is the granularity of the mapping of locations to orecs: A simplistic hash function, like the one in <a href="#f0065" id="cf0095">Fig. 20.11</a>, can lead to deterministic collisions (for example, with 4096 orecs, the first element of every array of 2<sup>16</sup> elements will be protected by the same orec); using a complex hash function introduces too much latency. One alternative is to validate by directly using the values returned from calls to the <span class="inlinecode">read</span> function. <a href="#f0075" id="cs0015">Figs. 20.13</a> and <a href="#f0080">20.14</a> present such an algorithm.</p><div class="pageavoid"><figure class="fig" id="f0075"><img alt="Image" height="356" src="images/B9780124159501000306/gr013.jpg" width="305"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 20.13</span> Software transactions with value-based validation (1/2).</div></figcaption></figure></div><div class="pageavoid"><figure class="fig" id="f0080"><img alt="Image" height="668" src="images/B9780124159501000306/gr014.jpg" width="527"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 20.14</span> Software transactions with value-based validation (2/2).</div></figcaption></figure></div><p class="text" id="p0525">This algorithm resembles our orec algorithm in that it delays transactional writes until commit time (redo logging). The main differences are that it uses a single sequence lock to coordinate when transactions commit, and it does not use the sequence lock to decide when transactions abort. Instead, changes to the sequence lock cause transactions to validate.</p><p class="text" id="p0530">The intuition behind this algorithm is that transactions can log the addresses they read, and the values they observed when they performed those reads. When a transaction commits, it increments the sequence lock to an odd value, writes back its entire write set, and then increments the sequence lock to a new even value. A transaction can trivially determine that it is valid if it sees that the sequence lock has the same even value as the last time at which the transaction was valid.</p><p class="text" id="p0535">If the sequence lock has changed since the last check, the transaction must wait until the sequence lock is even (unheld). Then it can reread every location in its read set, to ensure that the value is the same as it was when the transaction read it. The only catch is that the transaction must check its <i>entire</i> read set, without any intervening writer transaction commits. This manifests in the <span class="inlinecode">while</span> loop of the <span class="inlinecode">validate</span> function. Note that after a successful validation, a transaction is equivalent to one that started at the time of the validation, and hence can update its start time.</p><p class="text" id="p0540">One question that arises is whether a transaction can <i>start</i> while the lock is held. Rather than introduce waiting in the <span class="inlinecode">beginTx()</span> function, we subtract one from the start time if it would otherwise be odd. This means that a transaction might validate on its first load, instead of waiting to start.</p><p class="text" id="p0545">Like the orec algorithm, this approach to transactions is deadlock-free: It only has one lock, and thus there is no potential for deadlock! In addition, note that a transaction's progress is only impeded when it must validate, and every validation corresponds to the completion of a writer transaction. Hence, the algorithm is livelock-free. Unfortunately, starvation is possible, especially for long-running transactions concurrent with a stream of small writers: The long-running transaction might need to validate once per writer.</p></section></section><section><h2 class="h1hd" id="s0115"><a id="st0125"/>20.7 Combining hardware and software transactions</h2><p class="textfl" id="p0550">Part of the appeal of value-based validation is that it enables hybrid transactional systems, which use hardware transactional execution when possible, and fall back to software execution when a transaction cannot complete in hardware (e.g., because it is too big). One of the simplest approaches is to introduce a mechanism for dynamically switching between two phases: one in which all transactions use hardware support, and one in which none do.</p><p class="text" id="p0555">The phase change from hardware mode to software mode is relatively simple to achieve, using a mechanism similar to the fallback in hardware lock elision: The transaction system includes a global flag to indicate if the current mode is “hardware” or “software.” Every hardware-mode transaction begins by checking the flag. If the flag changes during the transaction's <span aria-label="Page 493" epub:type="pagebreak" id="page_493" role="doc-pagebreak"/>execution, it will abort, at which point it can switch to software mode. If a hardware transaction cannot complete because of capacity, then after it aborts, it atomically changes the flag value and then starts in software mode.</p><p class="text" id="p0560">The return from software to hardware is potentially more challenging. However, value-based validation provides a clean return: When the transaction that precipitated the switch to software mode is about to complete, its final step before validating is to use a transactional write to set the flag false (this write is performed by an augmented <span class="inlinecode">commitTx()</span> function). So long as every software-mode transaction begins by reading the flag (i.e., via a call to <span class="inlinecode">read()</span> from within <span class="inlinecode">beginTx()</span>), then when the distressed transaction commits and resets the mode, its commit will cause all concurrent transactions to validate, abort, and then resume in hardware mode.</p><p class="text" id="p0565">This is but one of many mechanisms for combining hardware and software transactional execution. Other approaches use a global lock or epochs (Section <a href="B978012415950100029X.xhtml">19.7</a>) to manage the transition between modes. Truly <i>hybrid</i> systems allow hardware and software transactions to run and commit concurrently.</p></section><section><h2 class="h1hd" id="s0120"><a id="st0130"/>20.8 Transactional data structure design</h2><p class="textfl" id="p0570">One of the most promising roles for TM is in the creation of high-performance concurrent data structures. As an example, consider the difficulty involved in creating a concurrent red/black tree: a lock-based implementation would have difficulty crafting a cycle-free lock acquisition order, because an operation does not know how much rebalancing is necessary until after it reaches its target node; a nonblocking implementation might need to atomically modify many memory locations in order to rebalance during an insert or removal. With transactions, these complex data structure maintenance operations can be done atomically with the modification operation, without requiring the programmer to craft a complex synchronization protocol.</p><p class="text" id="p0575">Another benefit of transactions is that they allow data structures to export rich interfaces. Consider a concurrent hashmap: a programmer may desire more methods than the traditional <span class="inlinecode">insert</span>/<span class="inlinecode">remove</span>/<span class="inlinecode">contains</span>. With transactions, a generic <span class="inlinecode">modifyKey(k,</span> <i>λ</i><span class="inlinecode">)</span> method becomes possible, wherein a programmer can atomically (1) find the entry with matching key, (2) apply the <i>λ</i> function to the value associated with that key, and (3) update the value with the computed result. TM is a pathway to composable, modular, generic concurrent data structures.</p><p class="text" id="p0580">While TM can transform an arbitrary sequential operation into a concurrent operation, it does not promise scalability. Programmers must make sure that their data structures do not have obvious scalability bottlenecks. For example, if every insertion and removal in a hashmap must update a count of the total number of items, then transactions cannot prevent concurrent counter updates from conflicting. Furthermore, TM is no different than locking in its requirement that all concurrent accesses to a datum agree on the synchronization mechanism being used. Just as it is not correct to allow one thread to perform an unsynchronized access to an item that is simultaneously locked by another thread, it is not correct to allow <span aria-label="Page 494" epub:type="pagebreak" id="page_494" role="doc-pagebreak"/>nontransactional accesses to an item that is simultaneously being accessed transactionally. Lastly, when using software TM, programs must take great care when transitioning memory from a state in which it is accessed transactionally to a state in which it is not. The most dangerous example is memory reclamation: If a transaction unlinks a node from a data structure, commits, and then tries to free that node, it must be sure that no concurrent (doomed to abort) transaction is still accessing that node. We explore this “privatization” problem in an exercise.</p></section><section><h2 class="h1hd" id="s0125"><a id="st0135"/>20.9 Chapter notes</h2><p class="textfl" id="p0585">The transition from Linux Kernel 2.4 to Linux Kernel 2.6 involved a significant effort to improve performance on multiprocessors. Sequence locks were one of the techniques that became widespread as a result <a epub:type="noteref" href="#br0490" id="cf0100" role="doc-noteref">[98]</a>. The challenges faced when using sequence locks in C++ are discussed in detail by Hans Boehm <a epub:type="noteref" href="#br0100" id="cf0105" role="doc-noteref">[20]</a>. We thank Hans for explaining the subtleties of sequence locks, and suggesting a solution using C++20's <img alt="Image" height="13" src="images/B9780124159501000306/fx007.jpg" width="113"/>.</p><p class="text" id="p0590">TLE in modern microprocessors is based on a more general mechanism called <i>hardware transactional memory</i>, first proposed by Maurice Herlihy and Eliot Moss <a epub:type="noteref" href="#br0370" id="cf0110" role="doc-noteref">[74]</a> as a general-purpose programming model for multiprocessors. Nir Shavit and Dan Touitou <a epub:type="noteref" href="#br0785" id="cf0115" role="doc-noteref">[157]</a> proposed the first TM that did not require specialized hardware, instead using software instrumentation on every load and store.</p><p class="text" id="p0595">The “orec” algorithm presented in this chapter is a variant of the TL2 algorithm of Dave Dice, Ori Shalev, and Nir Shavit <a epub:type="noteref" href="#br0175" id="cf0120" role="doc-noteref">[35]</a>. The value-based approach is due to Luke Dalessandro, Michael Spear, and Michael Scott <a epub:type="noteref" href="#br0165" id="cf0125" role="doc-noteref">[33]</a>.</p><p class="text" id="p0600">The use of transactional hardware for lock elision was developed by Ravi Rajwar and James Goodman <a epub:type="noteref" href="#br0730" id="cs0020" role="doc-noteref">[146</a>,<a epub:type="noteref" href="#br0725" role="doc-noteref">145]</a>. Like TM, there are software-only approaches to lock elision <a epub:type="noteref" href="#br0745" id="cf0130" role="doc-noteref">[149]</a>.</p><p class="text" id="p0605">A comparison of commercially available hardware systems that support TM can be found in <a epub:type="noteref" href="#br0665" id="cf0135" role="doc-noteref">[133]</a>. Harris, Larus, and Rajwar <a epub:type="noteref" href="#br0295" id="cf0140" role="doc-noteref">[59]</a> provide the authoritative survey of both hardware and software TM.</p></section><section><h2 class="h1hd" id="s0130"><a id="st0140"/>20.10 Exercises</h2><p class="textfl" id="p0610"/><div class="boxg1" id="enun0010"><p class="b1num">Exercise 20.1 </p><div><p class="b1textfl" id="p0615">Let <span class="inlinecode">G</span> be a global variable, and let <span class="inlinecode">H</span> be a variable allocated on the heap. Both <span class="inlinecode">G</span> and <span class="inlinecode">H</span> are structs with many fields, and a programmer wishes to protect each with a sequence lock. Why is it necessary to use a safe memory reclamation strategy for <span class="inlinecode">H</span>, but not for <span class="inlinecode">G</span>?</p></div></div><p class="textfl"/><p class="text" id="p0620"/><div class="boxg1" id="enun0015"><p class="b1num">Exercise 20.2 </p><div><p class="b1textfl" id="p0625">Consider <a href="#enun0010" id="cf0145">Exercise 20.1</a>. If the structs were protected by readers–writer locks, and a thread was going to read <span class="inlinecode">H</span>, would it need a safe memory reclamation strategy? Why or why not?</p></div></div><p class="textfl"/><p class="text" id="p0630"><span aria-label="Page 495" epub:type="pagebreak" id="page_495" role="doc-pagebreak"/></p><div class="boxg1" id="enun0020"><p class="b1num">Exercise 20.3 </p><div><p class="b1textfl" id="p0635">In our implementation of TM with orecs, we used a simple vector to store transaction read sets. Suppose there were 2<sup>16</sup> orecs, with a strong hash function for mapping addresses to orecs. How many randomly chosen accesses would a single transaction need to make before it would read the same orec twice?</p></div></div><p class="textfl"/><p class="text" id="p0640"/><div class="boxg1" id="enun0025"><p class="b1num">Exercise 20.4 </p><div><p class="b1textfl" id="p0645">In Section <a href="#s0110" id="cf0150">20.6.2</a>, we argued that false conflicts on orecs can limit throughput. As in <a href="#enun0020" id="cf0155">Exercise 20.3</a>, consider a system with 2<sup>16</sup> orecs. If every thread issued <i>W</i> writes, then with two threads, at what value of <i>W</i> would the probability of false conflicts exceed 50%?</p></div></div><p class="textfl"/><p class="text" id="p0650"/><div class="boxg1" id="enun0030"><p class="b1num">Exercise 20.5 </p><div><p class="b1textfl" id="p0655">Continuing the example from <a href="#enun0025" id="cf0160">Exercise 20.4</a>, if there were eight threads, at what value of <i>W</i> would the probability of false conflicts exceed 50%?</p></div></div><p class="textfl"/><p class="text" id="p0660"/><div class="boxg1" id="enun0035"><p class="b1num">Exercise 20.6 </p><div><p class="b1textfl" id="p0665">Repeat <a href="#enun0030" id="cf0165">Exercise 20.5</a>, but with 2<sup>20</sup> orecs.</p></div></div><p class="textfl"/><p class="text" id="p0670"/><div class="boxg1" id="enun0040"><p class="b1num">Exercise 20.7 </p><div><p class="b1textfl" id="p0675">Instead of buffering writes in a redo log, an STM implementation could update locations while it was executing, and maintain an undo log for restoring values if the transaction aborted. A subtle complication arises: When the transaction aborts, it cannot restore the old value of orecs when it releases them. Why not? Consider the case where transaction <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si1.png" style="vertical-align:middle" width="16"/></span> performs a write to location <i>X</i> and then aborts while reading location <i>Y</i>, and transaction <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si2.png" style="vertical-align:middle" width="17"/></span> performs a read to location <i>X</i> that is concurrent with both of <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si1.png" style="vertical-align:middle" width="16"/></span>'s operations.</p></div></div><p class="textfl"/><p class="text" id="p0680"/><div class="boxg1" id="enun0045"><p class="b1num">Exercise 20.8 </p><div><p class="b1textfl" id="p0685">Let <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si11.png" style="vertical-align:middle" width="19"/></span> be a transaction that has aborted several times in a row, with <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si12.png" style="vertical-align:middle" width="14"/></span> total transactions in the system. Suppose that the contention manager gives <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000306/si11.png" style="vertical-align:middle" width="19"/></span> two options:</p><div><ul><li class="b1bulllist" id="u0090">•  Block new transactions from starting, wait for all current transactions to commit, and then begin.</li><li class="b1bulllist" id="u0095">•  Block new transactions from starting, and begin immediately.</li></ul></div><p class="b1textfl"> Which option would you favor? Why? It might help to consider specific workload characteristics in justifying your answer.</p></div></div><p class="textfl"/><p class="text" id="p0700"/><div class="boxg1" id="enun0050"><p class="b1num">Exercise 20.9 </p><div><p class="b1textfl" id="p0705">Would the choice of software TM or hardware TM influence your answer to <a href="#enun0045" id="cf0170">Exercise 20.8</a>?</p></div></div><p class="textfl"/><p class="text" id="p0710"/><div class="boxg1" id="enun0055"><p class="b1num">Exercise 20.10 </p><div><p class="b1textfl" id="p0715">We claimed that it is necessary for a transaction to ensure the validity of its read set every time it reads a new location. If it does not, a destined-to-abort transaction may produce a visible fault. Create an interleaving between two transactions that could produce a divide-by-zero fault if a transaction does not validate after every read.</p></div></div><p class="textfl"/><p class="text" id="p0720"/><div class="boxg1" id="enun0060"><p class="b1num">Exercise 20.11 </p><div><p class="b1textfl" id="p0725">A common idiom in lock-based programming is to lock a data structure, unlink part of it, and then unlock the data structure. Doing so makes the unlinked part “private” to the thread who did the unlinking, because that part is no longer reachable to other threads.</p><p class="b1text" id="p0730"><span aria-label="Page 496" epub:type="pagebreak" id="page_496" role="doc-pagebreak"/>A challenge that transactional programming introduces is that speculative threads may not know that they are destined to abort, and their transactional accesses to the unlinked part could conflict with the nontransactional accesses by the unlinking thread.</p><p class="b1text" id="p0735">Create a workload where one thread's transaction privatizes a linked list by splitting it at some point, and another thread's transaction is traversing the list. Describe a fault that could occur in the transactional thread.</p></div></div><p class="textfl"/><p class="text" id="p0740"/><div class="boxg1" id="enun0065"><p class="b1num">Exercise 20.12 </p><div><p class="b1textfl" id="p0745">Consider your solution to <a href="#enun0060" id="cf0175">Exercise 20.11</a>. Would the algorithm in Section <a href="#s0105" id="cf0180">20.6.1</a> be vulnerable to that error? Why or why not?</p></div></div><p class="textfl"/><p class="text" id="p0750"/><div class="boxg1" id="enun0070"><p class="b1num">Exercise 20.13 </p><div><p class="b1textfl" id="p0755">Consider your solution to <a href="#enun0060" id="cf0185">Exercise 20.11</a>. Would the algorithm in Section <a href="#s0110" id="cf0190">20.6.2</a> be vulnerable to that error? Why or why not?</p></div></div><p class="textfl"/></section><footer><section epub:type="bibliography" role="doc-bibliography"><div id="bl0370"><h2 class="reftitle" id="st0145">Bibliography</h2><p class="reflist1" epub:type="biblioentry footnote" id="br0100" role="doc-biblioentry">[20] Hans-J. Boehm,  Can seqlocks get along with programming language memory models?  <i>Proceedings of the 2012 ACM SIGPLAN Workshop on Memory Systems Performance and Correctness</i>.  <i>Beijing, China</i>.  June 2012:12–20.</p><p class="reflist1" epub:type="biblioentry footnote" id="br0165" role="doc-biblioentry">[33] Luke Dalessandro, Michael Spear, Michael L. Scott,  NOrec: streamlining STM by abolishing ownership records,   <i>Proceedings of the 15th ACM Symposium on Principles and Practice of Parallel Programming</i>.  <i>Bangalore, India</i>.  January 2010.</p><p class="reflist1" epub:type="biblioentry footnote" id="br0175" role="doc-biblioentry">[35] Dave Dice, Ori Shalev, Nir Shavit,  Transactional locking II,   <i>Proceedings of the 20th International Symposium on Distributed Computing</i>.  <i>Stockholm, Sweden</i>.  September 2006.</p><p class="reflist1" epub:type="biblioentry footnote" id="br0295" role="doc-biblioentry">[59] Tim Harris, James R. Larus, Ravi Rajwar,  <i>Transactional Memory</i>. 2nd edition  <cite>Synthesis Lectures on Computer Architecture</cite>.  Morgan and Claypool; 2010.</p><p class="reflist1" epub:type="biblioentry footnote" id="br0370" role="doc-biblioentry">[74] Maurice P. Herlihy, J. Eliot B. Moss,  Transactional memory: architectural support for lock-free data structures,   <i>Proceedings of the 20th International Symposium on Computer Architecture</i>.  <i>San Diego, CA</i>.  May 1993.</p><p class="reflist1" epub:type="biblioentry footnote" id="br0490" role="doc-biblioentry">[98] Christoph Lameter,  Effective synchronization on Linux/NUMA systems,   <i>Proceedings of the May 2005 Gelato Federation Meeting</i>.  <i>San Jose, CA</i>.  May 2005.</p><p class="reflist2" epub:type="biblioentry footnote" id="br0665" role="doc-biblioentry">[133] Takuya Nakaike, Rei Odaira, Matthew Gaudet, Maged M. Michael, Hisanobu Tomari,  Quantitative comparison of hardware transactional memory for Blue Gene/Q, zEnterprise EC12, Intel Core, and POWER8,   <i>Proceedings of the 42nd Annual International Symposium on Computer Architecture</i>.  <i>Portland, OR</i>.  June 2015.</p><p class="reflist2" epub:type="biblioentry footnote" id="br0725" role="doc-biblioentry">[145] Ravi Rajwar, James R. Goodman,  Speculative lock elision: enabling highly concurrent multithreaded execution,   <i>Proceedings of the 34th IEEE/ACM International Symposium on Microarchitecture</i>.  <i>Austin, TX</i>.  December 2001.</p><p class="reflist2" epub:type="biblioentry footnote" id="br0730" role="doc-biblioentry">[146] Ravi Rajwar, James R. Goodman,  Transactional lock-free execution of lock-based programs,   <i>Proceedings of the 10th International Conference on Architectural Support for Programming Languages and Operating Systems</i>.  <i>ASPLOS-X</i>.  ACM Press; 2002:5–17.</p><p class="reflist2" epub:type="biblioentry footnote" id="br0745" role="doc-biblioentry">[149] Amitabha Roy, Steven Hand, Tim Harris,  A runtime system for software lock elision,   <i>Proceedings of the EuroSys2009 Conference</i>.  <i>Nuremberg, Germany</i>.  March 2009.</p><p class="reflist2" epub:type="biblioentry footnote" id="br0785" role="doc-biblioentry">[157] N. Shavit, D. Touitou,  Software transactional memory,   <cite><i>Distributed Computing</i></cite> February 1997;10(2):99–116.</p></div></section><section epub:type="rearnotes"><div class="ftnote"><hr/><p class="ftnote1" epub:type="footnote" id="fn001" role="doc-footnote"><sup><a epub:type="noteref" href="#cf0015" role="doc-noteref">1 </a></sup> <a id="np0010"/>“Kernel v2.4.19 <img alt="Image" height="13" src="images/B9780124159501000306/fx001.jpg" width="79"/>.”</p></div></section></footer></section></body></html>