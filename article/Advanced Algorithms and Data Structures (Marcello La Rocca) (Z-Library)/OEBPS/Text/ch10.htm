<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>10</title>
    
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
  <div class="tocheadb">
    <h1 class="tochead" id="heading_id_2"><a id="pgfId-998625"></a><a id="pgfId-998637"></a>10 Similarity Search Trees: Approximate nearest neighbors search for image retrieval</h1>
  </div>

  <p class="co-summary-head"><a id="pgfId-1019893"></a>This chapter covers</p>

  <ul class="calibre19">
    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1019925"></a>Discussing the limits of k-d trees</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1019926"></a>Describing image retrieval as a use case where k-d trees would struggle</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1019927"></a>Introducing a new data structure, the R-tree</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1019928"></a>Presenting SS-trees, a scalable variant of R-trees</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1019929"></a>Comparing SS-trees and k-d trees</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1019914"></a>Introducing approximate similarity search</li>
  </ul>

  <p class="body"><a id="pgfId-998749"></a>This chapter will be structured slightly differently from our book’s standard, because we will continue here a discussion started in chapter 8. There, we introduced the problem of searching multidimensional data for the nearest neighbor(s) of a generic point (possibly not in the dataset itself). In chapter 9, we introduce k-d trees, a data structure specifically invented to solve this problem.</p>

  <p class="body"><a id="pgfId-998766"></a>K-d trees are the best solution to date for indexing low- to medium-dimensional datasets that will completely fit in memory. When we have to operate on high-dimensional data or with big datasets that won’t fit in memory, k-d trees are not enough, and we will need to use more advanced data structures.</p>

  <p class="body"><a id="pgfId-998781"></a>In this chapter we first present a new problem, one that will push our indexing data structure beyond its limits, and then introduce two new data structures, R-trees and SS-trees, that can help us solve this category of problems efficiently.</p>

  <p class="body"><a id="pgfId-998790"></a>Brace yourself—this is going to be a long journey (and a long chapter!) through some of the most advanced material we have presented so far. We’ll try to make it through this journey step by step, section by section, so don’t let the length of this chapter intimidate you!</p>

  <h2 class="fm-head" id="heading_id_3"><a id="pgfId-998799"></a>10.1 Right where we left off</h2>

  <p class="body"><a id="pgfId-998818"></a>Let’s briefly recap where we left off in previous chapters. We were designing software for an e-commerce company, an application to find the closest warehouse selling a given product for any point on a very large map. Check out figure 9.4 to visualize it. To have a ballpark idea of the kind of scale we need, we want to serve millions of clients per day across the country, taking products from thousands of warehouses also spread across the map.</p>

  <p class="body"><a id="pgfId-998837"></a>In section 8.2, we have already established that a brute-force approach is not practical for applications at scale, and we need to resort to a brand-new data structure designed to handle multidimensional indexing. Chapter 9 described k-d trees, a milestone in multidimensional data indexing and a true game changer, which worked perfectly with the example we used in chapters 8 and 9 where we only needed to work with 2-D data. The only issue we faced is the fact that our dataset was dynamic and thus insertion/removal would produce an imbalanced tree, but we could rebuild the tree every so often (for instance, after 1% of its elements had been changed because of insertions or removals), and amortize the cost of the operation by running it in a background process (keeping the old version of the tree in the meantime, and either putting insert/delete on hold, or reapplying these operations to the new tree once it had been created and “promoted” to current).</p>

  <p class="body"><a id="pgfId-998856"></a>While in that case we could find workarounds, in other applications we won’t necessarily be so lucky. There are, in fact, intrinsic limitations that k-d trees can’t overcome:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-998869"></a>K-d trees are not self-balancing, so they perform best when they are constructed from a stable set of points, and when the number of inserts and removes is limited with respect to the total number of elements.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-998884"></a>The curse of dimensionality<a class="calibre14" id="marker-1009872"></a>: When we deal with high-dimensional spaces, k-d trees become inefficient, because running time for search is exponential in the dimension of the dataset. For points in the <code class="fm-code-in-text">k</code>-dimensional space, when <code class="fm-code-in-text">k ≈ 30,</code> k-d trees can’t give any advantage over brute-force search.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-998911"></a><a class="calibre14" id="id_Hlk17902585"></a>K-d trees don’t work well with paged memory, because they are not memory-efficient with respect to the locality of reference, as points are stored in tree nodes, so nearby points won’t lie close to memory areas.</p>
    </li>
  </ul>

  <h3 class="fm-head2" id="heading_id_4"><a id="pgfId-998929"></a>10.1.1 A new (more complex) example</h3>

  <p class="body"><a id="pgfId-998949"></a>To <a id="marker-1009876"></a><a id="marker-1009880"></a>illustrate a practical situation where k-d trees are not the recommended solution, let’s pivot on our warehouse search and imagine a different scenario, where we fast-forward 10 years. Our e-commerce company has evolved and doesn’t sell just groceries anymore, but also electronics and clothes. It’s almost 2010, and customers expect valuable recommendations when they browse our catalog; but even more importantly, the company’s marketing department expects that you, as CTO, make sure to increase sales by showing customers suggestions they actually like.</p>

  <p class="body"><a id="pgfId-998975"></a>For instance, if customers are browsing smartphones (the hottest product in the catalog, ramping up to rule the world of electronics, back in the day!), your application is supposed to show them more smartphones in a similar price/feature range. If they are looking at a cocktail dress, they should see more dresses that look similar to the one they (possibly) like.</p>

  <p class="body"><a id="pgfId-998988"></a>Now, these two problems look (and partially are) quite different, but they both boil down to the same core issue: given a product with a list of features, find one or more products with similar features. Obviously, the way we extract these feature lists from a consumer electronics product and a dress is very different!</p>

  <p class="body"><a id="pgfId-998997"></a>Let’s focus on the latter, illustrated in figure 10.1. Given an image of a dress, find other products in your catalog that look similar—this is a very stimulating problem, even today!</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F1.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1048946"></a>Figure 10.1 Feature extraction on an image dataset. Each image is translated into a feature vector (through what’s represented as a “black box” feature extractor, because we are not interested in the algorithm that creates this vectors). Then, if we have to search an entry <code class="fm-code-in-text">P</code>, we compare <code class="fm-code-in-text">P</code>’s feature vector to each of the images’ vectors, computing their mutual distance based on some metric. (In the figure, Euclidean distance. Notice that when looking for the minimum of these Euclidean distances, we can sometimes compute the squared distances, avoiding applying a square root operation for each entry.)</p>

  <p class="body"><a id="pgfId-999038"></a>The way we extract features from images completely changed in the last 10 years. In 2009, we used to extract edges, corners, and other geometrical features from the images, using dozens of algorithms specialized for the single feature, and then build higher-level features by hand (quite literally).</p>

  <p class="body"><a id="pgfId-999055"></a>Today, instead, we use deep learning for the task, training a CNN<a href="#pgfId-1011665"><sup class="footnotenumber">1</sup></a> on a larger dataset and then applying it to all the images in our catalog to generate their feature vectors.</p>

  <p class="body"><a id="pgfId-999066"></a>Once we have these feature vectors, though, the same question arises now as then: How do we efficiently search the most similar vectors to a given one?</p>

  <p class="body"><a id="pgfId-999075"></a>This is exactly the same problem we illustrated in chapter 8 for 2-D data, applied to a huge dataset (with tens of thousands of images/feature vectors), and where tuples have hundreds of features.</p>

  <p class="body"><a id="pgfId-999084"></a>Contrary to the feature extraction, the search algorithms haven’t changed much in the last 10 years, and the data structures that we introduce in this chapter, invented between the late 1990s and the early 2000s, are still cutting-edge choices for efficient search in the vector <a id="marker-1009884"></a><a id="marker-1009888"></a>space.</p>

  <h3 class="fm-head2" id="heading_id_5"><a id="pgfId-999100"></a>10.1.2 Overcoming k-d trees’ flaws</h3>

  <p class="body"><a id="pgfId-999114"></a>Back <a id="marker-1009892"></a><a id="marker-1009896"></a><a id="marker-1009900"></a><a id="marker-1009904"></a>in chapter 9, we also mentioned a couple of possible structural solutions to cope with the problems discussed in the previous section:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-999128"></a>Instead of partitioning points using a splitting line passing through a dataset’s points, we can divide a region into two balanced halves with respect to the number of points or the sub-region’s size.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-999148"></a>Instead of cycling through dimensions, we can choose at every step the dimension with the greatest spread or variance and store the choice made in each tree node.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-999166"></a>Instead of storing points in nodes, each node could describe a region of space and link (directly or indirectly) to an array containing the actual elements.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-999178"></a>These solutions are the basis of the data structures we will discuss in this chapter, <i class="calibre17">R-trees</i> <a id="marker-1009908"></a><a id="marker-1009912"></a><a id="marker-1009916"></a><a id="marker-1009920"></a>and <i class="calibre17">SS-trees</i>.</p>

  <h2 class="fm-head" id="heading_id_6"><a id="pgfId-999196"></a>10.2 R-tree</h2>

  <p class="body"><a id="pgfId-999208"></a>The <a id="marker-1009924"></a><a id="marker-1009928"></a>first evolution of k-d trees we will discuss are R-trees. Although we won’t delve into the details of their implementation, we are going to discuss the idea behind this solution, why they work, and their high-level mechanism.</p>

  <p class="body"><a id="pgfId-999222"></a>R-trees were introduced in 1984 by <a id="id_Hlk56332380"></a>Antonin Guttman in the paper “R-Trees. A Dynamic Index Structure For Spatial Searching.”</p>

  <p class="body"><a id="pgfId-999240"></a>They are inspired by B-trees,<a href="#pgfId-1011679"><sup class="footnotenumber">2</sup></a> balanced trees with a hierarchical structure. In particular, Guttman used as a starting point B+ trees, a variant where only leaf nodes contain data, while inner nodes only contain keys and serve the purpose of hierarchically partitioning data.</p>

  <h3 class="fm-head2" id="heading_id_7"><a id="pgfId-999255"></a>10.2.1 A step back: Introducing B-trees</h3>

  <p class="body"><a id="pgfId-999267"></a>Figure 10.2 <a id="marker-1009932"></a>shows an example of a B-tree, in particular a B+tree. These data structures were meant to index unidimensional data, partitioning it into pages,<a href="#pgfId-1011696"><sup class="footnotenumber">3</sup></a> providing efficient storage on disk and fast search (minimizing the number of pages loaded, and so the number of disk accesses).</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F2.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1048993"></a>Figure 10.2 B+ tree explained. The example shows a B+ tree with a branching factor of <code class="fm-code-in-text">d == 3</code>.</p>

  <p class="body"><a id="pgfId-999308"></a>Each node (both internal nodes and leaves) in a B-tree contains between <code class="fm-code-in-text">d-1</code> and <code class="fm-code-in-text">2*d-1</code> keys, where <code class="fm-code-in-text">d</code> is a fixed parameter for each tree, its branching factor:<a href="#pgfId-1011715"><sup class="footnotenumber">4</sup></a> the (minimum, in this case) number of children for each node. The only exception can be the root, which can possibly contain fewer than <code class="fm-code-in-text">d-1</code> keys. Keys are stored in an ordered list; this is fundamental to have a fast (logarithmic) search. In fact, each internal node with <code class="fm-code-in-text">m</code> keys, <code class="fm-code-in-text">k<sub class="subscript1">0</sub>, k<sub class="subscript1">1</sub>, ..., k<sub class="subscript1">m-1</sub>, d-1</code> <span class="cambria">≤</span> <code class="fm-code-in-text">m</code> <span class="cambria">≤</span> <code class="fm-code-in-text">2*d*-1</code>, also has exactly <code class="fm-code-in-text">m+1</code> children, <code class="fm-code-in-text">C<sub class="subscript1">0</sub>, C<sub class="subscript1">1</sub>, ..., C<sub class="subscript1">m-1</sub>, C<sub class="subscript1">m</sub></code>, such that <code class="fm-code-in-text">k &lt; k<sub class="subscript1">0</sub></code> for each key <code class="fm-code-in-text">k</code> in the subtree rooted in <code class="fm-code-in-text">C<sub class="subscript1">0</sub>; k<sub class="subscript1">0</sub></code> <span class="cambria">≤</span> <code class="fm-code-in-text">k &lt; k<sub class="subscript1">1</sub></code> for each key <code class="fm-code-in-text">k</code> in the subtree rooted in <code class="fm-code-in-text">C<sub class="subscript1">1</sub></code>; and so on.</p>

  <p class="body"><a id="pgfId-999380"></a>In a B-tree, keys and items are stored in the nodes, each key/item is stored exactly once in a single node, and the whole tree stores exactly <code class="fm-code-in-text">n</code> keys if the dataset has <code class="fm-code-in-text">n</code> items. In a B+ tree, internal nodes only contains keys, and only leaves contain pairs, each with keys and links to the items. This means that a B+tree storing <code class="fm-code-in-text">n</code> items has <code class="fm-code-in-text">n</code> leaves, and that keys in internal nodes are also stored in all its descendants (see how, in the example in figure 10.2, the keys <code class="fm-code-in-text">4</code>, <code class="fm-code-in-text">10,</code> and <code class="fm-code-in-text">45</code> are also stored in leaves).</p>

  <p class="body"><a id="pgfId-999418"></a>Storing links to items in the leaves, instead of having the actual items hosted in the tree, serves a double purpose:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-999429"></a>Nodes are more lightweight and easier to allocate/garbage collect.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-999442"></a>It allows storing all items in an array, or in a contiguous block of memory, exploiting the memory locality of neighboring items.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-999455"></a>When these trees are used to store huge collections of large items, these properties allow us to use memory paging efficiently. By having lightweight nodes, it is more likely the whole tree will fit in memory, while items can be stored on disk, and leaves can be loaded on a need-to basis. Because it is also likely that after accessing an item <code class="fm-code-in-text">X</code>, applications will need to access one of its contiguous items, by loading in memory the whole B-tree leaf containing <code class="fm-code-in-text">X,</code> we can reduce the disk reads as much as possible.</p>

  <p class="body"><a id="pgfId-999484"></a>Not surprisingly, for these reasons B-trees have been the core of many SQL database engines since their invention<a href="#pgfId-1011730"><sup class="footnotenumber">5</sup></a>—and even today they are still the data structure of choice for storing <a id="marker-1009936"></a>indices.</p>

  <h3 class="fm-head2" id="heading_id_8"><a id="pgfId-999498"></a>10.2.2 From B-Tree to R-tree</h3>

  <p class="body"><a id="pgfId-999510"></a>R-trees extend the main ideas behind B+trees to the multidimensional case. While for unidimensional data each node corresponds to an interval (the range from the left-most and right-most keys in its sub-tree, which are in turn its minimum and maximum keys), in R-trees each node <code class="fm-code-in-text">N</code> covers a rectangle (or a hyper-rectangle in the most generic case), whose corners are defined by the minimum and maximum of each coordinate over all the points in the subtree rooted at <code class="fm-code-in-text">N</code>.</p>

  <p class="body"><a id="pgfId-999529"></a>Similarly to B-trees, R-trees are also parametric. Instead of a branching factor <code class="fm-code-in-text">d</code> controlling the minimum number of entries per node, R-trees require their clients to provide two parameters on creation:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-999545"></a><code class="fm-code-in-text">M</code>, the maximum number of entries in a node; this value is usually set so that a full node will fit in a page of memory.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-999555"></a><code class="fm-code-in-text">m</code>, such that <code class="fm-code-in-text">m</code> <span class="cambria">≤</span> <code class="fm-code-in-text">M/2</code>, the minimum number of entries in a node. This parameter indirectly controls the minimum height of the tree, as we’ll see.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-999578"></a>Given values for these two parameters, R-trees abide by a few invariants:</p>

  <ol class="calibre18">
    <li class="fm-list-numbered">
      <p class="list"><a class="calibre14" id="pgfId-999587"></a>Every leaf contains between <code class="fm-code-in-text">m</code> and <code class="fm-code-in-text">M</code> points (except for the root, which can possibly have less than <code class="fm-code-in-text">m</code> points).</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-999607"></a>Each leaf node <code class="fm-code-in-text">L</code> has associated a hyper-rectangle <code class="fm-code-in-text">RL</code>, such that <code class="fm-code-in-text">RL</code> is the smallest rectangle containing all the points in the leaf.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-999628"></a>Every internal node has between <code class="fm-code-in-text">m</code> and <code class="fm-code-in-text">M</code> children (except for the root, which can possibly have less than <code class="fm-code-in-text">m</code> children).</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-999648"></a>Each internal node <code class="fm-code-in-text">N</code> has associated a bounding (hyper-)rectangle <code class="fm-code-in-text">R<sub class="subscript1">N</sub></code><a class="calibre14" id="id_Hlk17990640"></a>, such that <code class="fm-code-in-text">R<sub class="subscript1">N</sub></code> is the smallest rectangle, whose edges are parallel to the Cartesian axes, entirely containing all the bounding rectangles of <code class="fm-code-in-text">N</code>’s children.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-999672"></a>The root node has at least two children, unless it is a leaf.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-999687"></a>All leaves are at the same level.</p>
    </li>
  </ol>

  <p class="body"><a id="pgfId-999699"></a>Property number 6 tells us that R-trees are balanced, while from properties 1 and 3 we can infer that the maximum height of an R-tree containing <code class="fm-code-in-text">n</code> points is <code class="fm-code-in-text">log<sub class="subscript1">M</sub>(n)</code>.</p>

  <p class="body"><a id="pgfId-999714"></a>On insertion, if any node on the path from the root to the leaf holding the new point becomes larger than <code class="fm-code-in-text">M</code> entries, we will have to split it, creating two nodes, each with half the elements.</p>

  <p class="body"><a id="pgfId-999725"></a>On removal, if any node becomes smaller than <code class="fm-code-in-text">m</code> entries, we will have to merge it with one of its adjacent siblings.</p>

  <p class="body"><a id="pgfId-999736"></a>Invariants 2 and 4 require some extra work to be maintained true, but these bounding rectangles defined for each node are needed to allow fast search on the tree.</p>

  <p class="body"><a id="pgfId-999745"></a>Before describing how the search methods work, let’s take a closer look at an example of an R-tree in figures 10.3 and 10.4. We will stick to the 2-D case because it is easier to visualize, but as always, you have to imagine that real trees can hold 3-D, 4-D, or even 100-D points.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F3.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1049035"></a>Figure 10.3 Cartesian plane representation of a (possible) R-tree for our city maps as presented in the example of figure 9.4 (the names of the cities are omitted to avoid confusion). This R-tree contains 12 bounding rectangles, from <code class="fm-code-in-text">R<sub class="subscript1">1</sub></code> to <code class="fm-code-in-text">R<sub class="subscript1">12</sub></code>, organized in a hierarchical structure. Notice that rectangles can and do overlap, as shown in the bottom half.</p>

  <p class="body"><a id="pgfId-999784"></a>If we compare figure 10.3 to figure 9.6, showing how a k-d tree organizes the same dataset, it is immediately apparent how the two partitionings are completely different:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-999793"></a>R-trees create regions in the Cartesian plane in the shape of rectangles, while k-d trees split the plane along lines.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-999806"></a>While k-d trees alternate the dimension along which the split is done, R-trees don’t cycle through dimensions. Rather, at each level the sub-rectangles created can partition their bounding box in any or even all dimensions at the same time.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-999823"></a>The bounding rectangles can overlap, both across different sub-trees and even with siblings’ bounding boxes sharing the same parent. However, and this is crucial, no sub-rectangle extends outside its parent’s bounding box.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-999835"></a>Each internal node defines a so-called <i class="calibre15">bounding envelope</i><a class="calibre14" id="marker-1009940"></a>, that for R-trees is the smallest rectangle containing all the bounding envelopes of the node’s children.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-999851"></a>Figure 10.4 shows how these properties translate into a tree data structure; here the difference with k-d trees is even more evident!</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F4.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1049080"></a>Figure 10.4 The tree representation for the R-tree from figure 10.3. The parameters for this R-tree are <code class="fm-code-in-text">m==1</code> and <code class="fm-code-in-text">M==3</code>. Internal nodes only hold bounding boxes, while leaves hold the actual points (or, in general, <code class="fm-code-in-text">k</code>-dimensional entries). In the rest of the chapter we will use a more compact representation, for each node drawing just the list of its children.</p>

  <p class="body"><a id="pgfId-999889"></a>Each internal node is a list of rectangles (between <code class="fm-code-in-text">m</code> and <code class="fm-code-in-text">M</code> of them, as mentioned), while leaves are lists of (again, between <code class="fm-code-in-text">m</code> and <code class="fm-code-in-text">M</code>) points. Each rectangle is effectively determined by its children and could indeed be defined iteratively in terms of its children. For practical reasons such as improving the running time of the search methods, in practice we store the bounding box for each rectangle.</p>

  <p class="body"><a id="pgfId-999922"></a>Because the rectangles can only be parallel to the Cartesian axes, they are defined by two of their vertices: two tuples with <code class="fm-code-in-text">k</code> coordinates, one tuple for the minimum values of each coordinate, and one for the maximum value of each coordinate.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F5.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1049176"></a>Figure 10.5 R-trees entries, besides points, can also be rectangles or non-zero-measure entities. In this example, entities <code class="fm-code-in-text">R<sub class="subscript1">7</sub></code> to <code class="fm-code-in-text">R<sub class="subscript1">14</sub></code> are the tree’s entries, while <code class="fm-code-in-text">R<sub class="subscript1">3</sub></code> to <code class="fm-code-in-text">R<sub class="subscript1">6</sub></code> are the tree’s leaves.</p>

  <p class="body"><a id="pgfId-999938"></a>Notice how unlike k-d trees, an R-tree could handle a non-zero-measure object by simply considering its bounding boxes as special cases of rectangles, as illustrated in figure 10.5.</p>

  <h3 class="fm-head2" id="heading_id_9"><a id="pgfId-999980"></a>10.2.3 Inserting points in an R-tree</h3>

  <p class="body"><a id="pgfId-999994"></a>Now, <a id="marker-1009944"></a>of course, you might legitimately wonder how you get from a raw dataset to the R-tree in figure 10.5. After all, we just presented it and asked you to take it as a given.</p>

  <p class="body"><a id="pgfId-1000009"></a>Insertion for R-trees is similar to B-trees and has many steps in common with SS-trees, so we won’t duplicate the learning effort with a detailed description here.</p>

  <p class="body"><a id="pgfId-1000018"></a>At a high level, to insert a new point you will need to follow the following steps:</p>

  <ol class="calibre18">
    <li class="fm-list-numbered">
      <p class="list"><a class="calibre14" id="pgfId-1000027"></a>Find the leaf that should host the new point <code class="fm-code-in-text">P</code>. There are three possible cases:</p>

      <ol class="calibre35">
        <li class="fm-list-numbered1">
          <p class="list"><a class="calibre14" id="pgfId-1000041"></a><code class="fm-code-in-text">P</code> lies exactly within one of the leaves’ rectangles, <code class="fm-code-in-text">R</code>. Then just add <code class="fm-code-in-text">P</code> to <code class="fm-code-in-text">R</code> and move to the next step.</p>
        </li>

        <li class="fm-list-numbered1">
          <p class="list"><a class="calibre14" id="pgfId-1000063"></a><code class="fm-code-in-text">P</code> lies within the overlapping region between two or more leaves’ bounding rectangles. For example, referring to figure 10.6, it might lie in the intersection of <code class="fm-code-in-text">R<sub class="subscript1">12</sub></code> and <code class="fm-code-in-text">R<sub class="subscript1">14</sub></code>. In this case, we need to decide where to add <code class="fm-code-in-text">P;</code> the heuristic used to make these decisions will determine the shape of the tree (as an example, one heuristic could be just adding it to the rectangle with fewer elements).</p>
        </li>

        <li class="fm-list-numbered1">
          <p class="list"><a class="calibre14" id="pgfId-1000086"></a>If P lies outside of all rectangles at the leaves’ level, then we need to find the closest leaf <code class="fm-code-in-text">L</code> and add <code class="fm-code-in-text">P</code> to it (again, we can use more complex heuristics than just the Euclidean distance to decide).</p>
        </li>
      </ol>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1000102"></a>Add the points to the leaf’s rectangle <code class="fm-code-in-text">R</code>, and check how many points it contains afterward:</p>

      <ol class="calibre35">
        <li class="fm-list-numbered1">
          <p class="list"><a class="calibre14" id="pgfId-1000114"></a>If, after the new point is added, the leaf still has at most <code class="fm-code-in-text">M</code> points, then we are done.</p>
        </li>

        <li class="fm-list-numbered1">
          <p class="list"><a class="calibre14" id="pgfId-1000128"></a>Otherwise, we need to split <code class="fm-code-in-text">R</code> into two new rectangles, <code class="fm-code-in-text">R<sub class="subscript1">1</sub></code> and <code class="fm-code-in-text">R<sub class="subscript1">2</sub></code>, and go to step 3.</p>
        </li>
      </ol>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1000150"></a>Remove <code class="fm-code-in-text">R</code> from its parent <code class="fm-code-in-text">R<sub class="subscript1">P</sub></code> and add <code class="fm-code-in-text">R<sub class="subscript1">1</sub></code> and <code class="fm-code-in-text">R<sub class="subscript1">2</sub></code> to <code class="fm-code-in-text">R<sub class="subscript1">P</sub></code>. If <code class="fm-code-in-text">R<sub class="subscript1">P</sub></code> now has more than <code class="fm-code-in-text">M</code> children, split it and repeat this step recursively.</p>

      <ol class="calibre35">
        <li class="fm-list-numbered1">
          <p class="list"><a class="calibre14" id="pgfId-1000181"></a>If <code class="fm-code-in-text">R</code> was the root, we obviously can’t remove it from its parent; we just create a new root and set <code class="fm-code-in-text">R<sub class="subscript1">1</sub></code> and <code class="fm-code-in-text">R<sub class="subscript1">2</sub></code> as children.</p>
        </li>
      </ol>
    </li>
  </ol>

  <p class="body"><a id="pgfId-1000236"></a>To complete the insertion algorithm outlined here, we need to provide a few heuristics to break ties for overlapping rectangles and to choose the closest rectangle, but even more importantly, we haven’t said anything about how we are going to split a rectangle at points 2 and 3.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F6.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1049231"></a>Figure 10.6 Choosing the R-tree leaf’s rectangle to which a point should be added: the new point can lie within a leaf rectangle (<code class="fm-code-in-text">P<sub class="subscript1">A</sub></code>), within the intersection of two or more leaves’ rectangles (<code class="fm-code-in-text">P<sub class="subscript1">B</sub></code>), or outside any of the leaves (<code class="fm-code-in-text">P<sub class="subscript1">C</sub></code> and <code class="fm-code-in-text">P<sub class="subscript1">D</sub></code>).</p>

  <p class="body"><a id="pgfId-1000247"></a>This choice, together with the heuristic for choosing the insertion subtree, determines the behavior and shape (not to mention performance) of the R-tree.</p>

  <p class="body"><a id="pgfId-1000256"></a>Several heuristics have been studied over the years, each one aiming to optimize one or more usages of the tree. The split heuristics can be particularly complicated for internal nodes because we don’t just partition points, but <code class="fm-code-in-text">k</code>-dimensional shapes. Figure 10.7 shows how easily a naïve choice could lead to inefficient splits.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F7.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1049278"></a>Figure 10.7 An example of bad and good splits of an internal node’s rectangle, taken from the original paper by Antonin Guttman</p>

  <p class="body"><a id="pgfId-1000296"></a>Delving into these heuristics is out of the scope of this section; we refer the curious reader to the <span class="fm-hyperlink">original paper</span> by Antonin Guttman for a proper description. At this point, though, we can already reveal that the complexity of handling hyper-rectangles and obtaining good splits (and merges, after removals) is one of the main reasons that led to the introduction of <a id="marker-1009948"></a>SS-trees.</p>

  <h3 class="fm-head2" id="heading_id_10"><a id="pgfId-1000315"></a>10.2.4 Search</h3>

  <p class="body"><a id="pgfId-1000327"></a>Searching <a id="marker-1009952"></a>for a point or for the nearest neighbor (<i class="calibre17">NN</i> ) of a point in R-trees is very similar to what happens in k-d trees. We need to traverse the tree, pruning branches that can’t contain a point, or, for NN search, are certainly further away than the current minimum distance.</p>

  <p class="body"><a id="pgfId-1000344"></a>Figure 10.8 shows an example of an (unsuccessful) point search on our example R-tree. Remember that an unsuccessful search is the first step for inserting a new point, through which we can find the rectangle (or rectangles, in this case) where we should add the new point.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F8.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1049320"></a>Figure 10.8 Unsuccessful search on the R-tree in figures 10.4 and 10.5. The path of the search is highlighted in both the Cartesian and tree views, and curved arrows show the branches traversed in the tree. Notice the compact representation of the tree, compared to figure 10.5.</p>

  <p class="body"><a id="pgfId-1000387"></a>The search starts at the root, where we compare point <code class="fm-code-in-text">P</code>’s coordinates with the boundaries of each rectangle, <code class="fm-code-in-text">R<sub class="subscript1">1</sub></code> and <code class="fm-code-in-text">R<sub class="subscript1">2</sub></code><a id="id_Hlk56319462"></a>; <code class="fm-code-in-text">P</code> can only be within <code class="fm-code-in-text">R<sub class="subscript1">2</sub></code>, so this is the only branch we traverse.</p>

  <p class="body"><a id="pgfId-1000410"></a>At the next step, we go through <code class="fm-code-in-text">R<sub class="subscript1">2</sub></code>’s children, <code class="fm-code-in-text">R<sub class="subscript1">5</sub></code> and <code class="fm-code-in-text">R<sub class="subscript1">6</sub></code>. Both can contain <code class="fm-code-in-text">P</code>, so we need to traverse both branches at this level (as shown by the two curved arrows, leaving <code class="fm-code-in-text">R<sub class="subscript1">2</sub></code> in the bottom half of figure 10.8).</p>

  <p class="body"><a id="pgfId-1000446"></a>This means we need to go through the children of both rectangles <code class="fm-code-in-text">R<sub class="subscript1">5</sub></code> and <code class="fm-code-in-text">R<sub class="subscript1">6</sub></code>, checking from <code class="fm-code-in-text">R<sub class="subscript1">11</sub></code> to <code class="fm-code-in-text">R<sub class="subscript1">14</sub></code>. Of these, only <code class="fm-code-in-text">R<sub class="subscript1">12</sub></code> and <code class="fm-code-in-text">R<sub class="subscript1">14</sub></code> can contain <code class="fm-code-in-text">P</code>, so those are the only rectangles whose points we will check at the last step. Neither contains <code class="fm-code-in-text">P</code>, so the search method can return <code class="fm-code-in-text">false</code>, and optionally the two leaves’ rectangles that could host <code class="fm-code-in-text">P</code>, if inserted.</p>

  <p class="body"><a id="pgfId-1000485"></a>Nearest neighbor search works similarly, but instead of checking whether a point belongs to each rectangle, it keeps the distance of the current nearest neighbor and checks to see if each rectangle is closer than that (otherwise, it can prune it). This is similar to the rectangular region search in k-d trees, as described in section 9.3.6.</p>

  <p class="body"><a id="pgfId-1000494"></a>We won’t delve into NN-search for R-trees. Now that you should have a high-level understanding of this data structure, we are ready to move on to its evolution, the SS-tree.</p>

  <p class="body"><a id="pgfId-1000507"></a>It’s also worth mentioning that R-trees do not guarantee good worst-case performance, but in practice they usually perform better than k-d trees, so they were for a long time the de facto standard for similarity search and indexing of <a id="marker-1009956"></a>multidimensional <a id="marker-1009960"></a><a id="marker-1009964"></a>datasets.</p>

  <h2 class="fm-head" id="heading_id_11"><a id="pgfId-1000523"></a>10.3 Similarity search tree</h2>

  <p class="body"><a id="pgfId-1000535"></a>In section 10.2, we saw some of the key properties that influence the shape and performance of R-trees. Let’s recap them here:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1000550"></a>The splitting heuristic</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1000562"></a>The criteria used to choose the sub-tree to add new points (if more than one overlaps)</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1000574"></a>The distance metric</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1000586"></a>For R-trees, we assumed that aligned boxes, hyper-rectangles parallel to the Cartesian axes, are used as bounding envelopes for the nodes. If we lift this constraint, the shape of the bounding envelope becomes the fourth property of a more general class of similarity search trees.</p>

  <p class="body"><a id="pgfId-1000597"></a>And, indeed, at their core, the main difference between R-trees and SS-trees in their most basic versions, is the shape of bounding envelopes. As shown in figure 10.9, this variant (built on R-trees) uses spheres instead of rectangles.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F9.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1049473"></a>Figure 10.9 Representation of a possible SS-tree covering the same dataset of figures 10.4 and 10.5, with parameters <code class="fm-code-in-text">m==1</code> and <code class="fm-code-in-text">M==3</code>. As you can see, the tree structure is similar to R-trees’. For the sake of avoiding clutter, only a few spheres’ centroids and radii are shown. For the tree, we use the compact representation (as shown in figures 10.5 and 10.8).</p>

  <p class="body"><a id="pgfId-1000636"></a>Although it might seem like a small change, there is strong theoric and practical evidence that suggest using spheres reduces the average number of leaves touched by a similarity (nearest neighbor or region) query. We will discuss this point in more depth in section 10.5.1.</p>

  <p class="body"><a id="pgfId-1000651"></a>Each internal node <code class="fm-code-in-text">N</code> is therefore a sphere with a center and a radius. Those two properties are uniquely and completely determined by <code class="fm-code-in-text">N</code>’s children. <code class="fm-code-in-text">N</code>’s center is, in fact, the centroid of <code class="fm-code-in-text">N</code>’s children,<a href="#pgfId-1011746"><sup class="footnotenumber">6</sup></a> and the radius is the maximum distance between the centroid and <code class="fm-code-in-text">N</code>’s points.</p>

  <p class="body"><a id="pgfId-1000676"></a>To be fair, when we said that the only difference between R-trees and SS-trees was the shape of the bounding envelopes, we were guilty of omission. The choice of a different shape for the bounding envelopes also forces us to adopt a different splitting heuristic. In the case of SS-trees, instead of trying to reduce the spheres’ overlap on split, we aim to reduce the variance of each of the newly created nodes; therefore, the original splitting heuristic chooses the dimension with the highest variance and then splits the sorted list of children to reduce variance along that dimension (we’ll see this in more detail in the discussion about insertion in section 10.3.2).</p>

  <p class="body"><a id="pgfId-1000701"></a>As for R-trees, SS-trees have two parameters, <code class="fm-code-in-text">m</code> and <code class="fm-code-in-text">M</code>, respectively the minimum and maximum number of children each node (except the root) is allowed to have.</p>

  <p class="body"><a id="pgfId-1000714"></a>And like R-trees, bounding envelopes in an SS-tree might overlap. To reduce the overlap, some variants like <i class="calibre17">SS<sub class="subscript">+</sub>-trees</i> introduce a fifth property (also used in R-tree’s variants like <i class="calibre17">R*-trees</i>), another heuristic used on insert that performs major changes to restructure the tree; we will talk about SS+-trees later in this chapter, but for now we will focus on the implementation of plain SS-trees.</p>

  <p class="body"><a id="pgfId-1000739"></a>The first step toward a pseudo-implementation for our data structures is, as always, presenting a pseudo-class that models it. In this case, to model an SS-tree we are going to need a class modeling tree nodes. Once we build an SS-tree through its nodes, to access it we just need a pointer to the tree’s root. For convenience, as shown in listing 10.1, we will include this link and the values for parameters <code class="fm-code-in-text">m</code> and <code class="fm-code-in-text">M</code> in the <code class="fm-code-in-text">SsTree</code> class, as well as the dimensionality <code class="fm-code-in-text">k</code> of each data entry, and assume all these values are available from each of the tree nodes.</p>

  <p class="body"><a id="pgfId-1000769"></a>As we have seen, SS-trees (like R-trees) have two different kind of nodes, leaves and internal nodes, that are structurally and behaviorally different. The former stores <code class="fm-code-in-text">k</code>-dimensional tuples (references to the points in our dataset), and the latter only has links to its children (which are also nodes of the tree).</p>

  <p class="body"><a id="pgfId-1000786"></a>To keep things simple and as language-agnostic as possible, we will store both an array of children and an array of points into each node, and a Boolean flag will tell apart leaves from internal nodes. The <code class="fm-code-in-text">children</code> array<a id="marker-1009968"></a> will be empty for leaves and the <code class="fm-code-in-text">points</code> array<a id="marker-1009972"></a> will be empty for internal nodes.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020163"></a>Listing 10.1 The <code class="fm-code-in-text">SsTree</code> and <code class="fm-code-in-text">SsNode</code> classes</p>
  <pre class="programlisting"><b class="calibre21">class</b> SsNode
  <b class="calibre21">#type</b> tuple(k)
  centroid
  <b class="calibre21">#type</b> float
  radius
 
  <b class="calibre21">#type</b> SsNode[]
  children
 
  <b class="calibre21">#type</b> tuple(k)[]
  points
  
  <b class="calibre21">#type</b> boolean
  Leaf
 
  <b class="calibre21">function</b> SsNode(leaf, points=[], children=[])
 
<b class="calibre21">class</b> SsTree
  <b class="calibre21">#type</b> SsNode
  root
  <b class="calibre21">#type</b> integer 
  m
  <b class="calibre21">#type</b> integer 
  M
  <b class="calibre21">#type</b> integer 
  k
 
  <b class="calibre21">function</b> SsTree(k, m, M)</pre>

  <p class="body"><a id="pgfId-1001088"></a>Notice how in figure 10.9 we represented our tree nodes as a list of spheres, each of which has a link to a child. We could, of course, add a type <code class="fm-code-in-text">SsSphere</code><a id="marker-1009976"></a> and keep a link to each sphere’s only child node as a field of this new type. It wouldn’t make a great design, though, and would lead to data duplication (because then both <code class="fm-code-in-text">SsNode</code><a id="marker-1009980"></a> and <code class="fm-code-in-text">SsSphere</code> would hold fields for centroids and radius) and create an unnecessary level of indirection. Just keep in mind that when you look at the diagrams of SS-trees in these pages, what are shown as components of a tree node are actually its children.</p>

  <p class="body"><a id="pgfId-1001107"></a>One effective alternative to translate this into code in object-oriented programming is to use inheritance, defining a common abstract class (a class that can’t be instantiated to an actual object) or an interface, and two derived classes (one for leaves and one for internal nodes) that share a common data and behavior (defined in the base, abstract class), but are implemented differently. Listing 10.2 shows a possible pseudo-code description of this pattern.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020195"></a>Listing 10.2 Alternative Implementation for <code class="fm-code-in-text">SsNode</code>: <code class="fm-code-in-text">SsNodeOO</code></p>
  <pre class="programlisting"><b class="calibre21">abstract class</b> SsNodeOO
  <b class="calibre21">#type</b> tuple(k)
  centroid
  <b class="calibre21">#type</b> float
  radius
   
<b class="calibre21">class</b> SsInnerNode: SsNodeOO
  <b class="calibre21">#type</b> SsNode[]
  children
  <b class="calibre21">function</b> SsInnerNode(children=[])
 
<b class="calibre21">class</b> SsLeaf: SsNodeOO
  <b class="calibre21">#type</b> tuple(k)[]
  points
  <b class="calibre21">function</b> SsLeaf(points=[])</pre>

  <p class="body"><a id="pgfId-1001280"></a>Although the implementation using inheritance might result in some code duplication and greater effort being required to understand the code, it arguably provides a cleaner solution, removing the logic to choose the type of node that would otherwise be needed in each method of the class.</p>

  <p class="body"><a id="pgfId-1001289"></a>Although we won’t adopt this example in the rest of the chapter, the zealous reader might use it as a starting point to experiment with implementing SS-trees using this pattern.</p>

  <h3 class="fm-head2" id="heading_id_12"><a id="pgfId-1001299"></a>10.3.1 SS-tree search</h3>

  <p class="body"><a id="pgfId-1001313"></a>Now <a id="marker-1009984"></a><a id="marker-1009988"></a>we are ready to start describing <code class="fm-code-in-text">SsNode’s</code> methods. Although it would feel natural to start with insertion (we need to build a tree before searching it, after all), it is also true that as for many tree-based data structures, the first step to insert (or delete) an entry is searching the node where it should be inserted.</p>

  <p class="body"><a id="pgfId-1001331"></a>Hence, we will need the <code class="fm-code-in-text">search</code> method<a id="marker-1009992"></a> (meant as <i class="calibre17">exact element search</i><a id="marker-1009996"></a>) before we can insert a new item. While we will see how this step in the <code class="fm-code-in-text">insert</code> method<a id="marker-1010000"></a> is slightly different from plain <code class="fm-code-in-text">search</code>, it will still be easier to describe insertion after we have discussed traversing the tree.</p>

  <p class="body"><a id="pgfId-1001353"></a>Figures 10.10 and 10.11 show the steps of a call to <code class="fm-code-in-text">search</code> on our example SS-tree. To be fair, the SS-tree we’ll use in the rest of the chapter is derived from the one in figure 10.9. You might notice that there are a few more points (the orange stars), a few of the old points have been slightly moved, and we stripped all the points’ labels, replacing them with letters from <code class="fm-code-in-text">A</code> to <code class="fm-code-in-text">W</code>, in order to remove clutter and have cleaner diagrams. For the same reason, we’ll identify the point to search/insert/delete, in this and the following sections, as <code class="fm-code-in-text">Z</code> (to avoid clashes with points already in the tree).</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F10.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1049535"></a>Figure 10.10 Search on a SS-tree: the first few steps of searching for point <code class="fm-code-in-text">Z</code>. The SS-tree shown is derived from the one in figure 10.9, with a few minor changes; the name of the entries have been removed here and letters from <code class="fm-code-in-text">A</code> to <code class="fm-code-in-text">W</code> are used to reduce clutter. (Top) The first step of the search is comparing <code class="fm-code-in-text">Z</code> to the spheres in the tree’s root: for each of them, computes the distance between <code class="fm-code-in-text">Z</code> and its centroid, and checks if it’s smaller than the sphere’s radius. (Bottom) Since both <code class="fm-code-in-text">S<sub class="subscript1">1</sub></code> and <code class="fm-code-in-text">S<sub class="subscript1">2</sub></code> intersect <code class="fm-code-in-text">Z</code>, we need to traverse both branches and check spheres <code class="fm-code-in-text">S<sub class="subscript1">3</sub></code> to <code class="fm-code-in-text">S<sub class="subscript1">6</sub></code> for intersection with <code class="fm-code-in-text">Z</code>.</p>

  <p class="body"><a id="pgfId-1001439"></a>To continue with our image dataset example, suppose that we now would like to check to see if a specific image <code class="fm-code-in-text">Z</code> is in our dataset. One option would be comparing <code class="fm-code-in-text">Z</code> to all images in the dataset. Comparing two images might require some time (especially if, for instance, all images have the same size, and we can’t do a quick check on any other trivial image property to rule out obviously different pairs). Recalling that our dataset supposedly has tens of thousands of images, if we go this way, we should be prepared to take a long coffee break (or, depending on our hardware, leave our machine working for the night).</p>

  <p class="body"><a id="pgfId-1001465"></a>But, of course, by now readers must have learned that we shouldn’t despair, because this is the time we provide a better alternative!</p>

  <p class="body"><a id="pgfId-1001476"></a>And indeed, as we mentioned at the beginning of the chapter, we can create a collection of feature vectors for the images in our dataset, extract the feature vector for <code class="fm-code-in-text">Z</code>—let’s call it <code class="fm-code-in-text">F<sub class="subscript1">Z</sub></code>—and perform a search in the feature vectors space instead of directly searching the image dataset.</p>

  <p class="body"><a id="pgfId-1001490"></a>Now, comparing <code class="fm-code-in-text">F<sub class="subscript1">Z</sub></code> to tens or hundreds of thousands of other vectors could also be slow and expensive in terms of time, memory, and disk accesses.</p>

  <p class="body"><a id="pgfId-1001503"></a>If each memory page stored on disk can hold <code class="fm-code-in-text">M</code> feature vectors, we would have to perform <code class="fm-code-in-text">n/M</code> disk accesses and read <code class="fm-code-in-text">n*k</code> float values from disk.</p>

  <p class="body"><a id="pgfId-1001518"></a>And that’s exactly where an SS-tree comes into play. By using an SS-tree with at most <code class="fm-code-in-text">M</code> entries per node, and at least <code class="fm-code-in-text">m</code><span class="cambria">≤</span><code class="fm-code-in-text">M/2</code>, we can reduce the number of pages loaded from disk to<a href="#pgfId-1011766"><sup class="footnotenumber">7</sup></a> <code class="fm-code-in-text">2*log<sub class="subscript1">M</sub>(n)</code>, and the number of float values read to <code class="fm-code-in-text">~k*M*log<sub class="subscript1">M</sub>(n)</code>.</p>

  <p class="body"><a id="pgfId-1001545"></a>Listing 10.3 shows the pseudo-code for SS-tree’s <code class="fm-code-in-text">search</code> method<a id="marker-1010004"></a>. We can follow the steps from figures 10.10 and 10.11. Initially <code class="fm-code-in-text">node</code> will be the root of our example tree, so not a leaf; we’ll then go directly to line #7 and start cycling through <code class="fm-code-in-text">node</code>’s children, in this case <code class="fm-code-in-text">S<sub class="subscript1">1</sub></code> and <code class="fm-code-in-text">S<sub class="subscript1">2</sub></code>.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020223"></a>Listing 10.3 The <code class="fm-code-in-text">search</code> method</p>
  <pre class="programlisting"><b class="calibre21">function</b> search(node, target)                        <span class="fm-combinumeral">❶</span>
  <b class="calibre21">if</b> node.leaf <b class="calibre21">then</b>                                  <span class="fm-combinumeral">❷</span>
    <b class="calibre21">for</b> point in node.points <b class="calibre21">do</b>                      <span class="fm-combinumeral">❸</span>
      <b class="calibre21">if</b> point == target then
        <b class="calibre21">return</b> node                                  <span class="fm-combinumeral">❹</span>
  <b class="calibre21">else</b>
    <b class="calibre21">for</b> childNode in node.children <b class="calibre21">do</b>                <span class="fm-combinumeral">❺</span>
      <b class="calibre21">if</b> childNode.intersectsPoint(target) <b class="calibre21">then</b>      <span class="fm-combinumeral">❻</span>
        result ← search(childNode, target)           <span class="fm-combinumeral">❼</span>
        <b class="calibre21">if</b> result != <b class="calibre21">null then</b>                       <span class="fm-combinumeral">❼</span>
          <b class="calibre21">return</b> result                              <span class="fm-combinumeral">❼</span>
  <b class="calibre21">return null</b>                                        <span class="fm-combinumeral">❽</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1046875"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">search</code><a id="marker-1046879"></a> returns the tree leaf that contains a target point if the point is stored in the tree; it returns <code class="fm-code-in-text2">null</code> otherwise. We explicitly pass the root of the (sub)tree we want to search so we can reuse this function for sub-trees.</p>

  <p class="fm-code-annotation"><a id="pgfId-1046897"></a><span class="fm-combinumeral">❷</span> Checks if <code class="fm-code-in-text2">node</code><a id="marker-1046901"></a> is a leaf or an internal node</p>

  <p class="fm-code-annotation"><a id="pgfId-1046915"></a><span class="fm-combinumeral">❸</span> If <code class="fm-code-in-text2">node</code> is a leaf, goes through all the points held, and checks whether any match <code class="fm-code-in-text2">target</code></p>

  <p class="fm-code-annotation"><a id="pgfId-1046932"></a><span class="fm-combinumeral">❹</span> If a match is found, returns current leaf</p>

  <p class="fm-code-annotation"><a id="pgfId-1046949"></a><span class="fm-combinumeral">❺</span> Otherwise, if we are traversing an internal node, goes through all its children and checks which ones could contain <code class="fm-code-in-text2">target</code>. In other words, for each children <code class="fm-code-in-text2">childNode</code>,<a id="marker-1046954"></a> we check the distance between its centroid and the target point, and if this is smaller than the bounding envelope’s radius of <code class="fm-code-in-text2">childNode</code>, we recursively traverse <code class="fm-code-in-text2">childNode</code>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1046967"></a><span class="fm-combinumeral">❻</span> Checks if <code class="fm-code-in-text2">childNode</code> could contain <code class="fm-code-in-text2">target</code>; that is, if <code class="fm-code-in-text2">target</code> is within <code class="fm-code-in-text2">childNode</code>’s bounding envelope. See listing 10.4 for an implementation.</p>

  <p class="fm-code-annotation"><a id="pgfId-1046984"></a><span class="fm-combinumeral">❼</span> If that’s the case, performs a recursive search on <code class="fm-code-in-text2">childNode</code>’s branch, and if the result is an actual node (and not <code class="fm-code-in-text2">null</code>), we have found what we were looking for and we can return.</p>

  <p class="fm-code-annotation"><a id="pgfId-1047001"></a><span class="fm-combinumeral">❽</span> If no child of current node could contain the <code class="fm-code-in-text2">target</code>, or if we are at a leaf and no point matches <code class="fm-code-in-text2">target</code>, then we end up at this line and just return <code class="fm-code-in-text2">null</code> as the result of an unsuccessful search.</p>

  <p class="body"><a id="pgfId-1001896"></a>For each of them, we compute the distance between <code class="fm-code-in-text">target</code> (point <code class="fm-code-in-text">Z</code> in the figure) and the spheres’ centroids, as shown in listing 10.4, describing the pseudo-code implementation of method <code class="fm-code-in-text">SsNode::intersectsPoint</code><a id="marker-1010020"></a>. Since for both the spheres the computed (Euclidean) distance is smaller than their radii, this means that either (or both) could contain our target point, and therefore we need to traverse both <code class="fm-code-in-text">S<sub class="subscript1">1</sub></code> and <code class="fm-code-in-text">S<sub class="subscript1">2</sub></code> branches.</p>

  <p class="body"><a id="pgfId-1001945"></a>This is also apparent in figure 10.10, where point <code class="fm-code-in-text">Z</code> clearly lies in the intersection of spheres <code class="fm-code-in-text">S<sub class="subscript1">1</sub></code> and <code class="fm-code-in-text">S<sub class="subscript1">2</sub></code>.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F11.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1049577"></a>Figure 10.11 Search on an SS-tree: Continuing from figure 10.10, we traverse the tree up to leaves. At each step, the spheres highlighted are the ones whose children are being currently traversed (in other words, at each step the union of the highlighted spheres is the smallest area where the searched point could lie).</p>

  <p class="body"><a id="pgfId-1001960"></a>The next couple of steps in figures 10.10 (bottom half) and 10.11 execute the same lines of code, cycling through <code class="fm-code-in-text">node</code>’s children until we get to a leaf. It’s worth noting that this implementation will perform a depth-first traversal of the node: it will sequentially follow down to leaves, getting to leaves as fast as possible, back-tracking when needed. For the sake of space, these figures show these paths as they were traversed in parallel, which is totally possible with some modifications to the code (that would, however, be dependent on the programming language of an actual implementation, so we will stick with the simpler and less resource-intensive sequential version).</p>

  <p class="body"><a id="pgfId-1001984"></a>The method will sometime traverse branches where none of the children might contain the target. That’s the case, for instance, with the node containing <code class="fm-code-in-text">S<sub class="subscript1">3</sub></code> and <code class="fm-code-in-text">S<sub class="subscript1">4</sub></code>. The execution will just end up at line #12 of listing 10.3, returning <code class="fm-code-in-text">null</code>and back-tracking to the caller. It had initially traversed branch <code class="fm-code-in-text">S<sub class="subscript1">1</sub></code>; now the <code class="fm-code-in-text">for-each</code> loop at line #7 will just move on to branch <code class="fm-code-in-text">S<sub class="subscript1">2</sub></code>.</p>

  <p class="body"><a id="pgfId-1002013"></a>When we finally get to leaves <code class="fm-code-in-text">S<sub class="subscript1">12</sub>-S<sub class="subscript1">14</sub></code>, the execution will run the cycle at line #3, where we scan a leaf’s points searching for an exact match. If we find one, we can return the current leaf as the result of the search (we assume the tree doesn’t contain duplicates, of course).</p>

  <p class="body"><a id="pgfId-1002033"></a>Listing 10.4 shows a simple implementation for the method checking whether a point is within a node’s bounding envelope. As you can see, the implementation is very simple, because it just uses some basic geometry. Notice, however, that the <code class="fm-code-in-text">distance</code> function<a id="marker-1047531"></a> is a structural parameter of the SS-tree; it can be the Euclidean distance in a <code class="fm-code-in-text">k</code>-dimensional space, but it can also be a different <a id="marker-1047532"></a><a id="marker-1047533"></a>metric.<a href="#pgfId-1011795"><sup class="footnotenumber">8</sup></a></p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020251"></a>Listing 10.4 Method <code class="fm-code-in-text">SsNode::intersectsPoint</code></p>
  <pre class="programlisting"><b class="calibre21">function</b> SsNode:: intersectsPoint(point)                 <span class="fm-combinumeral">❶</span>
  <b class="calibre21">return</b> distance(<b class="calibre21">this</b>.centroid, point) &lt;= <b class="calibre21">this</b>.radius   <span class="fm-combinumeral">❷</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1046720"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">intersectsPoint</code><a id="marker-1046793"></a> is defined on <code class="fm-code-in-text2">SsNode</code><a id="marker-1046794"></a>. It takes a point and returns <code class="fm-code-in-text2">true</code><a id="marker-1046795"></a> if the point is within the bounding envelope of the node.</p>

  <p class="fm-code-annotation"><a id="pgfId-1046745"></a><span class="fm-combinumeral">❷</span> Since the bounding envelope is a hyper-sphere, it just needs to check that the distance between the node’s centroid and the argument point is within the node’s radius. Here, <code class="fm-code-in-text2">distance</code><a id="marker-1046811"></a> can be any valid metric function, including (by default) the Euclidean distance in <code class="fm-code-in-text2">R<sup class="superscript2">k</sup></code>.</p>

  <h3 class="fm-head2" id="heading_id_13"><a id="pgfId-1002146"></a>10.3.2 Insert</h3>

  <p class="body"><a id="pgfId-1002158"></a>As <a id="marker-1010052"></a><a id="marker-1010056"></a><a id="marker-1010060"></a>mentioned, insertion starts with a search step. While for more basic trees, such as <i class="calibre17">binary search trees</i><a id="marker-1010064"></a>, an unsuccessful search returns the one and only node where the new item can be added, for SS-trees we have the same issue we briefly discussed in section 10.2 for R-trees: since nodes can and do overlap, there could be more than one leaf where the new point could be added.</p>

  <p class="body"><a id="pgfId-1002179"></a>This is such a big deal that we mentioned it as the second property determining the SS-tree’s shape. We need to choose a heuristic method to select which branch to traverse, or to select one of the leaves that would already contain the new point.</p>

  <p class="body"><a id="pgfId-1002192"></a>SS-trees originally used a simple heuristic: at each step, they would select the one branch whose centroid is closest to the point that is being inserted (those rare ties that will be faced can be broken arbitrarily).</p>

  <p class="body"><a id="pgfId-1002201"></a>This is not always ideal, because it might lead to a situation like the one shown in figure 10.12, where a new point <code class="fm-code-in-text">Z</code> could be added to a leaf already covering it, and instead ends up in another leaf whose envelope becomes larger to accept <code class="fm-code-in-text">Z</code>, and ends up overlapping the other leaf. It is also possible, although unlikely, that the leaf selected is not actually the closest one to the target. Since at each level we traverse only the closest node, if the tree is not well balanced, it might happen that at some point during traversal the method bumps into a skewed sphere, with the center of mass far away from a small leaf—something like <code class="fm-code-in-text">S<sub class="subscript1">6</sub></code> in figure 10.12, whose child <code class="fm-code-in-text">S<sub class="subscript1">14</sub></code> lies far away from its center of mass.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F12.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1049622"></a>Figure 10.12 An example where the point <code class="fm-code-in-text">Z</code>, which will be inserted into the tree, is added to the closest leaf, <code class="fm-code-in-text">S<sub class="subscript1">14</sub></code>, whose bounding envelope becomes larger as a result, and overlaps another existing leaf, <code class="fm-code-in-text">S<sub class="subscript1">15</sub></code>, which could have held <code class="fm-code-in-text">Z</code> within its bounding envelope. In the bottom half, notice how <code class="fm-code-in-text">S<sub class="subscript1">14</sub></code>’s centroid moves as a result of adding the new point to the sphere.</p>

  <p class="body"><a id="pgfId-1002267"></a>On the other hand, using this heuristic greatly simplifies the code and improves the running time. This way, we have a worst-case bound (for this operation) of <code class="fm-code-in-text">O(log<sub class="subscript1">M</sub>(n))</code>, because we only follow one path from the root to a leaf. If we were to traverse all branches intersecting <code class="fm-code-in-text">Z</code>, in the worst case we could be forced to visit all leaves.</p>

  <p class="body"><a id="pgfId-1002290"></a>Moreover, the code would also become more complicated because we might have to handle differently the cases where no leaf, exactly one leaf, or more than one leaf intersecting <code class="fm-code-in-text">Z</code> are found.</p>

  <p class="body"><a id="pgfId-1002301"></a>So, we will use here the original heuristic described in the SS-tree first paper, shown in listing 10.5. It can be considered a simpler version of the search method described in section 10.3.1, since it will only traverse a single path in the tree. Figure 10.13 shows the difference with a call to the search method for the same tree and point (refer to figures 10.10 and 10.11 for a comparison).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020279"></a>Listing 10.5 The <code class="fm-code-in-text">searchParentLeaf</code> method</p>
  <pre class="programlisting"><b class="calibre21">function</b> searchParentLeaf(node, target)      <span class="fm-combinumeral">❶</span>
  <b class="calibre21">if</b> node.leaf <b class="calibre21">then</b>                          <span class="fm-combinumeral">❷</span>
    <b class="calibre21">return</b> node                                
  <b class="calibre21">else</b>
    child ← node.findClosestChild(target)    <span class="fm-combinumeral">❸</span>
    <b class="calibre21">return</b> searchParentLeaf(child, target)   <span class="fm-combinumeral">❹</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1046493"></a><span class="fm-combinumeral">❶</span> This search method returns the closest tree leaf to a target point.</p>

  <p class="fm-code-annotation"><a id="pgfId-1046514"></a><span class="fm-combinumeral">❷</span> Checks if <code class="fm-code-in-text2">node</code><a id="marker-1046518"></a> is a leaf. If it is, we can return it.</p>

  <p class="fm-code-annotation"><a id="pgfId-1046532"></a><span class="fm-combinumeral">❸</span> Otherwise, we are traversing an internal node and need to find which branch to go next. We run the heuristic <code class="fm-code-in-text2">findClosestChild</code><a id="marker-1046537"></a> to decide (see listing 10.8 for an implementation).</p>

  <p class="fm-code-annotation"><a id="pgfId-1046550"></a><span class="fm-combinumeral">❹</span> Recursively traverses the chosen branch and returns the result</p>

  <p class="body"><a id="pgfId-1002469"></a>However, listing 10.5 is just meant to illustrate how this traversal works. In the actual <code class="fm-code-in-text">insert</code> method,<a id="marker-1047599"></a> we won’t call it as a separate step, but rather integrate it. That’s because finding the closest leaf is just the first step; we are far from being done with insertion yet, and we might need to backtrack our steps. That’s why we are implementing <code class="fm-code-in-text">insert</code> as a recursive function, and each time a sub-call returns, we backtrack on the path from the root to current node.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F13.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1049668"></a>Figure 10.13 An example of the tree traversing for method <code class="fm-code-in-text">searchParentLeaf</code><a id="marker-1049667"></a>. In contrast with figures 10.10 and 10.11, here the steps are condensed into a single diagram for the sake of space. The fact that only one path is traversed allows this compact representation. Notice how at each step the distance between <code class="fm-code-in-text">Z</code> and the centroids in the current node are computed (in this figure, we used for distances the same level-based color code as for spheres, and the segments drawn for distances have one end in the center of the sphere they are computed from, so it’s easy to spot the distance to the root node, and to spheres at level 1, and so on), and only the branch with the shortest distance (drawn as a thicker, solid line) is chosen. The spheres’ branches traversed are highlighted on both representations.</p>

  <p class="body"><a id="pgfId-1002537"></a>Suppose, in fact, that we have found that we should add <code class="fm-code-in-text">Z</code> to some leaf <code class="fm-code-in-text">L</code>, that already contains <code class="fm-code-in-text">j</code> points. We know that <code class="fm-code-in-text">j</code> ≥ <code class="fm-code-in-text">m &gt; 1</code>, so the leaf is not empty, but there could be three very different situations:</p>

  <ol class="calibre18">
    <li class="fm-list-numbered">
      <p class="list"><a class="calibre14" id="pgfId-1002558"></a>If <code class="fm-code-in-text">L</code> already contains <code class="fm-code-in-text">Z</code>, we don’t do anything, assuming we don’t support duplicates (otherwise, we can refer to the remaining two cases).</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1002574"></a><code class="fm-code-in-text">j &lt; M</code>—In this case, we add <code class="fm-code-in-text">Z</code> to the list of <code class="fm-code-in-text">L</code>’s children, recompute the centroid and radius for <code class="fm-code-in-text">L</code>, and we are done. This case is shown in figure 10.12, where <code class="fm-code-in-text">L==S<sub class="subscript1">14</sub></code>. On the left side of the figure, you can see how the centroid and radius of the bounding envelopes are updated as a result of adding <code class="fm-code-in-text">Z</code> to <code class="fm-code-in-text">S<sub class="subscript1">14</sub></code>.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1002608"></a><code class="fm-code-in-text">j == M</code>—This is the most complicated case, because if we add another point to <code class="fm-code-in-text">L</code>, it will violate the invariant requiring that a leaf holds no more than <code class="fm-code-in-text">M</code> points. The only way to solve this is by splitting the leaf’s point into two sets and creating two new leaves that will be added to <code class="fm-code-in-text">L</code>’s parent, <code class="fm-code-in-text">N</code>. Unfortunately, by doing this we can end up in the same situation as if <code class="fm-code-in-text">N</code> already had <code class="fm-code-in-text">M</code> children. Again, the only way we can cope with this is by splitting <code class="fm-code-in-text">N</code>’s children into two sets (defining two spheres), removing <code class="fm-code-in-text">N</code> from its parent <code class="fm-code-in-text">P</code>, and adding the two new spheres to <code class="fm-code-in-text">P</code>. Obviously, <code class="fm-code-in-text">P</code> could also now have <code class="fm-code-in-text">M+1</code> children! Long story short, we need to backtrack to the root, and we can only stop if we get to a node that has less than <code class="fm-code-in-text">M</code> children, or if we do get to the root. If we have to split the root, then we will create a new root with just two children, and the height of the tree will grow by 1 (and that’s the only case where this can happen).</p>
    </li>
  </ol>

  <p class="body"><a id="pgfId-1002667"></a>Listing 10.6 shows an implementation of the <code class="fm-code-in-text">insert</code> method<a id="marker-1010084"></a> using the cases just described:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1002681"></a>The tree traversal, equivalent to the <code class="fm-code-in-text">searchParentLeaf</code> method<a class="calibre14" id="marker-1010088"></a>, appears at lines #10 and #11.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1002698"></a>Case 1 is handled at line #3, where we return <code class="fm-code-in-text">null</code> to let the caller know there is no further action required.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1002713"></a>Case 2 corresponds to lines #6 and #18 in the pseudo-code, also resulting in the method returning <code class="fm-code-in-text">null</code>.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1002729"></a>Case 3, which clearly is the most complicated option, is coded in lines #19 and #20.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1002742"></a>Backtracking is handled at lines #12 to #21.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1002754"></a>Figures 10.14 and 10.15 illustrate the third case, where we insert a point in a leaf that already contains <code class="fm-code-in-text">M</code> points. At a high level, insertion in SS-trees follows B-tree’s algorithm for <code class="fm-code-in-text">insert</code><a id="marker-1010092"></a>. The only difference is in the way we split nodes (in B-trees the list of elements is just split into two halves). Of course, in B-trees links to children and ordering are also handled differently, as we saw in section 10.2.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F14.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1049710"></a>Figure 10.14 Inserting a point in a full leaf. (Top) The search step to find the right leaf. (Center) A closeup of the area involved. <code class="fm-code-in-text">S<sub class="subscript1">9</sub></code> needs to be updated, recomputing its centroid and radius. T hen we can find the direction along which points have the highest variance (<code class="fm-code-in-text">y</code>, in the example) and split the points so that the variance of the two new point sets is minimal. Finally, we remove <code class="fm-code-in-text">S<sub class="subscript1">9</sub></code> from its parent <code class="fm-code-in-text">S<sub class="subscript1">4</sub></code> and add two new leaves containing the two point sets resulting from the split. (Bottom) The final result is that we now need to update <code class="fm-code-in-text">S<sub class="subscript1">4</sub></code>’s centroid and radius and backtrack.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F15.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1049755"></a>Figure 10.15 Backtracking in method <code class="fm-code-in-text">insert</code><a id="marker-1049754"></a> after splitting a leaf. Continuing from figure 10.14, after we split leaf <code class="fm-code-in-text">S<sub class="subscript1">9</sub></code> into nodes <code class="fm-code-in-text">S<sub class="subscript1">16</sub></code> and <code class="fm-code-in-text">S<sub class="subscript1">17</sub></code>, we backtrack to <code class="fm-code-in-text">S<sub class="subscript1">9</sub></code>’s parent <code class="fm-code-in-text">S<sub class="subscript1">4</sub></code>, and add these two new leaves to it, as shown at the end of figure 10.14. <code class="fm-code-in-text">S<sub class="subscript1">4</sub></code> now has four children, one too many. We need to split it as well. Here we show the result of splitting <code class="fm-code-in-text">S<sub class="subscript1">4</sub></code> into two new nodes, <code class="fm-code-in-text">S<sub class="subscript1">18</sub></code> and <code class="fm-code-in-text">S<sub class="subscript1">19</sub></code>, that will be added to <code class="fm-code-in-text">S<sub class="subscript1">4</sub></code>’s parent, <code class="fm-code-in-text">S<sub class="subscript1">1</sub></code>, to which, in turn, we will backtrack. Since it now has only three children (and <code class="fm-code-in-text">M==3</code>) we just recompute centroid and radius for <code class="fm-code-in-text">S<sub class="subscript1">1</sub></code>’s bounding envelope, and we can stop backtracking.</p>

  <p class="body"><a id="pgfId-1002885"></a>In listing 10.6 we used several helper functions<a href="#pgfId-1011810"><sup class="footnotenumber">9</sup></a> to perform insertion; however, there is still one case that is not handled. What happens when we get to the root and we need to split it?</p>

  <p class="body"><a id="pgfId-1002900"></a>The reason for not handling this case as part of the method in listing 10.6 is that we would need to update the root of the tree, and this is an operation that needs to be performed on the tree’s class, where we do have access to the root.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020324"></a>Listing 10.6 The <code class="fm-code-in-text">insert</code> method</p>
  <pre class="programlisting"><b class="calibre21">function</b> insert(node, point)                                <span class="fm-combinumeral">❶</span>
  <b class="calibre21">if</b> <b class="calibre21">this</b>.leaf <b class="calibre21">then</b>                                         <span class="fm-combinumeral">❷</span>
    <b class="calibre21">if</b> point <b class="calibre21">in this</b>.points <b class="calibre21">then</b>                            <span class="fm-combinumeral">❸</span>
      <b class="calibre21">return null</b>
    <b class="calibre21">this</b>.points.add(point)                                  <span class="fm-combinumeral">❹</span>
    <b class="calibre21">this</b>.updateBoundingEnvelope()                           <span class="fm-combinumeral">❺</span>
    <b class="calibre21">if</b> <b class="calibre21">this</b>.points.size &lt;= M <b class="calibre21">then</b>                           <span class="fm-combinumeral">❻</span>
      <b class="calibre21">return null</b>
  <b class="calibre21">else</b>
    closestChild ← <b class="calibre21">this</b>.findClosestChild()                  <span class="fm-combinumeral">❼</span>
    (newChild1, newChild2) ← insert(closestChild, point)    <span class="fm-combinumeral">❽</span>
    <b class="calibre21">if</b> newChild1 == <b class="calibre21">null then</b>                               <span class="fm-combinumeral">❾</span>
      node.updateBoundingEnvelope()
      <b class="calibre21">return null</b>
    <b class="calibre21">else</b>
      <b class="calibre21">this</b>.children.delete(closestChild)                    <span class="fm-combinumeral">❿</span>
      <b class="calibre21">this</b>.children.add(newChild1)                          <span class="fm-combinumeral">⓫</span>
      <b class="calibre21">this</b>.children.add(newChild2)                          <span class="fm-combinumeral">⓫</span>
      node.updateBoundingEnvelope()                         <span class="fm-combinumeral">⓬</span>
      <b class="calibre21">if</b> <b class="calibre21">this</b>.children.size &lt;= M <b class="calibre21">then</b>                       <span class="fm-combinumeral">⓭</span>
        <b class="calibre21">return null</b>
  <b class="calibre21">return this</b>.split()                                       <span class="fm-combinumeral">⓮</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1045426"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">insert</code><a id="marker-1045430"></a> takes a node and a point and adds the point to the node’s subtree. It is defined recursively and returns <code class="fm-code-in-text2">null</code> if <code class="fm-code-in-text2">node</code><a id="marker-1045432"></a> doesn’t need to be split as a result of the insertion; otherwise, it returns the pair of nodes resulting from splitting <code class="fm-code-in-text2">node</code>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1045452"></a><span class="fm-combinumeral">❷</span> Checks if <code class="fm-code-in-text2">node</code> is a leaf</p>

  <p class="fm-code-annotation"><a id="pgfId-1045469"></a><span class="fm-combinumeral">❸</span> If it is a leaf, checks if it already contains the argument among its points, and if it does, we can return</p>

  <p class="fm-code-annotation"><a id="pgfId-1045503"></a><span class="fm-combinumeral">❹</span> Otherwise, adds the point to the leaf</p>

  <p class="fm-code-annotation"><a id="pgfId-1045486"></a><span class="fm-combinumeral">❺</span> We need to recompute the centroid and radius for this leaf after adding the new point.</p>

  <p class="fm-code-annotation"><a id="pgfId-1045534"></a><span class="fm-combinumeral">❻</span> If we added a new point, we need to check whether this leaf now holds more than <code class="fm-code-in-text2">M</code> points. If there are no more than <code class="fm-code-in-text2">M</code>, we can return; otherwise, we continue to line #22.</p>

  <p class="fm-code-annotation"><a id="pgfId-1045551"></a><span class="fm-combinumeral">❼</span> If we are in an internal node, we need to find which branch to traverse, calling a helper method.</p>

  <p class="fm-code-annotation"><a id="pgfId-1045568"></a><span class="fm-combinumeral">❽</span> Recursively traverses the tree and inserts the new point, storing the outcome of the operation</p>

  <p class="fm-code-annotation"><a id="pgfId-1045585"></a><span class="fm-combinumeral">❾</span> If the recursive call returned <code class="fm-code-in-text2">null</code>, we only need to update this node’s bounding envelope, and then we can in turn return <code class="fm-code-in-text2">null</code> as well.</p>

  <p class="fm-code-annotation"><a id="pgfId-1045602"></a><span class="fm-combinumeral">❿</span> Otherwise, it means that <code class="fm-code-in-text2">closestchild</code> has been split, and we need to remove it from the list of children . . .</p>

  <p class="fm-code-annotation"><a id="pgfId-1045619"></a><span class="fm-combinumeral">⓫</span> . . . and add the two newly generated spheres in its place.</p>

  <p class="fm-code-annotation"><a id="pgfId-1045636"></a><span class="fm-combinumeral">⓬</span> We need to compute the centroid and radius for this node.</p>

  <p class="fm-code-annotation"><a id="pgfId-1045653"></a><span class="fm-combinumeral">⓭</span> If the number of children is still within the max allowed, we are done with backtracking.</p>

  <p class="fm-code-annotation"><a id="pgfId-1045670"></a><span class="fm-combinumeral">⓮</span> If it gets here, it means that the node needs to be split: create two new nodes and return them.</p>

  <p class="body"><a id="pgfId-1003394"></a>Therefore, we will give an explicit implementation of the tree’s method for <code class="fm-code-in-text">insert</code><a id="marker-1010108"></a>. Remember, we will actually only expose methods defined on the data structure classes (<code class="fm-code-in-text">KdTree</code><a id="marker-1010112"></a>, <code class="fm-code-in-text">SsTree</code><a id="marker-1010116"></a><code class="fm-code-in-text">,</code> and so on) and not on the nodes’ classes (such as <code class="fm-code-in-text">SsNode</code><a id="marker-1010120"></a>), but we usually omit the former’s when they are just wrappers around the nodes’ methods. Look at listing 10.7 to check out how we can handle root splits. Also, let me highlight this again: this code snippet is the only point where our tree’s height <a id="marker-1010124"></a><a id="marker-1010128"></a><a id="marker-1010132"></a>grows.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020352"></a>Listing 10.7 The <code class="fm-code-in-text">SsTree::insert</code> method</p>
  <pre class="programlisting"><b class="calibre21">function</b> SsTree::insert(point)                                      <span class="fm-combinumeral">❶</span>
  (newChild1, newChild2) ← insert(<b class="calibre21">this</b>.root, point)                 <span class="fm-combinumeral">❷</span>
  <b class="calibre21">if</b> newChild1 != <b class="calibre21">null then</b>                                         <span class="fm-combinumeral">❸</span>
    <b class="calibre21">this</b>.root = new SsNode(<b class="calibre21">false</b>, children=[newChild1, newChild2])  <span class="fm-combinumeral">❸</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1045193"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">insert</code><a id="marker-1045197"></a> is defined on <code class="fm-code-in-text2">SsTree</code><a id="marker-1045198"></a>. It takes a point and doesn’t return anything.</p>

  <p class="fm-code-annotation"><a id="pgfId-1045219"></a><span class="fm-combinumeral">❷</span> Calls the <code class="fm-code-in-text2">insert</code> function on the root and stores the result</p>

  <p class="fm-code-annotation"><a id="pgfId-1045236"></a><span class="fm-combinumeral">❸</span> If, and only if, the result of <code class="fm-code-in-text2">insert</code> is not <code class="fm-code-in-text2">null</code>, it needs to replace the old tree root with a newly created node, which will have as its children the two nodes resulting from splitting the old root.</p>

  <h3 class="fm-head2" id="heading_id_14"><a id="pgfId-1003562"></a>10.3.3 Insertion: Variance, means, and projections</h3>

  <p class="body"><a id="pgfId-1003578"></a>Now <a id="marker-1010144"></a><a id="marker-1010148"></a>let’s get into the details of the (many) helper methods we call in listing 10.6, starting with the heuristic method, described in listing 10.8, to find a node’s closest child to a point <code class="fm-code-in-text">Z</code>. As mentioned, we will just cycle through a node’s children, compute the distance between their centroids and <code class="fm-code-in-text">Z</code>, and choose the bounding envelope that minimizes it.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020397"></a>Listing 10.8 The <code class="fm-code-in-text">SsNode::findClosestChild</code> method</p>
  <pre class="programlisting"><b class="calibre21">function</b> SsNode::findClosestChild(target)                        <span class="fm-combinumeral">❶</span>
  <b class="calibre21">throw-if</b> <b class="calibre21">this</b>.leaf                                             <span class="fm-combinumeral">❷</span>
  minDistance ← <b class="calibre21">inf</b>                                              <span class="fm-combinumeral">❸</span>
  result ← <b class="calibre21">null</b>                                                  <span class="fm-combinumeral">❸</span>
  <b class="calibre21">for</b> childNode <b class="calibre21">in this</b>.children <b class="calibre21">do</b>                              <span class="fm-combinumeral">❹</span>
    <b class="calibre21">if</b> distance(childNode.centroid, point) &lt; minDistance <b class="calibre21">then</b>    <span class="fm-combinumeral">❺</span>
      minDistance ← distance(childNode.centroid, point)          <span class="fm-combinumeral">❻</span>
      result ← childNode                                         <span class="fm-combinumeral">❻</span>
  <b class="calibre21">return</b> result                                                  <span class="fm-combinumeral">❼</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1044563"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">findClosestChild</code><a id="marker-1044567"></a> is defined on <code class="fm-code-in-text2">SsNode</code><a id="marker-1044568"></a>. It takes a point <code class="fm-code-in-text2">target</code><a id="marker-1044570"></a> and returns the child of the current node whose distance to <code class="fm-code-in-text2">target</code> is minimal.</p>

  <p class="fm-code-annotation"><a id="pgfId-1044587"></a><span class="fm-combinumeral">❷</span> If we call this method on a leaf, there is something wrong. In some languages, we can use assert to make sure the invariant (not <code class="fm-code-in-text2">node.leaf</code>) is true.</p>

  <p class="fm-code-annotation"><a id="pgfId-1044604"></a><span class="fm-combinumeral">❸</span> Properly initializes the minimum distance, and the node that will be returned. Another implicit invariant is that an internal node has at least one child (there must be at least <code class="fm-code-in-text2">m</code>), so these values will be updated at least once.</p>

  <p class="fm-code-annotation"><a id="pgfId-1044621"></a><span class="fm-combinumeral">❹</span> Cycles through all children</p>

  <p class="fm-code-annotation"><a id="pgfId-1044641"></a><span class="fm-combinumeral">❺</span> Checks if the distance between the current child’s centroid and <code class="fm-code-in-text2">target</code> is smaller than the minimum found so far</p>

  <p class="fm-code-annotation"><a id="pgfId-1044658"></a><span class="fm-combinumeral">❻</span> If it is, stores the new minimum distance and updates the closest node</p>

  <p class="fm-code-annotation"><a id="pgfId-1044675"></a><span class="fm-combinumeral">❼</span> After the <code class="fm-code-in-text2">for</code> loop cycles through all children, returns the closest one found</p>

  <p class="body"><a id="pgfId-1003862"></a>Figure 10.16 shows what happens when we need to split a leaf. First we recompute the radius and centroid of the leaf after including the new point, and then we also compute the variance of the <code class="fm-code-in-text">M+1</code> points’ coordinates along the <code class="fm-code-in-text">k</code> directions of the axis in order to find the direction with the highest variance; this is particularly useful with skewed sets of points, like <code class="fm-code-in-text">S<sub class="subscript1">9</sub></code> in the example, and helps to reduce spheres volume and, in turn, overlap.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F16.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1049799"></a>Figure 10.16 Splitting a leaf along a non-optimal direction. In this case, the x axis is the direction with minimal variance. Comparing the final result to figure 10.14, although <code class="fm-code-in-text">S<sub class="subscript1">4</sub></code>’s shape doesn’t change significantly, <code class="fm-code-in-text">S<sub class="subscript1">16</sub></code> has more than doubled in size and completely overlaps <code class="fm-code-in-text">S<sub class="subscript1">17</sub></code>; this means that any search targeted within <code class="fm-code-in-text">S<sub class="subscript1">17</sub></code> will also have to traverse <code class="fm-code-in-text">S<sub class="subscript1">16</sub></code>.</p>

  <p class="body"><a id="pgfId-1003923"></a>If you refer to figure 10.16, you can see how a split along the <code class="fm-code-in-text">x</code> axis would have produced two sets with points <code class="fm-code-in-text">G</code> and <code class="fm-code-in-text">H</code> on one side, and <code class="fm-code-in-text">F</code> and <code class="fm-code-in-text">Z</code> on the other. Comparing the result with figure 10.14, there is no doubt about which is the best final result!</p>

  <p class="body"><a id="pgfId-1003946"></a>Of course, the outcome is not always so neat. If the direction of maximum variance is rotated at some angle with respect to the <code class="fm-code-in-text">x</code> axis (imagine, for instance, the same points rotated 45° clockwise WRT the leaf’s centroid), then neither axis direction will produce the optimal result. On average, however, this simpler solution does help.</p>

  <p class="body"><a id="pgfId-1003961"></a>So, how do we perform the split? We start with listing 10.9, which describes the method to find the direction with maximum variance. It’s a simple method performing a global maximum search in a linear space.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020425"></a>Listing 10.9 The <code class="fm-code-in-text">SsNode::directionOfMaxVariance</code> method</p>
  <pre class="programlisting"><b class="calibre21">function</b> SsNode::directionOfMaxVariance()                       <span class="fm-combinumeral">❶</span>
  maxVariance ← 0                                               <span class="fm-combinumeral">❷</span>
  directionIndex ← 0                                            <span class="fm-combinumeral">❷</span>
  centroids ← <b class="calibre21">this</b>.getEntriesCentroids()                        <span class="fm-combinumeral">❸</span>
  <b class="calibre21">for</b> i <b class="calibre21">in</b> {0..k-1} <b class="calibre21">do</b>                                          <span class="fm-combinumeral">❹</span>
    <b class="calibre21">if</b> varianceAlongDirection(centroids, i) &gt; maxVariance then  <span class="fm-combinumeral">❺</span>
      maxVariance ← varianceAlongDirection(centroids, i)        <span class="fm-combinumeral">❻</span>
      directionIndex ← i                                        <span class="fm-combinumeral">❻</span>
  <b class="calibre21">return</b> directionIndex                                         <span class="fm-combinumeral">❼</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1043953"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">directionOfMaxVariance</code><a id="marker-1043957"></a> is defined on <code class="fm-code-in-text2">SsNode</code><a id="marker-1043959"></a>. It returns the index of the direction along which the children of a node have maximum variance.</p>

  <p class="fm-code-annotation"><a id="pgfId-1043979"></a><span class="fm-combinumeral">❷</span> Properly initializes the maximum variance and the index of the direction with max variance</p>

  <p class="fm-code-annotation"><a id="pgfId-1043996"></a><span class="fm-combinumeral">❸</span> Gets the centroids of the items inside the node’s bounding envelope. For a leaf, those are the points held by the leaf, while for an internal node, the centroids of the node’s children.</p>

  <p class="fm-code-annotation"><a id="pgfId-1044013"></a><span class="fm-combinumeral">❹</span> Cycles through all directions: their indices, in a <code class="fm-code-in-text2">k</code>-dimensional space, go from <code class="fm-code-in-text2">0</code> to <code class="fm-code-in-text2">k-1</code>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1044030"></a><span class="fm-combinumeral">❺</span> Checks whether the variance along the <code class="fm-code-in-text2">i</code>-th axis is larger than the maximum found so far</p>

  <p class="fm-code-annotation"><a id="pgfId-1044047"></a><span class="fm-combinumeral">❻</span> If it is, stores the new maximum variance and updates the direction’s index</p>

  <p class="fm-code-annotation"><a id="pgfId-1044064"></a><span class="fm-combinumeral">❼</span> After the <code class="fm-code-in-text2">for</code> loop cycles through all axis’ directions, returns the index of the direction for which we have found the largest variance</p>

  <p class="body"><a id="pgfId-1004237"></a>We need, of course, to compute the variance at each step of the <code class="fm-code-in-text">for</code> loop at line #5. Perhaps this is the right time to remind you what variance is and how it is computed. Given a set <code class="fm-code-in-text">S</code> of real values, we define its mean <code class="fm-code-in-text">μ</code> as the ratio between the sum of the values and their multiplicities:</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F_EQ1.png"/></p>

  <p class="body"><a id="pgfId-1004263"></a>Once we’ve defined the mean, we can then define the variance (usually denoted as <span class="cambria">σ<sup class="superscript2">2</sup></span>) as the mean of the squares of the differences between <code class="fm-code-in-text">S</code>’s mean and each of its elements:</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F_EQ2.png"/></p>

  <p class="body"><a id="pgfId-1004286"></a>So, given a set of <code class="fm-code-in-text">n</code> points <code class="fm-code-in-text">P<sub class="calibre33">0</sub>..P<sub class="calibre33">n-1</sub></code>, each <code class="fm-code-in-text">P<sub class="calibre33">j</sub></code> with coordinates <code class="fm-code-in-text">(P<sub class="calibre33">(j,0)</sub>, P<sub class="calibre33">(j,1)</sub>, .., P<sub class="calibre33">(j,k-1)</sub>)</code>, the formulas for mean and variance along the direction of the <code class="fm-code-in-text">i</code>-th axis are</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F_EQ3.png"/></p>

  <p class="body"><a id="pgfId-1004333"></a>These formulas are easily translatable into code, and in most programming languages you will find an implementation of the method computing variance in core libraries; therefore, we won’t show the pseudo-code here. Instead, let’s see how both functions for variance and mean are used in the <code class="fm-code-in-text">updateBoundingEnvelope</code><a id="marker-1010172"></a> method (listing 10.10) that computes a node centroid and radius.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020532"></a>Listing 10.10 The S<code class="fm-code-in-text">sNode:: updateBoundingEnvelope</code> method</p>
  <pre class="programlisting"><b class="calibre21">function</b> SsNode::updateBoundingEnvelope()                                 <span class="fm-combinumeral">❶</span>
  points ← <b class="calibre21">this</b>.getCentroids()                                            <span class="fm-combinumeral">❷</span>
  <b class="calibre21">for</b> i <b class="calibre21">in</b> {0..k-1} <b class="calibre21">do</b>                                                    <span class="fm-combinumeral">❸</span>
    <b class="calibre21">this</b>.centroid[i] ← mean{point[i] for point in points}                 <span class="fm-combinumeral">❹</span>
  <b class="calibre21">this</b>.radius ← 
    max{distance(<b class="calibre21">this</b>.centroid, entry)+entry.radius for entry in points}  <span class="fm-combinumeral">❺</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1043548"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">updateBoundingEnvelope</code><a id="marker-1043552"></a> is defined on <code class="fm-code-in-text2">SsNode</code><a id="marker-1043553"></a>. It updates the centroid and radius for the current node.</p>

  <p class="fm-code-annotation"><a id="pgfId-1043574"></a><span class="fm-combinumeral">❷</span> Gets the centroids of the items inside the node’s bounding envelope. For a leaf, those are the points held by the leaf, while for an internal node, the centroids of the node’s children.</p>

  <p class="fm-code-annotation"><a id="pgfId-1043591"></a><span class="fm-combinumeral">❸</span> Cycles through the <code class="fm-code-in-text2">k</code> coordinates of the (<code class="fm-code-in-text2">k</code>-dimensional) space</p>

  <p class="fm-code-annotation"><a id="pgfId-1043608"></a><span class="fm-combinumeral">❹</span> For each coordinate, computes the centroid’s value as mean of the points’ values for that coordinate. For instance, for the <code class="fm-code-in-text2">x</code> axis, computes the mean of all <code class="fm-code-in-text2">x</code> coordinates over all points/children in the node.</p>

  <p class="fm-code-annotation"><a id="pgfId-1043625"></a><span class="fm-combinumeral">❺</span> The radius is the maximum distance between the node’s centroid and its children’s envelope. This distance includes the (Euclidean) distance between the two centroids, plus the radius of the children. We assume that points here have radius equal to <code class="fm-code-in-text2">0</code>.</p>

  <p class="body"><a id="pgfId-1004546"></a>This method computes the centroid for a node as the center of mass of its children’s centroids. Remember, for leaves, their children are just the points it contains, while for internal nodes, their children are other nodes.</p>

  <p class="body"><a id="pgfId-1004559"></a>The center of mass is a <code class="fm-code-in-text">k</code>-dimensional point, each of whose coordinates is the mean of the coordinates of all the other children’s centroids.<a href="#pgfId-1011827"><sup class="footnotenumber">10</sup></a></p>

  <p class="body"><a id="pgfId-1004572"></a>Once we have the new centroid, we need to update the radius of the node’s bounding envelope. This is defined as the minimum radius for which the bounding envelope includes all the bounding envelopes for the current node’s children; in turn, we can define it as the maximum distance between the current node’s centroid and any point in its children. Figure 10.17 shows how and why these distances are computed for each child: it’s the sum of the distance between the two centroids and the child’s radius <a id="marker-1010184"></a><a id="marker-1010188"></a>(as long as we assume that points have <code class="fm-code-in-text">radius==0</code>, this definition also works for leaves).</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F17.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1049841"></a>Figure 10.17 Computing the radius of an internal node. The point in <code class="fm-code-in-text">S<sub class="subscript1">C</sub></code> that is further away from <code class="fm-code-in-text">S<sub class="subscript1">A</sub></code>’s centroid <code class="fm-code-in-text">A</code> is the point on the bounding envelope that’s furthest from <code class="fm-code-in-text">C</code> along the opposite direction WRT <code class="fm-code-in-text">S<sub class="subscript1">A</sub></code>’s centroid, and its distance is therefore the sum of the distance <code class="fm-code-in-text">A-C</code> between the two centroids, plus <code class="fm-code-in-text">S<sub class="subscript1">C</sub></code>’s radius. If we choose another point <code class="fm-code-in-text">P</code> on the bounding envelope, its distance from <code class="fm-code-in-text">A</code> must be smaller than the distance <code class="fm-code-in-text">A-B</code>, because metrics by definition need to obey the triangular inequality, and the other two edges of triangle <code class="fm-code-in-text">ACP</code> are <code class="fm-code-in-text">AC</code> and <code class="fm-code-in-text">CP</code>, which is <code class="fm-code-in-text">S<sub class="subscript1">C</sub></code>’s radius. You can check that this is also true for any of the other envelopes in the figure.</p>

  <h3 class="fm-head2" id="heading_id_15"><a id="pgfId-1004648"></a>10.3.4 Insertion: Split nodes</h3>

  <p class="body"><a id="pgfId-1004662"></a>We c<a id="marker-1032979"></a><a id="marker-1032980"></a>an now move to the implementation of the <code class="fm-code-in-text">split</code><a id="marker-1032981"></a> method in listing 10.11.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020560"></a>Listing 10.11 The <code class="fm-code-in-text">SsNode::split</code> method</p>
  <pre class="programlisting"><b class="calibre21">function</b> SsNode::split()                                               <span class="fm-combinumeral">❶</span>
  splitIndex ← <b class="calibre21">this</b>.findSplitIndex(coordinateIndex)                    <span class="fm-combinumeral">❷</span>
  <b class="calibre21">if</b> <b class="calibre21">this</b>.leaf then
    newNode1 ← <b class="calibre21">new</b> SsNode(<b class="calibre21">true</b>, points=<b class="calibre21">this</b>.points[0..splitIndex-1])   <span class="fm-combinumeral">❸</span>
    newNode2 ← <b class="calibre21">new</b> SsNode(<b class="calibre21">true</b>, points=<b class="calibre21">this</b>.points[splitIndex..])      <span class="fm-combinumeral">❸</span>
  <b class="calibre21">else</b>
    newNode1 ← <b class="calibre21">new</b> SsNode(<b class="calibre21">false</b>, children=<b class="calibre21">this</b>.children[0.. index-1])  <span class="fm-combinumeral">❹</span>
    newNode2 ← <b class="calibre21">new</b> SsNode(<b class="calibre21">false</b>, children=<b class="calibre21">this</b>.children [index..])     <span class="fm-combinumeral">❹</span>
  <b class="calibre21">return</b> (newNode1, newNode2)                                          <span class="fm-combinumeral">❺</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1043157"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">split</code><a id="marker-1043161"></a> is defined on <code class="fm-code-in-text2">SsNode</code><a id="marker-1043162"></a>. It returns the two new nodes resulting from the split.</p>

  <p class="fm-code-annotation"><a id="pgfId-1043180"></a><span class="fm-combinumeral">❷</span> Finds the best “split index” for the list of points (leaves) or children (internal nodes)</p>

  <p class="fm-code-annotation"><a id="pgfId-1043200"></a><span class="fm-combinumeral">❸</span> If this is a leaf, the new nodes resulting from the split will be two leaves, each with part of the points of the current leaf. Given <code class="fm-code-in-text2">splitIndex</code>, the first leaf will have all points from the beginning of the list to <code class="fm-code-in-text2">splitIndex</code><a id="marker-1043205"></a> (not included), and the other leaf will have the rest of the <code class="fm-code-in-text2">points</code> list.</p>

  <p class="fm-code-annotation"><a id="pgfId-1043218"></a><span class="fm-combinumeral">❹</span> If this node is internal, then we create two new internal nodes, each with one of the partitions of the <code class="fm-code-in-text2">children</code> list.</p>

  <p class="fm-code-annotation"><a id="pgfId-1043235"></a><span class="fm-combinumeral">❺</span> Returns the pair of new <code class="fm-code-in-text2">SsNodes</code> created</p>

  <p class="body"><a id="pgfId-1004915"></a>This method looks relatively simple, because most of the leg work is performed by the auxiliary method <code class="fm-code-in-text">findSplitIndex</code><a id="marker-1010216"></a>, described in listing 10.12.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020588"></a>Listing 10.12 The <code class="fm-code-in-text">SsNode::findSplitIndex</code> method</p>
  <pre class="programlisting"><b class="calibre21">function</b> SsNode::findSplitIndex()                                     <span class="fm-combinumeral">❶</span>
  coordinateIndex ← <b class="calibre21">this</b>.directionOfMaxVariance()                     <span class="fm-combinumeral">❷</span>
  <b class="calibre21">this</b>.sortEntriesByCoordinate(coordinateIndex)                       <span class="fm-combinumeral">❸</span>
  points ← {point[coordinateIndex] for point <b class="calibre21">in this</b>.getCentroids()}  <span class="fm-combinumeral">❹</span>
  <b class="calibre21">return</b> minVarianceSplit(points, coordinateIndex)                    <span class="fm-combinumeral">❺</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1042802"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">findSplitIndex</code><a id="marker-1042806"></a> is defined on <code class="fm-code-in-text2">SsNode</code><a id="marker-1042807"></a>. It returns the optimal index for a node split. For a leaf, the index refers to the list of points, while for an internal node it refers to the children’s list. Either list will be sorted as a side effect of this method.</p>

  <p class="fm-code-annotation"><a id="pgfId-1042828"></a><span class="fm-combinumeral">❷</span> Finds along which axes the coordinates of the entries’ centroids have the highest variance</p>

  <p class="fm-code-annotation"><a id="pgfId-1042845"></a><span class="fm-combinumeral">❸</span> We need to sort the node’s entries (either points or children) by the chosen coordinate.</p>

  <p class="fm-code-annotation"><a id="pgfId-1042862"></a><span class="fm-combinumeral">❹</span> Gets a list of the centroids of this node’s entries: a list of points for a leaf, and a list of the children’s centroids, in case we are at an internal node. Then, for each centroid, extract only the coordinate given by <code class="fm-code-in-text2">coordinateIndex</code><a id="marker-1042867"></a>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1042880"></a><span class="fm-combinumeral">❺</span> Finds and returns which index will result in a partitioning with the minimum total variance</p>

  <p class="body"><a id="pgfId-1005084"></a>After finding the direction with maximum variance, we sort<a href="#pgfId-1011841"><sup class="footnotenumber">11</sup></a> points or children (depending on if a node is a leaf or an internal node) based on their coordinates for that same direction, and then, after getting the list of centroids for the node’s entries, we split this list, again along the direction of max variance. We’ll see how to do that in a moment.</p>

  <p class="body"><a id="pgfId-1005099"></a>Before that, we again ran into the method returning the centroids of the entries within the node’s bounding envelope, so it’s probably the right time to define it! As we mentioned before, the logic of the method is dichotomic:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1005108"></a>If the node is a leaf, this means that it returns the points contained in it.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1005121"></a>Otherwise it will return the centroids of the node’s children.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1005134"></a>Listing 10.13 puts this definition into pseudo-code.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020616"></a>Listing 10.13 The <code class="fm-code-in-text">SsNode::getEntriesCentroids</code> method</p>
  <pre class="programlisting"><b class="calibre21">function</b> SsNode::getEntriesCentroids()                  <span class="fm-combinumeral">❶</span>
  <b class="calibre21">if</b> <b class="calibre21">this</b>.leaf <b class="calibre21">then</b>                                     <span class="fm-combinumeral">❷</span>
    <b class="calibre21">return this</b>.points                                  <span class="fm-combinumeral">❷</span>
  <b class="calibre21">else</b>
    <b class="calibre21">return</b> {child.centroid <b class="calibre21">for</b> child <b class="calibre21">in this</b>.children}  <span class="fm-combinumeral">❸</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1042561"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">getEntriesCentroids</code><a id="marker-1042565"></a> is defined on <code class="fm-code-in-text2">SsNode</code><a id="marker-1042566"></a>. It returns the centroids of the entries within the node’s bounding envelope.</p>

  <p class="fm-code-annotation"><a id="pgfId-1042587"></a><span class="fm-combinumeral">❷</span> If the node is a leaf, we can just return its points.</p>

  <p class="fm-code-annotation"><a id="pgfId-1042604"></a><span class="fm-combinumeral">❸</span> Otherwise, we need to return a list of all the centroids of this node’s children. We use a construct typically called list-comprehension to denote this list (see appendix A).</p>

  <p class="body"><a id="pgfId-1005278"></a>After retrieving the index of the split point, we can actually split the node entries. Now we need two different conditional branches to handle leaves and internal nodes differently: we need to provide to the node constructors the right arguments, depending on the type of node we want to create. Once we have the new nodes constructed, all we need to do is return them.</p>

  <p class="body"><a id="pgfId-1005302"></a>Hang tight; we aren’t done yet. I know we have been going through this section for a while now, but we’re still missing one piece of the puzzle to finish our implementation of the <code class="fm-code-in-text">insert</code> method<a id="marker-1033121"></a>: the <code class="fm-code-in-text">splitPoints</code> helper function<a id="marker-1033122"></a>.</p>

  <p class="body"><a id="pgfId-1005325"></a>This method might seem trivial, but it’s actually a bit tricky to get it right. Let’s say it needs at least some thought.</p>

  <p class="body"><a id="pgfId-1005338"></a>So, let’s first go through an example, and then write some pseudo-code for it! Figure 10.18 illustrates the steps we need to perform such a split. We start with a node containing eight points. We don’t know, and don’t need to know, if those are dataset points or nodes’ centroids; it is irrelevant for this method.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F18.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1049883"></a>Figure 10.18 Splitting a set of points along the direction of maximum variance. (Top) The bounding envelope and its points to split; the direction of maximum variance is along the <code class="fm-code-in-text">y</code> axis (center), so we project all points on this axis. On the right, we rotate the axis for convenience and replace the point labels with indices. (Middle) Given that there are 8 points, we can infer <code class="fm-code-in-text">M</code> must be equal to <code class="fm-code-in-text">7</code>. Then <code class="fm-code-in-text">m</code> can be any value <span class="cambria">≤</span><code class="fm-code-in-text">3</code>. Since the algorithm chooses a single split index, partitioning the points on its two sides, and each partition needs to have at least <code class="fm-code-in-text">m</code> points, depending on the actual value of <code class="fm-code-in-text">m</code>, we can have a different number of choices for the split index. (Bottom) We show the three possible resulting splits for the case where <code class="fm-code-in-text">m==3</code>: the split index can be <code class="fm-code-in-text">3</code>, <code class="fm-code-in-text">4</code>, or <code class="fm-code-in-text">5</code>. We will choose the option for which the sum of the variances for the two sets is minimal.</p>

  <p class="body"><a id="pgfId-1005400"></a>Suppose we have computed the direction of maximum variance and that it is along the <code class="fm-code-in-text">y</code> axis; we then need to project the points along this axis, which is equivalent to only considering the <code class="fm-code-in-text">y</code> coordinate of the points because of the definition of our coordinate system.</p>

  <p class="body"><a id="pgfId-1005413"></a>In the diagram we show the projection of the points, since it’s visually more intuitive. For the same reason, we then rotate the axis and the projections 90° clockwise, remove the points’ labels, and index the projected points from left to right. In our code, we would have to sort our points according to the <code class="fm-code-in-text">y</code> coordinate (as we saw in listing 10.12), and then we can just consider their indices; an alternative could be using indirect sorting and keeping a table of sorted/unsorted indices, but this would substantially complicate the remaining code.</p>

  <p class="body"><a id="pgfId-1005437"></a>As shown, we have eight points to split. We can deduce that the parameter <code class="fm-code-in-text">M</code>, the maximum number of leaves/children for a tree node, is equal to 7, and thus <code class="fm-code-in-text">m</code>, the minimum number of entries, can only be equal to 2 or 3 (technically it could also be 1, but that’s a choice that would produce skewed trees, and usually it’s not even worth implementing these trees if we use <code class="fm-code-in-text">m==1</code>).</p>

  <p class="body"><a id="pgfId-1005458"></a>It’s worth mentioning again that the value for <code class="fm-code-in-text">m</code> <i class="calibre17">must</i> be chosen at the time of creation of our SS-Tree, and therefore it is fixed when we call <code class="fm-code-in-text">split</code>. Here we are just reasoning about how this choice influences how the splits are performed, and ultimately the structure of the tree.</p>

  <p class="body"><a id="pgfId-1005478"></a>And indeed, this value is crucial to the <code class="fm-code-in-text">split</code> method<a id="marker-1010248"></a>, because each of the two partitions created will need to have at least <code class="fm-code-in-text">m</code> points; therefore, since we are using a single index split,<a href="#pgfId-1011857"><sup class="footnotenumber">12</sup></a> the possible values for this split index go from <code class="fm-code-in-text">m</code> to <code class="fm-code-in-text">M-m</code>. In our example, as shown in the middle section of figure 10.18, this means</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1005502"></a>If <code class="fm-code-in-text">m==2</code>, then we can choose any index between 2 and 6 (5 choices).</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1005517"></a>If <code class="fm-code-in-text">m==3</code>, then the alternatives are between 3 and 5 (3 choices).</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1005531"></a>Now suppose we had chosen <code class="fm-code-in-text">m==3</code>. The bottom section of figure 10.18 shows the resulting split for each of the three alternative choices we have for the split index. We will have to choose the one that minimizes variance for both nodes (usually, we minimize the sum of the variances), but we only minimize variance along the direction we perform the split, so in the example we will only compute the variance of the y coordinates of the two sets of points. Unlike with R-trees, we won’t try to minimize the bounding envelopes’ overlap at this stage, although it turns out that reducing variance along the direction that had the highest variance brings us, as an indirect consequence, a reduction of the average overlap of the new nodes.</p>

  <p class="body"><a id="pgfId-1005549"></a>Also, with SS+-trees, we will tackle the issue of overlapping bounding envelopes separately.</p>

  <p class="body"><a id="pgfId-1005563"></a>For now, to finish with the insertion method, please look at listing 10.14 for an implementation of the <code class="fm-code-in-text">minVarianceSplit</code> method<a id="marker-1010252"></a>. As mentioned, it’s just a linear search among <code class="fm-code-in-text">M – 2*(m-1)</code> possible options for the split index of the points.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020649"></a>Listing 10.14 The <code class="fm-code-in-text">minVarianceSplit</code> method</p>
  <pre class="programlisting"><b class="calibre21">function</b> minVarianceSplit(values)                 <span class="fm-combinumeral">❶</span>
  minVariance ← <b class="calibre21">inf</b>                               <span class="fm-combinumeral">❷</span>
  splitIndex ← m                                  <span class="fm-combinumeral">❷</span>
  <b class="calibre21">for</b> i <b class="calibre21">in</b> {m, |values|-m} <b class="calibre21">do</b>                     <span class="fm-combinumeral">❸</span>
    variance1 ← variance(values[0..i-1])          <span class="fm-combinumeral">❹</span>
    variance2 ← variance(values[i..|values|-1])   <span class="fm-combinumeral">❹</span>
    <b class="calibre21">if</b> variance1 + variance2 &lt; minVariance <b class="calibre21">then</b>   <span class="fm-combinumeral">❺</span>
      minVariance ← variance1 + variance2         <span class="fm-combinumeral">❺</span>
      splitIndex ← i                              <span class="fm-combinumeral">❺</span>
  <b class="calibre21">return</b> splitIndex                               <span class="fm-combinumeral">❻</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1042051"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">minVarianceSplit</code><a id="marker-1042055"></a> takes a list of real values. The method returns the optimal index for a node split of the values. In particular, it returns the index of the first element of the second partition; the split is optimal with respect to the variance of the two sets. The method assumes the input is already sorted.</p>

  <p class="fm-code-annotation"><a id="pgfId-1042073"></a><span class="fm-combinumeral">❷</span> Initializes temporary variables for the minimum variance and the index where to split the list</p>

  <p class="fm-code-annotation"><a id="pgfId-1042093"></a><span class="fm-combinumeral">❸</span> Goes through all the possible values for the split index. One constraint is that both sets need to have at least <code class="fm-code-in-text2">m</code> points, so we can exclude all choices for which the first set has less than <code class="fm-code-in-text2">m</code> elements, as well as those where the second set is too small.</p>

  <p class="fm-code-annotation"><a id="pgfId-1042110"></a><span class="fm-combinumeral">❹</span> For each possible value <code class="fm-code-in-text2">i</code> for the split index, selects the points before and after the split, and computes the variances of the two sets</p>

  <p class="fm-code-annotation"><a id="pgfId-1042127"></a><span class="fm-combinumeral">❺</span> If the sum of the variances just computed is better than the best result so far, updates the temporary variables</p>

  <p class="fm-code-annotation"><a id="pgfId-1042144"></a><span class="fm-combinumeral">❻</span> Returns the best option found</p>

  <p class="body"><a id="pgfId-1005815"></a>And with this, we can finally close this section about <code class="fm-code-in-text">SsTree::insert</code><a id="marker-1010260"></a>. You might feel this was a very long road to get here, and you’d be right: this is probably the most complicated code we’ve described so far. Take your time to read the last few sub-sections multiple times, if it helps, and then brace yourself: we are going to delve into the <code class="fm-code-in-text">delete</code> method<a id="marker-1010264"></a> next, which is likely even more <a id="marker-1010268"></a><a id="marker-1010272"></a>complicated.</p>

  <h3 class="fm-head2" id="heading_id_16"><a id="pgfId-1005844"></a>10.3.5 Delete<a id="marker-1033342"></a><a id="marker-1033343"></a></h3>

  <p class="body"><a id="pgfId-1005856"></a>Like <code class="fm-code-in-text">insert</code><a id="marker-1010284"></a>, <code class="fm-code-in-text">delete</code><a id="marker-1010288"></a> in SS-trees is also heavily based on B-tree’s <code class="fm-code-in-text">delete</code>. The former is normally considered so complicated that many textbooks skip it altogether (for the sake of space), and implementing it is usually avoided as long as possible. The SS-tree version, of course, is even more complicated than the original one.</p>

  <p class="body"><a id="pgfId-1005887"></a>But one of the aspects where R-trees and SS-trees overcome k-d trees is that while the latter is guaranteed to be balanced only if initialized on a static dataset, both can remain balanced even when supporting dynamic datasets, with a large volume of insertions and removals. Giving up on <code class="fm-code-in-text">delete</code> would therefore mean turning down one of the main reasons we need this data structure.</p>

  <p class="body"><a id="pgfId-1005904"></a>The first (and easiest) step is finding the point we would like to delete, or better said, finding the leaf that holds that point. While for <code class="fm-code-in-text">insert</code> we would only traverse one path to the closest leaf, for <code class="fm-code-in-text">delete</code> we are back at the search algorithm described in section 10.3.1; however, as for <code class="fm-code-in-text">insert</code>, we will need to perform some backtracking, and hence rather than calling search, we will have to implement the same traversal in this new method.<a href="#pgfId-1011877"><sup class="footnotenumber">13</sup></a></p>

  <p class="body"><a id="pgfId-1005922"></a>Once we have found the right leaf <code class="fm-code-in-text">L</code>, assuming we do find the point <code class="fm-code-in-text">Z</code> in the tree (otherwise, we wouldn’t need to perform any change), we have a few possible situations—an easy one, a complicated one, and a seriously complicated one:</p>

  <ol class="calibre18">
    <li class="fm-list-numbered">
      <p class="list"><a class="calibre14" id="pgfId-1005937"></a>If the leaf contains more than <code class="fm-code-in-text">m</code> points, we just delete <code class="fm-code-in-text">Z</code> from <code class="fm-code-in-text">L</code>, and update its bounding envelope.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1005955"></a>Otherwise, after deleting <code class="fm-code-in-text">Z</code>, <code class="fm-code-in-text">L</code> will have only <code class="fm-code-in-text">m-1</code> points, and therefore it would violate one of the SS-tree’s invariants. We have a few options to cope with this:</p>

      <ol class="calibre35">
        <li class="fm-list-numbered1">
          <p class="list"><a class="calibre14" id="pgfId-1005975"></a>If <code class="fm-code-in-text">L</code> is the root, we are good, and we don’t have to do anything.</p>
        </li>

        <li class="fm-list-numbered1">
          <p class="list"><a class="calibre14" id="pgfId-1005989"></a>If <code class="fm-code-in-text">L</code> has at least one sibling <code class="fm-code-in-text">S</code> with more than <code class="fm-code-in-text">m</code> points, we can move one point from <code class="fm-code-in-text">S</code> to <code class="fm-code-in-text">L</code>. Although we will be careful to choose the closest point to <code class="fm-code-in-text">L</code> (among all its siblings with at least <code class="fm-code-in-text">m+1</code> points), this operation can potentially cause <code class="fm-code-in-text">L</code>’s bounding envelope to expand significantly (if only siblings far away from <code class="fm-code-in-text">L</code> have enough points) and unbalance the tree.</p>
        </li>

        <li class="fm-list-numbered1">
          <p class="list"><a class="calibre14" id="pgfId-1006019"></a>If no sibling of <code class="fm-code-in-text">L</code> can “lend” it a point, then we will have to merge <code class="fm-code-in-text">L</code> with one of its siblings. Again, we would then have to choose which sibling to merge it with and we might choose different strategies:</p>

          <ol class="calibre36">
            <li class="fm-list-bullet1">
              <p class="list"><a class="calibre14" id="pgfId-1006035"></a>Choosing the closest sibling</p>
            </li>

            <li class="fm-list-bullet1">
              <p class="list"><a class="calibre14" id="pgfId-1006047"></a>Choosing the sibling with larger overlap with <code class="fm-code-in-text">L</code></p>
            </li>

            <li class="fm-list-bullet1">
              <p class="list"><a class="calibre14" id="pgfId-1006060"></a>Choosing the sibling that minimizes the coordinates variance (over all axes)</p>
            </li>
          </ol>
        </li>
      </ol>
    </li>
  </ol>

  <p class="body"><a id="pgfId-1006072"></a>Case 2(c) is clearly the hardest to handle. Case 2(b), however, is relatively easy because, luckily, one difference with B-trees is that the node’s children don’t have to be sorted, so we don’t need to perform rotations when we move one point from <code class="fm-code-in-text">S</code> to <code class="fm-code-in-text">L</code>. In the middle-bottom sections of figure 10.19 you can see the result of node <code class="fm-code-in-text">S<sub class="subscript1">3</sub></code> “borrowing” one of the <code class="fm-code-in-text">S<sub class="subscript1">4</sub></code> children, <code class="fm-code-in-text">S<sub class="subscript1">9</sub></code>—it’s just as easy as that. Of course, the hardest part is deciding which sibling to borrow from and which of its children should be moved.</p>

  <p class="body"><a id="pgfId-1006128"></a>For case 2(b), merging two nodes will cause their parent to have one less child; we thus have to backtrack and verify that this node still has at least <code class="fm-code-in-text">m</code> children. This is shown in the top and middle sections of figure 10.19. The good news is that we can handle internal nodes exactly as we handle leaves, so we can reuse the same logic (and mostly the same code) for leaves and internal nodes.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F19.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1049928"></a>Figure 10.19 Deleting a point. This example shows, in order, cases 2(c), 2(b), and 1 described in this section.</p>

  <p class="body"><a id="pgfId-1006145"></a>Cases 1 (at the bottom of figure 10.19) and 2(a) are trivial, and we can easily implement them; the fact that when we get to the root we don’t have to do any extra action (like we have to for insert) makes the <code class="fm-code-in-text">SsTree::delete</code><a id="marker-1010292"></a> wrapper method trivial.</p>

  <p class="body"><a id="pgfId-1006158"></a>Enough with the examples; it’s time to write the body of the <code class="fm-code-in-text">delete</code> method, shown in listing 10.15.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020692"></a>Listing 10.15 The <code class="fm-code-in-text">delete</code> method</p>
  <pre class="programlisting"><b class="calibre21">function</b> delete(node, target)                                            <span class="fm-combinumeral">❶</span>
  <b class="calibre21">if</b> node.leaf <b class="calibre21">then</b>                                
    <b class="calibre21">if</b> node.points.contains(target) <b class="calibre21">then</b>                                 <span class="fm-combinumeral">❷</span>
      node.points.delete(target)                                         <span class="fm-combinumeral">❸</span>
      <b class="calibre21">return</b> (<b class="calibre21">true</b>, node.points.size() &lt; m)                              <span class="fm-combinumeral">❹</span>
    <b class="calibre21">else</b>
     <b class="calibre21">return</b> (<b class="calibre21">false</b>, false)                                               <span class="fm-combinumeral">❺</span>
  <b class="calibre21">else</b>
    nodeToFix ← <b class="calibre21">null</b>                                                     <span class="fm-combinumeral">❻</span>
    deleted ← <b class="calibre21">false</b>                                                      <span class="fm-combinumeral">❼</span>
    <b class="calibre21">for</b> childNode in node.children <b class="calibre21">do</b>                                    <span class="fm-combinumeral">❼</span>
      <b class="calibre21">if</b> childNode.intersectsPoint(target) <b class="calibre21">then</b>                          <span class="fm-combinumeral">❼</span>
        (deleted, violatesInvariants) ← delete(childNode, target)        <span class="fm-combinumeral">❽</span>
        <b class="calibre21">if</b> violatesInvariants == true <b class="calibre21">then</b>                               <span class="fm-combinumeral">❾</span>
          nodeToFix ← childNode                                          <span class="fm-combinumeral">❿</span>
        <b class="calibre21">if</b> deleted <b class="calibre21">then</b>                                                  <span class="fm-combinumeral">⓫</span>
          <b class="calibre21">break</b>
  <b class="calibre21">if</b> nodeToFix == <b class="calibre21">null then</b>                                              <span class="fm-combinumeral">⓬</span>
    <b class="calibre21">if</b> deleted then 
      node.updateBoundingEnvelope()                                      <span class="fm-combinumeral">⓭</span>
    <b class="calibre21">return</b> (deleted, <b class="calibre21">false</b>)                                              <span class="fm-combinumeral">⓮</span>
  <b class="calibre21">else</b>
    siblingsToBorrowFrom(nodeToFix)                                      <span class="fm-combinumeral">⓯</span>
    <b class="calibre21">if</b> not siblings.isEmpty() <b class="calibre21">then</b>                                       <span class="fm-combinumeral">⓰</span>
      nodeToFix.borrowFromSibling(siblings)                              <span class="fm-combinumeral">⓱</span>
    <b class="calibre21">else</b>
      node.mergeChildren(
        nodeToFix, node.findSiblingToMergeTo(nodeToFix))                 <span class="fm-combinumeral">⓲</span>
    node.updateBoundingEnvelope()                                        <span class="fm-combinumeral">⓳</span>
    <b class="calibre21">return</b> (<b class="calibre21">true</b>, node.children.size() &lt; m)                              <span class="fm-combinumeral">⓴</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1040338"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">delete</code><a id="marker-1040342"></a> takes a node and a point to delete from the node’s subtree. It is defined recursively and returns a pair of values: the first one tells if a point has been deleted in current subtree, and the second one is <code class="fm-code-in-text2">true</code><a id="marker-1040344"></a> if current node now violates SS-tree’s invariants. We assume both <code class="fm-code-in-text2">node</code><a id="marker-1040345"></a> and <code class="fm-code-in-text2">target</code><a id="marker-1040346"></a> are non-null.</p>

  <p class="fm-code-annotation"><a id="pgfId-1040366"></a><span class="fm-combinumeral">❷</span> If the current node is a leaf, checks that it does contain the point to delete and . . .</p>

  <p class="fm-code-annotation"><a id="pgfId-1040383"></a><span class="fm-combinumeral">❸</span> . . . removes the point . . .</p>

  <p class="fm-code-annotation"><a id="pgfId-1040400"></a><span class="fm-combinumeral">❹</span> . . . and returns (<code class="fm-code-in-text2">true</code>, <code class="fm-code-in-text2">true</code>) if the node now contains fewer than <code class="fm-code-in-text2">m</code> points, to let the caller know that it violates SS-tree invariants and needs fixing, or (<code class="fm-code-in-text2">true</code>, <code class="fm-code-in-text2">false</code>) otherwise, because the point was deleted in this subtree.</p>

  <p class="fm-code-annotation"><a id="pgfId-1040417"></a><span class="fm-combinumeral">❺</span> Otherwise, if this leaf doesn’t contain <code class="fm-code-in-text2">target</code>, we need to backtrack the search and traverse the next unexplored branch (the execution will return to line #13 of the call handling <code class="fm-code-in-text2">node</code>’s parent, unless <code class="fm-code-in-text2">node</code> is the root of the tree), but so far no change has been made, so it can return (<code class="fm-code-in-text2">false</code>, <code class="fm-code-in-text2">false</code>).</p>

  <p class="fm-code-annotation"><a id="pgfId-1040434"></a><span class="fm-combinumeral">❻</span> If <code class="fm-code-in-text2">node</code> is not a leaf, we need to continue the tree traversal by exploring <code class="fm-code-in-text2">node</code>’s branches. We start by initializing a couple of temporary variables to keep track of the outcome of recursive calls on <code class="fm-code-in-text2">node</code>’s children.</p>

  <p class="fm-code-annotation"><a id="pgfId-1040451"></a><span class="fm-combinumeral">❼</span> Cycles through all of <code class="fm-code-in-text2">node</code>’s children that intersect the target point to be deleted</p>

  <p class="fm-code-annotation"><a id="pgfId-1040468"></a><span class="fm-combinumeral">❽</span> Recursively traverses the next branch (one of the children that intersects <code class="fm-code-in-text2">target</code>), searching for the point and trying to delete it</p>

  <p class="fm-code-annotation"><a id="pgfId-1040485"></a><span class="fm-combinumeral">❾</span> If the recursive call returns <code class="fm-code-in-text2">true</code> for <code class="fm-code-in-text2">violatesInvariants</code><a id="marker-1040489"></a>, it means that the point has been found and deleted in this branch, and that <code class="fm-code-in-text2">childNode</code><a id="marker-1040491"></a> currently violates the SS-tree’s invariants, so its parent needs to do some fixing.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">❿</span> To that extent, we save current child in the temporary variable we had previously initialized.</p>

  <p class="fm-code-annotation"><a id="pgfId-1040504"></a><span class="fm-combinumeral">⓫</span> If a point has been deleted in the current node’s subtree, then exit the <code class="fm-code-in-text2">for</code> loop (we assume there are no duplicates in the tree, so a point can be in one and one branch only).</p>

  <p class="fm-code-annotation"><a id="pgfId-1040538"></a><span class="fm-combinumeral">⓬</span> Check if none of <code class="fm-code-in-text2">node</code>’s children violates SS-tree’s invariants. In that case, we won’t need to do any fix for the current node.</p>

  <p class="fm-code-annotation"><a id="pgfId-1040555"></a><span class="fm-combinumeral">⓭</span> However, if the point was deleted in this subtree, we still need to recompute the bounding envelope.</p>

  <p class="fm-code-annotation"><a id="pgfId-1040572"></a><span class="fm-combinumeral">⓮</span> Then we can return, letting the caller know if the point was deleted as part of this call, and that this node doesn’t violate any invariant.</p>

  <p class="fm-code-annotation"><a id="pgfId-1040589"></a><span class="fm-combinumeral">⓯</span> If, instead, one of the current node’s children does violate an invariant as the result of calling <code class="fm-code-in-text2">delete</code><a id="marker-1040594"></a> on it, the first thing we need to do is retrieve a list of the siblings of that child (stored in <code class="fm-code-in-text2">nodeToFix</code><a id="marker-1040595"></a>) but filtering in only those that in turn have more than <code class="fm-code-in-text2">m</code> children/points. We will try to move one of those entries (either children or points, for internal nodes and leaves respectively) from one of the siblings to the one child from which we deleted <code class="fm-code-in-text2">target</code>, and that now has too few children.</p>

  <p class="fm-code-annotation"><a id="pgfId-1040614"></a><span class="fm-combinumeral">⓰</span> Checks if there is any sibling of <code class="fm-code-in-text2">nodeToFix</code> that meets the criteria</p>

  <p class="fm-code-annotation"><a id="pgfId-1040631"></a><span class="fm-combinumeral">⓱</span> If <code class="fm-code-in-text2">nodeToFix</code> has at least one sibling with more than <code class="fm-code-in-text2">m</code> entries, moves one entry from one of the siblings to <code class="fm-code-in-text2">nodeTofix</code> (which will now be fixed, because it will have exactly <code class="fm-code-in-text2">m</code> points/children).</p>

  <p class="fm-code-annotation"><a id="pgfId-1040648"></a><span class="fm-combinumeral">⓲</span> Otherwise, if there is no sibling with more than <code class="fm-code-in-text2">m</code> elements, we will have to merge the node violating invariants with one of its siblings.</p>

  <p class="fm-code-annotation"><a id="pgfId-1040665"></a><span class="fm-combinumeral">⓳</span> Before we return, we still need to recompute the bounding envelope for the current node.</p>

  <p class="fm-code-annotation"><a id="pgfId-1040682"></a><span class="fm-combinumeral">⓴</span> If it gets here, we are at an internal node and the point has been deleted in <code class="fm-code-in-text2">node</code>’s subtree; checks also if <code class="fm-code-in-text2">node</code> now violates the invariant about the minimum number of children.</p>

  <p class="body"><a id="pgfId-1006948"></a>As you can see, this method is as complicated as <code class="fm-code-in-text">insert</code><a id="marker-1010328"></a> (possibly even more complicated!); thus, similarly to what we did for <code class="fm-code-in-text">insert</code>, we broke down the <code class="fm-code-in-text">delete</code> method<a id="marker-1010332"></a> using several helper functions to keep it leaner and cleaner.</p>

  <p class="body"><a id="pgfId-1006968"></a>This time, however, we won’t describe in detail all of the helper methods. All the methods involving finding something “closest to” a node, such as function <code class="fm-code-in-text">findSiblingToMergeTo</code><a id="marker-1021741"></a> in listing 10.15, are heuristics that depend on the definition of “closer” that we adopt. As mentioned when describing how <code class="fm-code-in-text">delete</code> works, we have a few choices, from shortest distance (which is also easy to implement) to lower overlap.</p>

  <p class="body"><a id="pgfId-1006995"></a>For the sake of space, we need to leave these implementations (including the choice of the proximity function) to the reader. If you refer to the material presented in this and the previous section, you should be able to easily implement the versions using Euclidean distance as a proximity criterion</p>

  <p class="body"><a id="pgfId-1007028"></a>So, to complete our description of the <code class="fm-code-in-text">delete</code> method<a id="marker-1010340"></a>, we can start from <code class="fm-code-in-text">findClosestEntryInNodesList</code><a id="marker-1010344"></a>. Listing 10.16 shows the pseudo-code for the method that is just another linear search within a list of nodes with the goal of finding the closest entry contained in any of the nodes in the list. Notice that we also return the parent node because it will be needed by the caller.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020729"></a>Listing 10.16 The <code class="fm-code-in-text">findClosestEntryInNodesList</code> method</p>
  <pre class="programlisting"><b class="calibre21">function</b> findClosestEntryInNodesList(nodes, targetNode)               <span class="fm-combinumeral">❶</span>
  closestEntry ← <b class="calibre21">null</b>                                                 <span class="fm-combinumeral">❷</span>
  closestNode ← <b class="calibre21">null</b>
  <b class="calibre21">for</b> node in nodes <b class="calibre21">do</b>                                                <span class="fm-combinumeral">❸</span>
    closestEntryInNode ← node.getClosestCentroidTo(targetNode)        <span class="fm-combinumeral">❹</span>
    <b class="calibre21">if</b> closerThan(closestEntryInNode, closestEntry, targetNode) <b class="calibre21">then</b>  <span class="fm-combinumeral">❺</span>
      closestEntry ← closestEntryInNode                               <span class="fm-combinumeral">❻</span>
      closestNode ← node                                              <span class="fm-combinumeral">❻</span>
  <b class="calibre21">return</b> (closestEntry, closestNode)                                  <span class="fm-combinumeral">❼</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1039725"></a><span class="fm-combinumeral">❶</span> Function <code class="fm-code-in-text2">findClosestEntryInNodesList</code><a id="marker-1040243"></a> takes a list of nodes and a target node and returns the closest entry to the target and the node in the list that contains it. An entry here is, again, meant as either a point (if <code class="fm-code-in-text2">nodes</code><a id="marker-1040245"></a> are leaves) or a child node (if <code class="fm-code-in-text2">nodes</code> contains internal nodes). The definition of “closest” is encapsulated in the two auxiliary methods called at lines #5 and #6.</p>

  <p class="fm-code-annotation"><a id="pgfId-1039751"></a><span class="fm-combinumeral">❷</span> Initializes the results to <code class="fm-code-in-text2">null</code>; it is assumed that at line #6 function <code class="fm-code-in-text2">closerThan</code><a id="marker-1039756"></a> will return the first argument, when <code class="fm-code-in-text2">closestEntry</code><a id="marker-1039757"></a> is null.</p>

  <p class="fm-code-annotation"><a id="pgfId-1039770"></a><span class="fm-combinumeral">❸</span> Cycles through all the nodes in the input list</p>

  <p class="fm-code-annotation"><a id="pgfId-1039787"></a><span class="fm-combinumeral">❹</span> For each node, gets its closest entry to <code class="fm-code-in-text2">targetNode</code><a id="marker-1039791"></a>. By default, closest can be meant as “with minimal Euclidean distance.”</p>

  <p class="fm-code-annotation"><a id="pgfId-1039805"></a><span class="fm-combinumeral">❺</span> Compares the entry just computed to the best result found so far</p>

  <p class="fm-code-annotation"><a id="pgfId-1039822"></a><span class="fm-combinumeral">❻</span> If the new entry is closer (by whatever definition of “closer” is assumed) then updates the temporary variables, with the results</p>

  <p class="fm-code-annotation"><a id="pgfId-1039846"></a><span class="fm-combinumeral">❼</span> Returns a pair with the closest entry and the node containing it for the caller’s benefit</p>

  <p class="body"><a id="pgfId-1007292"></a>Next, listing 10.17 describes the <code class="fm-code-in-text">borrowFromSibling</code> method<a id="marker-1010368"></a>, which moves an entry (respectively, a point, for leaves, or a child node, for internal nodes) to the node that currently violates the minimum points/children invariant, taking it from one of its siblings. Obviously, we need to choose a sibling that has more than <code class="fm-code-in-text">m</code> entries to avoid moving the issue around (the sibling will have one less entry afterward, and we don’t want it to violate the invariant itself!). For this implementation, we will assume that all elements of the list <code class="fm-code-in-text">siblings</code><a id="marker-1010372"></a>, passed in input, are non-null nodes with at least <code class="fm-code-in-text">m+1</code> entries, and also that <code class="fm-code-in-text">siblings</code> is not empty. If you are implementing this code in a language that supports assertions, you might want to add an assert to verify these conditions.<a href="#pgfId-1011919"><sup class="footnotenumber">14</sup></a></p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020757"></a>Listing 10.17 The <code class="fm-code-in-text">SsNode::borrowFromSibling</code> method</p>
  <pre class="programlisting"><b class="calibre21">function</b> borrowFromSibling(siblings)                <span class="fm-combinumeral">❶</span>
  (closestEntry, closestSibling) ←
      findClosestEntryInNodesList(siblings, this)   <span class="fm-combinumeral">❷</span>
  closestSibling.deleteEntry(closestEntry)          <span class="fm-combinumeral">❸</span>
  closestSibling.updateBoundingEnvelope()           <span class="fm-combinumeral">❸</span>
  <b class="calibre21">this</b>.addEntry(closestEntry)                       <span class="fm-combinumeral">❹</span>
  <b class="calibre21">this</b>.updateBoundingEnvelope()                     <span class="fm-combinumeral">❹</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1039419"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">borrowFromSibling</code><a id="marker-1039423"></a> is defined in class <code class="fm-code-in-text2">SsNode</code><a id="marker-1039424"></a>. It takes a non-empty list of siblings of the current node and moves the closest entry with regard to the current node from one of the siblings to the current node.</p>

  <p class="fm-code-annotation"><a id="pgfId-1039442"></a><span class="fm-combinumeral">❷</span> Searches the list of siblings for the closest entry to the current node. Here the definition of “closest” must be decided when designing the data structure. The helper function will return both the closest entry to be moved and the sibling that currently contains it.</p>

  <p class="fm-code-annotation"><a id="pgfId-1039462"></a><span class="fm-combinumeral">❸</span> Deletes the chosen entry from the node that currently contains it and updates its bounding envelope</p>

  <p class="fm-code-annotation"><a id="pgfId-1039479"></a><span class="fm-combinumeral">❹</span> Adds the chosen entry to the current node and re-computes its bounding envelope</p>

  <p class="body"><a id="pgfId-1007493"></a>If this condition is met, we want to find the best entry to “steal,” and usually this means the closest one to the destination node, but as mentioned, other criteria can be used. Once we find it, we just need to move the closest entry from the source to the destination of this transaction and update them accordingly.</p>

  <p class="body"><a id="pgfId-1007502"></a>If, instead, this condition is not met, and there is no sibling of the child violating invariants from which we can borrow an entry, it can mean one of two things:</p>

  <ol class="calibre18">
    <li class="fm-list-numbered">
      <p class="list"><a class="calibre14" id="pgfId-1007511"></a>There are no siblings: assuming <code class="fm-code-in-text">m</code>≥<code class="fm-code-in-text">2</code>, this can only happen if we are at the root, and it only has one child. In this case, there is nothing to do.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1007527"></a>There are siblings, but all of them have exactly <code class="fm-code-in-text">m</code> entries. In this case, since <code class="fm-code-in-text">m</code><span class="cambria">≤</span><code class="fm-code-in-text">M/2</code>, if we merge the invalid node with any of its siblings, we get a new node with <code class="fm-code-in-text">2*m-1&lt;M</code> entries—in other words, a valid node that doesn’t violate any invariant.</p>
    </li>
  </ol>

  <p class="body"><a id="pgfId-1007547"></a>Listing 10.18 shows how to handle both situations: we check whether the second argument is <code class="fm-code-in-text">null</code> to understand if we are in the former or latter case, and if we do need to perform a merge, we also clean up the parent’s node (which is the one node on which the <code class="fm-code-in-text">mergeChildren</code> method<a id="marker-1010384"></a> is called).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020791"></a>Listing 10.18 The <code class="fm-code-in-text">SsNode::mergeChildren</code> method</p>
  <pre class="programlisting"><b class="calibre21">function</b> mergeChildren(firstChild, secondChild)            <span class="fm-combinumeral">❶</span>
  <b class="calibre21">if</b> secondChild != <b class="calibre21">null then</b>                              <span class="fm-combinumeral">❷</span>
    newChild ← merge(firstChild, secondChild)              <span class="fm-combinumeral">❸</span>
    <b class="calibre21">this</b>.children.delete(firstChild)                       <span class="fm-combinumeral">❹</span>
    <b class="calibre21">this</b>.children.delete(secondChild)                      <span class="fm-combinumeral">❹</span>
    <b class="calibre21">this</b>.children.add(newChild)                            <span class="fm-combinumeral">❺</span>
 
<b class="calibre21">function</b> merge(firstNode, secondNode)                      <span class="fm-combinumeral">❻</span>
  assert(firstNode.leaf == secondNode.leaf)                <span class="fm-combinumeral">❼</span>
  <b class="calibre21">if</b> firstNode.leaf then
    <b class="calibre21">return new</b> SsNode(<b class="calibre21">true</b>, 
      points=firstNode.points + secondNode.points)         <span class="fm-combinumeral">❽</span>
  <b class="calibre21">else</b>
    <b class="calibre21">return new</b> SsNode(<b class="calibre21">false</b>, 
      children=firstNode.children + secondNode.children)   <span class="fm-combinumeral">❾</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1038818"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">mergeChildren</code><a id="marker-1038822"></a> is defined in class <code class="fm-code-in-text2">SsNode</code><a id="marker-1038823"></a>. It takes two children of the current node and merges them in a single node.</p>

  <p class="fm-code-annotation"><a id="pgfId-1038847"></a><span class="fm-combinumeral">❷</span> We assume the first argument is always non-null (we can add an <code class="fm-code-in-text2">assert</code> to check it, in those languages supporting assertions). If the second argument is <code class="fm-code-in-text2">null</code>, it means this method has been called on the root and it currently has just one child, so we don’t have to do anything. If we assume <code class="fm-code-in-text2">m</code>≥<code class="fm-code-in-text2">2</code>, in fact, this is only possible when <code class="fm-code-in-text2">node</code><a id="marker-1038904"></a> is tree’s root node (but still possible).</p>

  <p class="fm-code-annotation"><a id="pgfId-1038868"></a><span class="fm-combinumeral">❸</span> Performs the merge, creating a new node</p>

  <p class="fm-code-annotation"><a id="pgfId-1038885"></a><span class="fm-combinumeral">❹</span> Deletes the two former children from the current node</p>

  <p class="fm-code-annotation"><a id="pgfId-1038911"></a><span class="fm-combinumeral">❺</span> Adds the result of the call to merge to the list of this node’s children</p>

  <p class="fm-code-annotation"><a id="pgfId-1038928"></a><span class="fm-combinumeral">❻</span> Auxiliary function <code class="fm-code-in-text2">merge</code><a id="marker-1038932"></a> takes two nodes and returns a node that has the entries of both inputs</p>

  <p class="fm-code-annotation"><a id="pgfId-1038946"></a><span class="fm-combinumeral">❼</span> Verifies that either both nodes are leaves or neither is</p>

  <p class="fm-code-annotation"><a id="pgfId-1038963"></a><span class="fm-combinumeral">❽</span> If the nodes are leaves, returns a new node whose points are the union of the nodes’ sets of points</p>

  <p class="fm-code-annotation"><a id="pgfId-1038980"></a><span class="fm-combinumeral">❾</span> If the nodes are internal, creates a new node with children from both inputs</p>

  <p class="body"><a id="pgfId-1007892"></a>This was the last piece of pseudo-code we were missing for <code class="fm-code-in-text">delete</code><a id="marker-1010404"></a>. Before wrapping up this section, though, I’d like to exhort the reader to take another look at figure 10.19. The final result is a valid SS-tree that doesn’t violate any of its invariants, but let’s be honest, the result is not that great, right? Now we have one huge sphere, <code class="fm-code-in-text">S<sub class="subscript1">3</sub></code>, that is taking up almost all the space in its parent’s bounding envelope and significantly overlapping not just its sibling, but also its parent’s other branch.</p>

  <p class="body"><a id="pgfId-1007918"></a>If you remember, this was mentioned as a risk of handling merges in case 2(b) in the description for <code class="fm-code-in-text">delete</code>. It is, unfortunately, a common side effect of both node merges and moving nodes/points among siblings; especially when the choice of the entry to move is constrained, a far entry can be picked up for merge/move, and—as in the example in figure 10.19—in the long run, after many deletions, this can make the tree unbalanced.</p>

  <p class="body"><a id="pgfId-1007940"></a>We need to do better if we would like to keep our tree balanced and its performance acceptable, and in section 10.6 we will see a possible <a id="marker-1010408"></a><a id="marker-1010412"></a>solution: SS+-trees.</p>

  <h2 class="fm-head" id="heading_id_17"><a id="pgfId-1007959"></a>10.4 Similarity Search</h2>

  <p class="body"><a id="pgfId-1007971"></a>Before discussing how to improve the balancing of SS-trees, we can finish our discussion of their methods. So far, we have seen how to construct such a tree, but what can we use it for? Not surprisingly, nearest neighbor search is one of the main applications of this data structure; you probably guessed that. Range search, like for k-d trees, is another important application for SS-trees; both NN and range searches fall into the category of similarity search, queries on large, multidimensional spaces where our only criterion is the similarity between a pair of objects.</p>

  <p class="body"><a id="pgfId-1007990"></a>As we discussed for k-d trees in chapter 9, SS-trees can also (more easily) be extended to support approximated similarity search. If you remember, in section 9.4, we mentioned that approximate queries are a possible solution to k-d tree performance issues. We’ll talk in more depth about these methods in section 10.4.2.</p>

  <h3 class="fm-head2" id="heading_id_18"><a id="pgfId-1008009"></a>10.4.1 Nearest neighbor search</h3>

  <p class="body"><a id="pgfId-1008025"></a>The <a id="marker-1010416"></a><a id="marker-1010420"></a>nearest neighbor search algorithm is similar to what we saw for k-d trees; obviously, the tree structure is different, but the main change in the algorithm’s logic is the formula we need to use to check whether a branch intersects the NN query region (the sphere centered at the query point and with a radius equal to the distance to the current guess for the nearest neighbor)—that is, if a branch is close enough to the target point to be traversed. Also, while in k-d trees we check and update distance at every node, SS-trees (and R-trees) only host points in their leaves, so we will only update the initial distance after we traverse the tree to the first leaf.</p>

  <p class="body"><a id="pgfId-1008047"></a>To improve search performance, it’s important to traverse branches in the right order. While it is not obvious what this order is, a good starting point is sorting nodes based on their minimum distance from the query point. It is not guaranteed that the node that <i class="calibre17">potentially</i> has the closest point (meaning its bounding envelope is closest to the target point) will <i class="calibre17">actually</i> have a point so close to our target, and so we can’t be sure that this heuristic will produce the best ordering possible; however, on average it helps, compared to following a random order.</p>

  <p class="body"><a id="pgfId-1008060"></a>To remind you why this is important, we discussed in section 9.3.5 how getting to a better guess of the nearest neighbor distance helps us prune more branches, and thus improve search performance. In fact, if we know that there is a point within distance <code class="fm-code-in-text">D</code> from our query point, we can prune all branches whose bounding envelopes are further than <code class="fm-code-in-text">D</code>.</p>

  <p class="body"><a id="pgfId-1008081"></a>Listing 10.19 shows the code for the <code class="fm-code-in-text">nearestNeighbor</code> method<a id="marker-1010424"></a>, and figures 10.20 and 10.21 show examples of a call to the method on our example tree. As you can see, the code is quite compact: we just need to traverse all branches that intersect the sphere centered at the search points and whose radius is the distance to the current nearest neighbor, and update the best value found in the process.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F20.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1049980"></a>Figure 10.20 Nearest neighbor search. This figure shows the first steps of the call on the root of the tree. (Top) Initially, the search area is a sphere centered at the query point <code class="fm-code-in-text">Z</code>, and with infinite radius (although here it’s shown as a circle that includes the whole tree, for consistency). (Bottom) The search traverses the tree, choosing first the closest branches. <code class="fm-code-in-text">S<sub class="subscript1">5</sub></code>’s border is closer than <code class="fm-code-in-text">S<sub class="subscript1">6</sub></code>’s, so we visit the former first (although, as you can see, the opposite choice would be the best, but the algorithm can’t know that yet). Normally the distance is computed from a node’s bounding envelope, but since <code class="fm-code-in-text">Z</code> intersects both <code class="fm-code-in-text">S<sub class="subscript1">1</sub></code> and <code class="fm-code-in-text">S<sub class="subscript1">2</sub></code>, it first chooses the one whose centroid is closer. The algorithm goes in depth as much as possible, traversing a path to a leaf before back-tracking. Here we show the path to the first leaf: once there, we can update the query region that now is the sphere centered at <code class="fm-code-in-text">Z</code> with radius equal to the distance to <code class="fm-code-in-text">R</code>, the closest point in <code class="fm-code-in-text">S<sub class="subscript1">13</sub></code>, which is therefore saved as the best guess for the nearest neighbor.</p>

  <p class="body"><a id="pgfId-1008149"></a>This simplicity and cleanness are not unexpected. We have done the hard work of the design and creation of the data structure, and now we can enjoy the benefits!</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F21.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1050025"></a>Figure 10.21 Nearest neighbor search. A summary of the next steps in the traversal. Arrows are numbered to reflect the order of the recursive calls. (Top) After visiting <code class="fm-code-in-text">S<sub class="subscript1">13</sub></code> and finding <code class="fm-code-in-text">R</code> as the best guess for the nearest neighbor, <code class="fm-code-in-text">S<sub class="subscript1">12</sub></code> is skipped because it’s outside the update search region. Then the execution backtracks, and we get to <code class="fm-code-in-text">S<sub class="subscript1">5</sub></code>’s sibling, <code class="fm-code-in-text">S<sub class="subscript1">6</sub></code>, which still has a non-null intersection with the search region. (Bottom) Fast-forward to the end of the traversal. We need to traverse <code class="fm-code-in-text">S<sub class="subscript1">1</sub></code>’s branch as well, because <code class="fm-code-in-text">Z</code> lies within it. As a matter of fact, the search region intersects another leaf, <code class="fm-code-in-text">S<sub class="subscript1">10</sub></code>, so we need to traverse a path to it. As you can see, point <code class="fm-code-in-text">J</code> is close to being <code class="fm-code-in-text">Z</code>’s nearest neighbor, so it’s not unlikely that we would find the true NN in a later call.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020819"></a>Listing 10.19 The <code class="fm-code-in-text">nearestNeighbor</code> method for SS-tree</p>
  <pre class="programlisting"><b class="calibre21">function</b> nearestNeighbor(node, target, (nnDist, nn)=(inf, null))      <span class="fm-combinumeral">❶</span>
  <b class="calibre21">if</b> node.leaf <b class="calibre21">then</b>                                                   <span class="fm-combinumeral">❷</span>
    <b class="calibre21">for</b> point in node.points <b class="calibre21">do</b>                                       <span class="fm-combinumeral">❸</span>
      dist ← distance(point, target)                                  <span class="fm-combinumeral">❹</span>
      <b class="calibre21">if</b> dist &lt; nnDist <b class="calibre21">then</b>                                           <span class="fm-combinumeral">❺</span>
        (nnDist, nn) ← (dist, point)                                  <span class="fm-combinumeral">❺</span>
  <b class="calibre21">else</b>                                    
    sortedChildren ← sortNodesByDistance(node.children, target)       <span class="fm-combinumeral">❻</span>
    <b class="calibre21">for</b> child in sortedChildren <b class="calibre21">do</b>                                    <span class="fm-combinumeral">❼</span>
      <b class="calibre21">if</b> nodeDistance(child, target) &lt; nnDist <b class="calibre21">then</b>                    <span class="fm-combinumeral">❽</span>
        (nnDist, nn) ← nearestNeighbor(child, target, (nnDist, nn))   <span class="fm-combinumeral">❾</span>
  <b class="calibre21">return</b> (nnDist, nn)                                                 <span class="fm-combinumeral">❿</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1037981"></a><span class="fm-combinumeral">❶</span> Function <code class="fm-code-in-text2">nearestNeighbor</code><a id="marker-1038692"></a> returns the closest point to a given target. It takes the node to search and the query point. We also (optionally) pass the best values found so far for nearest neighbor (NN) and its distance to help pruning. These values default to <code class="fm-code-in-text2">null</code> and <code class="fm-code-in-text2">infinity</code> for a call on the tree root, unless we want to limit the search inside a spherical region (in that case, just pass the sphere’s radius as the initial value for <code class="fm-code-in-text2">nnDist</code>).</p>

  <p class="fm-code-annotation"><a id="pgfId-1038006"></a><span class="fm-combinumeral">❷</span> Checks if <code class="fm-code-in-text2">node</code><a id="marker-1038010"></a> is a leaf. Leaves are the only nodes containing points, and so are the only nodes where we can possibly update the nearest neighbor found.</p>

  <p class="fm-code-annotation"><a id="pgfId-1038024"></a><span class="fm-combinumeral">❸</span> Cycles through all points in this leaf</p>

  <p class="fm-code-annotation"><a id="pgfId-1038041"></a><span class="fm-combinumeral">❹</span> Computes the distance between current point and <code class="fm-code-in-text2">target</code><a id="marker-1038046"></a></p>

  <p class="fm-code-annotation"><a id="pgfId-1038059"></a><span class="fm-combinumeral">❺</span> If that distance is less than the current NN’s distance, we have to update the values stored for the NN and its distance.</p>

  <p class="fm-code-annotation"><a id="pgfId-1038076"></a><span class="fm-combinumeral">❻</span> If, instead, <code class="fm-code-in-text2">node</code> is an internal node, we need to cycle through all its children and possibly traverse their subtrees. We start by sorting <code class="fm-code-in-text2">node</code>’s children from the closest to the furthest with respect to <code class="fm-code-in-text2">target</code>. As mentioned, a different heuristic than the distance to the bounding envelope can be used here.</p>

  <p class="fm-code-annotation"><a id="pgfId-1038093"></a><span class="fm-combinumeral">❼</span> Cycles through all children in the order we sorted them</p>

  <p class="fm-code-annotation"><a id="pgfId-1038110"></a><span class="fm-combinumeral">❽</span> Checks if their bounding envelope intersects with the NN bounding sphere. In other words, if the distance from <code class="fm-code-in-text2">target</code> to the closest point within <code class="fm-code-in-text2">child</code>’s bounding envelope is smaller than the pruning distance (<code class="fm-code-in-text2">nnDist</code>).</p>

  <p class="fm-code-annotation"><a id="pgfId-1038127"></a><span class="fm-combinumeral">❾</span> If the distance between <code class="fm-code-in-text2">target</code> and <code class="fm-code-in-text2">child</code> is smaller than the pruning distance, then it traverses the subtree rooted at <code class="fm-code-in-text2">child</code>, updating the result.</p>

  <p class="fm-code-annotation"><a id="pgfId-1038144"></a><span class="fm-combinumeral">❿</span> All that remains is to return the updated values for the best result found so far.</p>

  <p class="body"><a id="pgfId-1008539"></a>Of the helper methods in listing 10.19, it’s important to spend some time explaining function <code class="fm-code-in-text">nodeDistance</code><a id="marker-1010440"></a>. If we refer to figure 10.22, you can see why the minimum distance between a node and a bounding envelope is equal to the distance between the centroids minus the envelope’s radius: we just used the formula for the distance between a point and a sphere, taken from geometry.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F22.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1050069"></a>Figure 10.22 Minimum distance to a bounding envelope. Consider the triangle <code class="fm-code-in-text">ZBC<sub class="subscript1">B</sub></code>. Then, for the triangular inequality, <code class="fm-code-in-text">|C<sub class="subscript1">B</sub>B|+|BZ|&gt;|ZC<sub class="subscript1">B</sub>|</code>, but <code class="fm-code-in-text">|ZC<sub class="subscript1">B</sub>|==|ZA|+|AC<sub class="subscript1">B</sub>|</code> and <code class="fm-code-in-text">|AC<sub class="subscript1">B</sub>|==|C<sub class="subscript1">B</sub>B|</code> (they are both radii), so ultimately <code class="fm-code-in-text">|C<sub class="subscript1">B</sub>B|+|BZ|&gt;|ZA|+|AC<sub class="subscript1">B</sub>|</code><span class="cambria">⇒</span><code class="fm-code-in-text">|BZ|&gt;|ZA|</code>. Therefore, the minimum distance is the length of segment from <code class="fm-code-in-text">Z</code> to <code class="fm-code-in-text">S<sub class="subscript1">B</sub></code>’s sphere, along the direction connecting its centroid to <code class="fm-code-in-text">Z</code>.</p>

  <p class="body"><a id="pgfId-1008555"></a>We can easily extend the nearest neighbor search algorithm to return the <code class="fm-code-in-text">n</code>-th nearest neighbor. Similar to what we did in chapter 9 for k-d trees, we just need to use a bounded priority queue that keeps at most <code class="fm-code-in-text">n</code> elements, and use the furthest of those <code class="fm-code-in-text">n</code> points as a reference, computing the pruning distance as the distance from this point to the search target (as long as we have found fewer than <code class="fm-code-in-text">n</code> points, the pruning distance will be infinity).</p>

  <p class="body"><a id="pgfId-1008645"></a>Likewise, we can add a threshold argument to the search function, which becomes the initial pruning distance (instead of passing infinity as the default value for <code class="fm-code-in-text">nnDist</code>), to also support search in spherical regions. Since these implementations can be trivially obtained, referring to chapter 9 for guidance, we leave them to the readers <a id="marker-1010444"></a><a id="marker-1010448"></a>(for a hint, check the implementation on the book’s <span class="fm-hyperlink">repo</span> on GitHub<a href="#pgfId-1011932"><sup class="footnotenumber">15</sup></a>).</p>

  <h3 class="fm-head2" id="heading_id_19"><a id="pgfId-1008665"></a>10.4.2 Region search</h3>

  <p class="body"><a id="pgfId-1008679"></a>Region <a id="marker-1010452"></a><a id="marker-1010456"></a><a id="marker-1010460"></a>search will be similar to what we have described for k-d trees—the only difference being how we need to compute the intersection between each node and the search region, besides the structural change due to the fact that points are only stored in leaves.</p>

  <p class="body"><a id="pgfId-1008694"></a>Listing 10.20 shows a generic implementation of this method for SS-trees that assumes the region passed as argument includes a method to check whether the region itself intersects a hyper-sphere (a node’s bounding envelope). Please refer to section 9.3.6 for a detailed explanation and examples about search on the most common types of regions, and their algebraic <a id="marker-1010464"></a><a id="marker-1010468"></a><a id="marker-1010472"></a>meaning.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020847"></a>Listing 10.20 The <code class="fm-code-in-text">pointsWithinRegion</code> method for SS-tree</p>
  <pre class="programlisting"><b class="calibre21">function</b> pointsWithinRegion(node, region)                    <span class="fm-combinumeral">❶</span>
  points ← []                                                <span class="fm-combinumeral">❷</span>
  <b class="calibre21">if</b> node.leaf <b class="calibre21">then</b>                                          <span class="fm-combinumeral">❸</span>
    <b class="calibre21">for</b> point in node.points <b class="calibre21">do</b>                              <span class="fm-combinumeral">❹</span>
      <b class="calibre21">if</b> region.intersectsPoint(point) <b class="calibre21">then</b>                  <span class="fm-combinumeral">❺</span>
        points.insert(point)                                 <span class="fm-combinumeral">❺</span>
  <b class="calibre21">else</b>    
   <b class="calibre21">for</b> child in node.children <b class="calibre21">do</b>                             <span class="fm-combinumeral">❻</span>
      <b class="calibre21">if</b> region.intersectsNode(child) <b class="calibre21">then</b>                   <span class="fm-combinumeral">❼</span>
        points.insertAll(pointsWithinRegion(child, region))  <span class="fm-combinumeral">❽</span>
  <b class="calibre21">return</b> points                                              <span class="fm-combinumeral">❾</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1037260"></a><span class="fm-combinumeral">❶</span> Function <code class="fm-code-in-text2">pointsWithinRegion</code><a id="marker-1037287"></a> takes a node on which the search must be performed, as well as the search region. It returns a list of points stored in the subtree rooted at <code class="fm-code-in-text2">node</code><a id="marker-1037289"></a> and lying within the search region (in other words, the intersection between the region and the <code class="fm-code-in-text2">node</code>’s bounding envelope).</p>

  <p class="fm-code-annotation"><a id="pgfId-1037302"></a><span class="fm-combinumeral">❷</span> Initializes the return value to an empty list</p>

  <p class="fm-code-annotation"><a id="pgfId-1037319"></a><span class="fm-combinumeral">❸</span> Checks if <code class="fm-code-in-text2">node</code> is a leaf</p>

  <p class="fm-code-annotation"><a id="pgfId-1037336"></a><span class="fm-combinumeral">❹</span> Cycles through all points in the current leaf</p>

  <p class="fm-code-annotation"><a id="pgfId-1037353"></a><span class="fm-combinumeral">❺</span> If the current point is within the search region, it’s added to the list of results. The onus of providing the right method to check if a point lies in a region is on the region’s class (so regions of different shapes can implement the method differently).</p>

  <p class="fm-code-annotation"><a id="pgfId-1037370"></a><span class="fm-combinumeral">❻</span> If, instead, <code class="fm-code-in-text2">node</code> is an internal node, cycles through its children</p>

  <p class="fm-code-annotation"><a id="pgfId-1037387"></a><span class="fm-combinumeral">❼</span> Checks to see if the search region intersects the current child. Again, the region’s class will have to implement this check.</p>

  <p class="fm-code-annotation"><a id="pgfId-1037404"></a><span class="fm-combinumeral">❽</span> If there is any intersection (so there might be points in common), we should call this method recursively on the current child, and then add all the results found, if any, to the list of points returned by this method call.</p>

  <p class="fm-code-annotation"><a id="pgfId-1037261"></a><span class="fm-combinumeral">❾</span> At this point, we can just return all the points we collected in this method call.</p>

  <h3 class="fm-head2" id="heading_id_20"><a id="pgfId-1009014"></a>10.4.3 Approximated similarity search</h3>

  <p class="body"><a id="pgfId-1009030"></a>As <a id="marker-1010484"></a><a id="marker-1010488"></a>mentioned, similarity search in k-d trees, as well as R-trees and SS-trees, suffers from what is called the <i class="calibre17">curse of</i> <i class="calibre17">dimensionality</i>: the methods on these data structures become exponentially slower with the growth of the number of dimensions of the search space. K-d trees also suffer from additional sparsity issues that become more relevant in higher dimensions.</p>

  <p class="body"><a id="pgfId-1009052"></a>While using R-trees and SS-trees can improve the balance of the trees and result in better trees and faster construction, there is still something more we can do to improve the performance of the similarity search methods.</p>

  <p class="body"><a id="pgfId-1009063"></a>These approximate search methods are indeed a tradeoff between accuracy and performance; there are a few different (and sometimes complementary) strategies that can be used to have faster approximate queries:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1009072"></a><i class="calibre15">Reduce the dimensionality of the objects</i>—Using algorithms such as PCA or Discrete Fourier Transform, it is possible to project the dataset’s object into a different, lower-dimensional space. The idea is that this space will maintain only the essential information to distinguish between different points in the dataset. With dynamic datasets, this method is obviously less effective.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1009089"></a><i class="calibre15">Reduce the number of branches traversed</i>—As we have seen in the previous section, our pruning strategy is quite conservative, meaning that we traverse a branch if there is any chance (even the slightest) that we can find a point in that branch closer than our current nearest neighbor. By using a more aggressive pruning strategy, we can reduce the number of branches (and ultimately dataset points) touched, as long as we accept that our results might not be the most accurate possible.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1009105"></a><i class="calibre15">Use an early termination strategy</i>—In this case, the search is stopped when the current result is judged good enough. The criterion to decide what’s “good enough” can be a threshold (for instance, when a NN closer than some distance is found), or a stop condition connected to the probability of finding a better match (for instance, if branches are visited from closer to further with regard to the query point, this probability decreases with the number of leaves visited).</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1009121"></a>We will focus on the second strategy, the pruning criterion. In particular, we can provide a method that, given a parameter <span class="cambria">ϵ</span>, called the <i class="calibre17">approximation error</i>, with <code class="fm-code-in-text">0.0</code> <span class="cambria">≤</span> <code class="fm-code-in-text"><span class="cambria">ϵ</span></code> <span class="cambria">≤</span> <code class="fm-code-in-text">0.5</code>, guarantees that in an approximated <code class="fm-code-in-text">n</code>-nearest neighbor search, the <code class="fm-code-in-text">n</code>-th nearest neighbor returned will be within a factor <code class="fm-code-in-text">(1+<span class="cambria">ϵ</span>)</code> from the true <code class="fm-code-in-text">n</code>-th nearest neighbor.</p>

  <p class="body"><a id="pgfId-1009148"></a>To explain this, let’s restrict<a href="#pgfId-1011948"><sup class="footnotenumber">16</sup></a> to the case where <code class="fm-code-in-text">n==1</code>, just plain NN search. Assume the approximated NN-search method, called on a point <code class="fm-code-in-text">P</code>, returns a point <code class="fm-code-in-text">Q</code>, while the real nearest neighbor of <code class="fm-code-in-text">P</code> would be another point <code class="fm-code-in-text">N<span class="cambria">≠</span>Q</code>. Then, if the distance between <code class="fm-code-in-text">P</code> and its <i class="calibre17">true</i> nearest neighbor <code class="fm-code-in-text">N</code> is <code class="fm-code-in-text">d</code>, the approximated search distance between <code class="fm-code-in-text">P</code> and <code class="fm-code-in-text">Q</code> will be at most <code class="fm-code-in-text">(1+<span class="cambria">ϵ</span>)*d</code>.</p>

  <p class="body"><a id="pgfId-1009191"></a>Guaranteeing this condition is easy. When we prune a branch, instead of checking to see if the distance between the target point and the branch’s bounding envelope is smaller than the current NN distance, we prune unless it is smaller than <code class="fm-code-in-text">1/(1+<span class="cambria">ϵ</span>)</code> times the NN’s distance.</p>

  <p class="body"><a id="pgfId-1009209"></a>If we denote with <code class="fm-code-in-text">Z</code> the query point, with <code class="fm-code-in-text">C</code> current nearest neighbor, and with <code class="fm-code-in-text">A</code> the closest point to <code class="fm-code-in-text">Z</code> in the branch pruned (see figure 10.21), we have, in fact</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F_EQ4.png"/></p>

  <p class="body"><a id="pgfId-1009235"></a>So, if the distance of the closest point in the branch is higher than the current NN’s distance over the reciprocal of <code class="fm-code-in-text">(1+<span class="cambria">ϵ</span>)</code>, we are guaranteed that the possible nearest neighbor held in the branch is no further than an epsilon factor from our current nearest neighbor.</p>

  <p class="body"><a id="pgfId-1009246"></a>Of course, there could be several points in the dataset that are within a factor <code class="fm-code-in-text">(1+<span class="cambria">ϵ</span>)</code> from the true nearest neighbor, so we are not guaranteed that we get the second-closest point, or the third, and so on.</p>

  <p class="body"><a id="pgfId-1009261"></a>However, the probability that these points exist is proportional to the size of the ring region with radii <code class="fm-code-in-text">nnDist</code> and <code class="fm-code-in-text">nnDist/(1+<span class="cambria">ϵ</span>)</code>, so the smaller we set <span class="cambria">ϵ</span>, the lower the chances we are missing closer points.</p>

  <p class="body"><a id="pgfId-1009276"></a>A more accurate estimate of the probability is given by the area of the intersection of the aforementioned ring with the bounding envelope of the node we skip. Figure 10.23 illustrates this idea, and shows the difference between SS-trees, k-d trees, and R-trees: the probability is maximum when the inner radius is just tangent to the area, and a sphere has a smaller intersection, with respect to any rectangle.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F23.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1050119"></a>Figure 10.23 The probability of missing <code class="fm-code-in-text">j</code> points by using an approximation error is proportional to the intersection of the pruned search area (the ring with radius <code class="fm-code-in-text">(<span class="cambria">ϵ</span>/(1+<span class="cambria">ϵ</span>))*nnDist)</code> and the bounding envelope of the node intersecting this region, but not the internal sphere with reduced radius. The maximum probability corresponds to a region tangent to the internal sphere. The figure shows how the intersection is smaller for spherical bounding envelopes than for rectangular ones.</p>

  <p class="body"><a id="pgfId-1009327"></a>If we set <code class="fm-code-in-text"><span class="cambria">ϵ</span>==0.0</code>, then we get back the exact search algorithm as a special case, because <code class="fm-code-in-text">nnDist/(1+epsilon)</code> becomes just <code class="fm-code-in-text">nnDist</code>.</p>

  <p class="body"><a id="pgfId-1009343"></a>When <code class="fm-code-in-text"><span class="cambria">ϵ</span>&gt;0.0</code>, a traversal might look like figure 10.24, where we use an example slightly different from the one in figures 10.20 and 10.21 to show how approximate NN search could miss the nearest neighbor.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F24.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1050156"></a>Figure 10.24 Approximated nearest neighbor search. The example is similar (almost identical) to the one in figures 10.20 and 10.21, to allow a close comparison. We show the verbose representation of the tree to make clearer the path followed in traversal. Node <code class="fm-code-in-text">S<sub class="subscript1">4</sub></code> contains <code class="fm-code-in-text">J</code>, the true NN, whose bit is further away from the query point <code class="fm-code-in-text">Z</code> than <code class="fm-code-in-text">S<sub class="subscript1">5</sub></code> and <code class="fm-code-in-text">S<sub class="subscript1">6</sub></code>, and outside the approximated query region (<span class="cambria">ϵ</span> has been chosen ad hoc, obviously, to cause this condition, in this example, <span class="cambria">ϵ</span><code class="fm-code-in-text">~=0.25</code>). Arrows are numbered in the order in which they are traversed.</p>

  <p class="body"><a id="pgfId-1009403"></a>In many domains we can be not just fine, but even happy, with approximate search. Take our example of the search through a dataset of images: Can we really be sure that an exact match is better than an approximate one? If we were working with geographic coordinates—say, on a map—then a factor <span class="cambria">ϵ</span> difference could have dire consequences (at the very best, taking a longer route would be expensive, but it might get as bad as safety issues). But when the task is finding the dresses that most closely resemble a purchase, then we can’t even guarantee the precision of our metric. Maybe a couple of feature vectors are slightly closer than another pair, but in the end, to the human eye, the latter images look more similar!</p>

  <p class="body"><a id="pgfId-1009418"></a>So, as long as the approximation error <span class="cambria">ϵ</span> is not too large, chances are that an approximated result for “most similar image” will be as good as, or maybe better, than the exact result.</p>

  <p class="body"><a id="pgfId-1009429"></a>The interested reader can find plenty of literature on the topic of approximated similarity search and delve deeper into the concepts that we could only superficially examine here. As a starting point I suggest the remarkable work <a id="marker-1010492"></a><a id="marker-1010496"></a>of Giuseppe Amato.<a href="#pgfId-1011970"><sup class="footnotenumber">17</sup></a></p>

  <h2 class="fm-head" id="heading_id_21"><a id="pgfId-1009443"></a>10.5 SS+-tree<a href="#pgfId-1011985"><sup class="footnotenumber3">18</sup></a></h2>

  <p class="body"><a id="pgfId-1009461"></a>So <a id="marker-1010500"></a><a id="marker-1010504"></a>far, we have used the original SS-tree structure, as described in the original paper by White and Jain<a href="#pgfId-1011999"><sup class="footnotenumber">19</sup></a>; SS-trees have been developed to reduce the nodes overlapping, and in turn the number of leaves traversed by a search on the tree, with respect to R-trees and k-d trees.</p>

  <h3 class="fm-head2" id="heading_id_22"><a id="pgfId-1009481"></a>10.5.1 Are SS-trees better?</h3>

  <p class="body"><a id="pgfId-1009497"></a>With <a id="marker-1010508"></a>respect to k-d trees, the main advantage of this new data structure is that it is self-balancing, so much so that all leaves are at the same height. Also, using bounding envelopes instead of splits parallel to a single axis mitigates the <i class="calibre17">curse of dimensionality</i><a id="marker-1010512"></a> because unidimensional splits only allow partitioning the search space along one direction at a time.</p>

  <p class="body"><a id="pgfId-1009513"></a>R-trees also use bounding envelops, but with a different shape: hyper-rectangles instead of hyper-spheres. While hyper-spheres can be stored more efficiently and allow for faster computation of their exact distance, hyper-rectangles can grow asymmetrically in different directions: this allows us to cover a node with a smaller volume, while hyper-spheres, being symmetrical in all directions, generally waste more space, with large regions without any point. And indeed, if you compare figure 10.4 to figure 10.8, you can see that rectangular bounding envelopes fit the data more tightly than the spherical ones of the SS-tree.</p>

  <p class="body"><a id="pgfId-1009526"></a>On the other hand, it can be proved that the decomposition in spherical regions minimizes the number of leaves traversed.<a href="#pgfId-1012014"><sup class="footnotenumber">20</sup></a></p>

  <p class="body"><a id="pgfId-1009537"></a>If we compare the growth of the volume of spheres and cubes in a <code class="fm-code-in-text">k</code>-dimensional spaces, for different values of <code class="fm-code-in-text">k</code>, given by these formulas</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F_EQ5.png"/></p>

  <p class="body"><a id="pgfId-1009559"></a>we can see that spheres grow more slowly than cubes, as also shown in figure 10.25.</p>

  <p class="body"><a id="pgfId-1009568"></a>And if a group of points is uniformly distributed along all directions and shaped as a spherical cluster, then a hyper-sphere is the type of bounding envelope that wastes the lowest volume, as you can see also for a 2-D space in figure 10.23, where a circle of radius <code class="fm-code-in-text">r</code> is inscribed in a square of side <code class="fm-code-in-text">2r</code>. If the points are distributed in a circular cluster, then all the areas between the circle and its circumscribed square (highlighted in figure 10.23) are potentially empty, and so wasted.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F25.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1050201"></a>Figure 10.25 Volume of spheres (lighter line) and cubes (darker line) for various radii, as a function of the number of dimensions of the search space</p>

  <p class="body"><a id="pgfId-1009629"></a>Experiments have confirmed that SS-trees using spherical bounding envelopes perform better on datasets uniformly distributed along all directions, while rectangular envelopes work better with skewed datasets.</p>

  <p class="body"><a id="pgfId-1009640"></a>Neither R-trees nor SS-trees can offer logarithmic worst-case upper bounds for their methods. In the (unlikely, but possible) worst case, all leaves of the tree will be traversed, and there are at most <code class="fm-code-in-text">n/m</code> of them. This means that each of the main operations on these data structures can take up to linear time in the size of the dataset. Table 10.1 summarizes their running time, comparing them to k-d <a id="marker-1010516"></a>trees.</p>

  <p class="fm-table-caption"><a id="pgfId-1021085"></a>Table 10.1 Operations provided by k-d tree, and their cost on a balanced k-d tree with <code class="fm-code-in-text">n</code> elements</p>

  <table border="1" class="contenttable" width="100%">
    <tr class="calibre8">
      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1021093"></a>Operation</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1021095"></a>k-d tree</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1021097"></a>R-tree</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1021099"></a>SS-tree</p>
      </th>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021101"></a><code class="fm-code-in-text2">Search</code><a id="marker-1021140"></a></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021103"></a><code class="fm-code-in-text2">O(log(n))</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021105"></a><code class="fm-code-in-text2">O(n)</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021107"></a><code class="fm-code-in-text2">O(n)</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021109"></a><code class="fm-code-in-text2">Insert</code><a id="marker-1021141"></a></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021111"></a><code class="fm-code-in-text2">O(log(n))</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021113"></a><code class="fm-code-in-text2">O(n)</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021115"></a><code class="fm-code-in-text2">O(n)</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021117"></a><code class="fm-code-in-text2">Remove</code><a id="marker-1021142"></a></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021119"></a><code class="fm-code-in-text2">O(n<sup class="superscript">1-1/k</sup>)</code><a href="#pgfId-1021173"><sup class="footnotenumber1">a</sup></a></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021121"></a><code class="fm-code-in-text2">O(n)</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021123"></a><code class="fm-code-in-text2">O(n)</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021125"></a><code class="fm-code-in-text2">nearestNeighbor</code><a id="marker-1021143"></a></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021127"></a><code class="fm-code-in-text2">O(2<sup class="superscript">k</sup> + log(n))</code><a href="#pgfId-1021173"><sup class="footnotenumber1">a</sup></a></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021129"></a><code class="fm-code-in-text2">O(n)</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021131"></a><code class="fm-code-in-text2">O(n)</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021133"></a><code class="fm-code-in-text2">pointsInRegion</code><a id="marker-1021144"></a></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021135"></a><code class="fm-code-in-text2">O(n)</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021137"></a><code class="fm-code-in-text2">O(n)</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1021139"></a><code class="fm-code-in-text2">O(n)</code></p>
      </td>
    </tr>
  </table>

  <p class="fm-footnote"><sup class="footnotenumber">a</sup> <a id="pgfId-1021173"></a>Amortized, for a k-d tree holding k-dimensional points.</p>

  <h3 class="fm-head2" id="heading_id_23"><a id="pgfId-1010871"></a>10.5.2 Mitigating hyper-sphere limitations</h3>

  <p class="body"><a id="pgfId-1010889"></a>Now <a id="marker-1011620"></a>this question arises: Is there anything we can do to limit the cons of using spherical bounding envelopes so that we can reap the advantages when we have symmetrical datasets, and limit the disadvantages with skewed ones?</p>

  <p class="body"><a id="pgfId-1010900"></a>To cope with skewed datasets, we could use ellipsoids instead of spheres, so that the clusters can grow in each direction independently. However, this would complicate search, because we would want to compute the radius along the direction connecting the centroid to the query point, which in the general case won’t lie on any of the axes.</p>

  <p class="body"><a id="pgfId-1010913"></a>A different approach to reduce the wasted area attempts to reduce the volume of the bounding spheres used. So far we have always used spheres whose center was a group of points’ center of mass, and whose radius was the distance to the furthest point, so that the sphere would cover all points in the cluster. This, however, is not the smallest possible sphere that covers all the points. Figure 10.26 shows an example of such a difference.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F26.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1050247"></a>Figure 10.26 Difference between the spheres with minimum radius centered at the cluster centroid (left), and the minimum covering sphere (right) for an example set of points.</p>

  <p class="body"><a id="pgfId-1010932"></a>Computing this smallest enclosing sphere in higher dimensions is, however, not feasible, because the algorithm to compute the exact values for its center (and radius) is exponential in the number of dimensions.</p>

  <p class="body"><a id="pgfId-1010962"></a>What can be done, however, is computing an approximation of the smallest enclosing sphere, starting with the center of mass of the cluster as an initial guess. At the very high level, the approximation algorithm tries at each iteration to move the center toward the farthest point in the dataset. After each iteration, the maximum distance the point can move is shortened, limited by the span of the previous update, thus ensuring convergence.</p>

  <p class="body"><a id="pgfId-1010978"></a>We won’t delve deeper into this algorithm in this context; the interested readers can read more about this method, starting, for instance, with this article<a href="#pgfId-1012029"><sup class="footnotenumber">21</sup></a> by Fischer et al.</p>

  <p class="body"><a id="pgfId-1010989"></a>For now, we will move to another way we could improve the balancing of the tree: reducing the node <a id="marker-1011624"></a>overlap.</p>

  <h3 class="fm-head2" id="heading_id_24"><a id="pgfId-1011000"></a>10.5.3 Improved split heuristic</h3>

  <p class="body"><a id="pgfId-1011016"></a>We <a id="marker-1011628"></a>saw in section 10.3 that splitting and merging nodes, as well as “borrowing” points/children from siblings, can result in skewed clusters that require bounding envelopes larger than necessary, and increase node overlap.</p>

  <p class="body"><a id="pgfId-1011028"></a>To counteract this effect, Kurniawati et al., in their work<a href="#pgfId-1012045"><sup class="footnotenumber">22</sup></a> on SS+-trees, introduce a new split heuristic that, instead of only partitioning points along the direction of maximum variance, tries to find the two groups such that each of them will collect the closest nearby points.</p>

  <p class="body"><a id="pgfId-1011043"></a>To achieve this result, a variant of the k-means clustering algorithm will be used, with two constraints:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1011052"></a>The number of clusters will be fixed and equal to <code class="fm-code-in-text">2.</code></p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1011066"></a>The maximum number of points per cluster is bound to <code class="fm-code-in-text">M</code>.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1011080"></a>We will talk in more depth about clustering and k-means in chapter 12, so please refer to section 12.2 to see the details of its implementation.</p>

  <p class="body"><a id="pgfId-1011089"></a>The running time for k-means, with at most <code class="fm-code-in-text">j</code> iterations on a dataset with <code class="fm-code-in-text">n</code> points, is <code class="fm-code-in-text">O(jkn)</code>,<code class="fm-code-in-text"><a class="calibre14" href="#pgfId-1012063"><sup class="footnotenumber4">23</sup></a></code> where, of course, <code class="fm-code-in-text">k</code> is the number of centroids.</p>

  <p class="body"><a id="pgfId-1011105"></a>Since for the split heuristic we have <code class="fm-code-in-text">k==2</code>, and the number of points in the node to split is <code class="fm-code-in-text">M+1</code>, the running time becomes <code class="fm-code-in-text">O(jdM)</code>, compared to <code class="fm-code-in-text">O(dM)</code> we had for the original split heuristic described in section 10.3. We can therefore trade off the quality of the result for performance by controlling the maximum number of iterations <code class="fm-code-in-text">j</code>. Figure 10.27 shows an example of how impactful this heuristic can be, and why the increase in running time is well worth it.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F27.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1050291"></a>Figure 10.27 An example of the different partitions produced by the split heuristic in the original SS-tree version (center) and the k-means split heuristic (left). For the original heuristic, the direction of maximum variance was along the <code class="fm-code-in-text">y</code> axis. For this example, we assume <code class="fm-code-in-text">M==12</code> and <code class="fm-code-in-text">m==6</code>.</p>

  <p class="body"><a id="pgfId-1011159"></a>Although newer, more complex clustering algorithms have been developed during the years (as we’ll see in chapter 12, where we’ll also describe DBSCAN and OPTICS), k-means is still a perfect match for SS-trees, because it naturally produces spherical clusters, each with a centroid equal to the center of mass of the points in the <a id="marker-1011632"></a>cluster.</p>

  <h3 class="fm-head2" id="heading_id_25"><a id="pgfId-1011172"></a>10.5.4 Reducing overlap</h3>

  <p class="body"><a id="pgfId-1011186"></a>The <a id="marker-1011636"></a>k-means split heuristic is a powerful tool to reduce node overlapping and keep the tree balanced and search fast, as we were reminded at the beginning of last section; however, we can unbalance a node while also deleting points, in particular during merge or when we move a point/child across siblings. Moreover, sometimes the overlapping can be the result of several operations on the tree and involve more than just two nodes, or even more than a single level.</p>

  <p class="body"><a id="pgfId-1011203"></a>Finally, the k-means split heuristic doesn’t have overlap minimization as a criterion, and because of the intrinsic behavior of k-means, the heuristic could produce results where a node with larger variance might completely overlap a smaller node.</p>

  <p class="body"><a id="pgfId-1011212"></a>To illustrate these situations, the top half of figure 10.28 shows several nodes and their parents, with a significant overlap.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch10_F28.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1050333"></a>Figure 10.28 An example where the grandchildren-split heuristic could be triggered and would lower the nodes overlap. The SS-tree in the example has <code class="fm-code-in-text">m==2</code> and <code class="fm-code-in-text">M&gt;=5</code>. The k-means split heuristic is run on the points <code class="fm-code-in-text">A-L</code>, with <code class="fm-code-in-text">k==5</code>. Notice how k-means can (and sometimes will) output fewer clusters than the initial number of centroids. In this case, only four clusters were returned.</p>

  <p class="body"><a id="pgfId-1011250"></a>To solve such a situation, SS+-trees introduce two new elements:</p>

  <ol class="calibre18">
    <li class="fm-list-numbered">
      <p class="list"><a class="calibre14" id="pgfId-1011262"></a>A check to discover such situations.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1011275"></a>A new heuristic that applies k-means to all grandchildren of a node <code class="fm-code-in-text">N</code> (no matter whether they are points or other nodes; in the latter case their centroids will be used for clustering). The clusters created will replace <code class="fm-code-in-text">N</code>’s children.</p>
    </li>
  </ol>

  <p class="body"><a id="pgfId-1011295"></a>To check the overlap, the first thing we need is the formula to compute the volume of intersections of two spheres. Unfortunately, computing the exact overlap of two hyperspheres, in the generic <code class="fm-code-in-text">k</code>-dimensional case, requires not just substantial work and good calculus skills to derive the formulas, but also robust computing resources, as it results in a formula that includes an integral whose computation is clearly expensive enough to cause you to question its usefulness in a heuristic.</p>

  <p class="body"><a id="pgfId-1011312"></a>An alternative approach is to check whether one of the two bounding envelopes is completely included in the other. Check that the center of one sphere is contained in the other one, and that the distance of the centroid of the smaller sphere is closer than <code class="fm-code-in-text">R-r</code> to the centroid of the larger one, where <code class="fm-code-in-text">R</code> is the radius of the larger sphere and, as you might expect, <code class="fm-code-in-text">r</code> is the radius of the smaller one.</p>

  <p class="body"><a id="pgfId-1011333"></a>Variants of this check can set a threshold for the ratio between the actual distance of the two centroids and <code class="fm-code-in-text">R-r</code>, using it for an approximation of the overlapping volume as this ratio gets closer to <code class="fm-code-in-text">1</code>.</p>

  <p class="body"><a id="pgfId-1011346"></a>The reorganization heuristic is then applied if the check’s condition is satisfied. A good understanding of k-means is needed to get into the details of this heuristic, so we’ll skip it in this context, and we encourage readers to refer to chapter 12 for a description of this clustering algorithm. Here, we will use an example to illustrate how the heuristic works.</p>

  <p class="body"><a id="pgfId-1011359"></a>In the example, the heuristic is called on node <code class="fm-code-in-text">S<sub class="subscript1">1</sub></code> and the clustering is run on its grandchildren, points <code class="fm-code-in-text">A-L</code>. As mentioned, these could also be centroids of other internal nodes, and the algorithm wouldn’t change.</p>

  <p class="body"><a id="pgfId-1011373"></a>The result is shown in the bottom half of figure 10.28. You might wonder why there are now only four children in node <code class="fm-code-in-text">S<sub class="subscript1">1</sub></code>: even if k-means was called with <code class="fm-code-in-text">k</code>, the number of initial clusters, equal to five (the number of <code class="fm-code-in-text">S<sub class="subscript1">1</sub></code>’s children), this clustering algorithm could output fewer than <code class="fm-code-in-text">k</code> clusters. If at any time during its second step, points assignment, one of the centroids doesn’t get any point assigned, that centroid just gets removed and the algorithm continues with one less cluster.</p>

  <p class="body"><a id="pgfId-1011404"></a>Both the check and the reorganization heuristic are resource-consuming; the latter in particular requires <code class="fm-code-in-text">O(jMk)</code> comparisons/assignments, if <code class="fm-code-in-text">j</code> is the number of maximum iterations we use in k-means. Therefore, in the original paper it was recommended to check the overlap situation after splitting nodes, but to apply the reorganization infrequently.</p>

  <p class="body"><a id="pgfId-1011417"></a>We can also easily run the check when an entry is moved across siblings, while it becomes less intuitive when we merge two nodes. In that case, we could always check all pairs of the merged node’s sibling, or—to limit the cost—just sample some pairs.</p>

  <p class="body"><a id="pgfId-1011434"></a>To limit the number of times we run the reorganization heuristic and avoid running it again on nodes that were recently re-clustered, we can introduce a threshold for the minimum number of points added/removed on the subtree rooted at each node, and only reorganize a node’s children when this threshold is passed. These methods prove to be effective in reducing the variance of the tree, producing more compact nodes.</p>

  <p class="body"><a id="pgfId-1035864"></a>But I’d like to conclude the discussion about these variants with a piece of advice: start implementing basic SS-trees (at this point, you should be ready to implement your own version), then profile them within your application (like we did for heaps in chapter 2), and only if SS-trees appear to be a bottleneck and improving their running time would reduce your application running time by at least 5-10%, try to implement one or more of the SS+-tree heuristics presented in <a id="marker-1011640"></a>this <a id="marker-1011644"></a><a id="marker-1011648"></a>section.</p>

  <h2 class="fm-head" id="heading_id_26"><a id="pgfId-1011476"></a>Summary</h2>

  <ul class="calibre19">
    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1011488"></a>To overcome the issues with k-d trees, alternative data structures such as R-trees and SS-trees have been introduced.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1011504"></a>The best data structure depends on the characteristics of the dataset and on how it needs to be used, the dimension of the dataset, the distribution (shape) of the data, whether the dataset is static or dynamic, and whether your application is search-intensive.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1011516"></a>Although R-trees and SS-trees don’t have any guarantee on the worst-case running time, in practice they perform better than k-d trees in many situations, and especially for higher-dimensional data.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1011530"></a>SS+-trees improve the structure of these trees by using heuristics that reduce the number of nodes overlapping.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1011545"></a>You can trade the quality of search results for performance by using approximated similarity searches.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1011557"></a>There are many domains where exact results for these searches are not important, because either we can accept a certain error margin, or we don’t have a strong similarity measure that can guarantee that the exact result will be the best choice.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1011571"></a>An example of such a domain is similarity search in an image <a class="calibre14" id="marker-1011652"></a><a class="calibre14" id="marker-1011656"></a><a class="calibre14" id="marker-1011660"></a>dataset.</p>
    </li>
  </ul>
  <hr class="calibre22"/>

  <p class="fm-footnote"><sup class="footnotenumber">1.</sup> <a id="pgfId-1011665"></a>Convolutional Neural Network, a type of deep neural network that is particularly well-suited to process images.</p>

  <p class="fm-footnote"><sup class="footnotenumber">2.</sup> <a id="pgfId-1011679"></a>A <span class="fm-hyperlink">B-tree</span> is a self-balancing tree optimized for efficiently storing large datasets on disk.</p>

  <p class="fm-footnote"><sup class="footnotenumber">3.</sup> <a id="pgfId-1011696"></a>A <i class="calibre17">memory page</i>, or just <i class="calibre17">page</i>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">4.</sup> <a id="pgfId-1011715"></a>An attentive reader will remember that we already discussed the branching factor in chapter 2 for d-ary heaps.</p>

  <p class="fm-footnote"><sup class="footnotenumber">5.</sup> <a id="pgfId-1011730"></a>See, for instance, <span class="fm-hyperlink"><a href="https://sqlity.net/en/2445/b-plus-tree/">https://sqlity.net/en/2445/b-plus-tree/</a></span>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">6.</sup> <a id="pgfId-1011746"></a>The centroid is defined as the center of mass of a set of points, whose coordinates are the weighted sum of the points’ coordinates. If N is a leaf, its center is the centroid of the points belonging to <code class="fm-code-in-text1">N</code>; if N is an internal node, then we consider the center of mass of the centroids of its children nodes.</p>

  <p class="fm-footnote"><sup class="footnotenumber">7.</sup> <a id="pgfId-1011766"></a>Since the height of the tree is at most log<sub class="subscript2">M</sub>(n), if m==M/2 (the choice with the largest height) log<sub class="subscript2">M</sub>/2(n)~= log<sub class="subscript2">M</sub>(n).</p>

  <p class="fm-footnote"><sup class="footnotenumber">8.</sup> <a id="pgfId-1011795"></a>As long as it satisfies the requirements for a valid metric: being always non-negative, being <code class="fm-code-in-text1">null</code> only between a point and itself, being symmetrical, and abiding by the triangular inequality.</p>

  <p class="fm-footnote"><sup class="footnotenumber">9.</sup> <a id="pgfId-1011810"></a>Remember: one of the golden rules of clean code is breaking up long, complex methods into smaller ones, so that each method is focused on one goal only.</p>

  <p class="fm-footnote"><sup class="footnotenumber">10.</sup> <a id="pgfId-1011827"></a>Assuming a point’s centroid is the point itself. We will also assume points have a radius equal to 0.</p>

  <p class="fm-footnote"><sup class="footnotenumber">11.</sup> <a id="pgfId-1011841"></a>Disclaimer: A function that returns a value <i class="calibre17">and</i> has a side effect is far from ideal and not the cleanest design. Using indirect sorting would be a better solution. Here, we used the simplest solution because of limited space, but be advised.</p>

  <p class="fm-footnote"><sup class="footnotenumber">12.</sup> <a id="pgfId-1011857"></a>For SS-trees we partition the ordered list of points by selecting the split index for the list, and then each point on the left of the index goes in one partition, and each point on the right in the other partition.</p>

  <p class="fm-footnote"><sup class="footnotenumber">13.</sup> <a id="pgfId-1011877"></a>We could reuse search<a id="marker-1011902"></a> as the first call in <code class="fm-code-in-text1">delete</code><a id="marker-1011906"></a> (and <code class="fm-code-in-text1">searchClosestLeaf</code><a id="marker-1011910"></a> in <code class="fm-code-in-text1">insert</code><a id="marker-1011914"></a>) if we store a pointer to its parent in each node, so that we can climb up the tree when needed.</p>

  <p class="fm-footnote"><sup class="footnotenumber">14.</sup> <a id="pgfId-1011919"></a>Assuming you implement this method as a private method. Assertions should never be used to check input, because they can be (and often are, in production) disabled. Checking arguments in private methods is not ideal and must be avoided when they are forwarded from user input. Ideally, you would only use assertions on invariants.</p>

  <p class="fm-footnote"><sup class="footnotenumber">15.</sup> <a id="pgfId-1011932"></a>See <span class="fm-hyperlink"><a href="https://github.com/mlarocca/AlgorithmsAndDataStructuresInAction#ss-tree">https://github.com/mlarocca/AlgorithmsAndDataStructuresInAction#ss-tree</a></span>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">16.</sup> <a id="pgfId-1011948"></a>This can easily be extended to <code class="fm-code-in-text1">n</code>-nearest neighbor by considering the distance of the <code class="fm-code-in-text1">n</code>-th NN instead.</p>

  <p class="fm-footnote"><sup class="footnotenumber">17.</sup> <a id="pgfId-1011970"></a>See Amato, Giuseppe. “Approximate similarity search in metric spaces.” Diss. Technical University of Dortmund, Germany, 2002.</p>

  <p class="fm-footnote"><sup class="footnotenumber">18.</sup> <a id="pgfId-1011985"></a>This section includes advanced material focused on theory.</p>

  <p class="fm-footnote"><sup class="footnotenumber">19.</sup> <a id="pgfId-1011999"></a>See White, David A., and Ramesh Jain. “Similarity indexing with the SS-tree.” Proceedings of the Twelfth International Conference on Data Engineering. IEEE, 1996.</p>

  <p class="fm-footnote"><sup class="footnotenumber">20.</sup> <a id="pgfId-1012014"></a>See Cleary, John Gerald. “Analysis of an algorithm for finding nearest neighbors in Euclidean space.” ACM Transactions on Mathematical Software (TOMS) 5.2 (1979): 183-192.</p>

  <p class="fm-footnote"><sup class="footnotenumber">21.</sup> <a id="pgfId-1012029"></a>See Fischer, Kaspar, Bernd Gärtner, and Martin Kutz. “Fast smallest-enclosing-ball computation in high dimensions.” European Symposium on Algorithms. Springer, Berlin, Heidelberg, 2003.</p>

  <p class="fm-footnote"><sup class="footnotenumber">22.</sup> <a id="pgfId-1012045"></a>“SS+ tree: an improved index structure for similarity searches in a high-dimensional feature space.” <i class="calibre17">Storage and Retrieval for Image and Video Databases</i> V. Vol. 3022. International Society for Optics and Photonics, 1997.</p>

  <p class="fm-footnote"><sup class="footnotenumber">23.</sup> <a id="pgfId-1012063"></a>To be thorough, we should also consider that computing each distance requires O(d) steps, so the running time, if d can vary, becomes O(djkn). While for SS-trees we have seen that it becomes important to keep in mind how algorithms perform when the dimension of the space grows, a linear dependency is definitely good news, and we can afford to omit it (as it is done by convention, considering the dimension fixed) without distorting our result.</p>
</body>
</html>
