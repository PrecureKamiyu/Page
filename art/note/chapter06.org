#+title: Memory and its Performance
* Beginning
  - Note taken on [2023-06-07 Wed 22:37] \\
    This chapter has huge amount of content, but arranged in a shitty way. Becareful.

我们来看一下 Tang 的目录. Gosh, 如果能够居中就好了.
1. 概述
2. Main Memory. 内存也称为 Main 存
3. Cache. Main 存和 CPU (Processor) 之间的
4. 虚拟 Memory. Main 存相关的.
5. 辅助存储器. External 存储器. 比如说硬盘.
* 概述
  - Note taken on [2023-06-07 Wed 22:24] \\
    And you don't have to remember the classification here. You can just learn it, with no pressure.
  - Note taken on [2023-06-07 Wed 22:22] \\
    This section is absolutely shit.

为什么研究存储器非常重要?
- CPU 的运行速度变得高的同时, 存储器的读取速度跟不上这个发展, 于是说计
  算机系统的运行速度很大程度上收到了存储器的制约.
- 另一方面, 当我们 IO 设备的数量不断增多, 若是 IO 设备和存储器之间的信
  息交换都是直接通过 CPU 来实现. 那么其将降低 CPU 的运转效率.

随后我们对存储器进行分类, 因为, 嘛, 反正我们有很多存储器. 随后, 这种分
类让我们看到的那些区别, 正是这些区别让我们有 Memory Heirarchy. 目前我
们有三种分类方法: 
1. 按照存储介质进行分类; 
2. 按照存取方式进行分类; 
3. 按照功能进行分类.
这些分类方法是让我们看见这些存储方式的特点. 总之是有好处的.
** 分类学
---------
按照存储介质进行分类:
1. SemiConductor 存储器. 这是一种 volatile 的存储器, 也就是 "断电会丢
   失" 的存储器.
2. 磁表面存储器. Magnetic Disk. 使用了磁介质来存储.
3. 磁芯存储器. ?什么价吧. 
4. 光盘存储器. 在读写过程之中应用了激光.

--------
然后我们还可以按照存储方式分类
1. RAM. 可读可写的存储器. 有 SRAM (Static RAM) 和 DRAM (Dynamic RAM).
2. ROM. 只可读的存储器. 有很多种, 比如说 EPROM, EEPROM, Flash
   Memory. 值得注意的是, Flash Memory 的功能并不是很一样, 虽然其也是
   ROM 的一种, 但是从结果上来看, 功能已经很不一样了.
3. 串行访问存储器. 简单来说, 就是和 Random accessing 相反的一个存在.
   RA 指的是, 能够通过地址来进行访问. 这种就是随机访问, 也就是说, 给定
   了一个地址, 我们能够直接访问到那个地址里面的数据. 串行访问存储器就
   是不能够做到这一点的存储器.

---------
然后我们还可以按照功能进行分类. 这里就不进行分类了, 因为分类是显然的.
这是由他们的物理特性决定的.

** Memory heirarchy

这里是一个类似于金字塔的东西. 记忆也是简单的. 就是说, 越接近 CPU, 存储
器就越快, 越贵; 越远离 CPU, 存储器就越慢, 越便宜, 于是相应的就越多. 我
们稍微看一下这个层级,简单来说, 我们有

#+begin_center
CPU <-> 缓存 <-> Main 存 <-> 辅存
#+end_center

Main 存就是我们常说的内存啦. 缓存的存在是为了加速 CPU 和 Main 存之间的
交互. 如果说我们知道了 hit rate 的概念, 当这个 hit rate 的数值接近于 1
的时候, 我们就可以说, CPU 能够以 "缓存的速度" 去访问 Main 存, 也就是说,
相当于缓存的大小被扩张为 Main 存的大小.

值得注意的是, Main 存以及 Cache 可以被称为 Internal Memory. 因为其是放
在板上的, 与之相对的是, 辅存被称为 External Memory. 这是说, 存储设备是
和 Chip 分开的. 于是我们能够知道访问 External Memory 的时候, 需要涉及
Bus. 应该.

---------

在 main 存和 CPU 之间还发展出来了虚拟存储. 简单来说, 这是一层抽象.  因
为我们的 Main 存的大小是实际上并没有那么大, 这层抽象让我们可以将一些并
不是 Main 存的地方 "看作" 是 Main 存. 剩下的细节交给系统和操作系统进行
处理.

这里面我们有两种概念: 1. 逻辑 (Logical) 地址; 2. 物理 (Physical) 地址.
前者就是一种逻辑上的地址, 已经被封装过了的地址, 而物理地址就是实际的,
Chip 上面的地址, 程序执行过程之中真正访问的地址.

** Main 存

#+begin_center
Word
#+end_center

字长便是 word 的长度. 如果说内部有 4 个 bytes, 这四个字节是可以独立寻
址的. 也就是说, 有一个特定的地址和其对应. 如果说地址长度是 n, word 的
长度是 2^m bytes 的话, "寻址范围" 为 2^{n-m} 以及 2^n.

我们能够有按 word寻址, 也能够按照 bytes 寻址

- 寻址范围
一般来说, 地址的长度就能够算出 "寻址范围", 虽然说是范围, 但实际上, 我
们可以直接认为是, "能够访问的单元个数的多少". 这里我们可以讨论两种寻址
方式的 "寻址范围". 是很简单的东西.

---------

#+begin_center
技术指标
#+end_center

简单来说, 有两个指标: 1. Capacity; 2. Speed. 

Capacity: 对于前者我们可以计算一个 Main 存之中的 bits 数量, 也可以计算
其中 bytes数量. 这很明显是废话. 一般来说, 我们以 bytes 为单位. 当我们
说出 1M 的存储容量的时候, 我们能够知道其地址长度为 20. 因为 2^20 约等
于 1M, 也即, 我们能够访问 1M 的数量的 bytes.


Speed: 指的是存储器获取到数据所用的时间. 这个指标可能还可以使用
Latency 来描述

带宽: 表示单位时间内, 存储器存取的信息量. 能够知道, 带宽和 bandwidth
和 Speed 之间有强关联.
* SemiConductors Chips
** 半导体存储芯片的基本结构

我们看芯片的接线就行了. *地址线*, 输入地址; *片选器*: 选择芯片内部的 RAM
芯片, 这是说, 一个存储器可以由很多个 RAM 芯片组成, 我们通过这个片选器
来选择具体是哪一个芯片.  *数据线*, 从芯片之中接出来的线, 大小为一个
word, 一般. *读写控制器*, 控制读写的使能.

** RAM
*** SRAM 和 DRAM

SRAM 使用了多个晶体管 (6个, 具体来说), 而 DRAM 使用了 1 个晶体管和一个
电容 (这是指一个存储单元内部用到的元件). 后者的造价便宜, 但是访问速度
比较慢, 并且需要刷新, 这个刷新是很有意思的概念, 这是说, DRAM 之中的数
据会 fade away, 于是说, 经常需要刷新. 其在读取之后, 内部的存储信息也会
失效, 于是需要将原本存储的信息再放回去.

我们先是介绍 DRAM 的结构: 我这里建议读者直接查看书本. 稍微了解到这个晶
体管是如何工作的. 这里说, 当我们读取数据的时候, 电容 (Capacitor) 放电,
于是我们得到了信息, 但是同时电容将电放出, 于是这里需要 recharge.

随后是介绍 SRAM 的结构: 我这里建议读者直接查看书本. 当我们知道了晶体管
是怎么工作的之后, 了解 SRAM 的结构就不是一件难事了. SRAM 里面是一个
Latch. 这是一个简单的 latch. 有两个端: A_1, A_2. 我们有一个 bit 的输入,
记为 B, 那么我们将 B 接入 A_1, bar B接入 A_2. 这就是一个 bit 的写操作.

OK, 去读, 去看图. 参见 Tang 第二版 76 页. 值得注意的是, Tang 写得一坨
答辩. 可以先去 81 页了解一下 DRAM 的工作原理, 因为 Tang 并没有介绍晶体
管是怎么工作的. 也可以参考 Stallings 的书.

*** SRAM 和 DRAM 的结构示意图

草泥马这写的是什么几把. 建议查看 [[https:en.wikipedia.org/wiki/Dynamic_random-access_memory][wikipedia]] for more information. 主要
是接线很多, 但是又不知道接线是用来干什么的. 首先我们要认知到, SRAM 和
DRAM 的基本存储单元是什么? 有多少个接线. 随后我们才能读懂书上的这些图.

DRAM 和 SRAM 的单元是类似的, 都有着: 
- 读选择线, 写选择线
- 读数据线, 写数据线
就是说, 一个单元格具有两个输入和两个输出. 这里有一点不同, 就是 DRAM 有
一个预充电信号. 
总之略, 最好还是看看书. 我这里就不进行抄写了.

*** 时序分析
略, 基本上是废话.

*** DRAM 的刷新方式

有三种刷新方式, 我们依次介绍其特点. 以一个 128 × 128 的 DRAM 为例子.
刷新 128 行需要 64 μs, 我们每 2ms 就需要刷新.

#+begin_center
集中刷新
#+end_center

2ms 之中抽出 64 μs 专门用来刷新. 

#+begin_center
分散刷新
#+end_center

每次进行一个读取操作的时候就进行一个行的刷新.

#+begin_center
异步刷新
#+end_center

每隔 \(\displaystyle \frac{64\, \mathrm{\mu s}}{128}\) 就刷新一次. 因
为刷新操作和读写操作并不是同步的, 于是称为异步的. 基本上都是废话.

** ROM

*** ROM 的简单分类学

- *基本ROM* (read-only memory) 介绍过的. 在一个节点上面放着一个电容, 导通的时候接入低电压
  (因为电容接地了); 如果没有电容, 读出的时候就是高电压.

- *PROM* (Programmable ROM) 其内部有一个熔丝, 通过是否熔断这个熔丝来达成
  program. 这种 program 是一次性的. 但是比基本ROM要方便.

- *EPROM* (Erasable) 可擦除的, optically erasable. 结构不介绍了. 几把. 

- *EEPROM* (electrically erasable) 可电擦除的, 不知道用来干嘛. 

- *Flash Memory* 用于手机等, 功能已经和 RAM 差不多了.

*** EPROM 的结构介绍

简单来说, 就是使用了一个特殊的晶体管, 这个晶体管叫什么, 雪崩注入式的晶
体管. 总之是一个很奇怪的名字. 这个晶体管之中有一个名为浮动栅的结构. 当
晶体管上面的一个 D 口接入了电源之后, 这个东西就能开始运作了, 其能够阻
断晶体管内部的电流的流通. 那么当这个电压接入的时候, 其存的就是 0. 没有
接入的话, 存的就是 1.

其实是很简单的东西. 我们稍微看一下就知道是什么了.

** main 存和 CPU 的链接. (shabi东西) 重点

这里可以出题, 傻逼, 就喜欢能出题的东西. 什么题? CPU 和 RAM 或者 ROM 之
间的 chip 链接. 大概是什么样:

给定了 chip 和一个 74138 译码器, 要你将 CPU 和 chip 之间连接起来. 这里
需要知道, 片选信号一般连入高位, 地址一般连入低位.

根据高位的这些信号决定片选信号的产生. 片选信号产生了之后, 会链接到 ROM
RAM 芯片的 CS 段上.

解题步骤为:
1. 根据地址范围写出相应的二进制地址. 以方便决定如何使用 74138 译码器.
2. 根据地址范围的大小, 决定使用的 chip
3. 分配 CPU 地址线. 一般来说这是简单的.
4. 决定片选信号. 查看第一步的二进制地址. 且, CPU 的 MREQ 信号一般要接
   入译码器的使能端.

还需要查看 Tang 99页的例题. 令人无语的题. 大概就考这种程度的东西. 真是
丢人. 令人叹息, 说到底就是喜欢这种垃圾.

** 存储器的校验 Parity

建议阅读 Stallings 一节.

*** 校验的电路结构

参考 Stallings 一节. 我们说我们有一串数据需要传输. 在传输之前, 我们通
过函数 f, 生成一个 K bits 的校验码. 传输了之后我们再次进行校验码的生成.

对比两次得到的校验码, 我们知道, 数据是否有损坏. 两次校验码取 XOR 得到
数据, 通过这点来得到信息. 这个 XOR 得到的结果称为 Syndrome Word

我们假设 N 是 数据的长度. 因为 K bits 的校验码, 其能够做到 2^K 的定位.
那么我们实际上有不等式:

\[
2 ^ K  - 1 \ge  N  + K 
\]

实际上我们还能够确认 K bits 的校验码在传输的过程之中是否有发生错误. 所
以说不等号后面加上了一个 K. 还有, 如果说 Syndrome Word 是0的话, 其就说
明这里并没有错误. 于是说不等号前面有一个 -1, 因为其中有一个值拿去放到
别的地方了.

*** Hamming Code

Hamming Code 是常见的 single error correction code. 其能够检测出一位数
据的错误. n其工作原理就是将某些位取 XOR 得到的结果. 直观理解请看
Stallings 的书.

在这里我们进一步采用一个模式, 这个模式能够让我们比较简单的生成 Hamming
Code. 我们将 Hamming Code 和 数据 bits 放到一排. 对于 2 的次幂的位置,
其上面放的是 Hamming Code 的位. 我们设 C1 C2 C4 为 Code 的位, 设 Dn 是第n个数据位. 然后我们有


#+begin_example
C1C2D1C4D2D3D4
#+end_example

上面是一个 4 位数据的校验码, 校验码是三位. 我们按照下面方式得出 Cn. 考虑位置码, 也就是位置的二进制码, 比如说第6位就是 110.
我们说 C1 的值为, 位置码个位数是 1 的数据位的 XOR. 类似的 C2 的值为, 位置码 第二位数 为 1 的数据位的 XOR.

| 位置   | 001 | 010 | 011 | 100 | 101 | 110 | 111 |
| 位置   |   1 |   2 |   3 |   4 |   5 |   6 |   7 |
| 数据   |  C1 |  C2 |  D1 |  C4 |  D2 |  D3 |  D4 |

#+begin_example
C1 = D1 ^ D2 ^ D4
#+end_example

答案是明显的

** TODO 提高访问速度的方式

*** 总结

总共分为两个部分: 1. 单字多体和多体并行; 2. 高性能存储芯片. 第二个部分主要抄袭 Stallings 的对应部分. 有兴趣的读者可以选择查看后者.

对于第一个部分: 什么是"体"? 体就是一个模块. 模块就是体. 在这里, 体是半个 RAM 或者别的东西. 其能够独立的工作, 结构上也相对独立, 也就是说, 其有独立的控制单元什么的, 我们用其来实现加速, 比如说利用流水线的思想.

对于第二个部分: 可以参考 Stallings. 其告诉了三种结构: 1. Synchronized DRAM; 2. Rambus DRAM; 3. Cache DRAM. 能够看出为什么这里是抄袭. 因为第二个部分针对的是 DRAM, 这样考虑的话, 这个部分应当放到前面来讲述, 但是 Tang 并没有这么做, 使得编排的逻辑并不是很规整. 并且, Cache 还没有介绍.

对于 SDRAM, 其思想很简单. 为 RAM 增加一个时钟. 我们利用起这个时钟. 一般来说, 当我们传输数据的时候, 需要和 CPU 同步, 并且应当指定地址. 但是 SDRAM 使用了 Burst Mode. 我输入了一个地址, 指定了传输数据的大小 (有多少个 word), 随后 SDRAM 就能够一直传输, 直到传输的数据达到了所需的大小. 这便是 burst mode. 有兴趣的可以查看 wikipedia. 

随后是 Rambus DRAM. 我不知道是什么

随后是 Cache DRAM. 关于这点, 可以查看 Patterson 相关部分. 其说明得更多. 简单来说, 就是运用了 Cache 的思想, 使用 SRAM 作为一个 buffer.

*** 单体多字

整一个 bandwidth 为多个字节的存储器, 设为 \(n\). 根据地址, 一次取出
\(n\) 个字节, 送入选择器之类的东西. 使得每隔 \(\frac{T}{n}\) 就能送入一个字节的数据.

*** 多体并行

一个正常的地址可以看为两个部分: 1. 体编号; 2. 体内地址. 前者告诉我们应当在哪一个体内寻找数据, 后者告诉我们在体内的哪里寻找数据.

这种将地址分为两个部分处理的东西称为交叉编址. 常用的有两种编址: 我们可以将低位地址看为体编号, 或者是相反, 将高位地址看作是体编号. 前者称为低位交叉编址, 后者称为高位交叉编址. 

多体并行运用了类似于流水线的思想. 当我们要 *交叉地访问不同体* 的时候, 速度是最大的. 如果说 *连续的数据* (地址连续) 都在 *一个体内*, 我们访问连续的数据的时候速度就没有变化. 高位编址就是这种情况. 连续的地址, 其高位不容易改变, 那么, 它们倾向于放在同一个体内. 于是高位编址对于 *访问连续的数据* 来说, 并没有加速作用. 相反地, 低位编址就能够加速.

注: 似乎 高位编址 又称为 顺序存储; 低位编址 又称为 交叉存储. 建议 Tang 同学下次抄教材的时候将别人的东西抄完整来.

*** 存控

看就完了, 这实际上是排队器. 这点 Patterson 有提及. 也就是将访问的请求进行一个排队. 理解是简单的.

*** Synchronized DRAM
*** Rambus DRAM
*** Cache DRAM

* Cache
** An introduction

This section tells the principle of cache and then tells some key concepts like *hit*, *miss* and so on.

Anyway. A cache is memory that lies between processor and main memory. It is used to speed up the accessing time. Some blocks of the data are loaded into cache. (Mind the word "block") And if processor want to access to the data, it will check cache first. And if the data is indeed in the cache, then the processor can just get the data via accessing to cache, none of main memory's business. 

So it will speed up the accessing time, since the cache is faster (and is more expensive than main memory).

If the data is on the cache, then it is called a *hit*; if not, it is called a *miss*. If a miss occurs, we will have to access to the main memory, and send the data to cache and to processor. The extra time that it takes is called *miss penalty*.

** TODO The Elements of Cache

Anyway, we are going to talk about the structure of the cache here. And moreover we are going to 
talk something about the attributes of a cache.

*** Hit and Miss

*** The Structure of the Cache

This part is rather simple, for we have already been familiar with the structure of cache. 

The data transferred between cache and main memory is by *blocks*. A block consists of words.

It use the principle of locality, to improve the performance. So we know that a block has usually more than one word.
The main memory is divided into blocks. Cache loads a block of data from memory from time to time. The *mapping* between the blocks in main memory and the block number in cache (that is called *line* number, which is the position that in cache).

A line in a cache is the basic unit of a cache. It consists of a block and some extra information field including tag field and valid-tag field. The name of line is used, to note the difference of the blocks that in main memory and that in cache, and to note that there is some other information in the cache.

*** Block Size and its Effect to Hit Rate

According to the principle of locality, the bigger block size can improve the hit rate, subsequently improving the performance. 

But if we consider the latency (that is the miss penalty), things get interesting, because if the improvement brought by the increase of hit rate is less greater than the degeneration brought by the increase of miss penalty, then the performance is actually being worse. For more information, you can check /Patterson/ for more information.

** TODO Mapping Strategy
   - State "DONE"       from "TODO"       [2023-06-07 Wed 15:38]

The mapping is from the blocks in the memory to the line in the cache, that is to say when given the position of a block, how do we find the corresponding position in the cache? There are some strategies of mapping.

The simplest one is called *Direct Mapping*. It is simple. There are also other ways called *associative mapping* and *set-associative mapping". 

*** Direct Mapping

*** Associative Mapping

*** Set-Associative Mapping

** TODO Write Policy

*** Why We have to maintain the consistency of the memory heirarchy

In computer science, a consistency model specifies a contract between ther programmer and a system. The system guarantees that if the programmer follows the rules for operations on memory, memory will be consistent and the results of reading, writing, or updating memory will be predictable. This is important because it allows for reliable and predicable behavior of programs that rely on shared memory.

*** Two strategies
There are ways to maintain the consistency. In short, there are two ways: write-through and write-back.

Let us look at write-through, to check how it maintain the
consistency. Write-through is to say, when you want to change some
data, if it is on cache, you need to change the content of cache and
that of the main memory.

**** Write Through

**** Write Back


** TODO Ways to Improve the Performance of Cache

One can use multiple level of cache.

** TODO Replacement Algorithm

When the cache is full, and we want to fill some blocks into the cache, we need the replacement algorithm to determine which blcok is about to be replaced. 

* TODO Virtual Memory
* TODO External Memory
** RAID from Stallings

** Some Other External Memory (not important)

包括但是不仅限于硬盘软盘光碟等等. 需要注意的是, 虽然说 Tang 的书上讲得挺tm头头是道的, 这些内容实际上是没什么讲的必要的. 都是垃圾, 简单来说.

在这里我们就不多介绍了, 实际上有兴趣的话, 可以自行查看wikipedia等等.

相对更重要的是 Stallings 的书上介绍的 RAID.