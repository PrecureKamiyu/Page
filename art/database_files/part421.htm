<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>23.4  Replication</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part420.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part422.htm">下一个 &gt;</a></p><p class="s65" style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: left;"><a name="bookmark488">23.4  </a><span style=" color: #00AEEF;">Replication</span><a name="bookmark533">&zwnj;</a></p><p style="padding-top: 11pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">One of the goals in using distributed databases is <span class="s63">high availability</span>; that is, the database must function almost all the time. In particular, since failures are more likely in large distributed systems, a distributed database must continue functioning even when there are various types of failures. The ability to continue functioning even during failures is referred to as <span class="s63">robustness</span>.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">For a distributed system to be robust, data must be replicated, allowing the data to be accessible even if a node containing a replica of the data fails.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The database system must keep track of the locations of the replicas of each data item in the database catalog. Replication can be at the level of individual data items, in which the catalog will have one entry for each data item, recording the nodes where it is replicated. Alternatively, replication can be done at the level of partitions of a relation, with an entire partition replicated at two or more nodes. The catalog would then have one entry for each partition, resulting in considerably lower overhead than having one entry for each data item.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">In this section we ﬁrst discuss (in Section 23.4.1) issues with consistency of values between replicas. We then discuss (in Section 23.4.2) how to extend concurrency con- trol techniques to deal with replicas, ignoring the issue of failures. Further extensions of the techniques to handle failures but modifying how reads and writes are executed are described in Section 23.4.3.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 7pt;padding-left: 119pt;text-indent: 0pt;text-align: left;">23.4.1 Consistency of Replicas</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Given that a data item (or partition) is replicated, the system should ideally ensure that the copies have the same value. Practically, given that some nodes may be disconnected or may have failed, it is impossible to ensure that all copies have the same value. Instead, the system must ensure that even if some replicas do not have the latest value, reads of a data item get to see the latest value that was written.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">More formally, the implementations of read and write operations on the replicas of a data item must follow a protocol that ensures the following property, called <span class="s63">lin- earizability</span>: Given a set of read and write operations on a data item,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-top: 8pt;padding-left: 145pt;text-indent: -16pt;text-align: justify;">1. <span class="p">there must be a linear ordering of the operations such that each read in the or- dering should see the value written by the most recent write preceding the read (or the initial value if there is no such write), and</span></p><p class="s13" style="padding-top: 6pt;padding-left: 145pt;text-indent: -17pt;text-align: justify;"><span class="s63">2. </span><span class="p">if an operation </span>o<span class="s98">1</span><span class="p"> ﬁnishes before an operation </span>o<span class="s98">2</span><span class="p"> begins (based on external time), then </span>o<span class="s93">1 </span><span class="s94">must precede </span>o<span class="s93">2 </span><span class="s94">in the linear order.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">Note that linearizability only addresses what happens to a single data item, and it is orthogonal to serializability.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 17pt;text-align: justify;">We ﬁrst consider approaches that write all copies of a data item and discuss limi- tations of this approach; in particular, to ensure availability during failure, failed nodes need to be removed from the set of replicas, which can be quite tricky as we will see.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">It is not possible, in general, to diﬀerentiate between node failure and network partition. The system can usually detect that a failure has occurred, but it may not be able to identify the type of failure. For example, suppose that node <i>N</i><span class="s98">1</span> is not able to communicate with <i>N</i><span class="s98">2</span>. It could be that <i>N</i><span class="s98">2</span> has failed. However, another possibility is that the link between <i>N</i><span class="s98">1</span> and <i>N</i><span class="s98">2</span> has failed, resulting in network partition. The problem is partly addressed by using multiple links between nodes, so that even if one link fails the nodes will remain connected. However, multiple link failures can still occur, so there are situations where we cannot be sure whether a node failure or network partition has occurred.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">There are protocols for data access that can continue working even if some nodes have failed, without any explicit actions to deal with the failures, as we shall see in Section 23.4.3.1. These protocols are based on ensuring a majority of nodes are writ- ten/read. With such protocols, actions to detect failed nodes and remove them from the system can be done in the background, and (re)integration of new or recovered nodes into the system can also be done without disrupting processing.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Although traditional database systems place a premium on consistency, there are many applications today that value availability more than consistency. The design of replication protocols is diﬀerent for such systems and is discussed in Section 23.6.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">In particular, one such alternative that is widely used for maintaining replicated data is to perform the update on a primary copy of the data item, and allow the trans- action to commit without updating the other copies. However, the update is subse- quently propagated to the other copies. Such propagation of updates, referred to as <i>asynchronous replication </i>or <i>lazy propagation of updates</i>, is discussed in Section 23.6.2.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">One drawback of asynchronous replication is that replicas may be out of date for some time following each update. Another drawback is that if the primary copy fails after a transaction commits, but before the updates were propagated to the replicas, the updates of the committed transaction may not be visible to subsequent transactions, leading to an inconsistency.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">On the other hand, a major beneﬁt of asynchronous replication is that exclusive locks can be released as soon as the transaction commits on the primary copy. In contrast, if other replicas have to be updated before the transaction commits, there may be a signiﬁcant delay in committing the transaction. In particular, if data is geo- graphically replicated to ensure availability despite failure of an entire data center, the network round-trip time to a remote data center could range from tens of milliseconds to nearby locations, up to hundreds of milliseconds for data centers that are on the other side of the world. If a transaction were to hold a lock on a data item for this duration, the number of transactions that can update that data item would be limited to approximately 10 to 100 transactions per second. For certain applications, for exam- ple, user data in a web application, 10 to 100 transactions per second for a single data item is quite suﬃcient. However, for applications where some data items are updated</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">by a large number of transactions each second, holding locks for such a long time is not acceptable. Asynchronous replication may be preferred in such cases.</p><p class="s68" style="padding-top: 9pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">23.4.2 Concurrency Control with Replicas</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">We discuss several alternative ways of dealing with locking in the presence of replica- tion of data items, in Section 23.4.2.1 to Section 23.4.2.4.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">In this section, we assume updates are done on all replicas of a data item. If any node containing a replica of a data item has failed, or is disconnected from the other nodes, that replica cannot be updated. We discuss how to perform reads and updates in the presence of failures later, in Section 23.4.3.</p><p class="s183" style="padding-top: 10pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">23.4.2.1 Primary Copy</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">When a system uses data replication, we can choose one of the replicas of a data item as the <span class="s63">primary copy</span>. For each data item <i>Q</i>, the primary copy of <i>Q </i>must reside in precisely one node, which we call the <span class="s63">primary node </span>of <i>Q</i>.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">When a transaction needs to lock a data item <i>Q</i>, it requests a lock at the primary node of <i>Q</i>. As before, the response to the request is delayed until it can be granted. The primary copy enables concurrency control for replicated data to be handled like that for unreplicated data. This similarity allows for a simple implementation. However, if the primary node of <i>Q </i>fails, lock information for <i>Q </i>would be lost, and <i>Q </i>would be inaccessible, even though other nodes containing a replica may be accessible.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">23.4.2.2 Majority Protocol</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">The <span class="s63">majority protocol </span>works this way: If data item <i>Q </i>is replicated in <i>n </i>diﬀerent nodes, then a lock-request message must be sent to more than one-half of the <i>n </i>nodes in which <i>Q </i>is stored. Each lock manager determines whether the lock can be granted immedi- ately (as far as it is concerned). As before, the response is delayed until the request can be granted. The transaction does not operate on <i>Q </i>until it has successfully obtained a lock on a majority of the replicas of <i>Q</i>.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">We assume for now that writes are performed on all replicas, requiring all nodes containing replicas to be available. However, the major beneﬁt of the majority protocol is that it can be extended to deal with node failures, as we shall see in Section 23.4.3.1. The protocol also deals with replicated data in a decentralized manner, thus avoiding the drawbacks of central control. However, it suﬀers from these disadvantages:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 139pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><b>Implementation</b>. The majority protocol is more complicated to implement than are the previous schemes. It requires at least 2(<i>n</i><span class="s15">∕</span>2 <span class="s15">+ </span>1) messages for handling lock requests and at least (<i>n</i><span class="s15">∕</span>2 <span class="s15">+ </span>1) messages for handling unlock requests.</p><p style="padding-top: 3pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><b>Deadlock handling</b>. In addition to the problem of global deadlocks due to the use of a distributed-lock-manager approach, it is possible for a deadlock to occur even if only one data item is being locked. As an illustration, consider a system with four nodes and full replication. Suppose that transactions <i>T</i><span class="s98">1</span> and <i>T</i><span class="s98">2</span> wish to lock</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-top: 5pt;padding-left: 107pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><span class="p">data item </span>Q <span class="p">in exclusive mode. Transaction </span>T<span class="s98">1</span><span class="p"> may succeed in locking </span>Q <span class="p">at nodes </span>N<span class="s130">1 </span><span class="s94">and </span>N<span class="s130">3</span><span class="s94">, while transaction </span>T<span class="s130">2 </span><span class="s94">may succeed in locking </span>Q <span class="p">at nodes </span>N<span class="s130">2 </span><span class="s94">and </span>N<span class="s130">4</span><span class="s94">. Each then must wait to acquire the third lock; hence, a deadlock has occurred.</span></p><p style="padding-left: 107pt;text-indent: 0pt;text-align: justify;">Luckily, we can avoid such deadlocks with relative ease by requiring all nodes to request locks on the replicas of a data item in the same predetermined order.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">23.4.2.3 Biased Protocol</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">The <span class="s63">biased protocol </span>is another approach to handling replication. The diﬀerence from the majority protocol is that requests for shared locks are given more favorable treat- ment than requests for exclusive locks.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 107pt;text-indent: -16pt;text-align: left;"><span class="s39">• </span><b>Shared locks</b>. When a transaction needs to lock data item <i>Q</i>, it simply requests a lock on <i>Q </i>from the lock manager at one node that contains a replica of <i>Q</i>.</p><p style="padding-top: 4pt;padding-left: 107pt;text-indent: -16pt;text-align: left;"><span class="s39">• </span><b>Exclusive locks</b>. When a transaction needs to lock data item <i>Q</i>, it requests a lock on <i>Q </i>from the lock manager at all nodes that contain a replica of <i>Q</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">As before, the response to the request is delayed until it can be granted.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The biased scheme has the advantage of imposing less overhead on <span class="s49">read </span>operations than does the majority protocol. This savings is especially signiﬁcant in common cases in which the frequency of <span class="s49">read </span>is much greater than the frequency of <span class="s49">write</span>. However, the additional overhead on writes is a disadvantage. Furthermore, the biased protocol shares the majority protocol’s disadvantage of complexity in handling deadlock.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">23.4.2.4 Quorum Consensus Protocol</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">The <span class="s63">quorum consensus </span>protocol is a generalization of the majority protocol. The quo- rum consensus protocol assigns each node a nonnegative weight. It assigns read and write operations on an item <i>x </i>two integers, called <span class="s63">read quorum </span><i>Q</i><span class="s145">r </span>and <span class="s63">write quorum </span><i>Q</i><span class="s97">w</span>, that must satisfy the following condition, where <i>S </i>is the total weight of all nodes at which <i>x </i>resides:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-top: 3pt;padding-left: 63pt;text-indent: 0pt;text-align: center;">Q<span class="s97">r </span><span class="s15">+ </span>Q<span class="s97">w </span><span class="s83">&gt; </span>S <b>and </b><span class="p">2 </span><span class="s15">∗ </span>Q<span class="s97">w </span><span class="s83">&gt; </span>S</p><p style="padding-top: 7pt;padding-left: 88pt;text-indent: 17pt;line-height: 94%;text-align: justify;">To execute a read operation, enough replicas must be locked that their total weight is at least <i>Q</i><span class="s97">r </span>. To execute a write operation, enough replicas must be locked so that their total weight is at least <i>Q</i><span class="s97">w</span>.</p><p style="padding-left: 106pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">A beneﬁt of the quorum consensus approach is that it can permit the cost of either</p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">read or write locking to be selectively reduced by appropriately deﬁning the read and write quorums. For instance, with a small read quorum, reads need to obtain fewer locks, but the write quorum will be higher, hence writes need to obtain more locks. Also, if higher weights are given to some nodes (e.g., those less likely to fail), fewer</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">nodes need to be accessed for acquiring locks. In fact, by setting weights and quorums appropriately, the quorum consensus protocol can simulate the majority protocol and the biased protocols.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Like the majority protocol, quorum consensus can be extended to work even in the presence of node failures, as we shall see in Section 23.4.3.1.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">23.4.3 Dealing with Failures</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Consider the following protocol to deal with replicated data. Writes must be success- fully performed at all replicas of a data item. Reads may read from any replica. When coupled with two-phase locking, such a protocol will ensure that reads will see the value written by the most recent write to the same data item. This protocol is also called the <span class="s63">read one, write all copies </span>protocol since all replicas must be written, and any replica can be read.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The problem with this protocol lies in what to do if some node is unavailable. To allow work to proceed in the event of failures, it may appear that we can use a “read one, write all available” protocol. In this approach, a read operation proceeds as in the <span class="s63">read one, write all </span>scheme; any available replica can be read, and a read lock is obtained at that replica. A write operation is shipped to all replicas, and write locks are acquired on all the replicas. If a node is down, the transaction manager proceeds without waiting for the node to recover. While this approach appears very attractive, it does not guarantee consistency of writes and reads. For example, a temporary communication failure may cause a node to appear to be unavailable, resulting in a write not being performed, but when the link is restored, the node is not aware that it has to perform some reintegration actions to catch up on writes it has lost. Further, if the network partitions, each partition may proceed to update the same data item, believing that nodes in the other partitions are all dead.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">23.4.3.1 Robustness Using the Majority-Based Protocol</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">The majority-based approach to distributed concurrency control in Section 23.4.2.2 can be modiﬁed to work in spite of failures. In this approach, each data object stores with it a version number to detect when it was last written. Whenever a transaction writes an object it also updates the version number in this way:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 139pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><span class="s40">If data object </span>a <span class="p">is replicated in </span>n <span class="p">diﬀerent nodes, then a lock-request message must be sent to more than one-half of the </span>n <span class="p">nodes at which </span>a <span class="p">is stored. The transaction does not operate on </span>a <span class="p">until it has successfully obtained a lock on a majority of the replicas of </span>a<span class="p">.</span></p><p style="padding-left: 139pt;text-indent: 15pt;text-align: justify;">Updates to the replicas can be committed atomically using <span class="s44">2PC</span>. (We assume for now that all replicas that were accessible stay accessible until commit, but we</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 107pt;text-indent: 0pt;text-align: left;">relax this requirement later in this section, where we also discuss alternatives to</p><p class="s42" style="padding-left: 107pt;text-indent: 0pt;text-align: left;">2PC<span class="s43">.)</span></p><p class="s39" style="padding-top: 4pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Read operations look at all replicas on which a lock has been obtained and read the value from the replica that has the highest version number. (Optionally, they may also write this value back to replicas with lower version numbers.) Writes read all the replicas just like reads to ﬁnd the highest version number (this step would normally have been performed earlier in the transaction by a read, and the result can be reused). The new version number is one more than the highest version number. The write operation writes all the replicas on which it has obtained locks and sets the version number at all the replicas to the new version number.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">Failures (whether network partitions or node failures) can be tolerated as long as (1) the nodes available at commit contain a majority of replicas of all the objects written to and (2) during reads, a majority of replicas are read to ﬁnd the version numbers. If these requirements are violated, the transaction must be aborted. As long as the requirements are satisﬁed, the two-phase commit protocol can be used, as usual, on the nodes that are available.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">In this scheme, reintegration is trivial; nothing needs to be done. This is because writes would have updated a majority of the replicas, while reads will read a majority of the replicas and ﬁnd at least one replica that has the latest version.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">However, the majority protocol using version numbers has some limitations, which can be avoided by using extensions or by using alternative protocols.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 113pt;text-indent: -16pt;text-align: justify;">1. <span class="p">The ﬁrst problem is how to deal with the failure of participants during an execu- tion of the two-phase commit protocol.</span></p><p style="padding-left: 113pt;text-indent: 15pt;text-align: justify;">This problem can be dealt with by an extension of the two-phase commit protocol that allows commit to happen even if some replicas are unavailable, as long as a majority of replicas of a partition conﬁrm that they are in prepared state. When participants recover or get reconnected, or otherwise discover that they do not have the latest updates, they need to query other nodes to catch up on missing updates. References that provide details of such solutions may be found in the bibliographic notes for this chapter, available online.</p><p class="s63" style="padding-top: 6pt;padding-left: 113pt;text-indent: -17pt;text-align: justify;">2. <span class="p">The second problem is how to deal with the failure of the coordinator during an execution of two-phase commit protocol, which could lead to the blocking problem. Consensus protocols, which we study in Section 23.8, provide a robust way of implementing two-phase commit without the risk of blocking even if the coordinator fails, as long as a majority of the nodes are up and connected, as we will see in Section 23.8.5.</span></p><p class="s63" style="padding-top: 6pt;padding-left: 113pt;text-indent: -17pt;text-align: justify;">3. <span class="p">The third problem is that reads pay a higher price, having to contact a majority of the copies. We study approaches to reducing the read overhead in Section 23.4.3.2.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">23.4.3.2 Reducing Read Cost</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">One approach to dealing with this problem is to use the idea of read and write quorums from the quorum consensus protocol; reads can read from a smaller read quorum, while writes have to successfully write to a larger write quorum. There is no change to the version numbering technique described earlier. The drawback of this approach is that a higher write quorum increases the chance of blocking of update transactions, due to failure or disconnection of nodes. As a special case of quorum consensus, we give unit weights to all nodes, set the read quorum to 1, and set the write quorum to <i>n </i>(all nodes). This corresponds to the read-any-write-all protocol we saw earlier. There is no need to use version numbers with this protocol. However, if even a single node containing a data item fails, no write to the item can proceed, since the write quorum will not be available.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">A second approach is to use the primary copy technique for concurrency control and force all updates to go through the primary copy. Reads can be satisﬁed by ac- cessing only one node, in contrast to the majority or quorum protocols. However, an issue with this approach is how to handle failures. If the primary copy node fails, and another node is assigned to act as the primary copy, it must ensure that it has the latest version of all data items. Subsequently, reads can be done at the primary copy, without having to read data from other copies.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">This approach requires that there be at most one node that can act as primary copy at a time, even in the event of network partitions. This can be ensured using leases as we saw earlier in Section 23.7. Furthermore, this approach requires an eﬃcient way for the new coordinator to ensure that it has the latest version of all data items. This can be done by having a log at each node and ensuring the logs are consistent with each other. This problem is by itself a nontrivial process, but it can be solved using distributed consensus protocols which we study in Section 23.8. Distributed consensus internally uses a majority scheme to ensure consistency of the logs. But it turns out that if distributed consensus is used to keep logs synchronized, there is no need for version numbering.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">In fact, consensus protocols provide a way of implementing fault-tolerant replica- tion of data, as we see later in Section 23.8.4. Many fault-tolerant storage system imple- mentations today are built using fault-tolerant replication of data based on consensus protocols.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">There is a variant of the primary copy scheme, called the <span class="s63">chain replication </span>protocol, where the replicas are ordered. Each update is sent to the ﬁrst replica, which records it locally and forwards it to the next replica, and so on. The update is completed when the last (tail) replica receives the update. Reads must be executed at the tail replica, to ensure that only updates that have been fully replicated are read. If a node in a replica chain fails, reconﬁguration is required to update the chain; further, the system must ensure that any incomplete updates are completed before processing further updates. Optimized versions of the chain replication scheme are used in several storage systems.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">References providing more details of the chain replication protocol may be found in the Further Reading section at the end of the chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 7pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">23.4.4 Reconfiguration and Reintegration</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">While nodes do fail, in most cases nodes recover soon, and the protocols described earlier can ensure that they will catch up with any updates that they missed.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">However, in some cases a node may fail permanently. The system must then be <span class="s63">reconfigured </span>to remove failed nodes, and to allow other nodes to take over the tasks assigned to the failed node. Further, the database catalog must be updated to remove the failed node from the list of replicas of all data items (or relation partitions) that were replicated at that node.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">As discussed earlier, a network failure may result in a node appearing to have failed, even if it has not actually failed. It is safe to remove such a node from the list of replicas; reads will no longer be routed to the node even though it may be accessible, but that will not cause any consistency problems.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">If a failed node that was removed from the system eventually recovers, it must be <span class="s63">reintegrated </span>into the system. When a failed node recovers, if it had replicas of any partition or data item, it must obtain the current values of these data items it stores. The database recovery log at a live site can be used to ﬁnd and perform all updates that happened when the node was down,</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Reintegration of a node is more complicated than it may seem to be at ﬁrst glance, since there may be updates to the data items processed during the time that the node is recovering. The database recovery log at a live site is used for catching up with the latest values for all data items at the node. Once it has caught up with the current value of all data items, the node should be added back into the list of replicas for the relevant partitions/data items, so it will receive all future updates. Locks are obtained on the partitions/data items, updates up to that point are applied from the log, and the node is added to the list of replicas for the partitions or data items, before releasing the locks. Subsequent updates will be applied directly to the node, since it will be in the list of replicas.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Reintegration is much easier with the majority-based protocols in Section 23.4.3.1, since the protocol can tolerate nodes with out-of-date data. In this case, a node can be reintegrated even before catching up on updates, and the node can catch up with missed updates subsequently.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Reconﬁguration depends on nodes having an up-to-date version of the catalog that records what table partitions (or data items) are replicated at what nodes; thus infor- mation must be consistent across all nodes in a system. The replication information in the catalog could be stored centrally, and consulted on each access, but such a de- sign would not be scalable since the central node would be consulted very frequently and would get overloaded. To avoid such a bottleneck, the catalog itself needs to be partitioned, and it may be replicated, for example, using the majority protocol.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part420.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part422.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
