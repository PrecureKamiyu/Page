<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>22.4  Other Operations</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part402.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part404.htm">下一个 &gt;</a></p><p class="s65" style="padding-top: 7pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">22.4  <span style=" color: #00AEEF;">Other Operations</span></p><p style="padding-top: 12pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">In this section, we discuss parallel processing of other relational operations, as well as parallel processing in the MapReduce framework.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="95" height="1" alt="image" src="Image_3193.png"/></span></p><p class="s111" style="padding-top: 3pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;"><span class="s77">1</span><span class="s78">Cost estimation should be done using histograms on join attributes. A heuristic approximation is to estimate the join cost at each node </span>N<span class="s363">i </span><span class="s80">as the sum of the sizes of </span>r<span class="s363">i </span><span class="s80">and </span>s<span class="s363">i </span><span class="s80">, and choose range partitioning vectors to balance the sum of the sizes.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: left;">22.4.1 Other Relational Operations</p><p style="padding-top: 6pt;padding-left: 52pt;text-indent: 0pt;text-align: center;">The evaluation of other relational operations also can be parallelized:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 139pt;text-indent: -16pt;line-height: 87%;text-align: justify;"><span class="s39">• </span><b>Selection</b>. Let the selection be <span class="s15">σ</span><span class="s137">θ</span>(<i>r</i>). Consider ﬁrst the case where <span class="s15">θ </span>is of the form <i>a</i><span class="s145">i </span><span class="s15">= </span><i>v</i>, where <i>a</i><span class="s145">i </span>is an attribute and <i>v </i>is a value. If the relation <i>r </i>is partitioned on <i>a</i><span class="s145">i</span>, the selection proceeds at a single node. If <span class="s15">θ </span>is of the form <i>l </i><span class="s86">≤ </span><i>a</i><span class="s145">i </span><span class="s86">≤ </span><i>u </i>— that is, <span class="s15">θ </span>is a range selection— and the relation has been range-partitioned on <i>a</i><span class="s97">i</span>, then the selection proceeds at each node whose partition overlaps with the speciﬁed range</p><p style="padding-left: 139pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">of values. In all other cases, the selection proceeds in parallel at all the nodes.</p><p class="s39" style="padding-top: 3pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s46">Duplicate elimination</span><span class="p">. Duplicates can be eliminated by sorting; either of the paral- lel sort techniques can be used, optimized to eliminate duplicates as soon as they appear during sorting. We can also parallelize duplicate elimination by partition- ing the tuples (by either range or hash partitioning) and eliminating duplicates locally at each node.</span></p><p class="s39" style="padding-top: 4pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s46">Projection</span><span class="p">. Projection without duplicate elimination can be performed as tuples are read in from disk in parallel. If duplicates are to be eliminated, either of the techniques just described can be used.</span></p><p class="s39" style="padding-top: 4pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s46">Aggregation</span><span class="p">. Consider an aggregation operation. We can parallelize the operation by partitioning the relation on the grouping attributes, and then computing the aggregate values locally at each node. Either hash partitioning or range partitioning can be used. If the relation is already partitioned on the grouping attributes, the ﬁrst step can be skipped.</span></p><p style="padding-left: 139pt;text-indent: 16pt;text-align: justify;">We can reduce the cost of transferring tuples during partitioning by partly computing aggregate values before partitioning, at least for the commonly used aggregate functions. Consider an aggregation operation on a relation <i>r</i>, using the <b>sum </b>aggregate function on attribute <i>B</i>, with grouping on attribute <i>A</i>. The system can perform the <b>sum </b>aggregation at each node <i>N</i><span class="s97">i </span>on those <i>r </i>tuples stored at <i>N</i><span class="s97">i</span>.</p><p style="padding-left: 139pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">This computation results in tuples with partial sums at each node; the result at</p><p class="s13" style="padding-left: 139pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">N<span class="s97">i </span><span class="p">has one tuple for each </span>A <span class="p">value present in </span>r <span class="p">tuples stored at </span>N<span class="s97">i</span><span class="p">, with the sum of the </span>B <span class="p">values of those tuples. The system then partitions the result of the local aggregation on the grouping attribute </span>A <span class="p">and performs the aggregation again (on tuples with the partial sums) at each node </span>N<span class="s145">i </span><span class="p">to get the ﬁnal result.</span></p><p style="padding-left: 139pt;text-indent: 0pt;text-align: justify;">As a result of this optimization, which is called <span class="s63">partial aggregation</span>, fewer tuples need to be sent to other nodes during partitioning. This idea can be extended easily to the <b>min </b>and <b>max </b>aggregate functions. Extensions to the <b>count </b>and <b>avg </b>aggregate functions are left for you to do in Exercise 22.2.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Skew handling for aggregation is easier than skew handling for joins, since the cost of aggregation is directly proportional to the input size. Usually, all that needs to be done is to use a good hash function to ensure the group-by attribute values are evenly distributed amongst the participating nodes. However, in some extreme cases, a few</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">values occur very frequently in the group-by attributes, and hashing can lead to uneven distribution of values. When applicable, partial aggregation is very eﬀective in avoiding skew in such situations. However, when partial aggregation is not applicable, skew can occur with aggregation.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Dynamic detection and handling of such skew can be done in some such cases: in case a node is found to be overloaded, some of the key values that are not yet processed by the node can be reassigned to another node, to balance the load. Such reassignment is greatly simpliﬁed if virtual-node partitioning is used; in that case, if a real node is found to be overloaded, some virtual nodes assigned to the overloaded real node, but not yet processed, are identiﬁed, and reassigned to other real nodes.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">More information on skew handling for join and other operators may be found in the Further Reading section at the end of the chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">22.4.2 Map and Reduce Operations</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Recall the MapReduce paradigm, described in Section 10.3, which is designed to ease the writing of parallel data processing programs.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Recall that the <span class="s49">map() </span>function provided by the programmer is invoked on each input record and emits zero or more output data items, which are then passed on to the <span class="s49">reduce() </span>function. Each data item output by a <span class="s49">map() </span>function consists of a record (<i>key, value</i>); we shall call the key as the <span class="s63">intermediate key</span>. In general, a <span class="s49">map() </span>function can emit multiple such records and since there are many input records, there are potentially many output records overall.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The MapReduce system takes all the records emitted by the <span class="s49">map() </span>functions, and groups them such that all records with a particular intermediate key are gathered to- gether. The <span class="s49">reduce() </span>function provided by the programmer is then invoked for each intermediate key and iterates over a collection of all values associated with that key.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Note that the map function can be thought of as a generalization of the project op- eration: both process a single record at a time, but for a given input record the project operation generates a single output record, whereas the map function can output mul- tiple records (including, as a special case, 0 records). Unlike the project operation, the output of a map function is usually intended to become the input of a reduce func- tion; hence, the output of a map function has an associated key that serves as a group by attribute. Recall that the reduce function takes as input a collection of values and outputs a result; with most of the reduce functions commonly in use, the result is an aggregate computed on the input values, and the reduce function is then essentially a user-deﬁned aggregation function.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">MapReduce systems are designed for parallel processing of data. A key require- ment for parallel processing is the ability to parallelize ﬁle input and output across multiple machines; otherwise, the single machine storing the data will become a bot- tleneck. Parallelization of ﬁle input and output can be done by using a distributed ﬁle system, such as the <i>Hadoop File System </i>(<i>HDFS</i>), discussed in Section 21.6, or by using a parallel/distributed storage system, discussed in Section 21.7. Recall that in such sys-</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s196" style="padding-left: 119pt;text-indent: 0pt;text-align: right;">copy</p><p class="s196" style="padding-top: 6pt;padding-left: 11pt;text-indent: 5pt;line-height: 78%;text-align: left;">User Program</p><p style="text-indent: 0pt;text-align: left;"><span><img width="364" height="195" alt="image" src="Image_3194.png"/></span></p><p class="s196" style="padding-top: 4pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">copy</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s196" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">copy</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s196" style="padding-left: 119pt;text-indent: 0pt;line-height: 10pt;text-align: right;">Part 1</p><p class="s196" style="padding-left: 119pt;text-indent: 0pt;line-height: 10pt;text-align: right;">Part 2</p><p class="s196" style="padding-left: 119pt;text-indent: 0pt;line-height: 10pt;text-align: right;">Part 3</p><p class="s196" style="padding-left: 119pt;text-indent: 0pt;line-height: 10pt;text-align: right;">Part 4</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s196" style="padding-left: 34pt;text-indent: 0pt;text-align: left;">Map 1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s196" style="padding-left: 34pt;text-indent: 0pt;text-align: left;">Map 2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s196" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">assign map</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s196" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">local write</p><p class="s196" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Master</p><p class="s196" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">assign reduce</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s196" style="text-indent: 0pt;text-align: left;">Reduce 1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s196" style="text-indent: 0pt;text-align: left;">Reduce 1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s196" style="padding-top: 7pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">write</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s196" style="padding-left: 18pt;text-indent: 0pt;text-align: left;">File 1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s196" style="padding-left: 18pt;text-indent: 0pt;text-align: left;">File 2</p><p class="s196" style="padding-left: 119pt;text-indent: 0pt;text-align: right;">Part p <span class="s489">read  </span><span class="s490">Map n</span></p><p class="s196" style="padding-top: 7pt;padding-left: 48pt;text-indent: 0pt;text-align: left;">Remote</p><p class="s196" style="padding-top: 4pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Reduce m</p><p class="s196" style="padding-top: 4pt;padding-left: 42pt;text-indent: 0pt;text-align: left;">File m</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s196" style="padding-left: 119pt;text-indent: 0pt;text-align: right;">Input file partitions</p><p class="s196" style="padding-left: 74pt;text-indent: 23pt;text-align: left;">Read, Sort Intermediate files</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s196" style="padding-left: 63pt;text-indent: 0pt;text-align: left;">Output files</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 192pt;text-indent: 0pt;text-align: left;">Figure 22.4 <span class="s74">Parallel processing of MapReduce job.</span></p><p style="padding-top: 8pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">tems, data are replicated (copied) across several (typically 3) machines, so that even if a few of the machines fail, the data are available from other machines that have copies of the data in the failed machine.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Conceptually, the map and reduce operations are parallelized in the same way that the relational operations project and aggregation are parallelized. Each node in the system has a number of concurrently executing <span class="s63">workers</span>, which are processes that execute map and reduce functions. The number of workers on one machine is often set to match the number of processor cores on the machine.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Parallel processing of MapReduce jobs is shown schematically in Figure 22.4. As shown in the ﬁgure, MapReduce systems split the input data into multiple pieces; the job of processing one such piece is called a <span class="s63">task</span>. Splitting can be done in units of ﬁles, and large ﬁles can be split into multiple parts. Tasks correspond to virtual nodes in our terminology, while workers correspond to real nodes. Note that with a multicore processor (as is standard today), MapReduce systems typically allocate one worker per core.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">MapReduce systems also have a scheduler, which assigns tasks to workers.<span class="s76">2</span> When- ever a worker completes a task, it is assigned a new task, until all tasks have been assigned.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">A key step between the map and reduce operations is the repartitioning of records output by the map step; these records are repartitioned based on their intermediate (reduce) key, such that all records with a particular key are assigned to the same reducer task. This could be done either by range-partitioning on the reduce key or by computing a hash function on the reduce key. In either case, the records are divided into multiple</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="95" height="1" alt="image" src="Image_3195.png"/></span></p><p class="s184" style="padding-top: 3pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;"><span class="s77">2</span><span class="s78">The scheduler is run on a dedicated node called the master node; the nodes that perform </span>map() <span class="s80">and </span>reduce() <span class="s80">tasks are called slave nodes in the Hadoop MapReduce terminology.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;"><a name="bookmark476">partitions, each of which is called a reduce task. A scheduler assigns reduce tasks to workers.</a><a name="bookmark521">&zwnj;</a></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">This step is identical to the repartitioning done for parallelizing the relational ag- gregation operation, with records partitioned into a number of virtual nodes based on their group-by key.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">To process the records in a particular <span class="s49">reduce </span>task, the records are sorted (or grouped) by the reduce key, so that all records with the same reduce-key value are brought together, and then the <span class="s49">reduce() </span>is executed on each group of reduce-key val- ues.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The reduce tasks are executed in parallel by the workers. When a worker completes a reduce task, another task is assigned to it, until all reduce tasks have been completed. A reduce task may have multiple diﬀerent reduce key values, but a particular call to the <span class="s49">reduce() </span>function is for a single reduce key; thus, the <span class="s49">reduce() </span>function is called for each key in the reduce task.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Tasks correspond to virtual nodes in the virtual-node partitioning scheme. There are far more tasks than there are nodes, and tasks are divided among the nodes. As discussed in Section 22.3.3, virtual-node partitioning reduces skew. Also note that as discussed in Section 22.4.1, skew can be reduced by partial aggregation, which corre- sponds to <i>combiners </i>in the MapReduce framework.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Further, MapReduce implementations typically also carry out dynamic detection and handling of skew, as discussed in Section 22.4.1.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Most MapReduce implementations include techniques to ensure that processing can be continued even if some nodes fail during query execution. Details are discussed further in Section 22.5.4.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part402.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part404.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
