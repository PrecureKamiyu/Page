<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>10.1 Background: Multiprocessor Architecture</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part105.htm">&lt; 上一个</a><span> | </span><a href="../ostep.html">内容</a><span> | </span><a href="part107.htm">下一个 &gt;</a></p><p class="s40" style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;">10.1 Background: Multiprocessor Architecture</p><p style="padding-top: 7pt;padding-left: 68pt;text-indent: 12pt;line-height: 89%;text-align: justify;">To understand the new issues surrounding multiprocessor schedul- ing, we have to understand a new and fundamental difference between single-CPU hardware and multi-CPU hardware. This difference centers around the use of hardware <b>caches </b>(e.g., Figure <span style=" color: #00AEEF;">10.1</span>), and exactly how data is shared across multiple processors. We now discuss this issue fur- ther, at a high level. Details are available elsewhere [CSG99], in particular in an upper-level or perhaps graduate computer architecture course.</p><p style="padding-left: 68pt;text-indent: 12pt;line-height: 89%;text-align: justify;">In a system with a single CPU, there are a hierarchy of <b>hardware caches </b>that in general help the processor run programs faster. Caches are small, fast memories that (in general) hold copies of <i>popular </i>data that is found in the main memory of the system. Main memory, in contrast, holds <i>all </i>of the data, but access to this larger memory is slower. By keep- ing frequently accessed data in a cache, the system can make the large, slow memory appear to be a fast one.</p><p style="padding-left: 68pt;text-indent: 12pt;line-height: 89%;text-align: justify;">As an example, consider a program that issues an explicit load instruc- tion to fetch a value from memory, and a simple system with only a single CPU; the CPU has a small cache (say 64 KB) and a large main memory.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-left: 127pt;text-indent: 0pt;text-align: left;"><span><img width="58" height="58" alt="image" src="Image_108.png"/></span>	<span><img width="58" height="58" alt="image" src="Image_109.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="98" height="29" alt="image" src="Image_110.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="text-indent: 0pt;text-align: left;">Memory</p><p style="text-indent: 0pt;text-align: left;"/><p class="s73" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">Cache</p><p style="text-indent: 0pt;text-align: left;"/><p class="s73" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">Cache</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">CPU</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">CPU</p><p style="text-indent: 0pt;text-align: left;"/><p class="s67" style="padding-top: 3pt;padding-left: 115pt;text-indent: 0pt;text-align: center;">Bus</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 76pt;text-indent: 0pt;text-align: left;">Figure 10.2: <b>Two CPUs With Caches Sharing Memory</b></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 41pt;text-indent: 0pt;line-height: 89%;text-align: justify;">The first time a program issues this load, the data resides in main mem- ory, and thus takes a long time to fetch (perhaps in the tens of nanosec- onds, or even hundreds). The processor, anticipating that the data may be reused, puts a copy of the loaded data into the CPU cache. If the pro- gram later fetches this same data item again, the CPU first checks for it in the cache; because it finds it there, the data is fetched much more quickly (say, just a few nanoseconds), and thus the program runs faster.</p><p style="padding-left: 41pt;text-indent: 11pt;line-height: 89%;text-align: justify;">Caches are thus based on the notion of <b>locality</b>, of which there are two kinds: <b>temporal locality </b>and <b>spatial locality</b>. The idea behind tem- poral locality is that when a piece of data is accessed, it is likely to be accessed again in the near future; imagine variables or even instructions themselves being accessed over and over again in a loop. The idea be- hind spatial locality is that if a program accesses a data item at address <span class="s43">x</span>, it is likely to access data items near <span class="s43">x </span>as well; here, think of a program streaming through an array, or instructions being executed one after the other. Because locality of these types exist in many programs, hardware systems can make good guesses about which data to put in a cache and thus work well.</p><p style="padding-left: 41pt;text-indent: 11pt;line-height: 90%;text-align: justify;">Now for the tricky part: what happens when you have multiple pro- cessors in a single system, with a single shared main memory, as we see in Figure <span style=" color: #00AEEF;">10.2</span>?</p><p style="padding-left: 41pt;text-indent: 11pt;line-height: 86%;text-align: justify;">As it turns out, caching with multiple CPUs is much more compli- cated. Imagine, for example, that a program running on CPU 1 reads a data item (with value <span class="s43">D</span>) at address <span class="s43">A</span>; because the data is not in the cache on CPU 1, the system fetches it from main memory, and gets the value <span class="s43">D</span>. The program then modifies the value at address <span class="s43">A</span>, just updat- ing its cache with the new value <span class="s43">D</span><span class="s74">′</span>; writing the data through all the way to main memory is slow, so the system will (usually) do that later. Then assume the OS decides to stop running the program and move it to CPU</p><p style="padding-left: 41pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">2. The program then re-reads the value at address <span class="s43">A</span>; there is no such data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 68pt;text-indent: 0pt;line-height: 80%;text-align: justify;">CPU 2’s cache, and thus the system fetches the value from main memory, and gets the old value <span class="s43">D </span>instead of the correct value <span class="s43">D</span><span class="s74">′</span>. Oops!</p><p style="padding-left: 80pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">This general problem is called the problem of <b>cache coherence</b>, and</p><p style="padding-left: 68pt;text-indent: 0pt;line-height: 89%;text-align: justify;">there is a vast research literature that describes many different subtleties involved with solving the problem [SHW11]. Here, we will skip all of the nuance and make some major points; take a computer architecture class (or three) to learn more.</p><p style="padding-left: 68pt;text-indent: 12pt;line-height: 89%;text-align: justify;">The basic solution is provided by the hardware: by monitoring mem- ory accesses, hardware can ensure that basically the “right thing” hap- pens and that the view of a single shared memory is preserved. One way to do this on a bus-based system (as described above) is to use an old technique known as <b>bus snooping </b>[G83]; each cache pays attention to memory updates by observing the bus that connects them to main mem- ory. When a CPU then sees an update for a data item it holds in its cache, it will notice the change and either <b>invalidate </b>its copy (i.e., remove it from its own cache) or <b>update </b>it (i.e., put the new value into its cache too). Write-back caches, as hinted at above, make this more complicated (because the write to main memory isn’t visible until later), but you can imagine how the basic scheme might work.</p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part105.htm">&lt; 上一个</a><span> | </span><a href="../ostep.html">内容</a><span> | </span><a href="part107.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
