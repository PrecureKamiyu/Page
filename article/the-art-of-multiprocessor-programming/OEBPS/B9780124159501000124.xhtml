<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML" lang="EN" xml:lang="en"><head><meta content="text/html; charset=utf-8" http-equiv="default-style"/><title>The Art of Multiprocessor Programming</title><link href="Elsevier_eBook.css" rel="stylesheet" type="text/css"/><link href="math.css" rel="stylesheet" type="text/css"/><link href="media.css" media="only screen" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4f1c4a5b-a3e2-48ff-98f3-ff17812cd57a" name="Adept.expected.resource"/></head><body><section epub:type="chapter" role="doc-chapter"><div aria-label="Page 49" epub:type="pagebreak" id="page_49" role="doc-pagebreak"/><div id="CN"><a id="c0010tit1"/></div><header><hgroup><h1 class="chaptitle" id="c0010tit">Chapter 3: Concurrent objects</h1></hgroup><section epub:type="preamble"><div class="abstract"><h2 class="h1hd" id="ab0010"><a id="st0010"/>Abstract</h2><p class="abspara">This chapter describes how to specify correctness for concurrent objects. All notions of correctness for concurrent objects are based on some notion of equivalence with sequential behavior. Sequential consistency is a strong condition that is useful for describing standalone systems. Linearizability is an even stronger condition that supports composition, which allows modular decomposition and analysis of systems. Quiescent consistency is a relaxed condition that is appropriate for some high-performance applications.</p><p class="abspara">This chapter also describes how to characterize systems by their progress guarantees. It defines several progress conditions—wait-freedom, lock-freedom, obstruction-freedom, deadlock-freedom, and starvation-freedom—classifying them as blocking or nonblocking.</p></div></section><section id="ks0010"><h3 class="h2hd" id="st0015">Keywords</h3><p class="keywords">sequential consistency; linearizability; quiescent consistency; blocking progress conditions; nonblocking progress conditions; lock-freedom; wait-freedom; obstruction-freedom; deadlock-freedom; starvation-freedom; compositionality</p></section></header><p class="textfl" id="p0010">The behavior of concurrent objects is best described through their safety and liveness properties, often called <i>correctness</i> and <i>progress</i>. In this chapter, we examine various ways of specifying correctness and progress.</p><p class="text" id="p0015">All notions of correctness for concurrent objects are based on some notion of equivalence with sequential behavior, but different notions are appropriate for different systems. We examine three correctness conditions. <i>Sequential consistency</i> is a strong condition, often useful for describing standalone systems such as hardware memory interfaces. <i>Linearizability</i> is an even stronger condition that supports <i>composition</i>: It is useful for describing systems composed from <i>linearizable</i> components. <i>Quiescent consistency</i> is appropriate for applications that require high performance at the cost of placing relatively weak constraints on object behavior.</p><p class="text" id="p0020">Along a different dimension, different method implementations provide different progress guarantees. Some are <i>blocking</i>, where the delay of one thread can prevent other threads from making progress; some are <i>nonblocking</i>, where the delay of a thread cannot delay other threads indefinitely.</p><section><h2 class="h1hd" id="s0010"><a id="st0020"/>3.1 Concurrency and correctness</h2><p class="textfl" id="p0025">What does it mean for a concurrent object to be correct? <a href="#f0010" id="cf0035">Fig. 3.1</a> shows a simple lock-based concurrent “first-in-first-out” (FIFO) queue. The <img alt="Image" height="9" src="images/B9780124159501000124/fx005.jpg" width="19"/>() and <img alt="Image" height="11" src="images/B9780124159501000124/fx006.jpg" width="19"/>() methods synchronize using a mutual exclusion lock of the kind studied in Chapter <a href="B9780124159501000112.xhtml">2</a>. We immediately intuit that this implementation should be correct: Because each method holds an exclusive lock the entire time it accesses and updates fields, the method calls take effect sequentially.</p><div class="pageavoid"><figure class="fig" id="f0010"><img alt="Image" height="536" src="images/B9780124159501000124/gr001.jpg" width="291"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 3.1</span> A lock-based FIFO queue. The queue's items are kept in an array <img alt="Image" height="9" src="images/B9780124159501000124/fx001.jpg" width="31"/>, where <img alt="Image" height="9" src="images/B9780124159501000124/fx002.jpg" width="25"/> is the index of the next item (if any) to dequeue, and <img alt="Image" height="9" src="images/B9780124159501000124/fx003.jpg" width="23"/> is the index of the first open array slot (modulo the capacity). The <img alt="Image" height="9" src="images/B9780124159501000124/fx004.jpg" width="25"/> field contains a lock that ensures that methods are mutually exclusive. Initially <img alt="Image" height="9" src="images/B9780124159501000124/fx002.jpg" width="25"/> and <img alt="Image" height="9" src="images/B9780124159501000124/fx003.jpg" width="23"/> are zero, and the queue is empty. If <img alt="Image" height="9" src="images/B9780124159501000124/fx005.jpg" width="19"/>() finds the queue is full (i.e., <img alt="Image" height="9" src="images/B9780124159501000124/fx002.jpg" width="25"/> and <img alt="Image" height="9" src="images/B9780124159501000124/fx003.jpg" width="23"/> differ by the queue capacity), then it throws an exception. Otherwise, there is room, so <img alt="Image" height="9" src="images/B9780124159501000124/fx005.jpg" width="19"/>() stores the item at array entry for <img alt="Image" height="9" src="images/B9780124159501000124/fx003.jpg" width="23"/>, and then increments <img alt="Image" height="9" src="images/B9780124159501000124/fx003.jpg" width="23"/>. The <img alt="Image" height="11" src="images/B9780124159501000124/fx006.jpg" width="19"/>() method works in a symmetric way.</div></figcaption></figure></div><p class="text" id="p0030">This idea is illustrated in <a href="#f0015" id="cf0040">Fig. 3.2</a>, which shows an execution in which thread <i>A</i> enqueues <i>a</i>, <i>B</i> enqueues <i>b</i>, and <i>C</i> dequeues twice, first throwing <img alt="Image" height="11" src="images/B9780124159501000124/fx007.jpg" width="91"/>, and second returning <i>b</i>. Overlapping intervals indicate concurrent method calls. All the method calls overlap in time. In this figure, as in others, time moves from left to right, and dark lines indicate intervals. The intervals for a single thread are displayed along a single horizontal line. When convenient, the thread name appears on the left. A bar represents an interval with a fixed start and stop time. A bar with dotted lines on the right represents an interval with a fixed start-time and an unknown stop-time. The label “<span class="hiddenClass"><mml:math><mml:mi>q</mml:mi><mml:mo>.</mml:mo><mml:mtext>enq</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000124/si1.png" style="vertical-align:middle" width="59"/></span>” means that a thread enqueues item <i>x</i> at object <i>q</i>, while “<span class="hiddenClass"><mml:math><mml:mi>q</mml:mi><mml:mo>.</mml:mo><mml:mtext>deq</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000124/si2.png" style="vertical-align:middle" width="59"/></span>” means that the thread dequeues item <i>x</i> from object <i>q</i>.</p><div class="pageavoid"><figure class="fig" id="f0015"><img alt="Image" height="291" src="images/B9780124159501000124/gr002.jpg" width="497"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 3.2</span> Lock-based queue execution. Here, <i>C</i> acquires the lock, observes the queue to be empty, releases the lock, and throws an exception. <i>B</i> acquires the lock, inserts <i>b</i>, and releases the lock. <i>A</i> acquires the lock, inserts <i>a</i>, and releases the lock. <i>C</i> reacquires the lock, dequeues <i>b</i>, releases the lock, and returns <i>b</i>.</div></figcaption></figure></div><p class="text" id="p0035"><span aria-label="Page 50" epub:type="pagebreak" id="page_50" role="doc-pagebreak"/>The timeline shows which thread holds the lock. Here, <i>C</i> acquires the lock, observes the queue to be empty, releases the lock, and throws an exception. It does not modify the queue. <i>B</i> acquires the lock, inserts <i>b</i>, and releases the lock. <i>A</i> acquires the lock, inserts <i>a</i>, and releases <span aria-label="Page 51" epub:type="pagebreak" id="page_51" role="doc-pagebreak"/>the lock. <i>C</i> reacquires the lock, dequeues <i>b</i>, releases the lock, and returns <i>b</i>. Each of these calls takes effect sequentially, and we can easily verify that dequeuing <i>b</i> before <i>a</i> is consistent with our understanding of sequential FIFO queue behavior.</p><p class="text" id="p0040">Unfortunately, it follows from Amdahl's law (Chapter <a href="B9780124159501000094.xhtml">1</a>) that concurrent objects whose methods hold exclusive locks, and therefore effectively execute one after the other, are less desirable than ones with finer-grained locking or no locks at all. We therefore need a way to specify the behavior required of concurrent objects, and to reason about their implementations, without relying on method-level locking.</p><p class="text" id="p0045">Consider the alternative concurrent queue implementation in <a href="#f0020" id="cf0045">Fig. 3.3</a>. It has almost the same internal representation as the lock-based queue of <a href="#f0010" id="cf0050">Fig. 3.1</a>; the only difference is the absence of a lock. We claim that this implementation is correct provided there is only a single enqueuer and a single dequeuer. But it is no longer easy to explain why. If the queue supported concurrent enqueues or concurrent dequeues, it would not even be clear what it means for a queue <i>to be FIFO</i>.</p><div class="pageavoid"><figure class="fig" id="f0020"><img alt="Image" height="322" src="images/B9780124159501000124/gr003.jpg" width="291"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 3.3</span> A single-enqueuer/single-dequeuer FIFO queue. The structure is identical to that of the lock-based FIFO queue, except that there is no need for the lock to coordinate access.</div></figcaption></figure></div><p class="text" id="p0050">The lock-based queue example illustrates a useful principle: It is easier to reason about the behavior of concurrent objects if we can somehow map their concurrent executions to sequential ones, and otherwise limit our reasoning to these sequential executions. This principle is the key to the correctness properties introduced in this chapter. Therefore, we begin by considering specifications of sequential objects.<span aria-label="Page 52" epub:type="pagebreak" id="page_52" role="doc-pagebreak"/></p></section><section><h2 class="h1hd" id="s0015"><a id="st0025"/>3.2 Sequential objects</h2><p class="textfl" id="p0055">An <i>object</i> in languages such as Java and C++ is a container for data and a set of <i>methods</i>, which are the only way to manipulate those data. Each object has a <i>class</i>, which defines the object's methods and how they behave. An object has a well-defined <i>state</i> (for example, the FIFO queue's current sequence of items). There are many ways to describe how an object's methods behave, ranging from formal specifications to plain English. The application program interface (API) documentation that we use every day lies somewhere in between.</p><p class="text" id="p0060">The API documentation typically says something like the following: If the object is in such-and-such a state before you call the method, then the object will be in some other state when the method returns, and the call will return a particular value, or throw a particular exception. This kind of description divides naturally into a <i>precondition</i>, which describes the object's state before invoking the method, and a <i>postcondition</i>, which describes the object's state and return value when the method returns. A change to an object's state is sometimes called a <i>side effect</i>.</p><p class="text" id="p0065">For example, a FIFO queue might be described as follows: The class provides two methods, <img alt="Image" height="9" src="images/B9780124159501000124/fx005.jpg" width="19"/>() and <img alt="Image" height="11" src="images/B9780124159501000124/fx006.jpg" width="19"/>(). The queue state is a sequence of items, possibly empty. If the queue state is a sequence <i>q</i> (precondition), then a call to <img alt="Image" height="9" src="images/B9780124159501000124/fx005.jpg" width="19"/>(<i>z</i>) leaves the queue in state <span class="hiddenClass"><mml:math><mml:mi>q</mml:mi><mml:mo>⋅</mml:mo><mml:mi>z</mml:mi></mml:math></span><span><img alt="Image" height="10" src="images/B9780124159501000124/si3.png" style="vertical-align:middle" width="29"/></span> (postcondition <span aria-label="Page 53" epub:type="pagebreak" id="page_53" role="doc-pagebreak"/>with side effect), where “⋅” denotes concatenation. If the queue object is nonempty, say, <span class="hiddenClass"><mml:math><mml:mi>a</mml:mi><mml:mo>⋅</mml:mo><mml:mi>q</mml:mi></mml:math></span><span><img alt="Image" height="10" src="images/B9780124159501000124/si4.png" style="vertical-align:middle" width="30"/></span> (precondition), then the <img alt="Image" height="11" src="images/B9780124159501000124/fx006.jpg" width="19"/>() method removes the sequence's first element <i>a</i>, leaving the queue in state <i>q</i>, and returns this element (postcondition). If, instead, the queue object is empty (precondition), the method throws <img alt="Image" height="11" src="images/B9780124159501000124/fx007.jpg" width="91"/> and leaves the queue state unchanged (postcondition, no side effect).</p><p class="text" id="p0070">This style of documentation, called a <i>sequential specification</i>, is so familiar that it is easy to overlook how elegant and powerful it is. The length of the object's documentation is linear in the number of methods, because each method can be described in isolation. There are a vast number of potential interactions among methods, and all such interactions are characterized succinctly by the methods' side effects on the object state. The object's documentation describes the object state before and after each call, and we can safely ignore any intermediate states that the object may assume while the method call is in progress.</p><p class="text" id="p0075">Defining objects in terms of preconditions and postconditions makes sense in a <i>sequential</i> model of computation, where a single thread manipulates a collection of objects. But this familiar style of documentation fails for objects shared by multiple threads. If an object's methods can be invoked concurrently by multiple threads, then method calls can overlap in time, and it no longer makes sense to talk about their order. What does it mean, for example, if <i>x</i> and <i>y</i> are enqueued onto a FIFO queue during overlapping intervals? Which will be dequeued first? Can we continue to describe methods in isolation, via preconditions and postconditions, or must we provide explicit descriptions of every possible interaction among every possible collection of concurrent method calls?</p><p class="text" id="p0080">Even the notion of an object's state becomes confusing. In a single-threaded program, an object must assume a meaningful state only between method calls.<sup><a epub:type="noteref" href="#fn001" id="cf0055" role="doc-noteref">1</a></sup> In a multithreaded program, however, overlapping method calls may be in progress at every instant, so a concurrent object might <i>never</i> be between method calls. Every method call must be prepared to encounter an object state that reflects the incomplete effects of concurrent method calls, a problem that does not arise in single-threaded programs.</p></section><section><h2 class="h1hd" id="s0020"><a id="st0030"/>3.3 Sequential consistency</h2><p class="textfl" id="p0085">One way to develop an intuition about how concurrent objects should behave is to review examples of concurrent computations involving simple objects, and decide, in each case, whether the behavior agrees with our intuition about how the objects should behave.</p><p class="text" id="p0090">Method calls take time. A <i>method call</i> is the interval that starts with an <i>invocation</i> event and continues until the corresponding <span aria-label="Page 54" epub:type="pagebreak" id="page_54" role="doc-pagebreak"/><i>response</i> event, if any. Method calls by concurrent threads may overlap, while method calls by a single thread are always sequential (nonoverlapping, one-after-the-other). We say a method call is <i>pending</i> if its invocation event has occurred, but its response event has not.</p><p class="text" id="p0095">For historical reasons, the object version of a read–write memory location is called a <i>register</i> (see Chapter <a href="B9780124159501000136.xhtml">4</a>). In <a href="#f0025" id="cf0060">Fig. 3.4</a>, two threads concurrently write −3 and 7 to a shared register <i>r</i> (as before, “<span class="hiddenClass"><mml:math><mml:mi>r</mml:mi><mml:mo>.</mml:mo><mml:mtext>read</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000124/si5.png" style="vertical-align:middle" width="60"/></span>” means that a thread reads value <i>x</i> from register object <i>r</i>, and similarly for “<span class="hiddenClass"><mml:math><mml:mi>r</mml:mi><mml:mo>.</mml:mo><mml:mtext>write</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000124/si6.png" style="vertical-align:middle" width="65"/></span>”). Later, one thread reads <i>r</i> and returns the value −7. This behavior is surprising. We expect to find either 7 or −3 in the register, not a mixture of both. This example suggests the following principle: </p><div class="boxg1" id="enun0010"><p class="b1num">Principle 3.3.1 </p><div><p class="b1textfl" id="p0100">Method calls should appear to happen in a one-at-a-time, sequential order.</p></div></div><p class="textfl"/><div class="pageavoid"><figure class="fig" id="f0025"><img alt="Image" height="107" src="images/B9780124159501000124/gr004.jpg" width="496"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 3.4</span> Why each method call should appear to take effect in one-at-a-time order. Two threads concurrently write −3 and 7 to a shared register <i>r</i>. Later, one thread reads <i>r</i> and returns the value −7. We expect to find either 7 or −3 in the register, not a mixture of both.</div></figcaption></figure></div><p class="text" id="p0105">By itself, this principle is too weak to be useful. For example, it permits reads to always return the object's initial state, even in sequential executions (i.e., executions in which method calls do not overlap). Or consider the execution in <a href="#f0030" id="cf0065">Fig. 3.5</a>, in which a single thread writes 7 and then −3 to a shared register <i>r</i>. Later, it reads <i>r</i> and returns 7. For some applications, this behavior might not be acceptable because the value the thread read is not the value it wrote most recently. The order in which a single thread issues method calls is called its <i>program order</i>. (Method calls by different threads are unrelated by program order.) In this example, we were surprised that operation calls did not take effect in program order. This example suggests the following principle: </p><div class="boxg1" id="enun0015"><p class="b1num">Principle 3.3.2 </p><div><p class="b1textfl" id="p0110">Method calls should appear to take effect in program order.</p></div></div><p class="textfl"/><div class="pageavoid"><figure class="fig" id="f0030"><img alt="Image" height="35" src="images/B9780124159501000124/gr005.jpg" width="497"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 3.5</span> Why method calls should appear to take effect in program order. This behavior is not acceptable because the value the thread read is not the last value it wrote (and no other thread writes to the register).</div></figcaption></figure></div><p class="text" id="p0115"><span aria-label="Page 55" epub:type="pagebreak" id="page_55" role="doc-pagebreak"/>This principle ensures that purely sequential computations behave the way we expect. Together, <a href="#enun0010" id="cs0010">Principles 3.3.1</a> and <a href="#enun0015">3.3.2</a> define <i>sequential consistency</i>, a correctness condition that is widely used in the literature on multiprocessor synchronization.</p><p class="text" id="p0120">Sequential consistency requires that method calls act as if they occurred in a sequential order consistent with program order. That is, there is a way to order all the method calls in any concurrent execution so that they (1) are consistent with program order and (2) meet the object's sequential specification. Multiple sequential orders may satisfy these conditions. For example, in <a href="#f0035" id="cf0070">Fig. 3.6</a>, thread <i>A</i> enqueues <i>x</i> while thread <i>B</i> enqueues <i>y</i>, and then <i>A</i> dequeues <i>y</i> while <i>B</i> dequeues <i>x</i>. Two sequential orders explain these results: (1) <i>A</i> enqueues <i>x</i>, <i>B</i> enqueues <i>y</i>, <i>B</i> dequeues <i>x</i>, and then <i>A</i> dequeues <i>y</i>, or (2) <i>B</i> enqueues <i>y</i>, <i>A</i> enqueues <i>x</i>, <i>A</i> dequeues <i>y</i>, and then <i>B</i> dequeues <i>x</i>. Both orders are consistent with the program order; either suffices to show that the execution is sequentially consistent.</p><div class="pageavoid"><figure class="fig" id="f0035"><img alt="Image" height="111" src="images/B9780124159501000124/gr006.jpg" width="497"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 3.6</span> There are two possible sequential orders that can justify this execution. Both orders are consistent with the method calls' program order, and either one is enough to show that the execution is sequentially consistent.</div></figcaption></figure></div><section><h3 class="h2hd" id="s0025"><a id="st0035"/>3.3.1 Sequential consistency versus real-time order</h3><p class="textfl" id="p0125">In <a href="#f0040" id="cf0075">Fig. 3.7</a>, thread <i>A</i> enqueues <i>x</i>, and later <i>B</i> enqueues <i>y</i>, and finally <i>A</i> dequeues <i>y</i>. This execution may <span aria-label="Page 56" epub:type="pagebreak" id="page_56" role="doc-pagebreak"/>violate our intuitive notion of how a FIFO queue should behave: The call enqueuing <i>x</i> finishes before the call enqueuing <i>y</i> starts, so <i>y</i> is enqueued after <i>x</i>. But it is dequeued before <i>x</i>. Nevertheless, this execution is sequentially consistent. Even though the call that enqueues <i>x</i> happens before the call that enqueues <i>y</i>, these calls are unrelated by program order, so sequential consistency is free to reorder them. When one operation completes before another begins, we say that the first operation precedes the second in the <i>real-time order</i>. This example shows that sequential consistency need not preserve the real-time order.</p><div class="pageavoid"><figure class="fig" id="f0040"><img alt="Image" height="114" src="images/B9780124159501000124/gr007.jpg" width="497"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 3.7</span> Sequential consistency versus real-time order. Thread <i>A</i> enqueues <i>x</i>, and later thread <i>B</i> enqueues <i>y</i>, and finally <i>A</i> dequeues <i>y</i>. This execution may violate our intuitive notion of how a FIFO queue should behave because the method call enqueuing <i>x</i> finishes before the method call enqueuing <i>y</i> starts, so <i>y</i> is enqueued after <i>x</i>. But it is dequeued before <i>x</i>. Nevertheless, this execution is sequentially consistent.</div></figcaption></figure></div><p class="text" id="p0130">One could argue whether it is acceptable to reorder method calls whose intervals do not overlap, even if they occur in different threads. For example, we might be unhappy if we deposit our paycheck on Monday, but the bank bounces our rent check the following Friday because it reordered our deposit after your withdrawal.</p></section><section><h3 class="h2hd" id="s0030"><a id="st0040"/>3.3.2 Sequential consistency is nonblocking</h3><p class="textfl" id="p0135">How much does sequential consistency limit concurrency? Specifically, under what circumstances does sequential consistency require one method call to block waiting for another to complete? Perhaps surprisingly, the answer is (essentially) <i>never</i>. More precisely, for any pending method call in a sequentially consistent concurrent execution, there is some sequentially consistent response, that is, a response to the invocation that could be given immediately without violating sequential consistency. We say that a correctness condition with this property is <i>nonblocking</i>. Sequential consistency is a <i>nonblocking</i> correctness condition.</p><p class="text" id="p0140"/><div class="boxg1" id="enun0020"><p class="b1num">Remark 3.3.1 </p><div><p class="b1textfl" id="p0145">The term <i>nonblocking</i> is used to denote several different notions. In this context, referring to correctness conditions, it means that for any pending call of a total method, there is a response that satisfies the correctness condition. In Section <a href="#s0095" id="cf0080">3.8</a>, referring to progress conditions, it means that a progress condition guarantees that the delay of one or more threads cannot prevent other threads from making progress. When referring to an object implementation, it means that the implementation meets a nonblocking progress condition. (It may even be used with finer granularity, referring to an individual method of an object implementation that cannot be prevented from making progress by the delay of other threads.) In the systems literature, a <i>nonblocking</i> operation returns immediately without waiting for the operation to take effect, whereas a <i>blocking</i> operation does not return until the operation is complete. (<i>Blocking</i> is also used to describe a lock implementation that suspends a thread that tries to acquire a lock that is held by another thread, as opposed to <i>spinning</i> implementations, which we discuss in Chapter <a href="B9780124159501000173.xhtml">7</a>). Unfortunately, these various uses are all too well established to change, but it should be clear from the context which meaning is intended.</p></div></div><p class="textfl"/><p class="text" id="p0150">Note that this observation does not mean that it is easy to figure out a sequentially consistent response for a pending method call, only that the correctness condition itself does not stand in the way. The observation <span aria-label="Page 57" epub:type="pagebreak" id="page_57" role="doc-pagebreak"/>holds only for <i>total methods</i>, which are defined for every object state (i.e., for any state on which a total method is invoked, there is some response allowed by the sequential specification). There is, of course, no sequentially consistent response to a method call if there is no response that satisfies the sequential specification. Our informal description of sequential consistency thus far is not sufficient to capture this and other important details, such as what it exactly means for an execution with pending method calls to be sequentially consistent. We make this notion more precise in Section <a href="#s0065" id="cf0085">3.6</a>.</p></section><section><h3 class="h2hd" id="s0035"><a id="st0045"/>3.3.3 Compositionality</h3><p class="textfl" id="p0155">Any sufficiently complex system must be designed and implemented in a <i>modular</i> fashion. Components are designed, implemented, and proved correct independently. Each component makes a clear distinction between its <i>implementation</i>, which is hidden, and its <i>interface</i>, which characterizes the guarantees it makes to the other components. For example, if a concurrent object's interface states that it is a sequentially consistent FIFO queue, then users of the queue need not know anything about how the queue is implemented. The result of composing individually correct components that rely only on one another's interfaces should itself be a correct system.</p><p class="text" id="p0160">A correctness property <span class="hiddenClass"><mml:math><mml:mi mathvariant="script">P</mml:mi></mml:math></span><span><img alt="Image" height="12" src="images/B9780124159501000124/si7.png" style="vertical-align:middle" width="15"/></span> is <i>compositional</i> if, whenever each object in the system satisfies <span class="hiddenClass"><mml:math><mml:mi mathvariant="script">P</mml:mi></mml:math></span><span><img alt="Image" height="12" src="images/B9780124159501000124/si7.png" style="vertical-align:middle" width="15"/></span>, the system as a whole satisfies <span class="hiddenClass"><mml:math><mml:mi mathvariant="script">P</mml:mi></mml:math></span><span><img alt="Image" height="12" src="images/B9780124159501000124/si7.png" style="vertical-align:middle" width="15"/></span>. Compositionality is important because it enables a system to be assembled easily from independently derived components. A system based on a noncompositional correctness property cannot rely solely on its components' interfaces: Some kind of additional constraints are needed to ensure that the components are actually compatible.</p><p class="text" id="p0165">Is sequential consistency compositional? That is, is the result of composing multiple sequentially consistent objects itself sequentially consistent? The answer, unfortunately, is <i>no</i>. In <a href="#f0045" id="cf0090">Fig. 3.8</a>, two threads, <i>A</i> and <i>B</i>, call enqueue and dequeue methods for two queue objects, <i>p</i> and <i>q</i>. It is not hard to see that <i>p</i> and <i>q</i> are each sequentially consistent: The sequence of method calls for <i>p</i> is the same as in the sequentially consistent execution shown in <a href="#f0040" id="cf0095">Fig. 3.7</a>, and the behavior of <i>q</i> is symmetric. Nevertheless, the execution as a whole is <i>not</i> sequentially consistent.</p><div class="pageavoid"><figure class="fig" id="f0045"><img alt="Image" height="110" src="images/B9780124159501000124/gr008.jpg" width="496"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 3.8</span> Sequential consistency is not compositional. Two threads, <i>A</i> and <i>B</i>, call enqueue and dequeue methods on two queue objects, <i>p</i> and <i>q</i>. It is not hard to see that <i>p</i> and <i>q</i> are each sequentially consistent, yet the execution as a whole is <i>not</i> sequentially consistent.</div></figcaption></figure></div><p class="text" id="p0170"><span aria-label="Page 58" epub:type="pagebreak" id="page_58" role="doc-pagebreak"/>To see that there is no correct sequential execution of these methods calls that is consistent with their program order, assume, by way of contradiction, that there is such an execution. We use the following shorthand: <img alt="Image" height="12" src="images/B9780124159501000124/fx008.jpg" width="176"/> means that any sequential execution must order <i>A</i>'s enqueue of <i>x</i> at <i>p</i> before <i>B</i>'s dequeue of <i>x</i> at <i>p</i>, and so on. Because <i>p</i> is FIFO and <i>A</i> dequeues <i>y</i> from <i>p</i>, <i>y</i> must have been enqueued at <i>p</i> before <i>x</i>:</p><div class="showClass"><p class="fig"><img alt="Image" height="12" src="images/B9780124159501000124/fx009.jpg" width="189"/><a id="deq1"/></p></div><p class="textfl"> Similarly, <i>x</i> must have been enqueued onto <i>q</i> before <i>y</i>:</p><div class="showClass"><p class="fig"><img alt="Image" height="12" src="images/B9780124159501000124/fx010.jpg" width="187"/><a id="deq2"/></p></div><p class="textfl"> But program order implies that</p><div class="showClass"><p class="fig"><img alt="Image" height="12" src="images/B9780124159501000124/fx011.jpg" width="430"/><a id="deq3"/></p></div><p class="textfl"> Together, these orderings form a cycle.</p></section></section><section><h2 class="h1hd" id="s0040"><a id="st0050"/>3.4 Linearizability</h2><p class="textfl" id="p0175">Sequential consistency has a serious drawback: it is not compositional. That is, the result of composing sequentially consistent components is not itself necessarily sequentially consistent. To fix this shortcoming, we replace the requirement that method calls appear to happen in program order with the following stronger constraint: </p><div class="boxg1" id="enun0025"><p class="b1num">Principle 3.4.1 </p><div><p class="b1textfl" id="p0180">Each method call should appear to take effect instantaneously at some moment between its invocation and response.</p></div></div><p class="textfl"/><p class="text" id="p0185">This principle states that the real-time order of method calls must be preserved. We call this correctness property <i>linearizability</i>. Every linearizable execution is sequentially consistent, but not vice versa.</p><section><h3 class="h2hd" id="s0045"><a id="st0055"/>3.4.1 Linearization points</h3><p class="textfl" id="p0190">The usual way to show that a concurrent object implementation is linearizable is to identify for each method a <i>linearization point</i>, an instant when the method takes effect. We say that a method <i>is linearized at</i> its linearization point. For lock-based implementations, any point within each method's critical section can serve as its linearization point. For implementations that do not use locks, the linearization point is typically a single step where the effects of the method call become visible to other method calls.</p><p class="text" id="p0195">For example, recall the single-enqueuer/single-dequeuer queue of <a href="#f0020" id="cf0100">Fig. 3.3</a>. This implementation has no critical sections, and yet we can identify linearization points for its methods. For example, if a <img alt="Image" height="11" src="images/B9780124159501000124/fx006.jpg" width="19"/>() method returns an item, its linearization point is when the <img alt="Image" height="9" src="images/B9780124159501000124/fx002.jpg" width="25"/> field is updated (line 17). If the queue is empty, the <img alt="Image" height="11" src="images/B9780124159501000124/fx006.jpg" width="19"/>() method is linearized when it reads the <img alt="Image" height="9" src="images/B9780124159501000124/fx003.jpg" width="23"/> field (line 14). The <img alt="Image" height="9" src="images/B9780124159501000124/fx005.jpg" width="19"/>() method is similar.<span aria-label="Page 59" epub:type="pagebreak" id="page_59" role="doc-pagebreak"/></p></section><section><h3 class="h2hd" id="s0050"><a id="st0060"/>3.4.2 Linearizability versus sequential consistency</h3><p class="textfl" id="p0200">Like sequential consistency, linearizability is nonblocking: There is a linearizable response to any pending call of a total method. In this way, linearizability does not limit concurrency.</p><p class="text" id="p0205">Threads that communicate only through a single shared object (e.g., the memory of a shared-memory multiprocessor) cannot distinguish between sequential consistency and linearizability. Only an external observer, who can see that one operation precedes another in the real-time order, can tell that a sequentially consistent object is not linearizable. For this reason, the difference between sequential consistency and linearizability is sometimes called <i>external consistency</i>. Sequential consistency is a good way to describe standalone systems, where composition is not an issue. However, if the threads share multiple objects, these objects may be external observers for each other, as we saw in <a href="#f0045" id="cf0105">Fig. 3.8</a>.</p><p class="text" id="p0210">Unlike sequential consistency, linearizability is compositional: The result of composing linearizable objects is linearizable. For this reason, linearizability is a good way to describe components of large systems, where components must be implemented and verified independently. Because we are interested in systems that compose, most (but not all) data structures considered in this book are linearizable.</p></section></section><section><h2 class="h1hd" id="s0055"><a id="st0065"/>3.5 Quiescent consistency</h2><p class="textfl" id="p0215">For some systems, implementors may be willing to trade consistency for performance. That is, we may relax the consistency condition to allow cheaper, faster, and/or more efficient implementations. One way to relax consistency is to enforce ordering only when an object is <i>quiescent</i>, that is, when it has no pending method calls. Instead of <a href="#enun0015" id="cs0015">Principles 3.3.2</a> and <a href="#enun0025">3.4.1</a>, we would adopt the following principle: </p><div class="boxg1" id="enun0030"><p class="b1num">Principle 3.5.1 </p><div><p class="b1textfl" id="p0220">Method calls separated by a period of quiescence should appear to take effect in their real-time order.</p></div></div><p class="textfl"/><p class="text" id="p0225">For example, suppose <i>A</i> and <i>B</i> concurrently enqueue <i>x</i> and <i>y</i> in a FIFO queue. The queue becomes quiescent, and then <i>C</i> enqueues <i>z</i>. We are not able to predict the relative order of <i>x</i> and <i>y</i> in the queue, but we do know they are ahead of <i>z</i>.</p><p class="text" id="p0230">Together, <a href="#enun0010" id="cs0020">Principles 3.3.1</a> and <a href="#enun0030">3.5.1</a> define a correctness property called <i>quiescent consistency</i>. Informally, it says that any time an object becomes quiescent, the execution so far is equivalent to some sequential execution of the completed calls.</p><p class="text" id="p0235">As an example of quiescent consistency, consider the shared counter from Chapter <a href="B9780124159501000094.xhtml">1</a>. A quiescently consistent shared counter would return numbers, not necessarily in the order of the <img alt="Image" height="11" src="images/B9780124159501000124/fx012.jpg" width="99"/>() requests, but always without duplicating or omitting a number. The execution of a quiescently consistent object is somewhat like a game of musical chairs: At any point, the music might stop, that is, the state could become quiescent. At that point, each pending method call must return an index so that all the indices together meet the specification of a sequential <span aria-label="Page 60" epub:type="pagebreak" id="page_60" role="doc-pagebreak"/>counter, implying no duplicated or omitted numbers. In other words, a quiescently consistent counter is an <i>index distribution</i> mechanism, useful as a “loop counter” in programs that do not care about the order in which indices are issued.</p><section><h3 class="h2hd" id="s0060"><a id="st0070"/>3.5.1 Properties of quiescent consistency</h3><p class="textfl" id="p0240">Note that sequential consistency and quiescent consistency are <i>incomparable</i>: There exist sequentially consistent executions that are not quiescently consistent, and vice versa. Quiescent consistency does not necessarily preserve program order, and sequential consistency is unaffected by quiescent periods. On the other hand, linearizability is stronger than both quiescent consistency and sequential consistency. That is, a linearizable object is both quiescently consistent and sequentially consistent.</p><p class="text" id="p0245">Like sequential consistency and linearizability, quiescent consistency is nonblocking: Any pending call to a total method in a quiescently consistent execution can be completed.</p><p class="text" id="p0250">Quiescent consistency is compositional: A system composed of quiescently consistent objects is itself quiescently consistent. It follows that quiescently consistent objects can be composed to construct more complex quiescently consistent objects. It is interesting to consider whether we could build useful systems using quiescent consistency rather than linearizability as the fundamental correctness property, and how the design of such systems would differ from existing system designs.</p></section></section><section><h2 class="h1hd" id="s0065"><a id="st0075"/>3.6 Formal definitions</h2><p class="textfl" id="p0255">We now consider more precise definitions. We focus on linearizability, since it is the property most often used in this book. We leave it as an exercise to provide analogous definitions for quiescent consistency and sequential consistency.</p><p class="text" id="p0260">Informally, a concurrent object is linearizable if each method call appears to take effect instantaneously at some moment between that method's invocation and return events. This statement suffices for most informal reasoning, but a more precise formulation is needed to cover some tricky cases (such as method calls that have not returned), and for more rigorous styles of argument.</p><section><h3 class="h2hd" id="s0070"><a id="st0080"/>3.6.1 Histories</h3><p class="textfl" id="p0265">We model the observable behavior of an execution of a concurrent system by a sequence of <i>events</i> called a <i>history</i>, where an event is an <i>invocation</i> or <i>response</i> of a method. We write a method invocation as <span class="hiddenClass"><mml:math><mml:mo stretchy="false">〈</mml:mo><mml:mi>x</mml:mi><mml:mo>.</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.25em"/><mml:mi>A</mml:mi><mml:mo stretchy="false">〉</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000124/si8.png" style="vertical-align:middle" width="80"/></span>, where <i>x</i> is an object, <i>m</i> is a method name, <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="12" src="images/B9780124159501000124/si9.png" style="vertical-align:middle" width="18"/></span> is a sequence of arguments, and <i>A</i> is a thread. We write a method response as <span class="hiddenClass"><mml:math><mml:mo stretchy="false">〈</mml:mo><mml:mi>x</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.25em"/><mml:mi>A</mml:mi><mml:mo stretchy="false">〉</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000124/si10.png" style="vertical-align:middle" width="80"/></span>, where <i>t</i> is either <i>OK</i> or an exception name, and <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="12" src="images/B9780124159501000124/si11.png" style="vertical-align:middle" width="16"/></span> is a sequence of result values.</p><p class="text" id="p0270">An invocation and a response <i>match</i> if they name the same object and thread. An invocation in <i>H</i> is <i>pending</i> if no matching response follows the invocation. <span aria-label="Page 61" epub:type="pagebreak" id="page_61" role="doc-pagebreak"/>A <i>method call</i> in a history <i>H</i> is a pair consisting of an invocation and either the next matching response in <i>H</i> or a special ⊥ value (pronounced “bottom”) if the invocation is pending (i.e., if there is no subsequent matching response). We say that a method call is <i>pending</i> if its invocation is pending, and that it is <i>complete</i> otherwise. A history is complete if all its method calls are complete. For a history <i>H</i>, we denote the subsequence of <i>H</i> consisting of all events of complete method calls (i.e., eliding all the pending invocations of <i>H</i>) by <span class="hiddenClass"><mml:math><mml:mrow><mml:mi mathvariant="italic">complete</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000124/si12.png" style="vertical-align:middle" width="85"/></span>.</p><p class="text" id="p0275">The <i>interval</i> of a method call in a history <i>H</i> is the history's sequence of events starting from its invocation and ending with its response, or the suffix of <i>H</i> starting from its invocation if the method call is pending. Two method calls <i>overlap</i> if their intervals overlap.</p><p class="text" id="p0280">A history is <i>sequential</i> if its first event is an invocation, and each invocation, except possibly the last, is followed immediately by a matching response, and each response is immediately preceded by an invocation. No method calls overlap in a sequential history, and a sequential history has at most one pending invocation.</p><p class="text" id="p0285">A <i>subhistory</i> of a history <i>H</i> is a subsequence of <i>H</i>. Sometimes we focus on a single thread or object: A <i>thread subhistory</i>, <span class="hiddenClass"><mml:math><mml:mi>H</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>A</mml:mi></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000124/si13.png" style="vertical-align:middle" width="32"/></span> (“<i>H</i> at <i>A</i>”), of a history <i>H</i> is the subsequence of all events in <i>H</i> whose thread names are <i>A</i>. An <i>object subhistory</i> <span class="hiddenClass"><mml:math><mml:mi>H</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>x</mml:mi></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000124/si14.png" style="vertical-align:middle" width="29"/></span> is similarly defined for an object <i>x</i>. We require each thread to complete each method call before calling another method: A history <i>H</i> is <i>well formed</i> if each thread subhistory is sequential. Henceforth, we consider only well-formed histories. Although thread subhistories of a well-formed history are always sequential, object subhistories need not be; method calls to the same object may overlap in a well-formed history. Finally, because what matters in the end is how each thread views the history, we say that two histories are <i>equivalent</i> if every thread has the same thread subhistory in both histories; that is, <i>H</i> and <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si15.png" style="vertical-align:middle" width="20"/></span> are equivalent if <span class="hiddenClass"><mml:math><mml:mi>H</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>A</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>A</mml:mi></mml:math></span><span><img alt="Image" height="16" src="images/B9780124159501000124/si16.png" style="vertical-align:middle" width="86"/></span> for every thread <i>A</i>.</p><p class="text" id="p0290">How can we tell whether a concurrent object is correct? Or, said differently, how do we define correctness for a concurrent object? The basic idea is to require a concurrent execution to be equivalent, in some sense, to some sequential history; the exact sense of equivalence is different for different correctness properties. We assume that we can tell whether a sequential object is correct, that is, whether a sequential object history is a legal history for the object's class. A <i>sequential specification</i> for an object is just a set of legal sequential histories for the object. A sequential history <i>H</i> is <i>legal</i> if each object subhistory is legal for that object.</p><p class="text" id="p0295">A method <i>m</i> of an object <i>x</i> is <i>total</i> if for every finite complete history <i>H</i> in the sequential specification of <i>x</i> and every invocation <span class="hiddenClass"><mml:math><mml:mo stretchy="false">〈</mml:mo><mml:mi>x</mml:mi><mml:mo>.</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.25em"/><mml:mi>A</mml:mi><mml:mo stretchy="false">〉</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000124/si8.png" style="vertical-align:middle" width="80"/></span> of <i>m</i>, there is a response <span class="hiddenClass"><mml:math><mml:mo stretchy="false">〈</mml:mo><mml:mi>x</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.25em"/><mml:mi>A</mml:mi><mml:mo stretchy="false">〉</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000124/si10.png" style="vertical-align:middle" width="80"/></span> such that <span class="hiddenClass"><mml:math><mml:mi>H</mml:mi><mml:mo>⋅</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:mi>x</mml:mi><mml:mo>.</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.25em"/><mml:mi>A</mml:mi><mml:mo stretchy="false">〉</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:mi>x</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.25em"/><mml:mi>A</mml:mi><mml:mo stretchy="false">〉</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000124/si17.png" style="vertical-align:middle" width="196"/></span> is in the sequential specification of <i>x</i>. A method is <i>partial</i> if it is not total.<span aria-label="Page 62" epub:type="pagebreak" id="page_62" role="doc-pagebreak"/></p></section><section><h3 class="h2hd" id="s0075"><a id="st0085"/>3.6.2 Linearizability</h3><p class="textfl" id="p0300">A key concept in defining linearizability is the <i>real-time order</i> of a history. Recall that a <i>(strict) partial order</i> →  on a set <i>X</i> is a relation that is irreflexive and transitive. That is, it is never true that <span class="hiddenClass"><mml:math><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>x</mml:mi></mml:math></span><span><img alt="Image" height="7" src="images/B9780124159501000124/si18.png" style="vertical-align:middle" width="45"/></span>, and whenever <span class="hiddenClass"><mml:math><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>y</mml:mi></mml:math></span><span><img alt="Image" height="10" src="images/B9780124159501000124/si19.png" style="vertical-align:middle" width="45"/></span> and <span class="hiddenClass"><mml:math><mml:mi>y</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>z</mml:mi></mml:math></span><span><img alt="Image" height="10" src="images/B9780124159501000124/si20.png" style="vertical-align:middle" width="43"/></span>, then <span class="hiddenClass"><mml:math><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>z</mml:mi></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si21.png" style="vertical-align:middle" width="44"/></span>. Note that there may be distinct <i>x</i> and <i>y</i> such that neither <span class="hiddenClass"><mml:math><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>y</mml:mi></mml:math></span><span><img alt="Image" height="10" src="images/B9780124159501000124/si19.png" style="vertical-align:middle" width="45"/></span> nor <span class="hiddenClass"><mml:math><mml:mi>y</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>x</mml:mi></mml:math></span><span><img alt="Image" height="10" src="images/B9780124159501000124/si22.png" style="vertical-align:middle" width="44"/></span>. A <i>total order</i> &lt; on <i>X</i> is a partial order such that for all distinct <i>x</i> and <i>y</i> in <i>X</i>, either <span class="hiddenClass"><mml:math><mml:mi>x</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">&lt;</mml:mo><mml:mi>y</mml:mi></mml:math></span><span><img alt="Image" height="10" src="images/B9780124159501000124/si23.png" style="vertical-align:middle" width="39"/></span> or <span class="hiddenClass"><mml:math><mml:mi>y</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">&lt;</mml:mo><mml:mi>x</mml:mi></mml:math></span><span><img alt="Image" height="10" src="images/B9780124159501000124/si24.png" style="vertical-align:middle" width="38"/></span>.</p><p class="text" id="p0305">Any partial order can be extended to a total order. </p><div class="boxg1" id="enun0035"><p class="b1num">Fact 3.6.1 </p><div><p class="b1textfl" id="p0310">If →  is a partial order on <i>X</i>, then there exists a total order &lt; on <i>X</i> such that if <span class="hiddenClass"><mml:math><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>y</mml:mi></mml:math></span><span><img alt="Image" height="10" src="images/B9780124159501000124/si19.png" style="vertical-align:middle" width="45"/></span> then <span class="hiddenClass"><mml:math><mml:mi>x</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">&lt;</mml:mo><mml:mi>y</mml:mi></mml:math></span><span><img alt="Image" height="10" src="images/B9780124159501000124/si23.png" style="vertical-align:middle" width="39"/></span>.</p></div></div><p class="textfl"/><p class="text" id="p0315">We say that a method call <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si25.png" style="vertical-align:middle" width="22"/></span> <i>precedes</i> a method call <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si26.png" style="vertical-align:middle" width="21"/></span> in history <i>H</i> if <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si25.png" style="vertical-align:middle" width="22"/></span> finishes before <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si26.png" style="vertical-align:middle" width="21"/></span> starts, that is, if <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si25.png" style="vertical-align:middle" width="22"/></span>'s response event occurs before <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si26.png" style="vertical-align:middle" width="21"/></span>'s invocation event in <i>H</i>. This notion is important enough to introduce some shorthand notation: Given a history <i>H</i> containing method calls <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si25.png" style="vertical-align:middle" width="22"/></span> and <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si26.png" style="vertical-align:middle" width="21"/></span>, we write <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si27.png" style="vertical-align:middle" width="76"/></span> if <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si25.png" style="vertical-align:middle" width="22"/></span> precedes <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si26.png" style="vertical-align:middle" width="21"/></span> in <i>H</i>. We leave it as an exercise to show that <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si28.png" style="vertical-align:middle" width="30"/></span> is a partial order. Note that if <i>H</i> is sequential, then <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si28.png" style="vertical-align:middle" width="30"/></span> is a total order. Given a history <i>H</i> and an object <i>x</i> such that <span class="hiddenClass"><mml:math><mml:mi>H</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>x</mml:mi></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000124/si14.png" style="vertical-align:middle" width="29"/></span> contains method calls <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si25.png" style="vertical-align:middle" width="22"/></span> and <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si26.png" style="vertical-align:middle" width="21"/></span>, when <i>H</i> is clear from the context, we write <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si29.png" style="vertical-align:middle" width="72"/></span> if <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si25.png" style="vertical-align:middle" width="22"/></span> precedes <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si26.png" style="vertical-align:middle" width="21"/></span> in <span class="hiddenClass"><mml:math><mml:mi>H</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>x</mml:mi></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000124/si14.png" style="vertical-align:middle" width="29"/></span>.</p><p class="text" id="p0320">For linearizability, the basic rule is that if one method call precedes another, then the earlier call must take effect before the later call (each call must linearize within its interval, and the interval of the earlier interval is entirely in front of the interval of the later call). By contrast, if two method calls overlap, then their order is ambiguous, and we are free to order them in any convenient way.</p><p class="text" id="p0325"/><div class="boxg1" id="enun0040"><p class="b1num">Definition 3.6.2 </p><div><p class="b1textfl" id="p0330">A legal sequential history <i>S</i> is a <i>linearization</i> of a history <i>H</i> if <i>H</i> can be extended to a history <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si15.png" style="vertical-align:middle" width="20"/></span> by appending zero or more responses such that: </p><p class="definition" id="p0335"><span class="def_term"><b>L1</b></span> <span class="hiddenClass"><mml:math><mml:mrow><mml:mi mathvariant="italic">complete</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="16" src="images/B9780124159501000124/si30.png" style="vertical-align:middle" width="89"/></span> is equivalent to <i>S</i>, and</p><p class="definition" id="p0340"><span class="def_term"><b>L2</b></span> if method call <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si25.png" style="vertical-align:middle" width="22"/></span> precedes method call <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si26.png" style="vertical-align:middle" width="21"/></span> in <i>H</i>, then the same is true in <i>S</i> (i.e., <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="11" src="images/B9780124159501000124/si31.png" style="vertical-align:middle" width="77"/></span> implies <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si32.png" style="vertical-align:middle" width="72"/></span>).</p><p class="textfl"> <i>H</i> is <i>linearizable</i> if there is a linearization of <i>H</i>.</p></div></div><p class="textfl"/><p class="text" id="p0345">Informally, extending <i>H</i> to <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si15.png" style="vertical-align:middle" width="20"/></span> captures the idea that some pending invocations may have taken effect, even though their responses have not yet been returned to the caller. <a href="#f0050" id="cf0110">Fig. 3.9</a> illustrates the notion: We must complete the pending <img alt="Image" height="9" src="images/B9780124159501000124/fx005.jpg" width="19"/>(<i>x</i>) method call to justify the <img alt="Image" height="11" src="images/B9780124159501000124/fx006.jpg" width="19"/>() call that returns <i>x</i>. The second condition says that if one method call precedes another in the original history, then that ordering must be preserved in the linearization.<span aria-label="Page 63" epub:type="pagebreak" id="page_63" role="doc-pagebreak"/></p><div class="pageavoid"><figure class="fig" id="f0050"><img alt="Image" height="115" src="images/B9780124159501000124/gr009.jpg" width="497"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 3.9</span> The pending <img alt="Image" height="9" src="images/B9780124159501000124/fx005.jpg" width="19"/>(<i>x</i>) method call must take effect early to justify the <img alt="Image" height="11" src="images/B9780124159501000124/fx006.jpg" width="19"/>() call that returns <i>x</i>.</div></figcaption></figure></div></section><section><h3 class="h2hd" id="s0080"><a id="st0090"/>3.6.3 Linearizability is compositional</h3><p class="textfl" id="p0350">Linearizability is compositional.</p><p class="text" id="p0355"/><div class="boxg1" id="enun0045"><p class="b1num">Theorem 3.6.3 </p><div><p class="b1textfl" id="p0360"><i>H</i> is linearizable if, and only if, for each object <i>x</i>, <span class="hiddenClass"><mml:math><mml:mi>H</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>x</mml:mi></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000124/si14.png" style="vertical-align:middle" width="29"/></span> is linearizable.</p></div></div><p class="textfl"> </p><div class="boxg1" id="enun0050"><p class="b1num">Proof </p><div><p class="b1textfl" id="p0365">The “only if” part is left as an exercise.</p><p class="b1text" id="p0370">For each object <i>x</i>, pick a linearization of <span class="hiddenClass"><mml:math><mml:mi>H</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>x</mml:mi></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000124/si14.png" style="vertical-align:middle" width="29"/></span>. Let <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si33.png" style="vertical-align:middle" width="19"/></span> be the set of responses appended to <span class="hiddenClass"><mml:math><mml:mi>H</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>x</mml:mi></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000124/si14.png" style="vertical-align:middle" width="29"/></span> to construct that linearization, and let <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si34.png" style="vertical-align:middle" width="26"/></span> be the corresponding linearization order. Let <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si15.png" style="vertical-align:middle" width="20"/></span> be the history constructed by appending to <i>H</i> each response in <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si33.png" style="vertical-align:middle" width="19"/></span> (the order in which they are appended does not matter).</p><p class="b1text" id="p0375">We argue by induction on the number of method calls in <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si15.png" style="vertical-align:middle" width="20"/></span>. For the base case, if <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si15.png" style="vertical-align:middle" width="20"/></span> contains no method calls, we are done. Otherwise, assume the claim for every <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si15.png" style="vertical-align:middle" width="20"/></span> containing fewer than <span class="hiddenClass"><mml:math><mml:mi>k</mml:mi><mml:mo>⩾</mml:mo><mml:mn>1</mml:mn></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000124/si35.png" style="vertical-align:middle" width="36"/></span> method calls. For each object <i>x</i>, consider the last method call in <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>x</mml:mi></mml:math></span><span><img alt="Image" height="16" src="images/B9780124159501000124/si36.png" style="vertical-align:middle" width="33"/></span>. One of these calls <i>m</i> must be maximal with respect to <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="9" src="images/B9780124159501000124/si28.png" style="vertical-align:middle" width="30"/></span>; that is, there is no <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si37.png" style="vertical-align:middle" width="19"/></span> such that <span class="hiddenClass"><mml:math><mml:mi>m</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="16" src="images/B9780124159501000124/si38.png" style="vertical-align:middle" width="67"/></span>. Let <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si39.png" style="vertical-align:middle" width="18"/></span> be the history defined by removing <i>m</i> from <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si15.png" style="vertical-align:middle" width="20"/></span>. Because <i>m</i> is maximal, <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si15.png" style="vertical-align:middle" width="20"/></span> is equivalent to <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mi>m</mml:mi></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si40.png" style="vertical-align:middle" width="41"/></span>. By the induction hypothesis, <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si39.png" style="vertical-align:middle" width="18"/></span> is linearizable to a sequential history <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si41.png" style="vertical-align:middle" width="15"/></span>, and both <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si15.png" style="vertical-align:middle" width="20"/></span> and <i>H</i> are linearizable to <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mi>m</mml:mi></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si42.png" style="vertical-align:middle" width="38"/></span>. □</p></div></div><p class="textfl"/></section><section><h3 class="h2hd" id="s0085"><a id="st0095"/>3.6.4 Linearizability is nonblocking</h3><p class="textfl" id="p0380">Linearizability is a <i>nonblocking</i> property: A pending invocation of a total method is never required to wait for another pending invocation to complete.</p><p class="text" id="p0385"/><div class="boxg1" id="enun0055"><p class="b1num">Theorem 3.6.4 </p><div><p class="b1textfl" id="p0390">If <i>m</i> is a total method of an object <i>x</i> and <span class="hiddenClass"><mml:math><mml:mo stretchy="false">〈</mml:mo><mml:mi>x</mml:mi><mml:mo>.</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.25em"/><mml:mi>P</mml:mi><mml:mo stretchy="false">〉</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000124/si43.png" style="vertical-align:middle" width="81"/></span> is a pending invocation in a linearizable history <i>H</i>, then there exists a response <span class="hiddenClass"><mml:math><mml:mo stretchy="false">〈</mml:mo><mml:mi>x</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.25em"/><mml:mi>P</mml:mi><mml:mo stretchy="false">〉</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000124/si44.png" style="vertical-align:middle" width="80"/></span> such that <span class="hiddenClass"><mml:math><mml:mi>H</mml:mi><mml:mo>⋅</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:mi>x</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.25em"/><mml:mi>P</mml:mi><mml:mo stretchy="false">〉</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000124/si45.png" style="vertical-align:middle" width="106"/></span> is linearizable.</p></div></div><p class="textfl"> </p><div class="boxg1" id="enun0060"><p class="b1num">Proof </p><div><p class="b1textfl" id="p0395">Let <i>S</i> be any linearization of <i>H</i>. If <i>S</i> includes a response <span class="hiddenClass"><mml:math><mml:mo stretchy="false">〈</mml:mo><mml:mi>x</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.25em"/><mml:mi>P</mml:mi><mml:mo stretchy="false">〉</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000124/si44.png" style="vertical-align:middle" width="80"/></span> to <span class="hiddenClass"><mml:math><mml:mo stretchy="false">〈</mml:mo><mml:mi>x</mml:mi><mml:mo>.</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.25em"/><mml:mi>P</mml:mi><mml:mo stretchy="false">〉</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000124/si43.png" style="vertical-align:middle" width="81"/></span>, we are done, since <i>S</i> is also a linearization of <span class="hiddenClass"><mml:math><mml:mi>H</mml:mi><mml:mo>⋅</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:mi>x</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.25em"/><mml:mi>P</mml:mi><mml:mo stretchy="false">〉</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000124/si45.png" style="vertical-align:middle" width="106"/></span>. Otherwise, <span class="hiddenClass"><mml:math><mml:mo stretchy="false">〈</mml:mo><mml:mi>x</mml:mi><mml:mo>.</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.25em"/><mml:mi>P</mml:mi><mml:mo stretchy="false">〉</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000124/si43.png" style="vertical-align:middle" width="81"/></span> does not appear in <i>S</i> either, since a linearization, by definition, has no pending invocations. Because the method is total, there exists a response <span class="hiddenClass"><mml:math><mml:mo stretchy="false">〈</mml:mo><mml:mi>x</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.25em"/><mml:mi>P</mml:mi><mml:mo stretchy="false">〉</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000124/si44.png" style="vertical-align:middle" width="80"/></span> such that</p><p class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo linebreak="badbreak" linebreakstyle="after">=</mml:mo><mml:mi>S</mml:mi><mml:mo>⋅</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:mi>x</mml:mi><mml:mo>.</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.25em"/><mml:mi>P</mml:mi><mml:mo stretchy="false">〉</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:mi>x</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.25em"/><mml:mi>P</mml:mi><mml:mo stretchy="false">〉</mml:mo></mml:math></p><div class="showClass"><p class="fig"><img alt="Image" height="22" src="images/B9780124159501000124/si46.png" width="308"/><a id="deq4"/></p></div><p class="b1textfl"> is a legal sequential history. <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si41.png" style="vertical-align:middle" width="15"/></span> is a linearization of <span class="hiddenClass"><mml:math><mml:mi>H</mml:mi><mml:mo>⋅</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:mi>x</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>⁎</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.25em"/><mml:mi>P</mml:mi><mml:mo stretchy="false">〉</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000124/si45.png" style="vertical-align:middle" width="106"/></span>, and hence is also a linearization of <i>H</i>. □</p></div></div><p class="textfl"/><p class="text" id="p0400">This theorem implies that linearizability by itself never forces a thread with a pending invocation of a total method to block. Of course, blocking (or even deadlock) may occur as artifacts of particular implementations of linearizability, but it is not inherent to the correctness property itself. This theorem suggests that linearizability is an appropriate correctness property for systems where concurrency and real-time response are important.</p><p class="text" id="p0405"><span aria-label="Page 64" epub:type="pagebreak" id="page_64" role="doc-pagebreak"/>The nonblocking property does not rule out blocking in situations where it is explicitly intended. For example, it may be sensible for a thread attempting to dequeue from an empty queue to block, waiting until another thread enqueues an item. A queue specification would capture this intention by making the <img alt="Image" height="11" src="images/B9780124159501000124/fx006.jpg" width="19"/>() method's specification partial, leaving its effect undefined when applied to an empty queue. The most natural concurrent interpretation of a partial sequential specification is simply to wait until the object reaches a state in which that method call's response is defined.</p></section></section><section><h2 class="h1hd" id="s0090"><a id="st0100"/>3.7 Memory consistency models</h2><p class="textfl" id="p0410">We can consider the memory read and written by a program as a single object—the composition of many registers—shared by all threads of the program. This shared memory is often the only means of communication among threads (i.e., the only way that threads can observe the effects of other threads). Its correctness property is called the <i>memory consistency model</i>, or <i>memory model</i> for short.</p><p class="text" id="p0415">Early concurrent programs assumed sequentially consistent memory. Indeed, the notion of sequential consistency was introduced to capture the assumptions implicit in those programs. However, the memory of most modern multiprocessor systems is <i>not</i> sequentially consistent: Compilers and hardware may reorder memory reads and writes in complex ways. Most of the time no one can tell, because the vast majority of reads and writes are not used for synchronization. These systems also provide synchronization primitives that inhibit reordering.</p><p class="text" id="p0420">We follow this approach in the first part of this book, where we focus on the principles of multiprocessor programming. For example, the pseudocode for the various lock algorithms in Chapter <a href="B9780124159501000112.xhtml">2</a> assumes that if a thread writes two locations, one after the other, then the two writes are made visible to other threads in the same order, so that any thread that sees the later write will also see the earlier write. However, Java does not guarantee this ordering for ordinary reads and writes. As mentioned in Pragma <a href="B9780124159501000112.xhtml">2.3.1</a>, these locations would need to be declared <img alt="Image" height="9" src="images/B9780124159501000124/fx013.jpg" width="53"/> to work in real systems. We omit these declarations because these algorithms are not practical in any case, and the declarations would clutter the code and obscure the ideas embodied in those algorithms. In the second part of the book, where we discuss practical algorithms, we include these declarations. (We describe the Java memory model in Appendix <a href="B9780124159501000318.xhtml">A.3</a>.)<span aria-label="Page 65" epub:type="pagebreak" id="page_65" role="doc-pagebreak"/></p></section><section><h2 class="h1hd" id="s0095"><a id="st0105"/>3.8 Progress conditions</h2><p class="textfl" id="p0425">The nonblocking property of linearizability (and sequential consistency and quiescent consistency) ensures that any pending invocation has a correct response. But linearizability does not tell us how to compute such a response, nor even require an implementation to produce a response at all. Consider, for example, the lock-based queue shown in <a href="#f0010" id="cf0115">Fig. 3.1</a>. Suppose the queue is initially empty, and thread <i>A</i> halts halfway through enqueuing <i>x</i>, while holding the lock, and <i>B</i> then invokes <img alt="Image" height="11" src="images/B9780124159501000124/fx006.jpg" width="19"/>(). The nonblocking property guarantees that there is a correct response to <i>B</i>'s call to <img alt="Image" height="11" src="images/B9780124159501000124/fx006.jpg" width="19"/>(); indeed, there are two: It could throw an exception or return <i>x</i>. In this implementation, however, <i>B</i> is unable to acquire the lock, and will be delayed as long as <i>A</i> is delayed.</p><p class="text" id="p0430">Such an implementation is called <i>blocking</i>, because delaying one thread can prevent others from making progress. Unexpected thread delays are common in multiprocessors. A cache miss might delay a processor for a hundred cycles, a page fault for a few million cycles, preemption by the operating system for hundreds of millions of cycles. These delays depend on the specifics of the machine and the operating system. The part of the system that determines when threads take steps is called the <i>scheduler</i>, and the order in which threads take steps is the <i>schedule</i>.</p><p class="text" id="p0435">In this section, we consider <i>progress conditions</i>, which require implementations to produce responses to pending invocations. Ideally, we would like to say simply that every pending invocation gets a response. Of course, this is not possible if the threads with pending invocations stop taking steps. So we require progress only for those threads that keep taking steps.</p><section><h3 class="h2hd" id="s0100"><a id="st0110"/>3.8.1 Wait-freedom</h3><p class="textfl" id="p0440">A method of an object implementation is <i>wait-free</i> if every call finishes its execution in a finite number of steps; that is, if a thread with a pending invocation to a wait-free method keeps taking steps, it completes in a finite number of steps. We say that an object implementation is <i>wait-free</i> if all its methods are wait-free, and that a class is <i>wait-free</i> if every object of that class is wait-free.</p><p class="text" id="p0445">The queue shown in <a href="#f0020" id="cf0120">Fig. 3.3</a> is wait-free. For example, if <i>B</i> invokes <img alt="Image" height="11" src="images/B9780124159501000124/fx006.jpg" width="19"/>() while <i>A</i> is halted halfway through enqueuing <i>x</i>, then <i>B</i> will either throw <img alt="Image" height="11" src="images/B9780124159501000124/fx007.jpg" width="91"/> (if <i>A</i> halted before incrementing <img alt="Image" height="9" src="images/B9780124159501000124/fx003.jpg" width="23"/>) or it will return <i>x</i> (if <i>A</i> halted afterward). In contrast, the lock-based queue is not wait-free because <i>B</i> may take an unbounded number of steps unsuccessfully trying to acquire the lock.</p><p class="text" id="p0450">We say that wait-freedom is a <i>nonblocking</i> progress condition<sup><a epub:type="noteref" href="#fn002" id="cf0125" role="doc-noteref">2</a></sup> because a wait-free implementation cannot be blocking: An arbitrary delay by one thread (say, one holding a lock) cannot prevent other threads from making progress.</p></section><section><h3 class="h2hd" id="s0105"><a id="st0115"/>3.8.2 Lock-freedom</h3><p class="textfl" id="p0455">Wait-freedom is attractive because it guarantees that every thread that takes steps makes progress. However, wait-free algorithms can be inefficient, and sometimes we are willing to settle for a weaker progress guarantee.</p><p class="text" id="p0460">One way to relax the progress condition is to guarantee progress only to <i>some</i> thread, rather than <i>every</i> thread. A method of an object implementation is <i>lock-free</i> if executing the method guarantees that <i>some</i> method <span aria-label="Page 66" epub:type="pagebreak" id="page_66" role="doc-pagebreak"/>call finishes in a finite number of steps; that is, if a thread with a pending invocation to a lock-free method keeps taking steps, then within a finite number of its steps, some pending call to a method of that object (not necessarily the lock-free method) completes. An object implementation is <i>lock-free</i> if all its methods are lock-free. We say that lock-freedom guarantees <i>minimal progress</i> because executing a lock-free method guarantees that the system as a whole makes progress, but not that any thread in particular makes progress. In contrast, wait-freedom guarantees <i>maximal progress</i>: Every thread that keeps taking steps makes progress.</p><p class="text" id="p0465">Clearly, any wait-free method implementation is also lock-free, but not vice versa. Although lock-freedom is weaker than wait-freedom, if a program executes only a finite number of method calls, then lock-freedom is equivalent to wait-freedom for that program.</p><p class="text" id="p0470">Lock-free algorithms admit the possibility that some threads could starve. As a practical matter, there are many situations in which starvation, while possible, is extremely unlikely, so a fast lock-free algorithm may be preferable to a slower wait-free algorithm. We consider several lock-free concurrent objects in later chapters.</p><p class="text" id="p0475">Lock-freedom is also a nonblocking progress condition: A delayed thread does not prevent other threads from making progress as long as the system as a whole keeps taking steps.</p></section><section><h3 class="h2hd" id="s0110"><a id="st0120"/>3.8.3 Obstruction-freedom</h3><p class="textfl" id="p0480">Another way to relax the progress condition is to guarantee progress only under certain assumptions about how threads are scheduled, that is, about the order in which threads take steps. For example, an implementation may guarantee progress only if no other threads actively interfere with it. We say that a thread executes <i>in isolation</i> in an interval if no other threads take steps in that interval. A method of an object implementation is <i>obstruction-free</i> if, from any point after which it executes in isolation, it finishes in a finite number of steps; that is, if a thread with a pending invocation to an obstruction-free method executes in isolation from any point (not necessarily from its invocation), it completes in a finite number of steps.</p><p class="text" id="p0485">Like other nonblocking progress conditions, obstruction-freedom ensures that a thread cannot be blocked by the delay of other threads. Obstruction-freedom guarantees progress to every thread that executes in isolation, so like wait-freedom, it guarantees maximal progress.</p><p class="text" id="p0490">By guaranteeing progress only when one thread is scheduled to execute in isolation (i.e., preventing other threads from taking steps concurrently), obstruction-freedom seems to defy most operating system schedulers, which try to ensure a schedule in which every thread keeps taking steps (such a schedule is called <i>fair</i>). In practice, however, there is no problem. Ensuring progress for an obstruction-free method does not require pausing all other threads, only those threads that <i>conflict</i>, meaning those that are executing method calls on the same object. In later chapters, we consider a variety of <i>contention management techniques</i> to reduce or eliminate conflicting concurrent method calls. The simplest such technique <span aria-label="Page 67" epub:type="pagebreak" id="page_67" role="doc-pagebreak"/>is to introduce a <i>back-off</i> mechanism: a thread that detects a conflict pauses to give an earlier thread time to finish. Choosing when to back off, and for how long, is discussed in detail in Chapter <a href="B9780124159501000173.xhtml">7</a>.</p></section><section><h3 class="h2hd" id="s0115"><a id="st0125"/>3.8.4 Blocking progress conditions</h3><p class="textfl" id="p0495">In Chapter <a href="B9780124159501000112.xhtml">2</a>, we defined two progress conditions for lock implementations: <i>deadlock-freedom</i> and <i>starvation-freedom</i>. Analogous to lock-freedom and wait-freedom, respectively, deadlock-freedom guarantees that some thread makes progress and starvation-freedom guarantees that every thread makes progress <i>provided the lock is not held by some other thread</i>. The caveat that the lock is not held is necessary because, while one thread holds the lock, no other thread can acquire it without violating mutual exclusion, the correctness property for locks. To guarantee progress, we must also assume that a thread holding a lock will eventually release it. This assumption has two parts: (a) Each thread that acquires the lock must release it after a finite number of steps, and (b) the scheduler must allow a thread holding the lock to keep taking steps.</p><p class="text" id="p0500">We can generalize deadlock-freedom and starvation-freedom to concurrent objects by making a similar assumption for threads executing method calls. Specifically, we assume that the scheduler is fair; that is, it allows every thread with a pending method call to take steps. (The first part of the assumption must be guaranteed by the concurrent object implementation.) We say that a method of an object implementation is <i>starvation-free</i> if it completes in a finite number of steps provided that every thread with a pending method call keeps taking steps. We say that a method of an object implementation is <i>deadlock-free</i> if, whenever there is a pending call to that method and every thread with a pending method call keeps taking steps, some method call completes in a finite number of steps.</p><p class="text" id="p0505">Deadlock-freedom and starvation-freedom are useful progress conditions when the operating system guarantees that every thread keeps taking steps, and particularly that each thread takes a step <i>in a timely manner</i>. We say these properties are <i>blocking progress conditions</i> because they admit blocking implementations where the delay of a single thread can prevent all other threads from making progress.</p><p class="text" id="p0510">A class whose methods rely on lock-based synchronization can guarantee, at best, a blocking progress condition. Does this observation mean that lock-based algorithms should be avoided? Not necessarily. If preemption in the middle of a critical section is sufficiently rare, then blocking progress conditions may be effectively indistinguishable from their nonblocking counterparts. If preemption is common enough to cause concern, or if the cost of preemption-based delay is sufficiently high, then it is sensible to consider nonblocking progress conditions.<span aria-label="Page 68" epub:type="pagebreak" id="page_68" role="doc-pagebreak"/></p></section><section><h3 class="h2hd" id="s0120"><a id="st0130"/>3.8.5 Characterizing progress conditions</h3><p class="textfl" id="p0515">We now consider the various progress conditions and how they relate to one another. For example, wait-freedom and lock-freedom guarantee progress no matter how threads are scheduled. We say that they are <i>independent</i> progress conditions. By contrast, obstruction-freedom, starvation-freedom, and deadlock-freedom are all <i>dependent</i> progress conditions, where progress is guaranteed only if the underlying operating system satisfies certain properties: fair scheduling for starvation-freedom and deadlock-freedom, isolated execution for obstruction-freedom. Also, as we already discussed, wait-freedom, lock-freedom, and obstruction-freedom are all nonblocking progress conditions, whereas starvation-freedom and deadlock-freedom are blocking.</p><p class="text" id="p0520">We can also characterize these progress conditions by whether they guarantee maximal or minimal progress under their respective system assumptions: Wait-freedom, starvation-freedom, and obstruction-freedom guarantee maximal progress, whereas lock-freedom and deadlock-freedom guarantee only minimal progress.</p><p class="text" id="p0525"><a href="#f0055" id="cf0135">Fig. 3.10</a> summarizes this discussion with a table that shows the various progress conditions and their properties. There is a “hole” in this table because any condition that guarantees minimal progress to threads that execute in isolation also guarantees maximal progress to these threads.</p><div class="pageavoid"><figure class="fig" id="f0055"><img alt="Image" height="194" src="images/B9780124159501000124/gr010.jpg" width="496"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 3.10</span> Progress conditions and their properties.</div></figcaption></figure></div><p class="text" id="p0530">Picking a progress condition for a concurrent object implementation depends on both the needs of the application and the characteristics of the underlying platform. Wait-freedom and lock-freedom have strong theoretical properties, they work on just about any platform, and they provide guarantees useful to real-time applications such as music, electronic games, and other interactive applications. The dependent obstruction-free, deadlock-free, and starvation-free properties rely on guarantees provided by the underlying platform. Given those guarantees, however, the dependent properties often admit simpler and more efficient implementations.<span aria-label="Page 69" epub:type="pagebreak" id="page_69" role="doc-pagebreak"/></p></section></section><section><h2 class="h1hd" id="s0125"><a id="st0135"/>3.9 Remarks</h2><p class="textfl" id="p0535">Which correctness condition is right for your application? It depends on your application's needs. A lightly loaded printer server might be satisfied with a quiescently consistent queue of jobs, since the order in which documents are printed is of little importance. A banking server that must execute customer requests in program order (e.g., transfer $100 from savings to checking, then write a check for $50), might require a sequentially consistent queue. A stock trading server that is required to be fair, ensuring that orders from different customers must be executed in the order they arrive, would require a linearizable queue.</p><p class="text" id="p0540">Which progress condition is right for your application? Again, it depends on the application's needs. In a way, this is a trick question. Different methods, even ones for the same object, might have different progress conditions. For example, the table lookup method of a firewall program, which checks whether a packet source is suspect, is called frequently and is time-critical, so we might want it to be wait-free. By contrast, the method that updates table entries, which is rarely called, might be implemented using mutual exclusion. As we shall see, it is quite natural to write applications whose methods differ in their progress guarantees.</p><p class="text" id="p0545">So what progress condition is right for a particular operation? Programmers typically intend any operation they execute to eventually complete. That is, they want maximal progress. However, ensuring progress requires assumptions about the underlying platform. For example, how does the operating system schedule threads for execution? The choice of progress condition reflects what the programmer is willing to assume to guarantee that an operation will complete. For any progress guarantee, the programmer must assume that the thread executing the operation is eventually scheduled. For certain critical operations, the programmer may be unwilling to assume more than that, incurring extra overhead to ensure progress. For other operations, stronger assumptions, such as fairness or a particular priority scheme for scheduling, may be acceptable, enabling less expensive solutions.</p><p class="text" id="p0550">The following joke circulated in Italy in the 1920s: According to Mussolini, the ideal citizen is intelligent, honest, and fascist. Unfortunately, no one is perfect, which explains why everyone you meet is either intelligent and fascist but not honest, honest and fascist but not intelligent, or honest and intelligent but not fascist.</p><p class="text" id="p0555">As programmers, it would be ideal to have linearizable hardware, linearizable data structures, and good performance. Unfortunately, technology is imperfect, and for the time being, hardware that performs well is usually not even sequentially consistent. As the joke goes, that leaves open the possibility that data structures might still be linearizable while performing well. Nevertheless, there are many challenges to make this vision work, and the remainder of this book presents a road map toward attaining this goal.<span aria-label="Page 70" epub:type="pagebreak" id="page_70" role="doc-pagebreak"/></p></section><section><h2 class="h1hd" id="s0130"><a id="st0140"/>3.10 Chapter notes</h2><p class="textfl" id="p0560">Leslie Lamport <a epub:type="noteref" href="#br0510" id="cf0140" role="doc-noteref">[102]</a> introduced the notion of <i>sequential consistency</i>, while Christos Papadimitriou <a epub:type="noteref" href="#br0685" id="cf0145" role="doc-noteref">[137]</a> formulated the canonical formal characterization of <i>serializability</i>. William Weihl <a epub:type="noteref" href="#br0830" id="cf0150" role="doc-noteref">[166]</a> was the first to point out the importance of <i>compositionality</i> (which he called <i>locality</i>). Maurice Herlihy and Jeannette Wing <a epub:type="noteref" href="#br0375" id="cf0155" role="doc-noteref">[75]</a> introduced the notion of <i>linearizability</i>. <i>Quiescent consistency</i> was introduced implicitly by James Aspnes, Maurice Herlihy, and Nir Shavit <a epub:type="noteref" href="#br0070" id="cf0160" role="doc-noteref">[14]</a>, and more explicitly by Nir Shavit and Asaph Zemach <a epub:type="noteref" href="#br0790" id="cf0165" role="doc-noteref">[158]</a>. Leslie Lamport <a epub:type="noteref" href="#br0495" id="cs0025" role="doc-noteref">[99</a>,<a epub:type="noteref" href="#br0525" role="doc-noteref">105]</a> introduced the notion of an <i>atomic register</i>.</p><p class="text" id="p0565">The two-thread queue is considered folklore; as far as we are aware, it first appeared in print in a paper by Leslie Lamport <a epub:type="noteref" href="#br0515" id="cf0170" role="doc-noteref">[103]</a>.</p><p class="text" id="p0570">To the best of our knowledge, the notion of <i>wait-freedom</i> first appeared implicitly in Leslie Lamport's Bakery algorithm <a epub:type="noteref" href="#br0500" id="cf0175" role="doc-noteref">[100]</a>. <i>Lock-freedom</i> has had several historical meanings and only in recent years has it converged to its current definition. The notions of <i>dependent progress</i> and of <i>minimal</i> and <i>maximal progress</i> and the table of progress conditions were introduced by Maurice Herlihy and Nir Shavit <a epub:type="noteref" href="#br0360" id="cf0180" role="doc-noteref">[72]</a>. <i>Obstruction-freedom</i> was introduced by Maurice Herlihy, Victor Luchangco, and Mark Moir <a epub:type="noteref" href="#br0340" id="cf0185" role="doc-noteref">[68]</a>.</p></section><section><h2 class="h1hd" id="s0135"><a id="st0145"/>3.11 Exercises</h2><p class="textfl" id="p0575"/><div class="boxg1" id="enun0065"><p class="b1num">Exercise 3.1 </p><div><p class="b1textfl" id="p0580">Explain why quiescent consistency is compositional.</p></div></div><p class="textfl"/><p class="text" id="p0585"/><div class="boxg1" id="enun0070"><p class="b1num">Exercise 3.2 </p><div><p class="b1textfl" id="p0590">Consider a <i>memory object</i> that encompasses two register components. We know that if both registers are quiescently consistent, then so is the memory. Does the converse hold? That is, if the memory is quiescently consistent, are the individual registers quiescently consistent? Outline a proof, or give a counterexample.</p></div></div><p class="textfl"/><p class="text" id="p0595"/><div class="boxg1" id="enun0075"><p class="b1num">Exercise 3.3 </p><div><p class="b1textfl" id="p0600">Give an example of an execution that is quiescently consistent but not sequentially consistent, and another that is sequentially consistent but not quiescently consistent.</p></div></div><p class="textfl"/><p class="text" id="p0605"/><div class="boxg1" id="enun0080"><p class="b1num">Exercise 3.4 </p><div><p class="b1textfl" id="p0610">For each of the histories shown in <a href="#f0060" id="cs0030">Figs. 3.11</a> and <a href="#f0065">3.12</a>, are they quiescently consistent? Sequentially consistent? Linearizable? Justify your answer.</p><div class="pageavoid"><figure class="fig" id="f0060"><img alt="Image" height="178" src="images/B9780124159501000124/gr011.jpg" width="497"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 3.11</span> First history for Exercise <a href="#enun0080" id="cf0010">3.4</a>.</div></figcaption></figure></div><div class="pageavoid"><figure class="fig" id="f0065"><img alt="Image" height="182" src="images/B9780124159501000124/gr012.jpg" width="497"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 3.12</span> Second history for Exercise <a href="#enun0080" id="cf0015">3.4</a>.</div></figcaption></figure></div></div></div><p class="textfl"/><p class="text" id="p0615"/><div class="boxg1" id="enun0085"><p class="b1num">Exercise 3.5 </p><div><p class="b1textfl" id="p0620">If we drop condition L2 from the linearizability definition, is the resulting property the same as sequential consistency? Explain.</p></div></div><p class="textfl"/><p class="text" id="p0625"><span aria-label="Page 71" epub:type="pagebreak" id="page_71" role="doc-pagebreak"/></p><div class="boxg1" id="enun0090"><p class="b1num">Exercise 3.6 </p><div><p class="b1textfl" id="p0630">Prove the “only if” part of <a href="#enun0045" id="cf0190">Theorem 3.6.3</a>.</p></div></div><p class="textfl"/><p class="text" id="p0635"/><div class="boxg1" id="enun0095"><p class="b1num">Exercise 3.7 </p><div><p class="b1textfl" id="p0640">The <img alt="Image" height="11" src="images/B9780124159501000124/fx014.jpg" width="87"/> class (in the <span class="sans-serif">java.util.concurrent.atomic</span> package) is a container for an integer value. One of its methods is</p><div class="pageavoid"><figure class="fig" id="f0070"><img alt="Image" class="img" height="11" src="images/B9780124159501000124/fx015.jpg" width="276"/></figure></div><p class="textfl"/><p class="b1text" id="p0645">This method compares the object's current value with <img alt="Image" height="11" src="images/B9780124159501000124/fx016.jpg" width="39"/>. If the values are equal, then it atomically replaces the object's value with <img alt="Image" height="11" src="images/B9780124159501000124/fx017.jpg" width="38"/> and returns <i>true</i>. Otherwise, it leaves the object's value unchanged, and returns <i>false</i>. This class also provides</p><div class="pageavoid"><figure class="fig" id="f0075"><img alt="Image" class="img" height="11" src="images/B9780124159501000124/fx018.jpg" width="51"/></figure></div><p class="textfl"> which returns the object's value.</p><p class="b1text" id="p0650">Consider the FIFO queue implementation shown in <a href="#f0080" id="cf0195">Fig. 3.13</a>. It stores its items in an array <img alt="Image" height="9" src="images/B9780124159501000124/fx001.jpg" width="31"/>, which, for simplicity, we assume has unbounded size. It has two <img alt="Image" height="11" src="images/B9780124159501000124/fx014.jpg" width="87"/> fields: <img alt="Image" height="9" src="images/B9780124159501000124/fx002.jpg" width="25"/> is the index of the next slot from which to remove an item, and <img alt="Image" height="9" src="images/B9780124159501000124/fx003.jpg" width="23"/> is the index of the next slot in which to place an item. Give an example showing that this implementation is <i>not</i> linearizable.</p><div class="pageavoid"><figure class="fig" id="f0080"><img alt="Image" height="372" src="images/B9780124159501000124/gr013.jpg" width="322"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 3.13</span> <img alt="Image" height="11" src="images/B9780124159501000124/fx019.jpg" width="38"/> implementation for Exercise <a href="#enun0095" id="cf0020">3.7</a>.</div></figcaption></figure></div></div></div><p class="textfl"/><p class="text" id="p0655"/><div class="boxg1" id="enun0100"><p class="b1num">Exercise 3.8 </p><div><p class="b1textfl" id="p0660">Consider the following rather unusual implementation of a method <i>m</i>: In every history, the <i>i</i>-th time a thread calls <i>m</i>, the call returns after <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000124/si47.png" style="vertical-align:middle" width="14"/></span> steps. Is this method wait-free?</p></div></div><p class="textfl"/><p class="text" id="p0665"/><div class="boxg1" id="enun0105"><p class="b1num">Exercise 3.9 </p><div><p class="b1textfl" id="p0670">Consider a system with an object <i>x</i> and <i>n</i> threads. Determine if each of the following properties are equivalent to saying <i>x</i> is deadlock-free, starvation-free, obstruction-free, lock-free, wait-free, or none of these. Briefly justify your answers.</p><div><ol><li class="b1numlist" id="o0010">1.  For every infinite history <i>H</i> of <i>x</i>, an infinite number of method calls complete.</li><li class="b1numlist" id="o0015">2.  For every finite history <i>H</i> of <i>x</i>, there is an infinite history <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>H</mml:mi><mml:mo>⋅</mml:mo><mml:mi>G</mml:mi></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si48.png" style="vertical-align:middle" width="77"/></span>.</li><li class="b1numlist" id="o0020">3.  For every infinite history <i>H</i> of <i>x</i>, every thread takes an infinite number of steps.</li><li class="b1numlist" id="o0025">4.  For every infinite history <i>H</i> of <i>x</i>, every thread that takes an infinite number of steps in <i>H</i> completes an infinite number of method calls. <span aria-label="Page 72" epub:type="pagebreak" id="page_72" role="doc-pagebreak"/></li><li class="b1numlist" id="o0030">5.  For every finite history <i>H</i> of <i>x</i>, there are <i>n</i> infinite histories <span class="hiddenClass"><mml:math><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>H</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="19" src="images/B9780124159501000124/si49.png" style="vertical-align:middle" width="81"/></span> where only thread <i>i</i> takes steps in <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si50.png" style="vertical-align:middle" width="18"/></span>, where it completes an infinite number of method calls.</li><li class="b1numlist" id="o0035">6.  For every finite history <i>H</i> of <i>x</i>, there is an infinite history <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>H</mml:mi><mml:mo>⋅</mml:mo><mml:mi>G</mml:mi></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000124/si48.png" style="vertical-align:middle" width="77"/></span> where every thread completes an infinite number of method calls in <i>G</i>.</li></ol></div><p class="b1textfl"/></div></div><p class="textfl"/><p class="text" id="p0705"/><div class="boxg1" id="enun0110"><p class="b1num">Exercise 3.10 </p><div><p class="b1textfl" id="p0710">This exercise examines the queue implementation in <a href="#f0085" id="cf0200">Fig. 3.14</a>, whose <img alt="Image" height="9" src="images/B9780124159501000124/fx005.jpg" width="19"/>() method does not have a single fixed linearization point in the code.</p><div class="pageavoid"><figure class="fig" id="f0085"><img alt="Image" height="470" src="images/B9780124159501000124/gr014.jpg" width="460"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 3.14</span> Herlihy–Wing queue for Exercise <a href="#enun0110" id="cf0025">3.10</a>.</div></figcaption></figure></div><p class="b1text" id="p0715">The queue stores its items in an <img alt="Image" height="9" src="images/B9780124159501000124/fx001.jpg" width="31"/> array, which, for simplicity, we assume is unbounded. The <img alt="Image" height="9" src="images/B9780124159501000124/fx003.jpg" width="23"/> field is an <img alt="Image" height="11" src="images/B9780124159501000124/fx014.jpg" width="87"/>, initially zero.</p><p class="b1text" id="p0720">The <img alt="Image" height="9" src="images/B9780124159501000124/fx005.jpg" width="19"/>() method reserves a slot by incrementing <img alt="Image" height="9" src="images/B9780124159501000124/fx003.jpg" width="23"/>, and then stores the item at that location. Note that these two steps are not atomic: There is an interval after <img alt="Image" height="9" src="images/B9780124159501000124/fx003.jpg" width="23"/> has been incremented but before the item has been stored in the array.</p><p class="b1text" id="p0725">The <img alt="Image" height="11" src="images/B9780124159501000124/fx006.jpg" width="19"/>() method reads the value of <img alt="Image" height="9" src="images/B9780124159501000124/fx003.jpg" width="23"/>, and then traverses the array in ascending order from slot zero to the tail. For each slot, it swaps <i>null</i> with the current contents, returning the first non-<i>null</i> item it finds. If all slots are <i>null</i>, the procedure is restarted.</p><div><ul><li class="b1bulllist" id="u0010">•  Give an execution showing that the linearization point for <img alt="Image" height="9" src="images/B9780124159501000124/fx005.jpg" width="19"/>()<span aria-label="Page 73" epub:type="pagebreak" id="page_73" role="doc-pagebreak"/> cannot occur at line 15. (Hint: Give an execution in which two <img alt="Image" height="9" src="images/B9780124159501000124/fx005.jpg" width="19"/>() calls are not linearized in the order they execute line 15.)</li><li class="b1bulllist" id="u0015">•  Give another execution showing that the linearization point for <img alt="Image" height="9" src="images/B9780124159501000124/fx005.jpg" width="19"/>() cannot occur at line 16.</li><li class="b1bulllist" id="u0020">•  Since these are the only two memory accesses in <img alt="Image" height="9" src="images/B9780124159501000124/fx005.jpg" width="19"/>(), we must conclude that <img alt="Image" height="9" src="images/B9780124159501000124/fx005.jpg" width="19"/>() has no single linearization point. Does this mean <img alt="Image" height="9" src="images/B9780124159501000124/fx005.jpg" width="19"/>() is not linearizable?</li></ul></div><p class="b1textfl"/></div></div><p class="textfl"/><p class="text" id="p0745"/><div class="boxg1" id="enun0115"><p class="b1num">Exercise 3.11 </p><div><p class="b1textfl" id="p0750">This exercise examines a stack implementation (<a href="#f0090" id="cf0205">Fig. 3.15</a>) whose <img alt="Image" height="11" src="images/B9780124159501000124/fx020.jpg" width="25"/>() method does not have a single fixed linearization point in the code.</p><div class="pageavoid"><figure class="fig" id="f0090"><img alt="Image" height="404" src="images/B9780124159501000124/gr015.jpg" width="321"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 3.15</span> Afek–Gafni–Morrison stack for Exercise <a href="#enun0115" id="cf0030">3.11</a>.</div></figcaption></figure></div><p class="b1text" id="p0755">The stack stores its items in an <img alt="Image" height="9" src="images/B9780124159501000124/fx001.jpg" width="31"/> array, which, for simplicity, we assume is unbounded. The <img alt="Image" height="11" src="images/B9780124159501000124/fx021.jpg" width="18"/> field is an <img alt="Image" height="11" src="images/B9780124159501000124/fx014.jpg" width="87"/>, initially zero.</p><p class="b1text" id="p0760"><span aria-label="Page 74" epub:type="pagebreak" id="page_74" role="doc-pagebreak"/>The <img alt="Image" height="11" src="images/B9780124159501000124/fx020.jpg" width="25"/>() method reserves a slot by incrementing <img alt="Image" height="11" src="images/B9780124159501000124/fx021.jpg" width="18"/>, and then stores the item at that location. Note that these two steps are not atomic: There is an interval after <img alt="Image" height="11" src="images/B9780124159501000124/fx021.jpg" width="18"/> has been incremented but before the item has been stored in the array.</p><p class="b1text" id="p0765">The <img alt="Image" height="9" src="images/B9780124159501000124/fx022.jpg" width="18"/>() method reads the value of <img alt="Image" height="11" src="images/B9780124159501000124/fx021.jpg" width="18"/> and then traverses the array in descending order from the top to slot zero. For each slot, it swaps <i>null</i> with the current contents, returning the first <i>nonnull</i> item it finds. If all slots are <i>null</i>, the method returns <i>null</i>, indicating an empty stack.</p><div><ul><li class="b1bulllist" id="u0025">•  Give an execution showing that the linearization point for <img alt="Image" height="11" src="images/B9780124159501000124/fx020.jpg" width="25"/>() cannot occur at line 11. (Hint: Give an execution in which two <img alt="Image" height="11" src="images/B9780124159501000124/fx020.jpg" width="25"/>() calls are not linearized in the order they execute line 11.)</li><li class="b1bulllist" id="u0030">•  Give another execution showing that the linearization point for <img alt="Image" height="11" src="images/B9780124159501000124/fx020.jpg" width="25"/>() cannot occur at line 12.</li><li class="b1bulllist" id="u0035">•  Since these are the only two memory accesses in <img alt="Image" height="11" src="images/B9780124159501000124/fx020.jpg" width="25"/>(), we conclude that <img alt="Image" height="11" src="images/B9780124159501000124/fx020.jpg" width="25"/>() has no single fixed linearization point. Does this mean <img alt="Image" height="11" src="images/B9780124159501000124/fx020.jpg" width="25"/>() is not linearizable?</li></ul></div><p class="b1textfl"/></div></div><p class="textfl"/><p class="text" id="p0785"/><div class="boxg1" id="enun0120"><p class="b1num">Exercise 3.12 </p><div><p class="b1textfl" id="p0790">Prove that sequential consistency is nonblocking.</p></div></div><p class="textfl"/></section><footer><section epub:type="bibliography" role="doc-bibliography"><div id="bl0445"><h2 class="reftitle" id="st0150">Bibliography</h2><p class="reflist1" epub:type="biblioentry footnote" id="br0070" role="doc-biblioentry">[14] James Aspnes, Maurice Herlihy, Nir Shavit,  Counting networks,   <cite><i>Journal of the ACM</i></cite> 1994;41(5):1020–1048.</p><p class="reflist1" epub:type="biblioentry footnote" id="br0340" role="doc-biblioentry">[68] M. Herlihy, V. Luchangco, M. Moir,  Obstruction-free synchronization: double-ended queues as an example,   <i>Proceedings of the 23rd International Conference on Distributed Computing Systems</i>.  IEEE; 2003:522–529.</p><p class="reflist1" epub:type="biblioentry footnote" id="br0360" role="doc-biblioentry">[72] Maurice Herlihy, Nir Shavit,  On the nature of progress,   <i>Proceedings of the 15th International Conference on Principles of Distributed Systems</i>.  <i>OPODIS'11</i>.  Berlin, Heidelberg: Springer-Verlag; 2011:313–328.</p><p class="reflist1" epub:type="biblioentry footnote" id="br0375" role="doc-biblioentry">[75] Maurice P. Herlihy, Jeannette M. Wing,  Linearizability: a correctness condition for concurrent objects,   <cite><i>ACM Transactions on Programming Languages and Systems</i></cite> 1990;12(3):463–492.</p><p class="reflist1" epub:type="biblioentry footnote" id="br0495" role="doc-biblioentry">[99] L. Lamport,  On interprocess communication,   <cite><i>Distributed Computing</i></cite> 1986;1:77–101.</p><p class="reflist2" epub:type="biblioentry footnote" id="br0500" role="doc-biblioentry">[100] Leslie Lamport,  A new solution of Dijkstra's concurrent programming problem,   <cite><i>Communications of the ACM</i></cite> 1974;17(5):543–545.</p><p class="reflist2" epub:type="biblioentry footnote" id="br0510" role="doc-biblioentry">[102] Leslie Lamport,  How to make a multiprocessor computer that correctly executes multiprocess programs,   <cite><i>IEEE Transactions on Computers</i></cite> September 1979;C-28(9):690.</p><p class="reflist2" epub:type="biblioentry footnote" id="br0515" role="doc-biblioentry">[103] Leslie Lamport,  Specifying concurrent program modules,   <cite><i>ACM Transactions on Programming Languages and Systems</i></cite> 1983;5(2):190–222.</p><p class="reflist2" epub:type="biblioentry footnote" id="br0525" role="doc-biblioentry">[105] Leslie Lamport,  On interprocess communication (part II),   <cite><i>Distributed Computing</i></cite> January 1986;1(1):203–213.</p><p class="reflist2" epub:type="biblioentry footnote" id="br0685" role="doc-biblioentry">[137] Christos H. Papadimitriou,  The serializability of concurrent database updates,   <cite><i>Journal of the ACM</i></cite> 1979;26(4):631–653.</p><p class="reflist2" epub:type="biblioentry footnote" id="br0790" role="doc-biblioentry">[158] Nir Shavit, Asaph Zemach,  Diffracting trees,   <cite><i>ACM Transactions on Computer Systems</i></cite> 1996;14(4):385–428.</p><p class="reflist2" epub:type="biblioentry footnote" id="br0830" role="doc-biblioentry">[166] W.E. Weihl,  Local atomicity properties: modular concurrency control for abstract data types,   <cite><i>ACM Transactions on Programming Languages and Systems</i></cite> 1989;11(2):249–282.</p></div></section><section epub:type="rearnotes"><div class="ftnote"><hr/><p class="ftnote1" epub:type="footnote" id="fn001" role="doc-footnote"><sup><a epub:type="noteref" href="#cf0055" role="doc-noteref">1 </a></sup> <a id="np0010"/>“There is an exception: Care must be taken if one method partially changes an object's state and then calls another method of that same object.”</p><p class="ftnote1" epub:type="footnote" id="fn002" role="doc-footnote"><sup><a epub:type="noteref" href="#cf0125" role="doc-noteref">2 </a></sup> <a id="np0015"/>“See <a href="#enun0020" id="cf0130">Remark 3.3.1</a> for various ways in which the term <i>nonblocking</i> is used.”</p></div></section></footer></section></body></html>