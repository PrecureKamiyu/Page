<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>18</title>
    
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
  <div class="tocheadb">
    <h1 class="tochead" id="heading_id_2"><a id="pgfId-998721"></a><a id="pgfId-998733"></a>18 Genetic algorithms: <a id="id_Hlk45449838"></a>Biologically inspired, fast-converging optimization</h1>
  </div>

  <p class="co-summary-head"><a id="pgfId-1015709"></a>This chapter covers</p>

  <ul class="calibre19">
    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1015752"></a>Introducing the genetic algorithm</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1015753"></a>Exploring whether genetic algorithms are better than simulated annealing</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1015754"></a>Solving the “Packing to Mars” problem with genetic algorithms</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1015755"></a>Solving TSP and assigning deliveries to trucks with genetic algorithms</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1015756"></a>Creating a genetic algorithm to solve minimum vertex cover</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1015734"></a>Discussing applications of genetic algorithms</li>
  </ul>

  <p class="body"><a id="pgfId-998843"></a>While <i class="calibre17">gradient descent</i><a id="marker-999587"></a> and <i class="calibre17">simulated annealing</i><a id="marker-999591"></a> are great optimization techniques, they both have shortcomings: the former is fast but has a tendency to get stuck in local minima and needs the cost function to be differentiable, while the latter can be quite slow in converging.</p>

  <p class="body"><a id="pgfId-998862"></a>In this chapter we are going to learn about the <i class="calibre17">genetic algorithm</i><a id="marker-999595"></a>, yet another optimization technique that uses an approach inspired by nature to overcome both issues, providing more resilience to local minima by evolving a pool of solutions and at the same time speeding up convergence.</p>

  <p class="body"><a id="pgfId-998874"></a>We will apply this technique to a few hard problems that can’t be solved efficiently with deterministic algorithms. We’ll start by using 0–1 knapsack, which we discovered in chapter 1, to explain the theory behind genetic algorithms. Then we’ll briefly discuss a genetic algorithm for <i class="calibre17">TSP</i>,<a id="marker-1015909"></a><a href="#pgfId-1008452"><sup class="footnotenumber">1</sup></a> and show how (and why) it converges faster than simulated annealing; finally, we will also introduce two new problems:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-998896"></a><i class="calibre15">Vertex cover</i><a class="calibre14" id="marker-1015913"></a>, which is useful in many areas, from network security to bioinformatics</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-998911"></a><i class="calibre15">Maximum flow</i><a class="calibre14" id="marker-1015878"></a>, fundamental to network connectivity, compiler optimization, and much more</p>
    </li>
  </ul>

  <h2 class="fm-head" id="heading_id_3"><a id="pgfId-998925"></a>18.1 Genetic algorithms</h2>

  <p class="body"><a id="pgfId-998939"></a>When it comes to optimization algorithms, the gist is that we are trying to replace a brute-force search over the whole problem space with a local search that filters out as many points in the problem space as possible: finding the same result by searching a much smaller domain.</p>

  <p class="body"><a id="pgfId-998948"></a>Optimization algorithms explore the neighborhood of current solutions, which are points in the search space, and do this either deterministically or randomly, trying to spot areas of interest, promising regions where we expect to find better solutions than what the search has previously found.</p>

  <p class="body"><a id="pgfId-998957"></a>The way we perform this filtering, the range of the search and how “local” it actually is, if we move randomly or in a deterministic way: these are the key factors that characterize the different techniques we have described in chapters 16 and 17.</p>

  <p class="body"><a id="pgfId-998970"></a>In the previous chapter we discovered simulated annealing and discussed how it converges to near-optimal solutions by allowing transitions uphill, while gradient descent only moves downhill, toward better solutions, and as such it gets easily stuck in local minima. Simulated annealing can therefore work better than gradient descent when the cost function has many local (sub-)minima.</p>

  <p class="body"><a id="pgfId-998983"></a>While powerful, simulated annealing can’t be perfect, and indeed we have also seen that there are two main drawbacks to consider:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-998992"></a>The algorithm is slow to converge. While gradient descent, which is deterministic, takes the fastest route downhill,<a class="calibre14" href="#pgfId-1008466"><sup class="footnotenumber">2</sup></a> simulated annealing (which instead is stochastic) randomly wanders across the cost function’s landscape, and as such it might require many attempts before finding the right direction.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-999012"></a>With certain shapes for the cost functions, where local/global minima are in narrow valleys, the probability that simulated annealing randomly “walks” into those valleys can be low, so low that it can fail to reach a near-optimal solution altogether.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-999024"></a>In section 17.1 we analyzed in some depth why simulated annealing works well, and how it creates this sort of dynamic filtering, indirectly restricting (based on their cost) the domain points that can be reached at a given stage. If you remember, initially the algorithm jumps from point to point, as likely to go uphill as downhill, exploring the landscape at large.</p>

  <p class="body"><a id="pgfId-999037"></a>We have also seen how it can happen that the optimization finds a near-optimal solution in the early stages, and then moves away from it, because in the initial phase it’s likely to accept transitions uphill. The problem is that simulated annealing has no memory, it doesn’t keep track of the past solutions, and—especially if we allow long-range transitions—there is a concrete risk that the algorithm will never get back to a solution that was as good.</p>

  <p class="body"><a id="pgfId-999050"></a>An even more probable risk is that the algorithm does find the valley containing global minimum at some early-ish stage, not necessarily getting any closer to the end of the valley (perhaps landing just somewhere close to the entrance of the valley), and then moves away and never manages to enter it again.</p>

  <p class="body"><a id="pgfId-999063"></a>Figure 18.1 illustrates these situations. In section 17.1 we also discussed how simulated annealing with restart can keep track of past solutions and—when stuck—randomly restart from one of these past positions, to check if a solution better than the current best is found.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F1.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032138"></a>Figure 18.1 An example of a scenario where simulated annealing would find a promising (<code class="fm-code-in-text">P<sub class="subscript1">i</sub></code>), or even good (<code class="fm-code-in-text">Q<sub class="subscript1">i</sub></code>) solution in an early stage (as suggested by the temperature of the system, <code class="fm-code-in-text">T<sub class="subscript1">i</sub></code>, still close to <code class="fm-code-in-text">T<sub class="subscript1">0</sub></code>): since all transitions, even uphill, are likely accepted when the temperature is high, the algorithm might move away from the sweet spot in the cost function landscape, and possibly never find its way back.</p>

  <p class="body"><a id="pgfId-999109"></a>This workaround can help somewhat with the former situation, where we get to a great solution early, but it’s unlikely to improve the latter (just finding a promising position at the entrance of a valley), because we only save at most a small number of the best previously found solutions. As such, even if the optimization had managed to find the beginning of a path to global minimum, we would forget about it, unless we were lucky enough to land close to the bottom of the valley.</p>

  <h3 class="fm-head2" id="heading_id_4"><a id="pgfId-999133"></a>18.1.1 Inspired by nature</h3>

  <p class="body"><a id="pgfId-999147"></a>Simulated <a id="marker-999611"></a><a id="marker-999615"></a>annealing was inspired by metallurgy, mimicking its cooling process to drive a system from chaotic to ordered behavior. Nature, as a matter of fact, has often been a great source of inspiration for mathematics and computer science. Just think about neural networks, probably the most pervasive of these examples, at the time of writing.</p>

  <p class="body"><a id="pgfId-999163"></a>For biological processes, such as neural networks, it’s not hard to imagine why. They have been adapted and perfected over millions of years, and since their efficiency is tightly connected to organisms’ survival, we can find clever and efficient solutions everywhere in nature.</p>

  <p class="body"><a id="pgfId-999180"></a>Genetic algorithms are yet another example of biologically inspired algorithms and, even more, they are based on this very principle of evolution in response to stimuli from the environment.</p>

  <p class="body"><a id="pgfId-999191"></a>As shown in figure 18.2, a genetic algorithm is an optimization algorithm that maintains a pool of solutions at each iteration. Compared to simulated annealing, this allows maintaining a larger degree of diversity, probing different areas of the cost function’s landscape at the same time.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F2.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032180"></a>Figure 18.2 An example of how genetic algorithms would tackle the scenario in figure 18.1. At a generic iteration (in the figure, we assume we are at the <code class="fm-code-in-text">i</code>-th iteration), which could indifferently be toward the beginning or the end of the simulation, the algorithm maintains a pool of possible solutions. At the next iteration, the pool (as a whole) will transition to a new set of candidate solutions (based on the current ones—we’ll see how later in this section). Usually the new solutions can be anywhere in the domain.</p>

  <p class="body"><a id="pgfId-999228"></a>One thing that can’t be shown in this “cost function” kind of graphic is that genetic algorithms also evolve the solutions by recombining them: this allows us to take the strengths of different solutions and merge them into a better one. Take, for instance, the TSP problem, which we described in section 17.2 (we’ll also talk about it again in the next section). Imagine that we have two poor solutions, each having the best possible sequence for a different half of the vertices, but each with terrible solutions for the remaining half. By combining these two solutions in the right<a href="#pgfId-1008497"><sup class="footnotenumber">3</sup></a> way, we can take the good halves from each and get a great candidate solution, possibly (if you are lucky!) even the best possible.</p>

  <p class="body"><a id="pgfId-999254"></a>This idea of combining solutions is something that we don’t find either in gradient descent (obviously!) or in simulated annealing, where a single solution is kept and “studied” at any time.</p>

  <p class="body"><a id="pgfId-999263"></a>We’ll see that this new idea is embodied in a new operator, the <i class="calibre17">crossover operator</i>, that derives from the biological analogy used for this technique. But to avoid getting ahead of ourselves, we still need to reveal what inspired genetic algorithms; in doing so, we will also make clear where the name comes from.</p>

  <p class="body"><a id="pgfId-999274"></a>A genetic algorithm is an optimization meta-heuristic based on the principles of genetics and natural selection. We mentioned that this optimization technique maintains a pool of solutions; what we hadn’t said is that these solutions will mimic a population that has evolved through several generations—each organism in the population is defined by a chromosome (or, sometimes, a pair of chromosomes), that encodes a single solution to the problem that’s optimized.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre14" id="pgfId-1032220"></a>Biologically inspired algorithms</p>

    <p class="fm-sidebar-text"><a id="pgfId-1032221"></a>Most living organisms on this planet are made up of cells,<a href="#pgfId-1008512"><sup class="footnotenumber1">4</sup></a> each of which carries the same<a href="#pgfId-1008527"><sup class="footnotenumber1">5</sup></a> genetic material in a set of chromosomes (usually more than one, for advanced organisms such as animals or plants). Each chromosome is a DNA (deoxyribonucleic acid) molecule that can be broken down into a sequence of nucleotides, each of which is, in turn, a sequence of nitrogen-containing nucleobases encoding information. The information contained in each cell’s DNA is used by cells to drive their behavior and synthesize proteins. Figure 18.3 succinctly illustrate these concepts.</p>
  </div>

  <p class="body"><a id="pgfId-999362"></a>So, in a nutshell, chromosomes encode information determining an organism’s behavior (<i class="calibre17">genetics</i>), and in turn how well an organism can adapt to its environment, survive, and generate offspring (<i class="calibre17">natural selection</i><a id="marker-1032283"></a>).</p>

  <p class="body"><a id="pgfId-999380"></a>Computer scientist John Holland, inspired by this mechanism, in the early 1970s devised<a href="#pgfId-1008542"><sup class="footnotenumber">6</sup></a> a way to use these two principles to evolve artificial systems. One of his students, David Goldberg (considered for a long time the expert of reference on this topic), popularized this approach through his research and his book at the end of the 1980s.<a href="#pgfId-1008562"><sup class="footnotenumber">7</sup></a></p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F3.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032323"></a>Figure 18.3 From cells to DNA: A cell’s genome is contained in its nucleus, inside structures called chromosomes, whose telomeres, when unraveled, reveal double helixes of DNA, a sequence made of just four nucleobases (hence we can consider DNA a base-4 encoding). Source: <span class="fm-hyperlink"><a href="https://commons.wikimedia.org/w/index.php?curid=10856406">https://commons.wikimedia.org/w/index.php?curid=10856406</a></span>, author: KES47.</p>

  <p class="body"><a id="pgfId-999399"></a>The core idea, shown in figure 18.4 in the next section, is to encode a solution as a chromosome and assign each chromosome to an organism. Then, naturally, it follows that based on how good a solution is, we derive how fit the corresponding organism is for its environment. Fitness, in turn, is a proxy for the chances of an organism to reproduce and pass its genetic material to the next generation through its offspring. As in nature, the alpha-individuals of a population, the stronger (or faster, or smarter) ones, tend to have a higher chance of surviving longer and finding a mate.</p>

  <p class="body"><a id="pgfId-999416"></a>But perhaps the best way to understand genetic algorithms is by seeing them in action. To that extent, and to keep things concrete, while we describe all the building blocks of this meta-algorithm, we’ll develop an example along the way to help readers visualize how each component works. Do you remember the 0–1 knapsack problem? We are going all the way back to chapter 1, when we introduced this problem to model our “packing to Mars” situation. Back then, we mentioned there is a pseudo-polynomial dynamic algorithm that can provide the absolute best solution, but it can be too slow when the capacity of the knapsack is large. Branch-and-bound approximated algorithms exist that can compute near-optimal solutions (with some guarantees on the upper bound and in reasonable time), although the efficient ones are usually quite complex.<a href="#pgfId-1008576"><sup class="footnotenumber">8</sup></a></p>

  <p class="body"><a id="pgfId-999439"></a>To get a fast, clear, simple optimization algorithm, we will implement a genetic algorithm that can find approximated, near-optimal solutions to the knapsack problem.</p>

  <p class="body"><a id="pgfId-999448"></a>Before we delve into the details of genetic algorithms, I suppose you could use a refresher. Let’s quickly review the problem definition, and the instance we were using.</p>

  <p class="fm-callout"><a id="pgfId-999461"></a><span class="fm-callout-head">Note</span> In the generic 0–1 knapsack problem, we have a container with a limited capacity <code class="fm-code-in-text2">M</code>, and a set of goods, each characterized by a weight (or any other measure of capacity) <code class="fm-code-in-text2">w</code>, and a value <code class="fm-code-in-text2">v</code>. If we add an item to our knapsack (or any generic container we decide to use!) it must be all of it; we can’t add fractions of items. The sum of the weights of all available items exceeds the capacity of the knapsack, so we need to choose a subset of items, and the goal, therefore, is to choose the particular subset that maximizes the value of the goods carried.</p>

  <p class="body"><a id="pgfId-999482"></a>In section 1.4.1, we tackle a more concrete problem: given the list of goods shown (for your convenience) in table 18.1, we’d like to fill a crate that can carry at most 1 ton, not with as much food as possible, but with the largest total calorie count possible.</p>

  <p class="body"><a id="pgfId-999491"></a>As such, we could easily check that wheat flour, rice, and tomatoes could sum up to the maximum carriable weight, but they wouldn’t give the maximum number of calories.</p>

  <p class="body"><a id="pgfId-999502"></a>In chapter 1, we briefly described the key consideration that is used by the branch-and-bound heuristics to approximate this problem. It computes the value by weight (in this case, calories by weight) for each of the goods, and prefers food with higher ratios; although that’s used as a starting point by the Martello-Toth algorithm, just choosing products in descending order of per-kilo values won’t give us the best possible solution.</p>

  <p class="body"><a id="pgfId-999523"></a>And, in fact, the dynamic programming algorithm that exactly solves the 0–1 knapsack problem won’t even compute this ratio; we will not use it here either, so it’s not even shown in table 18.1.</p>

  <p class="fm-table-caption"><a id="pgfId-1016860"></a>Table 18.1 A recap of the available goods for the mission to Mars, with their weight and calories</p>

  <table border="1" class="contenttable" width="100%">
    <tr class="calibre8">
      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1016866"></a>Food</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1016868"></a>Weight (kgs)</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1016870"></a>Total calories</p>
      </th>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016872"></a>Potatoes</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016874"></a>800</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016876"></a>1,502,000</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016878"></a>Wheat Flour</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016880"></a>400</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016882"></a>1,444,000</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016884"></a>Rice</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016886"></a>300</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016888"></a>1,122,000</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016890"></a>Beans (can)</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016892"></a>300</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016894"></a>690,000</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016896"></a>Tomatoes (can)</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016898"></a>300</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016900"></a>237,000</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016902"></a>Strawberry jam</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016904"></a>50</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016906"></a>130,000</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016908"></a>Peanut butter</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016910"></a>20</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016912"></a>117,800</p>
      </td>
    </tr>
  </table>

  <p class="body"><a id="pgfId-999622"></a>If you’d like to have a closer look at the problem, please feel free to skim through section 1.4; let’s now move on to discussing the main components of genetic algorithms<a id="marker-1005163"></a> (GA).</p>

  <p class="body"><a id="pgfId-999914"></a>To completely define an optimization algorithm, we need to specify the following <a id="marker-1005167"></a><a id="marker-1005171"></a>components:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-999926"></a><i class="calibre15">How to encode a solution</i>—Chromosomes, for GA</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-999942"></a><i class="calibre15">Transitional operators</i>—Crossover and mutations</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-999956"></a><i class="calibre15">A way to measure cost</i>—Fitness function</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-999970"></a><i class="calibre15">How system evolves</i>—Generations and natural selection</p>
    </li>
  </ul>

  <h3 class="fm-head2" id="heading_id_5"><a id="pgfId-999984"></a>18.1.2 Chromosomes</h3>

  <p class="body"><a id="pgfId-999996"></a>As <a id="marker-1005175"></a><a id="marker-1005179"></a>we mentioned, chromosomes encode a solution to the problem we need to solve. In the original work by Holland, chromosomes were only supposed to be strings of bits, sequences of zeros and ones. Not all problems can be encoded with a binary string (we’ll see an example in section 18.2 with TSP), and so a more generic version was derived later, allowing <i class="calibre17">genes</i> (each of the values in a chromosome) to be continuous, real values.</p>

  <p class="body"><a id="pgfId-1000017"></a>Conceptually, those versions can be considered equivalent, but whenever we don’t use binary strings for the chromosomes, we will likely require restrictions on the possible values they can store: the operators acting on organisms will have to be adjusted in order to check and maintain these constraints.</p>

  <p class="body"><a id="pgfId-1000026"></a>Luckily for us, the 0–1 knapsack is the perfect example to discuss the original genetic algorithm, because a solution to this problem can be encoded exactly as a binary string. For each item, a zero means we leave it on Earth, while a 1 means we add it to the crate going to Mars.</p>

  <p class="body"><a id="pgfId-1000039"></a>Figure 18.4 shows a bit-string representation for a chromosome that can be used in our example. Two different solutions (or <i class="calibre17">phenotypes</i><a id="marker-1015873"></a>, in biological terms) can be represented by two chromosomes (or <i class="calibre17">genotypes</i>): their difference boils down to the difference between the two bit-strings.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F4.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032369"></a>Figure 18.4 Examples of encoding chromosomes for the knapsack problem (the specific instance described in section 1.4). Two figures are shown: the differences in the genotypes (the two strings) are reflected in the phenotypes (the goods taken to Mars: wheat flour and tomatoes versus wheat flour, rice, and beans). The latter chromosome encodes the best solution for the instance of the problem summed up in table 18.1.</p>

  <p class="body"><a id="pgfId-1000076"></a>It’s worth noting that for <a id="Last"></a>this example we have a 1:1 mapping between genotype and phenotype: a solution (the phenotype) is uniquely identified by its representation (the genotype) and so two organisms with the same genotype will translate to the same solution and in turn to the same fitness value (as we’ll discuss in a few sections). In the context of our 0–1 knapsack example, if two organisms have the same genotype, both solutions will add the same set of goods to the crate to Mars.</p>

  <p class="body"><a id="pgfId-1000096"></a>The equivalence between genotype and phenotype is, however, not observed in nature, where the difference between them is clear, because the genome only encodes developmental rules, not precise outcomes, and an organism is also determined by its interaction with the environment.<a href="#pgfId-1008596"><sup class="footnotenumber">9</sup></a></p>

  <p class="body"><a id="pgfId-1000115"></a>Likewise, in some simulated problems fitness is not always fully determined by genotype. <span>O</span>ne of the first examples of an application of genetic algorithms I studied was the work<a href="#pgfId-1008614"><sup class="footnotenumber">10</sup></a> of Floreano and Mondada at EPFL, where they evolved the neural networks of small two-wheel robots, simulating the evolution of gatherers, and later of predator and prey populations (in a sort of hide and seek game).</p>

  <p class="body"><a id="pgfId-1000141"></a>In this experiment, the genotype was the set of weights for a robot’s neural network, which, incidentally, also completely identify the phenotype. Each organism’s fitness, though, was later determined by its interaction with the environment and with other robots.</p>

  <p class="body"><a id="pgfId-1000156"></a>If we were to extend their original work by allowing online learning for the robots (today, it wouldn’t be hard to imagine applying stochastic<a id="marker-1022948"></a> or mini-batch backpropagation<a id="marker-1022949"></a> to evolve the NN’s weights based on the input from the environment), then neither would the phenotype be completely determined by the genotype, at least not after the first update, because after that, the way weights change is heavily influenced by the interaction of the robot with the environment.</p>

  <p class="body"><a id="pgfId-1000173"></a>With respect to figure 18.4, we also need to make one more thing clear: the actual representation of chromosomes depends on the actual, specific problem we are solving. In this case, it depends on the specific instance of the 0–1 knapsack (although a generic representation for all 0–1 knapsacks can and should be designed).</p>

  <p class="body"><a id="pgfId-1000186"></a>If we want to design code for the genetic algorithm main method, instead, we should be concerned with designing organisms, for example, modeling them with a class that will be provided the actual chromosome (and as we’ll see, the methods doing modifications on them) whether at runtime through composition, as arguments (using the strategy design pattern), or at compile time through inheritance (the template design pattern).</p>

  <p class="fm-callout"><a id="pgfId-1000199"></a><span class="fm-callout-head">Note</span> The genetic algorithm is a meta-algorithm, a template that is completed by the code specific to each problem tackled. As such, there is part of the code that is generic—the structural code defining the optimization technique—and another part that is specific to the problem. At this point, while describing the components of genetic algorithms, we’ll provide the pseudo-code for the template, and since we are using the knapsack problem as an example, we’ll also provide some pseudocode to implement examples of the specific methods for the 0–1 knapsack, with these snippets clearly marked.</p>

  <p class="body"><a id="pgfId-1000220"></a>Listing 18.1 shows a possible implementation of the <code class="fm-code-in-text">Individual</code> class<a id="marker-1023056"></a>, modeling each individual organism in the population <a id="marker-1023058"></a><a id="marker-1023059"></a>evolved.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1016990"></a>Listing 18.1 Class <code class="fm-code-in-text">Individual</code></p>
  <pre class="programlisting"><b class="strong">class</b> Individual                     <span class="fm-combinumeral">❶</span>
  <b class="strong">#type array</b> 
  chromosome                         <span class="fm-combinumeral">❷</span>
 
  <b class="calibre21">function</b> Individual(chromosome)    <span class="fm-combinumeral">❸</span>
 
  <b class="calibre21">function</b> fitness()                 <span class="fm-combinumeral">❹</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1019298"></a><span class="fm-combinumeral">❶</span> Class <code class="fm-code-in-text2">Individual</code><a id="marker-1030794"></a> models each organism in the population to evolve.</p>

  <p class="fm-code-annotation"><a id="pgfId-1019330"></a><span class="fm-combinumeral">❷</span> Each individual has a genome made of a chromosome (multi-chromosome organisms are, in theory, also possible, though not as common).</p>

  <p class="fm-code-annotation"><a id="pgfId-1019361"></a><span class="fm-combinumeral">❸</span> We assume the chromosomes are provided as arguments to the constructor (implementation, which is trivial, isn’t shown). Alternatively, there can be a static generator method on this class.</p>

  <p class="fm-code-annotation"><a id="pgfId-1019396"></a><span class="fm-combinumeral">❹</span> Each individual can be queried for its fitness (possibly this needs to take arguments if the fitness depends on the environment).</p>

  <h3 class="fm-head2" id="heading_id_6"><a id="pgfId-1000364"></a>18.1.3 Population</h3>

  <p class="body"><a id="pgfId-1000376"></a>The <a id="marker-1005211"></a><a id="marker-1005215"></a>most apparent difference between simulated annealing and genetic algorithms is that instead of tuning a single solution, genetic algorithms will evolve a population of individuals.</p>

  <p class="body"><a id="pgfId-1000388"></a>Listing 18.2 shows a possible implementation for a method that initializes the population that will be used for the genetic algorithm. This is also a templated method, since it will have to be provided with the actual code for initializing the single chromosomes (which, again, will depend on the actual problem tackled).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1017020"></a>Listing 18.2 Method <code class="fm-code-in-text">initPopulation</code></p>
  <pre class="programlisting"><b class="strong">function</b> initPopulation(size, chromosomeInitializer)         <span class="fm-combinumeral">❶</span>
  population ← []                                            <span class="fm-combinumeral">❷</span>
  <b class="calibre21">for</b> i <b class="strong">in</b> {1,...,size} do
    population.add(<b class="strong">new</b> Individual(chromosomeInitializer())   <span class="fm-combinumeral">❸</span>
  <b class="calibre21">return</b> population  </pre>

  <p class="fm-code-annotation"><a id="pgfId-1019461"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">initPopulation</code><a id="marker-1030601"></a> takes the size of the population to create (assumed, or checked, to be positive) and a function to initialize individual chromosomes. It returns the newly created population.</p>

  <p class="fm-code-annotation"><a id="pgfId-1019529"></a><span class="fm-combinumeral">❷</span> Init <code class="fm-code-in-text2">population</code> as an empty list</p>

  <p class="fm-code-annotation"><a id="pgfId-1019479"></a><span class="fm-combinumeral">❸</span> Adds as many new individuals as necessary, each with a newly created genome</p>

  <p class="body"><a id="pgfId-1000524"></a>There are a few different strategies that can be used for initialization. You can rely on random initialization to provide diversity in your initial population. This is the original and still most common approach, but in certain situations you might also add constraints on the chromosomes (to avoid invalid solutions) or even decide to provide an initial population (that can, for instance, be the output of a different algorithm or of a previous iteration).</p>

  <p class="body"><a id="pgfId-1000546"></a>For a generic instance of the 0–1 knapsack problem, since chromosomes are just bit strings, we are good with a random generator, as shown in listing 18.3.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1017402"></a>Listing 18.3 0–1 knapsack: chromosome generation</p>
  <pre class="programlisting"><b class="strong">function</b> knapsackChromosomeInitializer(genesNumber)     <span class="fm-combinumeral">❶</span>
  chromosome ← []                                       <span class="fm-combinumeral">❷</span>
  <b class="calibre21">for</b> i <b class="fm-bold">in</b> {0,...,genesNumber-1} do
    chromosome[i] ← randomBool().toInt()                <span class="fm-combinumeral">❸</span>
  <b class="calibre21">return</b> chromosome    </pre>

  <p class="fm-code-annotation"><a id="pgfId-1019574"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">knapsackChromosomeInitializer</code><a id="marker-1030391"></a> takes the size of the chromosome to create (assumed, or checked, to be positive) and returns a random bit string (or, in this case, a bit array for the sake of simplicity).</p>

  <p class="fm-code-annotation"><a id="pgfId-1019632"></a><span class="fm-combinumeral">❷</span> Init <code class="fm-code-in-text2">chromosome</code> as an empty list</p>

  <p class="fm-code-annotation"><a id="pgfId-1019656"></a><span class="fm-combinumeral">❸</span> Generates as many random bits as necessary. Here we used a Java-like pattern, but it’s also possible to directly use a method that returns random integers between <code class="fm-code-in-text2">0</code> and <code class="fm-code-in-text2">1</code>.</p>

  <p class="body"><a id="pgfId-1000687"></a>There is a caveat: not all randomly generated strings are acceptable solutions for a given instance of the knapsack problem. For example, a string with all values set to 1 would certainly mean (for a non-trivial instance) that the weight constraint has been violated. As we’ll see next, we can assign a low fitness to solutions that violate this constraint; alternatively, when we generate chromosomes, we can add a check to avoid violating it in the first <a id="marker-1005227"></a><a id="marker-1005231"></a>place.</p>

  <h3 class="fm-head2" id="heading_id_7"><a id="pgfId-1000699"></a>18.1.4 Fitness</h3>

  <p class="body"><a id="pgfId-1000711"></a>Connected <a id="marker-1015861"></a><a id="marker-1015862"></a>to the definitions of chromosome and organism, there is the notion of <i class="calibre17">fitness</i> of an individual. As you can see in listing 18.1, the <code class="fm-code-in-text">Individual</code> class<a id="marker-1015864"></a> has a <code class="fm-code-in-text">fitness</code> method<a id="marker-1015865"></a> returning a value that measures how well an organism adapts to its environment.</p>

  <p class="body"><a id="pgfId-1000737"></a>The term <i class="calibre17">fitness</i> naturally implies a maximization problem, because higher fitness is usually associated with better performance in an environment. Conversely, for many problems we usually want to express our problems’ goals as <i class="calibre17">cost functions</i><a id="marker-1005251"></a> to be minimized, so we have two choices: either implement the genetic algorithm template in such a way that a lower value for fitness means better performance, or reformulate our cost functions according to the choice we made. For instance, if we need to find the point of highest elevation on a map, we can either implement a maximization algorithm or set the fitness function to <code class="fm-code-in-text">f(P)=-elevation(P)</code>, for any given point on the map, and stick with our usual minimization strategy (as we saw in chapters 16 and 17 for <code class="fm-code-in-text">gradient descent</code> and <code class="fm-code-in-text">simulated annealing</code>).</p>

  <p class="body"><a id="pgfId-1000765"></a>Consider our example with the 0-1 knapsack problem: the goal is naturally expressed by a value function that we want to maximize, the sum of the nutritional values of the goods added to the crate.</p>

  <p class="body"><a id="pgfId-1000778"></a>The first chromosome in figure 18.4, for example, is <code class="fm-code-in-text">0100100</code>, which means we sum the calories of rice and tomatoes, for a total of <code class="fm-code-in-text">1,359,000</code> calories. What happens when we exceed the weight that the crate can carry, for instance, with the chromosome <code class="fm-code-in-text">1111111</code>? To spot these edge cases, we need to check the total weight while computing the fitness and assign a special value (for instance, in this case, <code class="fm-code-in-text">0</code>) to those solutions that violate the constraint on the weight.</p>

  <p class="body"><a id="pgfId-1000798"></a>An alternative can be making sure that this situation never occurs by carefully checking it in the operators that create and modify individuals (initialization, crossover, and mutations, as we’ll see). This solution, however, may have consequences on the optimization process, artificially reducing the plurality of features in the population.</p>

  <p class="body"><a id="pgfId-1000809"></a>At this point, if the template method that we have implemented for our genetic algorithms tries to maximize the fitness of individuals, we are golden. What if, instead, our implementation strives for lower fitness? For this particular case, we might have an easy solution: we sum the values of the goods discarded, corresponding to zeroes in a chromosome. For any given solution, the lower this sum is, the higher will be the sum of the calories of the goods in the crate. Of course, if we go this way, we have to assign a very high value (even infinity) to the solutions where the weight threshold is <a id="marker-1005255"></a><a id="marker-1005259"></a>exceeded.</p>

  <h3 class="fm-head2" id="heading_id_8"><a id="pgfId-1000829"></a>18.1.5 Natural selection</h3>

  <p class="body"><a id="pgfId-1000843"></a>We <a id="marker-1015852"></a><a id="marker-1015853"></a><a id="marker-1015854"></a>now have the tools to create a single organism, generate a whole population, and check individual fitness. In turn, this allows us to mimic a basic natural process that guides the evolution of populations in the wild: <i class="calibre17">natural selection</i>.</p>

  <p class="body"><a id="pgfId-1000864"></a>In nature, we see it everywhere: the strongest, fastest individuals, the ones that are best at hunting or at hiding, tend to survive longer, and in turn (or in addition to that) they have greater chances of mating and transmitting their genes to the next generation.</p>

  <p class="body"><a id="pgfId-1000873"></a>The goal of genetic algorithms is to use a similar mechanism to have the population converge toward good solutions. In this analogy, the genes of individuals with high fitness<a href="#pgfId-1008642"><sup class="footnotenumber">11</sup></a> encode features that give them an advantage over the competitors.</p>

  <p class="body"><a id="pgfId-1000888"></a>In our knapsack example, a good feature could be including food with high calories- per-weight ratio. Solutions including potatoes will be individuals with low fitness, and that therefore will struggle to survive to the next generations. As an example, in the predator-prey simulation mentioned in section 18.1.2, the prey that developed a strategy to hide behind objects were more efficient in escaping predators.</p>

  <p class="body"><a id="pgfId-1000905"></a>Listing 18.4 shows, through some pseudocode, how natural selection (in genetic algorithms) works. This method regulates how a population transitions between the current generation and the next one, taking the old population as input, and returning a new population, randomly created from the original organisms through a set of operators.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1017645"></a>Listing 18.4 Natural selection</p>
  <pre class="programlisting"><b class="strong">function</b> naturalSelection(
<span class="fm-code-continuation-arrow">➥</span> population, elitism, selectForMating, crossover, mutations)   <span class="fm-combinumeral">❶</span>
  newPopulation ← elitism(population)                            <span class="fm-combinumeral">❷</span>
  <b class="calibre21">while</b>  |newPopulation| &lt; |population| <b class="calibre21">do</b>                       <span class="fm-combinumeral">❸</span>
    p1 ← selectForMating(population)                             <span class="fm-combinumeral">❹</span>
    p2 ← selectForMating(population)
    newIndividual ← crossover.apply(p1, p2)                      <span class="fm-combinumeral">❺</span>
    <b class="strong">for</b> mutation <b class="strong">in</b> mutations <b class="strong">do</b>
      mutation.apply(newIndividual)                              <span class="fm-combinumeral">❻</span>
    newPopulation.add(newIndividual)                             <span class="fm-combinumeral">❼</span>
  <b class="calibre21">return new</b>Population  </pre>

  <p class="fm-code-annotation"><a id="pgfId-1019733"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">naturalSelection</code><a id="marker-1029904"></a> takes the current population and returns a new population evolved from the input one. The function also needs to be provided the operators that will change the population, either in the arguments list or, for instance, through inheritance.</p>

  <p class="fm-code-annotation"><a id="pgfId-1019768"></a><span class="fm-combinumeral">❷</span> The first thing to do is to initialize the new population. This can be done by simply creating a new, empty list. A more generic alternative, however, is to allow passing a method for this initialization, which could, for instance, be used to include elitism.</p>

  <p class="fm-code-annotation"><a id="pgfId-1019833"></a><span class="fm-combinumeral">❸</span> Add new individuals, one by one, until the new population matches the old one in size.</p>

  <p class="fm-code-annotation"><a id="pgfId-1019864"></a><span class="fm-combinumeral">❹</span> Select two individuals from the old population. The selection method is passed as an argument (equivalently, as for the others, it can be provided through inheritance).</p>

  <p class="fm-code-annotation"><a id="pgfId-1019943"></a><span class="fm-combinumeral">❺</span> Combines the two organisms using crossover; details of this and the other operators are, as discussed, for the most part specific to the problem, and will be discussed later in this section. In order for this template to work, however, we require that crossovers and mutations abide by a common interface, and that they are implemented as objects providing an <code class="fm-code-in-text2">apply</code> method<a id="marker-1029974"></a> (we’ll also discuss wrappers, to make this easy).</p>

  <p class="fm-code-annotation"><a id="pgfId-1019911"></a><span class="fm-combinumeral">❻</span> For each possible mutation, apply it to the result of crossover (as we’ll see, each mutation is characterized by a probability of happening, so it will be randomly decided if any mutation is applied to organisms).</p>

  <p class="fm-code-annotation"><a id="pgfId-1019802"></a><span class="fm-combinumeral">❼</span> Adds the newly generated individual to the output list</p>

  <p class="body"><a id="pgfId-1001134"></a>This natural selection process is applied at each iteration of the algorithm. When the algorithm starts, it’s always with a fully formed population outputted by the initialization method (its size is determined at runtime through a method argument, as we have seen). Individuals in the input population are evaluated for their fitness and then go through a process of selection that determines which ones are going to either pass to the next generation unaltered or propagate their genes through mating.</p>

  <p class="body"><a id="pgfId-1001151"></a>In the genetic algorithm’s simplest form, the new population is just initialized as an empty list (line #2), a list which is then <i class="calibre17">populated</i> (pun intended) with the new individuals resulting from selection and mating (#3–#6).</p>

  <p class="body"><a id="pgfId-1001162"></a>For mating, there are, obviously, differences with respect to the biological analogy. First and foremost, in genetic algorithms sexual reproduction between two <i class="calibre17">asexual</i> organisms is generally<a href="#pgfId-1008657"><sup class="footnotenumber">12</sup></a> used: moreover, the recombination of genetic material between the two parents doesn’t follow any biologically meaningful rules and the actual details of how crossover is performed are for the most part left to the specific problem that is solved.</p>

  <p class="body"><a id="pgfId-1001186"></a>Finally, we add mutations to the genetic material after crossover, modeling crossover and mutations as separate phases.</p>

  <p class="body"><a id="pgfId-1001195"></a>Figure 18.5 illustrates the general mechanism with an analogy to the biological process.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F5.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032414"></a>Figure 18.5 Natural selection in genetic algorithms, in an analogy with the biological process.</p>

  <p class="body"><a id="pgfId-1001204"></a>We start with an initial population where individuals have a common basis, but also some peculiar characteristics. The diversity and variance in the population largely depends on which stage of the simulated evolution we are in. As we’ll see, our aim is having greater diversity at the beginning of the simulation and then have the population converge toward a few homogenous, high-fitness groups.</p>

  <p class="body"><a id="pgfId-1001242"></a>In our example, our population of ducks shows a few unique features, from the shapes of their beak, to their colors/patterns, to the shape of wings and tails. If you look closely, you should be able to spot them and to figure out how these changes are encoded in their chromosomes.</p>

  <p class="body"><a id="pgfId-1001255"></a>This is because, as we have discussed, each feature in an individuals’ phenotype is determined by a difference in their genotype, a <code class="fm-code-in-text">1</code> that becomes a <code class="fm-code-in-text">0</code>, or vice versa (or maybe a few bits that need to flip together, or even, if we move past binary strings, a gene assigned with a different real number).</p>

  <p class="body"><a id="pgfId-1001268"></a>In order to perform selection, we need to evaluate each <i class="calibre17">duck</i>’s fitness. This can be as easy as feeding each chromosome to a function or as complicated as running a simulation for hours (as in the predator-prey setting we described) and seeing which individuals survive longer or perform a real-world task better.</p>

  <p class="body"><a id="pgfId-1001287"></a>We’ll only talk about selection for mating in the next subsection, but one thing is generally true regardless of the details of the selection mechanism chosen: organisms with higher fitness will have greater chances of being chosen to pass their genes to the next generation. This is crucial, maybe the only really crucial part, because without this “meritocracy,” no optimization would be performed.</p>

  <p class="body"><a id="pgfId-1001296"></a>Once we have selected two individuals, we need to recombine their genetic material. We’ll also tackle crossover and mutations in their own section, but one thing that we have already mentioned is that these operators, acting on chromosomes, are largely determined by the specific problem that is being solved.</p>

  <p class="body"><a id="pgfId-1001309"></a>In our example in figure 18.5, our little duckling takes its feathering pattern and color from one parent and its tail shape from the other. Then a further mutation kicks in changing the feathering again from a checkerboard pattern to a polka dot pattern.</p>

  <p class="body"><a id="pgfId-1001322"></a>The new population, formed by repeating this process many times, should show a larger presence of those genes/features associated with high-fitness individuals in the initial population.</p>

  <p class="body"><a id="pgfId-1001333"></a>Now that we have seen how the broad natural selection process works, it’s time to dive into the specifics of its <a id="marker-1005283"></a><a id="marker-1005287"></a><a id="marker-1005291"></a>steps.</p>

  <h3 class="fm-head2" id="heading_id_9"><a id="pgfId-1001346"></a>18.1.6 Selecting individuals for mating</h3>

  <p class="body"><a id="pgfId-1001362"></a>Let’s <a id="marker-1005295"></a><a id="marker-1005299"></a>start with selection: we’ll go back to our knapsack problem example to better explain it.</p>

  <p class="body"><a id="pgfId-1001374"></a>There are several techniques that can possibly be used for selecting, at each iteration, the individuals that will mate or progress; some of the most successful alternatives are</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1001383"></a>Elitism</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1001395"></a>Thresholding</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1001407"></a>Tournament selection</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1001419"></a>Roulette wheel selection</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1001433"></a>But there are also many other possible techniques used. Considering our restricted list, the first two techniques are filters on the old population, while the remaining two are methods to select organisms for mating.</p>

  <p class="body"><a id="pgfId-1001443"></a>Elitism and thresholding, illustrated in figure 18.6, are used to decide what organisms can or can’t transmit their genomes to the next generation.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F6.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032466"></a>Figure 18.6 Elitism and thresholding illustrated with an example (0-1 knapsack problem). Elitism takes the best individual(s) (one or a few of the highest-fitness entries) and directly brings them to the next generation without mating or mutations. Thresholding takes care of the opposite problem: it’s a hard barrier that stops low-fitness individuals from passing their genes to the next generation. It’s possible to discard a certain fixed number of individuals, or all individuals below some threshold. For instance, in this example (where we maximize fitness value), all individuals below 80% of the fitness value of the best individual will be discarded and never selected for mating.</p>

  <p class="body"><a id="pgfId-1001471"></a><i class="calibre17">Elitism</i> allows the best individuals to pass completely unaltered to the next generation. Whether or not to apply this principle is up to the algorithm designer; as always, it might work better with some problems and worse with others.</p>

  <p class="body"><a id="pgfId-1001487"></a>Without elitism, all organisms in the current iteration would be replaced in the next one, so that all organisms would “live” exactly one generation; with this workaround, we can instead simulate a longer lifespan for particularly well-fitting individuals—the better their fitness in comparison to the average population’s, the longer they will be kept.</p>

  <p class="body"><a id="pgfId-1001500"></a>In turn, this will also ensure that the genes of the <i class="calibre17">alpha organisms</i><a id="marker-1005303"></a> will be present, untouched, in the next generation too (although it’s not certain they will still be the top-fitness individuals in the next generation).</p>

  <p class="body"><a id="pgfId-1001518"></a>One notable effect of using elitism is that the fitness of the best element in the population is monotonically improving over generations (that, however, might not be true for the average fitness).</p>

  <p class="body"><a id="pgfId-1001527"></a><i class="calibre17">Thresholding</i> has the opposite goal: it will prevent the lowest-fitness organisms from transmitting their genes to the next generations, which in turn reduces the probability that the algorithm explores less-promising areas of the fitness function’s landscape.</p>

  <p class="body"><a id="pgfId-1001537"></a>The way it works is simple. We set a threshold for the fitness of the individuals that are allowed to be selected for mating and ignore the ones outside the requirements. We can discard a fixed number of organisms, such as the five individuals with the worst fitness value, or discard a variable number of individuals based on their fitness value.</p>

  <p class="body"><a id="pgfId-1001558"></a>In the latter scenario, the threshold on the fitness value is usually set dynamically (changing each generation) and based on the fitness value of the best individual for that generation. With a static, absolute value (which would require domain knowledge to be set), the risk is that it would be ineffective (resulting in the whole population being filtered in) or even worse, fatal for the simulation when it’s too stringent, resulting in all, or almost all, individuals being filtered out in the early stages.</p>

  <p class="body"><a id="pgfId-1001571"></a>In the example in figure 18.6 we suggested setting the threshold to 80% of the best individual’s fitness value.<a href="#pgfId-1008681"><sup class="footnotenumber">13</sup></a> Whether or not thresholding should be applied and what the threshold ratio should be completely depend on the actual problem to be solved.</p>

  <p class="body"><a id="pgfId-1001585"></a>After filtering the initial population, and possibly escorting the best individuals to the next generation, we are still tasked with recreating the remaining individuals in the new population. To do so, we implement <i class="calibre17">crossover</i><a id="marker-1005307"></a>, which will be discussed in the next sub-section, as a way to generate a new organism from two parents, and <i class="calibre17">mutations</i><a id="marker-1005311"></a>, to provide genetic diversity to the new generations. The necessary step before being able to apply crossover is, therefore, selecting those two parents. As you can see in listing 18.4, we are going to perform this selection several times at each iteration.</p>

  <p class="body"><a id="pgfId-1001604"></a>There are many possible ways to select individuals for mating—we are going to discuss two of the most common here.</p>

  <p class="body"><a id="pgfId-1001617"></a><i class="calibre17">Tournament selection</i><a id="id_Hlk44590396"></a> is one of the simplest (and easiest-to-implement) selection techniques used in genetic algorithms; it’s illustrated through an example in figure 18.7. Conceptually, we randomly select a handful of organisms and then have them “compete” in a tournament where only the winner gets the right to mate. This is similar to what we see everywhere in wildlife, where (usually) male adults fight for the right to mate with females (ever watched a documentary on deer crossing horns during mating season?).</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F7.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032511"></a>Figure 18.7 Tournament selection applied to our instance of the 0-1 knapsack problem</p>

  <p class="body"><a id="pgfId-1001656"></a>Now, in reality we don’t actually code a tournament, unless we are running a tournament simulation!<a href="#pgfId-1008696"><sup class="footnotenumber">14</sup></a> As shown in listing 18.5, we just randomly select <code class="fm-code-in-text">k</code> individuals and take the one with the best fitness. The exact number, <code class="fm-code-in-text">k</code>, can vary depending on the problem and on the size of the population, but usually somewhere around 3 to 5 elements could be a good choice. Consider that the larger the pool of individuals participating in the tournaments, the lower the chance for low-fitness individuals to be chosen.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1017690"></a>Listing 18.5 Tournament selection</p>
  <pre class="programlisting"><b class="strong">function</b> tournamentSelection(population, k)                 <span class="fm-combinumeral">❶</span>
  bestFitness ← lowestPossibleFitness()                    <span class="fm-combinumeral">❷</span>
  <b class="calibre21">for</b> i <b class="strong">in</b> {1,..,k} <b class="calibre21">do</b>                                      <span class="fm-combinumeral">❸</span>
    j ← randomInt(|population|)                            <span class="fm-combinumeral">❹</span>
    <b class="strong">if</b> isHigher(population[j].fitness, bestFitness) then    <span class="fm-combinumeral">❺</span>
      bestElement ← population[j]
      bestFitness ← bestElement.fitness                    <span class="fm-combinumeral">❻</span>
  <b class="calibre21">return</b> bestElement  </pre>

  <p class="fm-code-annotation"><a id="pgfId-1020133"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">tournamentSelection</code><a id="marker-1029500"></a> takes current population and the number of elements that should participate in each tourney, and returns an individual, the best among the ones selected for the “tournament.”</p>

  <p class="fm-code-annotation"><a id="pgfId-1020165"></a><span class="fm-combinumeral">❷</span> Init the temporary variable storing the best fitness found. Since this is a generic method and we don’t know if, in concrete problems, the best fitness will mean highest or lowest values, we use a helper function returning the “lowest” possible fitness, <code class="fm-code-in-text2">0</code> for functions that will be maximized, infinity for the ones to be minimized.</p>

  <p class="fm-code-annotation"><a id="pgfId-1020196"></a><span class="fm-combinumeral">❸</span> Repeat <code class="fm-code-in-text2">k</code> times</p>

  <p class="fm-code-annotation"><a id="pgfId-1020227"></a><span class="fm-combinumeral">❹</span> Randomly selects an index between <code class="fm-code-in-text2">0</code> and the population’s size. Indirectly, this selects an individual. In this implementation there are no measures to avoid duplicated selections, which can be fine when the population size is large (and consequently probability of duplicates is low), but still, you need to be aware of this.</p>

  <p class="fm-code-annotation"><a id="pgfId-1020297"></a><span class="fm-combinumeral">❺</span> Checks if this element is better than the current best; again, we use a helper function to abstract on the meaning of good/high fitness</p>

  <p class="fm-code-annotation"><a id="pgfId-1020265"></a><span class="fm-combinumeral">❻</span> If we found a new best, just updates the temporary variables</p>

  <p class="body"><a id="pgfId-1001854"></a>For instance, for the third-highest individual to be chosen, neither the best nor the second-best fitness organisms can be chosen (and, of course, the third-best must be), so the probability this happens with a pool of <code class="fm-code-in-text">k</code> individuals is<a href="#pgfId-1008713"><sup class="footnotenumber">15</sup></a></p>
  <pre class="programlisting">1/n * [(n-2)/n]<sup class="superscript2">k-1</sup></pre>

  <p class="body"><a id="pgfId-1001874"></a>assuming <code class="fm-code-in-text">n</code> is the size of the population.</p>

  <p class="body"><a id="pgfId-1001885"></a>For the lowest fitness individual (after, possibly, applying thresholding), it becomes</p>
  <pre class="programlisting">1/n * [1/n]<sup class="superscript2">k-1</sup> = 1/n<sup class="superscript2">k</sup></pre>

  <p class="body"><a id="pgfId-1001906"></a>The generic formula for the <code class="fm-code-in-text">m</code>-th best fitness becomes</p>
  <pre class="programlisting">1/n * [(n-m+1)/n]<sup class="superscript2">k-1</sup></pre>

  <p class="body"><a id="pgfId-1001929"></a>As you can see, beyond the details and the actual exact probability, the chances of any individual (except the first) are decreasing exponentially with <code class="fm-code-in-text">k</code> (while polynomially with <code class="fm-code-in-text">m</code>).</p>

  <p class="body"><a id="pgfId-1001944"></a>It goes without saying that we need to apply tournament selection twice to get the pair of parents we need to generate a single element in the new population.</p>

  <p class="body"><a id="pgfId-1001955"></a><i class="calibre17">Roulette wheel selection</i><a id="marker-1015828"></a> is definitely more complicated to implement than tournament selection, but the high-level idea is the same: higher-fitness individuals must have more chances to be selected.</p>

  <p class="body"><a id="pgfId-1001968"></a>As we have seen, in tournament selection the probability that an element with low fitness is chosen decreases polynomially with the rank of the element (its position in the list of organisms sorted by fitness); in particular, since the probability will be <code class="fm-code-in-text">O([(n-m)/n]<sup class="superscript1">k</sup>)</code>, the decrease will be super-linear, because <code class="fm-code-in-text">k</code> is certainly greater than <code class="fm-code-in-text">1</code>.</p>

  <p class="body"><a id="pgfId-1001988"></a>If, instead, we would like for lower-fitness elements to get a real chance of being selected, we could resort to a fairer selection method. One option is assigning the same probability to all organisms, but then we wouldn’t really reward high-fitness individuals anymore.<a href="#pgfId-1008731"><sup class="footnotenumber">16</sup></a> A good balance would be, for example, making sure that the probability of selecting each individual is proportional to its fitness.</p>

  <p class="body"><a id="pgfId-1002002"></a>If we decide on the latter, we can use the roulette wheel selection. Each organism is assigned a section on a “roulette wheel” whose angle (and in turn the length of the arc subtended) is proportional to the fitness of the organism.</p>

  <p class="body"><a id="pgfId-1002015"></a>One way to obtain this is to compute the sum of the fitness of all individuals and then, for each one, see the percentage of the cumulative fitness for the whole population it accounts for.<a href="#pgfId-1008749"><sup class="footnotenumber">17</sup></a> For instance, figure 18.8 shows how to apply this to our example population for the knapsack problem. The angles of each organism’s sector are computed with the formula <code class="fm-code-in-text">θ=2</code><span class="cambria">ϖ</span><code class="fm-code-in-text">*f</code>, where <code class="fm-code-in-text">f</code> is the normalized fitness of the given individual, aka the fraction of the individual’s fitness over the total sum for the whole population.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F8.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032557"></a>Figure 18.8 Roulette wheel selection once again applied to the example of 0-1 knapsack problem we are discussing in this section</p>

  <p class="body"><a id="pgfId-1002058"></a>Then each time we need to select a new parent for mating, we spin the wheel and see where it stops: higher fitness will have a larger probability of being picked, but lower ones still have a shot.</p>

  <p class="fm-callout"><a id="pgfId-1002071"></a><span class="fm-callout-head">Note</span> Notice that if we choose to use rank instead of fitness, we’ll need to sort the population first (time required: <code class="fm-code-in-text2">O(n*log(n)))</code>. Sticking to fitness, we only need a linear number of operations to compute the total and the percentages, and since this is computed at each iteration, for large populations we can have a relevant savings. Likewise, tournament selection also doesn’t require prior sorting of the population.</p>

  <p class="body"><a id="pgfId-1002090"></a>When it comes to implementing this technique, however, we are clearly not going to bother with building an actual wheel (or even a simulated one)!</p>

  <p class="body"><a id="pgfId-1002099"></a>The easiest way we can achieve the same result is by building an array like the one shown in figure 18.9, where the <code class="fm-code-in-text">i</code>-th element contains the sum of the normalized<a href="#pgfId-1008764"><sup class="footnotenumber">18</sup></a> fitness of the first <code class="fm-code-in-text">i</code> individuals in the current population (taken in the order they are stored in the population array).</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F10.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032602"></a>Figure 18.9 Roulette wheel selection in practice. We use an array with cumulative percentages; the difference between <code class="fm-code-in-text">A[i]</code> and <code class="fm-code-in-text">A[i-1]</code> is exactly the ratio between the <code class="fm-code-in-text">i</code>-th element’s fitness and the sum of all elements’ fitness.</p>

  <p class="body"><a id="pgfId-1002145"></a>For example, if we refer to the population shown in figure 18.8, the first element of the wheel-array holds the <code class="fm-code-in-text">population[0].normalizedFitness = 0.16</code>, the second element holds <code class="fm-code-in-text">population[0].normalizedFitness + population[1].normalizedFitness = 0.16 + 0.15</code>, and so on, as summarized in figure 18.9. Notice that the last element sums up exactly to 1. Ideally, we can imagine a hidden first element, not shown in the figure, whose value is 0 (we’ll see that this helps with writing cleaner, succinct code).</p>

  <p class="body"><a id="pgfId-1002169"></a>To select an element, we draw a random number <code class="fm-code-in-text">r</code> from a regular distribution of real numbers between <code class="fm-code-in-text">0</code> and <code class="fm-code-in-text">1</code> (not included). Equivalently, in the wheel analogy, we would draw a random angle <code class="fm-code-in-text">θ</code> between 0° and 360° (not included) to find how much we should spin the wheel. The relation between the two numbers would be <code class="fm-code-in-text">θ=r*360</code>.</p>

  <p class="body"><a id="pgfId-1002194"></a>Listing 18.6 shows the method to generate a wheel-array like the one in figure 18.9.</p>

  <p class="body"><a id="pgfId-1002203"></a>To find where our wheel’s pin is pointing, we need to run a search on the array. We look for the smallest element that’s larger than <code class="fm-code-in-text">r</code>, the random number drawn by the selection routine. We can modify the binary search algorithm to perform this task and limit the runtime to <code class="fm-code-in-text">O(log(n))</code> for each element selected; therefore, for each iteration, we will have to perform <code class="fm-code-in-text">O(n*log(n))</code> operations to select parents and apply crossover.</p>

  <p class="body"><a id="pgfId-1002228"></a>Alternatively, a linear search is also possible. It’s a lot easier to write and less likely to be buggy, but that makes the running times grow to <code class="fm-code-in-text">O(n)</code> and <code class="fm-code-in-text">O(n<sup class="superscript1">2</sup>)</code> respectively.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1017776"></a>Listing 18.6 Creating a selection roulette wheel</p>
  <pre class="programlisting"><b class="strong">function</b> createWheel(population)                                      <span class="fm-combinumeral">❶</span>
  totalFitness ← <b class="strong">sum</b>({population[i].fitness, i=0,...,|population|-1}  <span class="fm-combinumeral">❷</span>
  wheel ← [0]                                                         <span class="fm-combinumeral">❸</span>
  
for i <b class="strong">in</b> {1,..,|population|} <b class="calibre21">do</b>                                       <span class="fm-combinumeral">❹</span>
  wheel[i] ← wheel[i-1] + population[i-1].fitness / totalFitness      <span class="fm-combinumeral">❺</span>
pop(wheel, 0)                                                         <span class="fm-combinumeral">❻</span>
return wheel  </pre>

  <p class="fm-code-annotation"><a id="pgfId-1020430"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">createWheel</code><a id="marker-1029131"></a> takes current population and returns an array whose generic <code class="fm-code-in-text2">i</code>-th elements is the cumulative fitness of all population’s individuals, from the first to the <code class="fm-code-in-text2">i</code>-th</p>

  <p class="fm-code-annotation"><a id="pgfId-1020465"></a><span class="fm-combinumeral">❷</span> Computes the total sum of all individuals’ fitness values (here we use the simplified notation discussed in appendix A)</p>

  <p class="fm-code-annotation"><a id="pgfId-1020499"></a><span class="fm-combinumeral">❸</span> Initializes the array for the “roulette wheel.” We set the first element to <code class="fm-code-in-text2">0</code> for convenience, so that we don’t have to handle a special case for the first individual outside the next <code class="fm-code-in-text2">for</code> loop.</p>

  <p class="fm-code-annotation"><a id="pgfId-1020625"></a><span class="fm-combinumeral">❹</span> Cycles through all individuals in the population</p>

  <p class="fm-code-annotation"><a id="pgfId-1020591"></a><span class="fm-combinumeral">❺</span> Each element in the roulette wheel’s array is the sum of the normalized fitness of all individuals before it (already stored in <code class="fm-code-in-text2">wheel[i-1]</code>) plus the ratio between the current individual’s fitness and the sum of all organisms’ fitness.</p>

  <p class="fm-code-annotation"><a id="pgfId-1020557"></a><span class="fm-combinumeral">❻</span> Optionally, we can now drop the first element of the array, containing a <code class="fm-code-in-text2">0</code>. In many languages, this operation requires <code class="fm-code-in-text2">O(n)</code> assignments, so we might want to avoid it. If we keep the first value, the search method can easily take that into account, for instance, by starting search from the element at index <code class="fm-code-in-text2">1</code> and subtracting one from the index found before returning.</p>

  <p class="body"><a id="pgfId-1002460"></a>Choose wisely depending on the time constraints you have, as well as your domain knowledge. We leave the implementation of your preferred method of search as an exercise, and I strongly suggest you start with the simplest method, test it thoroughly, and then, if you really need the speedup, attempt to write the binary search method and compare their <a id="marker-1005327"></a><a id="marker-1005331"></a><a id="marker-1005335"></a><a id="marker-1005339"></a>results.</p>

  <h3 class="fm-head2" id="heading_id_10"><a id="pgfId-1002478"></a>18.1.7 Crossover</h3>

  <p class="body"><a id="pgfId-1002490"></a>Once we have selected a pair of parents, we are ready for the next step: as we outlined in listing 18.4, it’s time for crossover.</p>

  <p class="body"><a id="pgfId-1002503"></a>We mentioned this already, but just to give you a quick recap: crossover simulates mating and sexual reproduction of animal<a href="#pgfId-1008780"><sup class="footnotenumber">19</sup></a> populations in nature, stimulating diversity in the offspring by allowing the recombination of subsets of features from both parents in each child.</p>

  <p class="body"><a id="pgfId-1002514"></a>In the analogous algorithmic process, crossover corresponds to wide-range search of the fitness function landscape, since we generally recombine large sections of the genome (and hence of the solutions) carried by each of the parents. This is equivalent to a long leap in the problem space (and cost function landscape).</p>

  <p class="body"><a id="pgfId-1002527"></a>For instance, take our example problem: the 0-1 knapsack and packing goods for a journey in space. Figure 18.10 shows a possible definition for the crossover method for this problem: we choose a single crossover point, an index at which we cut both chromosomes; then one part of each chromosome (at the opposite sides of the crossover point) will be used for recombination. In this case, we just glue the two parts together.</p>

  <p class="body"><a id="pgfId-1002571"></a>Randomness is and should be a huge part of crossover. In our example, we can see it in the choice of the crossover point. Some choices (as in figure 18.10 (A)) will lead to improvements in the child’s fitness, while other crossover points (as in figure 18.10 (B)) will lead to poorer or sometimes even disastrous results.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F11.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032644"></a>Figure 18.10 Examples of crossover for the knapsack problem. Once the two parents are selected, we need to choose a crossover point, and then recombine the two genomes. Depending on this choice, we can have an improvement (A) or a worsening (B) in fitness for the children.</p>

  <p class="body"><a id="pgfId-1002592"></a>This single-crossover-point technique, if you noticed, actually produces two pairs of opposite sub-sequences (left-right and right-left of the crossover point). While in our example we only used one of the pairs and discarded the other, some implementations of the genetic algorithm use both pairs to produce two children with all the possible combinations<a href="#pgfId-1008795"><sup class="footnotenumber">20</sup></a> (either keeping them both or just keeping the one with better fitness).</p>

  <p class="body"><a id="pgfId-1002609"></a>But nobody says we need to stick with the single-point crossover; that’s just one of many alternatives. For instance, we could have a two-point crossover, selecting a segment of the first chromosome (and, implicitly, one or two remaining segments of the other), or we could even randomly select each value from one of the parents’ chromosomes. Both examples are shown in figure 18.11.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F12.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032689"></a>Figure 18.11 More examples of crossover for the knapsack problem. (A) Two-point recombination: select a segment from the first chromosome, and the rest from the other. (B) For each gene (aka value), randomly decide from which parent it will be copied.</p>

  <p class="body"><a id="pgfId-1002648"></a>And many more alternatives are possible: as we said before, crossover can only be defined on the actual chromosomes, and depending on their structure and constraints (it ultimately depends on the structure of the candidate solution), different ways of recombining chromosomes are possible. We’ll see plenty of examples later in this chapter.</p>

  <p class="body"><a id="pgfId-1002673"></a>One last thing: it is customary to associate a <i class="calibre17">crossover chance</i><a id="marker-1005343"></a>, aka crossover ratio, to our crossover operators. This represents the probability that crossover/mating actually happens between the selected individuals. For instance, with a crossover ratio of 0.7, there is a 70% chance that crossover does happen for any pair of organisms selected for mating.</p>

  <p class="body"><a id="pgfId-1002693"></a>As shown in listing 18.7, every time we apply crossover, we randomly decide what to do, based on the crossover chance. If the choice is not to apply crossover, then there are a few alternatives; most commonly, we randomly select one of the parents that move to the next step. Notice that this is different from <i class="calibre17">elitism</i> because the organism outputted by crossover will still undergo mutations, while any individual selected by elitism will be copied completely unaltered to the new population.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1017810"></a>Listing 18.7 A wrapper class for crossover operator</p>
  <pre class="programlisting"><b class="strong">class</b> CrossoverOperator                                              <span class="fm-combinumeral">❶</span>
  <b class="strong">#type function</b>
  method                                                             <span class="fm-combinumeral">❷</span>
  <b class="strong">#type float</b>
  chance                                                             <span class="fm-combinumeral">❸</span>
 
  <b class="calibre21">function</b> CrossoverOperator(crossoverMethod, crossoverChance)       <span class="fm-combinumeral">❹</span>
 
  <b class="calibre21">function</b> apply(p1, p2)                                             <span class="fm-combinumeral">❺</span>
    <b class="calibre21">if</b> random() &lt; <b class="calibre21">this</b>.chance then
      <b class="calibre21">return new</b> Individual(method(p1.chromosome, p2.chromosome))    <span class="fm-combinumeral">❻</span>
    <b class="calibre21">else</b>
      <b class="calibre21">return</b> choose(p1, p2)                                          <span class="fm-combinumeral">❼</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1020793"></a><span class="fm-combinumeral">❶</span> Class <code class="fm-code-in-text2">CrossoverOperator</code><a id="marker-1028603"></a> models a wrapper for the actual crossover method specific to individual problems. While the crossover method changes with the problem definition, it’s good to have a uniform, stable API that can be used in the main method for the genetic algorithm.</p>

  <p class="fm-code-annotation"><a id="pgfId-1020829"></a><span class="fm-combinumeral">❷</span> We need to store a reference/pointer to the actual method to run.</p>

  <p class="fm-code-annotation"><a id="pgfId-1020860"></a><span class="fm-combinumeral">❸</span> We also need to store the probability that this method is applied to the parents.</p>

  <p class="fm-code-annotation"><a id="pgfId-1020893"></a><span class="fm-combinumeral">❹</span> The constructor (body omitted) takes two arguments to initialize the two class’ attributes.</p>

  <p class="fm-code-annotation"><a id="pgfId-1020932"></a><span class="fm-combinumeral">❺</span> This method will take two organisms, the two parents, and output the organism that will be passed to the next generation.</p>

  <p class="fm-code-annotation"><a id="pgfId-1020997"></a><span class="fm-combinumeral">❻</span> With probability proportional to <code class="fm-code-in-text2">crossoverChance</code><a id="marker-1028688"></a>, it will apply the crossover method passed at construction, and then return a new individual created recombining (in some problem-dependent way) its parents’ chromosomes.</p>

  <p class="fm-code-annotation"><a id="pgfId-1020966"></a><span class="fm-combinumeral">❼</span> If the crossover is not to be applied, then we have a choice of what to return: in this case, we just return one of the two parents, randomly choosing it.</p>

  <p class="body"><a id="pgfId-1002962"></a>Remember that listing 18.7 shows a wrapper for the actual crossover operator, compatible with the template we provided in listing 18.4 for natural selection’s generic template. The actual method will be crafted and passed every time a specific problem instance is addressed.</p>

  <p class="body"><a id="pgfId-1002971"></a>Listing 18.8 shows the single-point crossover operator that we discussed for the 0-1 knapsack <a id="marker-1005355"></a><a id="marker-1005359"></a>problem.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1017838"></a>Listing 18.8 0-1 Knapsack: crossover</p>
  <pre class="programlisting"><b class="strong">function</b> knapsackCossover(chromosome1, chromosome2)            <span class="fm-combinumeral">❶</span>
  i ← randomInt(1, |chromosome1|-1)                            <span class="fm-combinumeral">❷</span>
     <b class="calibre21">return</b> chromosome1[0:i] + chromosome2[i:|chromosome2|]    <span class="fm-combinumeral">❸</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1021046"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">knapsackCossover</code><a id="marker-1028415"></a> takes two chromosomes (as bit strings) and returns a new chromosome obtained by recombining the head of the first one, with the tail of the second one.</p>

  <p class="fm-code-annotation"><a id="pgfId-1021081"></a><span class="fm-combinumeral">❷</span> Chooses a cut point, making sure at least one gene is taken from each parent</p>

  <p class="fm-code-annotation"><a id="pgfId-1021112"></a><span class="fm-combinumeral">❸</span> Returns the combination of the head of <code class="fm-code-in-text2">chromosome1</code> (up to index <code class="fm-code-in-text2">i</code>, excluded) and the tail of <code class="fm-code-in-text2">chromosome2</code> (from the index <code class="fm-code-in-text2">i</code> to the end)</p>

  <h3 class="fm-head2" id="heading_id_11"><a id="pgfId-1003074"></a>18.1.8 Mutations</h3>

  <p class="body"><a id="pgfId-1003086"></a>After <a id="marker-1005367"></a><a id="marker-1005371"></a><a id="marker-1005375"></a>a new organism is created through crossover, the genetic algorithm applies mutations to its newly recombined genome. In nature, as we have seen, the recombination of parents’ genomes and mutations happens simultaneously<a href="#pgfId-1008811"><sup class="footnotenumber">21</sup></a> during sexual reproduction. Conceptually, this works analogously in genetic algorithms, although the two operators are implemented separately. If crossover is regarded as wide-range search, mutations are often used for small-range, local search.</p>

  <p class="body"><a id="pgfId-1003110"></a>We have seen that crossover promotes diversity in the new population, so why do we need mutations?</p>

  <p class="body"><a id="pgfId-1003119"></a>The answer is simple and best provided with an example (shown in figure 18.12). The population considered in this example shares the same values for three of the genes. This means that no matter how you recombine the organisms’ genes during crossover, the resulting children will all have beans and strawberries included in the solution and potatoes excluded from it. This is a real risk for problems where chromosomes are large (carrying a lot of information), especially if the length of the chromosome is larger or comparable to the size of the population. The danger is that no matter how long we run our simulation, or how many iterations and crossovers we perform, we won’t be able to flip the values for some genes, which in turn means that the areas of the problem spaces that we can explore are limited by the choice of the initial population. And this, of course, would be a terrible limitation.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F13.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032731"></a>Figure 18.12 An example of a population where, without mutations, there wouldn’t be enough genetic diversity. All individuals have the same value for three of their genes.</p>

  <p class="body"><a id="pgfId-1003165"></a>To prevent this issue and increase the diversity in the genomic pool of our population, we add a mechanism that can change each gene independently and for any organism.</p>

  <p class="body"><a id="pgfId-1003174"></a>In Holland’s original work, as we had mentioned, chromosomes were bit strings, and the mutation was also conceived as domain-agnostic. The mutation operator would be applied to each organism’s chromosome, and for each gene (that is, each bit) it would toss a coin<a href="#pgfId-1008833"><sup class="footnotenumber">22</sup></a> and decide if the bit should be flipped. This is shown in figure 18.13.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F14.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032776"></a>Figure 18.13 An example of mutation for the 0-1 knapsack problem. The mutation rate here is overly boosted (for the sake of presentation). In a real scenario it would likely be set somewhere between 1% and 0.1% (also depending on the size of the chromosomes). A too-large mutation ratio can hinder the stability of the algorithm’s convergence toward the minimum, voiding the benefits of crossover and selection.</p>

  <p class="body"><a id="pgfId-1003215"></a>In modern genetic algorithms, however, chromosomes can take different shapes and have constraints, so mutations can become much more complex or be applied to the chromosome as a whole, rather than to single genes. As we’ll see for TSP, for instance, a mutation operator can be applied (with a certain probability) just once to the whole chromosome (instead of to each of its genes) and swap two vertices in the tour.</p>

  <p class="body"><a id="pgfId-1003226"></a>The wrapper class for the mutation operator is identical to the one for crossover shown in listing 18.7, apart from minor differences in handling arguments, especially the ones to the wrapped methods: we only have one organism to pass, but it’s a good idea to also forward the mutation chance to the wrapped method. This is necessary for bit-wise mutations, such as the one we discussed for the knapsack problem, implemented in <a id="marker-1005379"></a><a id="marker-1005383"></a><a id="marker-1005387"></a>listing 18.9.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1017878"></a>Listing 18.9 0-1 knapsack: Bit-wise mutation</p>
  <pre class="programlisting"><b class="strong">function</b> knapsackMutation(chromosome, mutationChance)   <span class="fm-combinumeral">❶</span>
  <b class="calibre21">for</b> i <b class="strong">in</b> {0,..,|chromosome|} <b class="calibre21">do</b>                       <span class="fm-combinumeral">❷</span>
    <b class="strong">if</b> random() &lt; mutationChance <b class="strong">then</b>
      chromosome[i] ← 1 – chromosome[i]                 <span class="fm-combinumeral">❸</span>
  <b class="strong">return</b> chromosome</pre>

  <p class="fm-code-annotation"><a id="pgfId-1021221"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">knapsackMutation</code><a id="marker-1028235"></a> takes a chromosome (as a bit string) and a probability of mutation (as a real number between <code class="fm-code-in-text2">0</code> and <code class="fm-code-in-text2">1</code>) and applies mutations to the chromosome. The method, in this implementation, alters the argument, but it’s possible and often cleaner to clone the input and return a new object as output, unless this cloning becomes a bottleneck. Weigh the pros and cons and run some profiling before making a choice.</p>

  <p class="fm-code-annotation"><a id="pgfId-1021254"></a><span class="fm-combinumeral">❷</span> Cycles through each gene (that is, bit) in the chromosome</p>

  <p class="fm-code-annotation"><a id="pgfId-1021285"></a><span class="fm-combinumeral">❸</span> With probability <code class="fm-code-in-text2">mutationChance(*100)</code>, flip current bit</p>

  <h3 class="fm-head2" id="heading_id_12"><a id="pgfId-1003382"></a>18.1.9 The genetic algorithm template</h3>

  <p class="body"><a id="pgfId-1003400"></a>Listing 18.10 <a id="marker-1005395"></a><a id="marker-1005399"></a>provides an implementation of the main method for the genetic algorithm template, and also concludes our discussion of the 0-1 knapsack problem. We have all we need now to actually run the algorithm on our instance and find out that the best possible solution is bringing wheat flour, rice, and beans. To see a solution in action, and possibly apply it to larger problem instances with hundreds of possible items to pack, you can use the implementation of the genetic algorithm provided by JsGraphs on GitHub<a href="#pgfId-1008850"><sup class="footnotenumber">23</sup></a> and implement the methods for crossover, mutations, and random initialization that we discussed in this section. It will be a great exercise to test your understanding of this technique and delve deeper into its <a id="marker-1005403"></a><a id="marker-1005407"></a>details.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1017906"></a>Listing 18.10 A generic implementation of the genetic algorithm</p>
  <pre class="programlisting"><b class="strong">function</b> geneticAlgorithm(                                              <span class="fm-combinumeral">❶</span>
<span class="fm-code-continuation-arrow">➥</span> populationSize, chromosomeInitilizer, elitism, selectForMating,
<span class="fm-code-continuation-arrow">➥</span> crossover, mutations, maxSteps)    
  population ← initPopulation(populationSize, chromosomeInitilizer)    <span class="fm-combinumeral">❷</span>
  <b class="strong">for</b> k <b class="strong">in</b> {1..maxSteps} <b class="strong">do</b>                                             <span class="fm-combinumeral">❸</span>
    population ← naturalSelection(
    <span class="fm-code-continuation-arrow">➥</span> population, elitism, selectForMating, crossover, mutations)      <span class="fm-combinumeral">❹</span>
  <b class="strong">return</b> findBest(population).chromosome                                <span class="fm-combinumeral">❺</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1021325"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">geneticAlgorithm</code><a id="marker-1027873"></a> is a template method that implements the backbone of a genetic algorithm and can be adapted to run on several problems by passing the specialized methods for the concrete problems’ instances.</p>

  <p class="fm-code-annotation"><a id="pgfId-1021360"></a><span class="fm-combinumeral">❷</span> Initializes the population</p>

  <p class="fm-code-annotation"><a id="pgfId-1021391"></a><span class="fm-combinumeral">❸</span> Repeats <code class="fm-code-in-text2">maxSteps</code><a id="marker-1027927"></a> times. Each iteration is a new generation in the simulation.</p>

  <p class="fm-code-annotation"><a id="pgfId-1021458"></a><span class="fm-combinumeral">❹</span> Let natural selection take its course . . .</p>

  <p class="fm-code-annotation"><a id="pgfId-1021426"></a><span class="fm-combinumeral">❺</span> Finally, finds the best individual in the population and returns its chromosome (which is the best solution found). Method <code class="fm-code-in-text2">findBest</code><a id="marker-1027961"></a> can also be supplied as a parameter, if needed.</p>

  <h3 class="fm-head2" id="heading_id_13"><a id="pgfId-1003620"></a><a id="id_Hlk45479390"></a>18.1.10 When does the genetic algorithm work best?</h3>

  <p class="body"><a id="pgfId-1003642"></a>To <a id="marker-1015818"></a><a id="marker-1015819"></a>recap what we have discussed so far, in chapters 16 and 17, we have presented a few techniques that can be used in optimization problems to find near-optimal solutions without exploring the whole problem space. Each of them comes with some strengths and some pain points:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1003658"></a><i class="calibre15">Gradient descent</i><a class="calibre14" id="marker-1015823"></a> (chapter 16) converges fast but tends to get stuck in local minima. It also requires the cost function to be differentiable (as such, it couldn’t be used in experimental settings or game theory, for instance in the predator-prey robotic evolution experiment described earlier in this section).</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1003676"></a><i class="calibre15">Random sampling</i><a class="calibre14" id="marker-1015814"></a> (chapter 16) overcomes the need for differentiability and the issue with local minima, but it’s (unbearably) slow in converging (<i class="calibre15">when</i> it manages to converge).</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1003696"></a><i class="calibre15">Simulated annealing</i> <a class="calibre14" id="marker-1015808"></a>(chapter 17) has the same advantages of random sampling, but evolves in a more controlled and steady way toward minima. Nevertheless, convergence can still be slow, and it has a hard time finding minima when they lie in narrow valleys.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1003717"></a>At the beginning of the chapter, we discussed how the genetic algorithm can overcome many of the issues of these heuristics; for instance, it can speed up convergence, compared to simulated annealing, by keeping a pool of solutions and evolving them together.</p>

  <p class="body"><a id="pgfId-1003732"></a>To conclude this section, I’d like to provide yet another criterion that can help you choose which optimization technique you should use. It involves another term, <i class="calibre17">epistasis</i><a id="marker-1005443"></a>, that is borrowed from biology: its mean is <i class="calibre17">gene interaction</i>, and in our algorithmic analogy, it expresses the presence of dependent variables—in other words, variables whose values depend on other variables.</p>

  <p class="body"><a id="pgfId-1003756"></a>Let’s first go through an example to better explain this.</p>

  <p class="body"><a id="pgfId-1003765"></a>Each gene on a chromosome can be considered a separate variable that can assume values within a certain domain. For the 0-1 knapsack problem, each gene is an independent variable, because the value it assumes won’t directly influence other variables. (If we enforce the constraint on the weight of each solution, however, flipping a gene to <code class="fm-code-in-text">1</code> might force us to flip one or more other genes to <code class="fm-code-in-text">0</code>. That’s a loose indirect dependency, anyway.)</p>

  <p class="body"><a id="pgfId-1003792"></a>For the TSP, as we’ll see, we assign a vertex to each gene with the constraint that there can’t be any duplicate. As such, assigning a variable will impose constraints on all other variables (that won’t be able to be assigned the same value), so we have a more direct, although still loose, dependency.</p>

  <p class="body"><a id="pgfId-1003805"></a>If our problem was optimizing the energy efficiency of a house that was being designed, and some variables were its area, the number of rooms, and the amount of wood needed for the floors, the latter would be dependent on the other two, because the area to cover with wood flooring would depend on the square feet and rooms of the house. Changing the size of the house would immediately force us to change the amount of wood used if the floor’s thickness remains fixed (the possibility of changing its thickness makes it meaningful having a separate variable for this).</p>

  <p class="body"><a id="pgfId-1003816"></a>For 0-1 knapsack, we say that the problem has low variable interaction, and so low epistasis; the house optimization has high epistasis, while for the TSP, its epistasis is somewhere in the middle.</p>

  <p class="body"><a id="pgfId-1003829"></a>Now, the interesting thing is that the degree of variable interaction contributes to the shape of the landscape of the objective function that we want to optimize: the higher the epistasis, the wavier the landscape will likely look.</p>

  <p class="body"><a id="pgfId-1003846"></a>But above all, when the degree of interaction is high, varying one variable also changes the value of other variables, and this means making a bigger leap in the cost function’s landscape and problem domain, making it harder to explore the surroundings of solutions and fine-tune the algorithm’s results.</p>

  <p class="body"><a id="pgfId-1003857"></a>Knowing the epistasis of a problem can guide our choice of the best algorithm to apply:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1003866"></a>With low epistasis, minimum-seeking algorithms like gradient-descent work best.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1003879"></a>With high epistasis, it’s best to prefer random search, so simulated annealing, or for very high variables interaction, random sampling.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1003893"></a>What about genetic algorithms? It turns out that they work best in a wide range of medium to high epistasis.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1003905"></a>Therefore, when we design the cost/fitness function for a problem (which, let me state this clearly once again, is one of the most crucial steps to a good solution), we need to be careful about the degree of interaction in order to choose the best technique that we can apply.</p>

  <p class="body"><a id="pgfId-1003914"></a>During the design phase, we can also try to reduce the dependent variables, whenever possible. This will allow us to use more powerful techniques and ultimately get better solutions.</p>

  <p class="body"><a id="pgfId-1003927"></a>Now that we have concluded our discussion of the components and theory of genetic algorithms, we are ready to delve into a couple of practical applications to see how powerful this <a id="marker-1005447"></a><a id="marker-1005451"></a>technique is.</p>

  <h2 class="fm-head" id="heading_id_14"><a id="pgfId-1003939"></a>18.2 TSP</h2>

  <p class="body"><a id="pgfId-1003951"></a>We <a id="marker-1023997"></a><a id="marker-1023998"></a><a id="marker-1023999"></a>described the <i class="calibre17">travelling salesman problem</i> (or <i class="calibre17">TSP</i><a id="marker-1024000"></a> for short) in chapter 17, where we also provided a solution based on simulated annealing.</p>

  <p class="body"><a id="pgfId-1003971"></a>Figure 18.14 shows the same example we produced in the previous chapter, an instance of the problem with 10 cities for which the best solution has cost <code class="fm-code-in-text">1625</code> (miles), the total length of the following path:</p>
  <pre class="programlisting">[“New Carthage”, “Syracuse”, “Buffalo”, “Pittsburgh”, “Harrisburg”, 
<span class="fm-code-continuation-arrow">➥</span> “Opal City”, “Metropolis”, “Gotham City”, “Civic City” ]
 </pre>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F15.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032827"></a>Figure 18.14 An instance of the traveling salesman problem: the best solution, outlined in this figure, has cost <code class="fm-code-in-text">1625</code>.</p>

  <p class="body"><a id="pgfId-1004014"></a>Now we would like to tackle the same problem with a genetic algorithm and see if we can speed up convergence to a good solution (and possibly improve the average solution: remember that these optimization algorithms output near-optimal solutions, and so they provide no guarantee of always finding the best possible one).</p>

  <h3 class="fm-head2" id="heading_id_15"><a id="pgfId-1004029"></a>18.2.1 Fitness, chromosomes, and initialization</h3>

  <p class="body"><a id="pgfId-1004047"></a>A <a id="marker-1005471"></a><a id="marker-1005475"></a><a id="marker-1005479"></a><a id="marker-1005483"></a><a id="marker-1005487"></a><a id="marker-1005491"></a>good starting point, in general, for all problems, is the design of how solutions are encoded and of the cost function. While with the 0-1 knapsack we wanted to maximize a quantity—the total value of the entries added to the knapsack—TSP is a minimization problem, so we’ll assume the use of a minimization algorithm (as mentioned, otherwise we can just maximize the opposite or inverse of the cost).</p>

  <p class="body"><a id="pgfId-1004067"></a>Therefore, we can use the same definition for the fitness function of the genetic algorithm as the cost function we saw in section 17.2, and in particular in listing 17.2: that is, just the sum of the distances between adjacent vertices, as shown in figure 18.14.</p>

  <p class="body"><a id="pgfId-1004080"></a>We can even reuse the same encoding for solutions: they are permutations of the graph’s vertices. This can easily be translated into chromosomes that won’t be bit strings anymore in this case; we need each gene to be an integer value (the index of one of the vertices), with the implicit constraint that there can’t be two genes holding the same value.</p>

  <p class="body"><a id="pgfId-1004095"></a>We can initialize any solution as a random permutation of the indices between <code class="fm-code-in-text">0</code> and <code class="fm-code-in-text">n-1</code>, where <code class="fm-code-in-text">n</code> is the number of vertices in the graph, just like we did for simulated annealing. We can even pass the same method as the <code class="fm-code-in-text">chromosomeInitializer</code><a id="marker-1005495"></a> argument to method <code class="fm-code-in-text">initPopulation</code><a id="marker-1005499"></a> in <a id="marker-1005503"></a><a id="marker-1005507"></a><a id="marker-1005511"></a><a id="marker-1005515"></a><a id="marker-1005519"></a><a id="marker-1005523"></a>listing 18.2.</p>

  <h3 class="fm-head2" id="heading_id_16"><a id="pgfId-1004127"></a>18.2.2 Mutations</h3>

  <p class="body"><a id="pgfId-1004139"></a>Even <a id="marker-1024073"></a><a id="marker-1024074"></a><a id="marker-1024075"></a>for mutations, our job can be (at least for an initial assessment) pretty much limited to reusing methods developed in chapter 17 for local search, and if we’d like to compare to the performance of simulated annealing, we also <i class="calibre17">shouldn’t</i> add new kinds of mutations.</p>

  <p class="body"><a id="pgfId-1004156"></a>There are, however, a few differences. First, while in listing 17.3 we implement an ensemble method, where with some probability one of the transitions will be applied, in genetic algorithms, this ensemble mechanism is already implicit for mutations, and thus we have to provide each mutation (aka local search) method separately.</p>

  <p class="body"><a id="pgfId-1004167"></a>Another difference is that for the genetic algorithm it doesn’t really make sense to have a mutation that randomly recreates an individual from scratch. This goes against the principles of natural selection, and we have seen that genome diversity in the population is already supported by crossover and local mutations.</p>

  <p class="body"><a id="pgfId-1004180"></a>Therefore, we can extract two methods from listing 17.3, one for swapping adjacent vertices, and one for swapping random vertices, and in turn create two mutation operators from them. (Both are shown in listing 18.11. Warning: These methods, for the sake of simplicity, alter their inputs—while this is usually fine for GA mutations, you should be aware of that and state it clearly in the method’s documentation.) To see these methods in action, see figure 17.13 in the previous chapter.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1017984"></a>Listing 18.11 TSP mutations</p>
  <pre class="programlisting"><b class="strong">function</b> swapAdjacent(P)       <span class="fm-combinumeral">❶</span>
  i ← randomInt(0, |P|)        <span class="fm-combinumeral">❷</span>
  swap(i, (i+1) % |P|)
  <b class="calibre21">return</b> P  
 
<b class="strong">function</b> swapAny(P)            <span class="fm-combinumeral">❸</span>
  i ← randomInt(0, |P|)        <span class="fm-combinumeral">❹</span>
  j ← randomInt(0, |P|)
  swap(i, j)
  <b class="calibre21">return</b> P  </pre>

  <p class="fm-code-annotation"><a id="pgfId-1021537"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">swapAdjacent</code><a id="marker-1027633"></a> takes a chromosome, aka candidate solution <code class="fm-code-in-text2">P</code> (a point in the problem space, that is, a permutation of the list of vertices in the graph), and returns the same chromosome after swapping an adjacent pair of vertices.</p>

  <p class="fm-code-annotation"><a id="pgfId-1021581"></a><span class="fm-combinumeral">❷</span> Randomly chooses an index in the array and swaps two adjacent vertices</p>

  <p class="fm-code-annotation"><a id="pgfId-1021615"></a><span class="fm-combinumeral">❸</span> Method <code class="fm-code-in-text2">swapAny</code><a id="marker-1027670"></a> also takes a chromosome and returns the same chromosome, after swapping a random pair of vertices.</p>

  <p class="fm-code-annotation"><a id="pgfId-1021649"></a><span class="fm-combinumeral">❹</span> Selects the vertices to swap. Notice that the two indices might in theory be equal.</p>

  <p class="body"><a id="pgfId-1004381"></a>A final, though important, consideration: the mutation chance in GAs will typically be much smaller than the ratios we used for the same methods in simulated annealing (because, obviously, for simulated annealing these “mutations” were the only way to explore the problem domain at any iteration).</p>

  <p class="body"><a id="pgfId-1004391"></a>As a first attempt, however, we tried to use the same probabilities to try and make a fairer comparison. A couple of sections later, we’ll discuss how it <a id="marker-1005547"></a><a id="marker-1005551"></a><a id="marker-1005555"></a>went.</p>

  <h3 class="fm-head2" id="heading_id_17"><a id="pgfId-1004410"></a>18.2.3 Crossover</h3>

  <p class="body"><a id="pgfId-1004422"></a>In <a id="marker-1005559"></a><a id="marker-1005563"></a>crossover lies the real novelty we are adding, with respect to simulated annealing: the latter had no way to combine two good solutions. Truth be told, there wasn’t even a way to keep multiple solutions in the first place!</p>

  <p class="body"><a id="pgfId-1004438"></a>There are many ways in which we can combine two permutations of the same sequence. OK, perhaps not that many, but there is definitely more than one. Since we don’t need to make things even more complicated, let’s go for the simplest way—or at least the simplest I can think of.</p>

  <p class="body"><a id="pgfId-1004451"></a>We are still going to choose a single cutoff point, like for 0-1 knapsack, but then things get interesting. Let’s see an example to help us understand. Figure 18.15 shows this crossover method in action for the example we have used since chapter 17, a graph with 10 vertices.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F16.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032869"></a>Figure 18.15 Crossover method for TSP. While the first half of the new chromosome can be directly copied from the first parent, deriving the second half from the second parent requires a lot more effort.</p>

  <p class="body"><a id="pgfId-1004485"></a>After randomly choosing a cut point, we copy all the genes in the first parent’s chromosome to the child’s genome, from the gene at index <code class="fm-code-in-text">0</code> to the one at the point of cut.</p>

  <p class="body"><a id="pgfId-1004496"></a>Now we need to fill the rest of the child’s chromosome, but we can’t just copy the sequence after the cut point (like we did for the knapsack’s bit strings), because the last four genes in the second organism’s chromosome contain vertices <code class="fm-code-in-text">0</code>, <code class="fm-code-in-text">2,</code> and <code class="fm-code-in-text">9</code>, which have already been assigned as values to genes in the first half of the new chromosome.</p>

  <p class="body"><a id="pgfId-1004512"></a>What we can do, instead, is go through the whole second chromosome from start to finish, and each time we find one of the vertices that was not copied from chromosome 1 to the child (vertices 1, 4, 8, and 5), we add this vertex to the new chromosome. This way, the net result is that the first part of the new chromosome, from its beginning to the cut point, is identical to its first parent’s, while the vertices in the second part, after the cut point, appear in the same order as they do in the second parent (where, however, they might not have been adjacent to each other).</p>

  <p class="body"><a id="pgfId-1004525"></a>Listing 18.12 provides a possible implementation for this <a id="marker-1005567"></a><a id="marker-1005571"></a>method.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1018012"></a>Listing 18.12 TSP: crossover</p>
  <pre class="programlisting"><b class="strong">function</b> tspCossover(chromosome1, chromosome2)                <span class="fm-combinumeral">❶</span>
  i ← randomInt(1, |chromosome1|-1)                           <span class="fm-combinumeral">❷</span>
  newChromosome ← chromosome1[0:i]                            <span class="fm-combinumeral">❸</span>
  genesFromChromosome1 ← <b class="strong">new</b> Set(newChromosome)               <span class="fm-combinumeral">❹</span>
  <b class="strong">for</b> j <b class="strong">in</b> {0,..,|chromosome2|-1} <b class="strong">do</b>                          <span class="fm-combinumeral">❺</span>
    <b class="strong">if</b> <b class="strong">not</b> chromosome2[j] <b class="strong">in</b> genesFromChromosome1 <b class="strong">then</b>        <span class="fm-combinumeral">❻</span>
      newChromosome.add(chromosome2[j])
  <b class="calibre21">return new</b>Chromosome  </pre>

  <p class="fm-code-annotation"><a id="pgfId-1021735"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">tspCossover</code><a id="marker-1027228"></a> takes two chromosomes (as arrays) and returns a new chromosome obtained by recombining the inputs.</p>

  <p class="fm-code-annotation"><a id="pgfId-1021770"></a><span class="fm-combinumeral">❷</span> Chooses a cut point, making sure that at least one gene is taken from each parent</p>

  <p class="fm-code-annotation"><a id="pgfId-1021801"></a><span class="fm-combinumeral">❸</span> Initializes the new chromosome with the head of <code class="fm-code-in-text2">chromosome1</code> (all genes from its start up to index <code class="fm-code-in-text2">i</code>, excluded)</p>

  <p class="fm-code-annotation"><a id="pgfId-1021832"></a><span class="fm-combinumeral">❹</span> Stores the genes taken from <code class="fm-code-in-text2">chromosome1</code> in a set: this will help with optimizing performance later.</p>

  <p class="fm-code-annotation"><a id="pgfId-1021863"></a><span class="fm-combinumeral">❺</span> To fill the remaining genes of <code class="fm-code-in-text2">newChromosome</code>, we need to cycle over the whole <code class="fm-code-in-text2">chromosome2</code>, examining all its genes.</p>

  <p class="fm-code-annotation"><a id="pgfId-1021896"></a><span class="fm-combinumeral">❻</span> If the <code class="fm-code-in-text2">j</code>-th gene’s value is a vertex that wasn’t yet added to the new chromosome, then append it to the end of it. For greater efficiency, we use a set to perform search in amortized constant time. Also, notice that we don’t need to update the set, because there are no duplicates.</p>

  <h3 class="fm-head2" id="heading_id_18"><a id="pgfId-1004746"></a>18.2.4 Results and parameters tuning</h3>

  <p class="body"><a id="pgfId-1004762"></a>Now <a id="marker-1005579"></a><a id="marker-1005583"></a>that we have all the pieces of the genetic algorithm defined and implemented, there are a couple of questions that we would like to answer:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1004774"></a>Do we get any improvement over simulated annealing?</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1004786"></a>How much do crossover and mutations influence the performance of the algorithm?</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1004798"></a>You might have guessed it: now it’s profiling time!</p>

  <p class="body"><a id="pgfId-1004807"></a>The former question is easier to answer. Simulated annealing, in our tests in section 17.2.4, based on JsGraphs’ implementation<a id="marker-1005587"></a>, required approximately 600 iterations to converge to the best solution, and we were able to obtain an average cost equal to <code class="fm-code-in-text">1668.248</code> (remember, Monte Carlo algorithms<a id="marker-1005591"></a>, like simulated annealing and the genetic algorithm, don’t always return the best result—there is no guarantee of that).</p>

  <p class="body"><a id="pgfId-1004831"></a>You can also take a look on JsGraphs’ GitHub at the implementation<span class="fm-hyperlink"><a href="#pgfId-1008867"><sup class="footnotenumber">24</sup></a></span> of a genetic algorithm to solve TSP. Running this algorithm with a population of 10 individuals, with the same mutations rates we used for simulated annealing, and a crossover chance of 0.7, the algorithm takes on average 10 generations to converge to the optimal solution (cost: <code class="fm-code-in-text">1625</code>). For a fair comparison we need to compare the iterations of simulated annealing (where a single solution is kept) to the product of the number of iterations of GA by the population size. Still, we go down from 600 to 100, a nice improvement.</p>

  <p class="body"><a id="pgfId-1004857"></a>But where we really understand the advantage we get is by computing the average cost on the same number of “total” iterations, for instance 1,000 iterations of simulated annealing versus a population of 25 individuals evolved for 40 iterations (or any combination whose product is 1,000). Without too much effort spent tuning the parameters, we were able to obtain an average cost equal to <code class="fm-code-in-text">1652.84</code>, using high chances for both crossover and random vertex swap mutation. With a population of 100 elements, the average goes down to <code class="fm-code-in-text">1636.8</code>.</p>

  <p class="body"><a id="pgfId-1004874"></a>Another interesting factor we would like to understand is this: How do crossover and mutations chances influence the evolution of the population?</p>

  <p class="body"><a id="pgfId-1004885"></a>To answer that, let’s try to run the algorithm with only one of these operators enabled and draw a curve of the best organism’s fitness. In order to get a better understanding and clearer charts, we’ll use a more complicated instance of the problem: a complete graph with 50 vertices (and random weights for the edges).</p>

  <p class="body"><a id="pgfId-1004898"></a>Figure 18.16 shows a chart with the plots of the optimization trend for three runs of the algorithm on a population with 100 organisms for 100 generations. The first thing you are likely to notice is that the crossover operator plateaus at some (early) point, but this shouldn’t be a surprise, right? We have already discussed the role of crossover and its limitations. In particular, it can recombine the genomes of the initial population, but if none of the organisms shows a certain feature,<a href="#pgfId-1008883"><sup class="footnotenumber">25</sup></a> then crossover can only improve fitness so much.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F17.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032914"></a>Figure 18.16 How TSP population evolves when only mutations versus only crossover are enabled. Crossover-only evolution plateaus soon enough, while the mutation operator swapping random pairs is the most successful. On the <code class="fm-code-in-text">x</code> axis, there is the number of generations, while on the <code class="fm-code-in-text">y</code> axis is the cost of the tour.</p>

  <p class="body"><a id="pgfId-1004938"></a>We can also see that mutation 2 is more effective than mutation 1, which also plateaus at some point: here as well, it shouldn’t be surprising. If you read chapter 17 about simulated annealing, where we introduced both these mutations, you saw that swapping only adjacent vertices is a small-range local search, which makes it more difficult to get out of local minima.</p>

  <p class="body"><a id="pgfId-1004953"></a>So far, so good, but what would happen if we were to increase the population size? This would increase the genome diversity in the initial population, so it’s legitimate to expect crossover will be more effective, drawing from a larger pool.</p>

  <p class="body"><a id="pgfId-1004962"></a>Figure 18.17 confirms this idea. Crossover makes the algorithm evolve faster in the initial phases and manages to get as good a solution as the one obtained using only the most effective mutation.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F18.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032956"></a>Figure 18.17 Same analysis as figure 18.16, but with a 10-fold population. With a larger population, crossover produces better results in earlier generations, and even gets to a result comparable to the mutations.</p>

  <p class="body"><a id="pgfId-1004994"></a>This was a particularly happy case, though. Running the crossover-only version a few more times, as in figure 18.18, shows that the quality of the best solution found depends on the diversity and quality of the initial population. If we are lucky during initialization, the algorithm will get to an optimal solution; otherwise—as expected—using only the crossover, the algorithm will plateau to a sub-optimal solution. When enabling only the mutations, the final result (not shown on the chart) has less variance across different runs, and the evolution plots of various attempts look similar.</p>

  <p class="body"><a id="pgfId-1005017"></a>Yet another consideration to make is that both charts in figures 18.16 and 18.17 also confirm that the “adjacent pairs swap” mutation is less relevant to the algorithm and can be put aside in favor of the “all pairs swap” one.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F19.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032998"></a>Figure 18.18 Using the same setup as for figure 18.17, we ran a few experiments using crossover-only (all with the same crossover chance, 0.7). This chart shows how there is a huge variance in the final results that depends entirely on the initial choice of the population (because there is no mutation).</p>

  <p class="body"><a id="pgfId-1005047"></a>So, do we get any advantage in using crossover <i class="calibre17">and</i> mutations at the same time? To understand that, we need to abandon our charts (which provide an idea for the trend of the simulation, but are not statistically relevant, because they are based on just a few runs), and run a more thorough analysis, summarized in table 18.2.</p>

  <p class="fm-table-caption"><a id="pgfId-1018183"></a>Table 18.2 Average best solution for the TSP genetic algorithm, varying the operators’ chance. The results are sorted by tour cost, and we highlighted the most significant rows.</p>

  <table border="1" class="contenttable" width="100%">
    <tr class="calibre8">
      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1018191"></a>Crossover</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1018193"></a>Adjacent swap</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1018195"></a>Random swap</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1018197"></a>Average tour cost</p>
      </th>
    </tr>

    <tr class="contenttable-tr">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018199"></a><code class="fm-code-in-text2">0.2</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018201"></a><code class="fm-code-in-text2">0</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018203"></a><code class="fm-code-in-text2">0.5</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018205"></a><code class="fm-code-in-text2">6326.447</code></p>
      </td>
    </tr>

    <tr class="contenttable-tr1">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018207"></a><code class="fm-code-in-text2">0.3</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018209"></a><code class="fm-code-in-text2">0.002</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018211"></a><code class="fm-code-in-text2">0.5</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018213"></a><code class="fm-code-in-text2">6379.628</code></p>
      </td>
    </tr>

    <tr class="contenttable-tr1">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018215"></a><code class="fm-code-in-text2">0.2</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018217"></a><code class="fm-code-in-text2">0.02</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018219"></a><code class="fm-code-in-text2">0.5</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018221"></a><code class="fm-code-in-text2">6409.664</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018223"></a><code class="fm-code-in-text2">0.1</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018225"></a><code class="fm-code-in-text2">0.002</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018227"></a><code class="fm-code-in-text2">0.5</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018229"></a><code class="fm-code-in-text2">6428.103</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018231"></a><code class="fm-code-in-text2">0.2</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018233"></a><code class="fm-code-in-text2">0</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018235"></a><code class="fm-code-in-text2">0.2</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018237"></a><code class="fm-code-in-text2">6555.593</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018239"></a><code class="fm-code-in-text2">0.2</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018241"></a><code class="fm-code-in-text2">0.2</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018243"></a><code class="fm-code-in-text2">0.5</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018245"></a><code class="fm-code-in-text2">6753.441</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018247"></a><code class="fm-code-in-text2">0.2</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018249"></a><code class="fm-code-in-text2">0.2</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018251"></a><code class="fm-code-in-text2">0.2</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018253"></a><code class="fm-code-in-text2">6823.971</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018255"></a><code class="fm-code-in-text2">0.2</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018257"></a><code class="fm-code-in-text2">0</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018259"></a><code class="fm-code-in-text2">0.7</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018261"></a><code class="fm-code-in-text2">7016.033</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018263"></a><code class="fm-code-in-text2">0</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018265"></a><code class="fm-code-in-text2">0</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018267"></a><code class="fm-code-in-text2">0.2</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018269"></a><code class="fm-code-in-text2">7110.798</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018271"></a><code class="fm-code-in-text2">0</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018273"></a><code class="fm-code-in-text2">0</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018275"></a><code class="fm-code-in-text2">0.7</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018277"></a><code class="fm-code-in-text2">7143.873</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018279"></a><code class="fm-code-in-text2">0.7</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018281"></a><code class="fm-code-in-text2">0</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018283"></a><code class="fm-code-in-text2">0.7</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018285"></a><code class="fm-code-in-text2">7818.061</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018287"></a><code class="fm-code-in-text2">0</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018289"></a><code class="fm-code-in-text2">0.7</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018291"></a><code class="fm-code-in-text2">0</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018293"></a><code class="fm-code-in-text2">10428.917</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018295"></a><code class="fm-code-in-text2">0.7</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018297"></a><code class="fm-code-in-text2">0</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018299"></a><code class="fm-code-in-text2">0</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018301"></a><code class="fm-code-in-text2">11655.949</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018303"></a><code class="fm-code-in-text2">0.2</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018305"></a><code class="fm-code-in-text2">0</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018307"></a><code class="fm-code-in-text2">0</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1018309"></a><code class="fm-code-in-text2">15210.800</code></p>
      </td>
    </tr>
  </table>

  <p class="body"><a id="pgfId-1005598"></a>As you can see, the best results have been found using a mix of crossover and mutations. Using random swaps mutation is more important than using crossover, while the adjacent swaps mutation is not only less important, but can even become detrimental when its chance is too high—the same way it happens for crossover. This might feel puzzling—after all, the worst you’d have expected was, maybe, that a mutation proved useless. How can we explain the fact that a higher chance for crossover and mutation 1 translates into worse solutions, on average? Well, the point is that even if we use elitism, when the other organisms in the new generations are affected by many changes, there is a higher chance that their fitness will get worse, and this effect becomes stronger with each generation, as the average fitness becomes better. If a solution is close to the optimal one, it’s much easier for a random change’s result to be pejorative than ameliorative.</p>

  <p class="body"><a id="pgfId-1006251"></a>That’s because, unlike simulated annealing, where we reject pejorative changes in the final stages of the simulation, GAs accept them without questioning. Besides the small fraction of high-fitness elements potentially guaranteed by elitism, the rest of the population can drive the average fitness down when too many random changes happen at the same time.</p>

  <p class="body"><a id="pgfId-1006264"></a>One final word of caution: these results are still to be taken with a grain of salt, because the average is computed on “only” a thousand runs, and because we haven’t explored extensively the domain of parameter <a id="marker-1008231"></a><a id="marker-1008235"></a>values.</p>

  <p class="fm-callout"><a id="pgfId-1006278"></a><span class="fm-callout-head">Note</span> To do parameter fine-tuning in a systematic way, there is an effective protocol often used in machine learning. We start with a set of coarse-grained, heterogeneously spaced values to find the order of magnitude for the best solution (for instance, for the mutation chance, we could choose [0.01, 0.05, 0.1, 0.2, 0.5, 0.8, 1]), and then refine our search by repeating (one or more times) the process around the most promising values. (For instance, assuming the best results were obtained with 0.2 and 0.5, then we test [0.15, 0.2, 0.25, 0.3, ..., 0.45, 0.5, 0.55, 0.6].) Since we have multiple parameters, and we need to measure the results for all the combinations for the three of them (as shown in table 18.2), we really want to keep these lists as short as possible and refine them in several steps.</p>

  <h3 class="fm-head2" id="heading_id_19"><a id="pgfId-1006304"></a>18.2.5 Beyond TSP: Optimizing the routes of the whole fleet</h3>

  <p class="body"><a id="pgfId-1006322"></a>As <a id="marker-1008239"></a><a id="marker-1008243"></a>we mentioned in chapter 17, solving TSP is nice, but only allows us to optimize the route of a single truck. With the business thriving, it’s natural to have several trucks taking care of shipments from each warehouse (or possibly from more than one warehouse at the same time).</p>

  <p class="body"><a id="pgfId-1006334"></a>Each truck has a limited capacity, so we need to take parcel size into account (rings a bell?) and, moreover, each parcel’s destination can significantly change the route needed to deliver all packages assigned to a truck.</p>

  <p class="body"><a id="pgfId-1006349"></a>This is an extremely complicated problem to optimize, because for each assignment of parcels to the available trucks, we need to optimize the trucks’ routes one by one.</p>

  <p class="body"><a id="pgfId-1006358"></a>To simplify the problem, what’s often done is splitting the area covered by a warehouse into zones and finding an a priori optimal assignment of these zones to trucks. This, however, forces us to settle for sub-optimal solutions and to include some redundancy in the trucks’ capacity to cope with the fluctuations in demand.</p>

  <p class="body"><a id="pgfId-1006369"></a>An alternative is running a TSP optimization for each truck after assigning the deliveries to all vehicles, as part of the fitness function computation, and then sum whatever metric is used (miles run, or time spent, or gasoline consumed) across all of them.</p>

  <p class="body"><a id="pgfId-1006378"></a>As you can imagine, optimizing the larger problem is not easy, because for any change in the assignments of parcels to vehicles, we need to rerun at least two TSP optimizations.</p>

  <p class="body"><a id="pgfId-1006387"></a>In terms of <i class="calibre17">epistasis</i> (as defined in section 18.1.9), the fitness function has a high degree of dependence between its variables, so a genetic algorithm could be effective, but a random search could also be a <a id="marker-1008247"></a><a id="marker-1008251"></a>viable <a id="marker-1008255"></a><a id="marker-1008259"></a><a id="marker-1008263"></a>alternative.</p>

  <h2 class="fm-head" id="heading_id_20"><a id="pgfId-1006405"></a>18.3 Minimum vertex cover</h2>

  <p class="body"><a id="pgfId-1006421"></a>Now <a id="marker-1008267"></a><a id="marker-1008271"></a>we can be quite satisfied with our genetic algorithm for TSP, and we can move on to new, interesting, and of course <i class="calibre17">NP-Hard</i> problems on graphs.</p>

  <p class="body"><a id="pgfId-1006439"></a>There is a rich body of literature on this kind of problem; some writings are milestones and benchmarks in computer science and some appear in many practical applications throughout engineering.</p>

  <p class="body"><a id="pgfId-1006450"></a><i class="calibre17">Minimum vertex cover</i><a id="marker-1008275"></a> is important both theoretically and practically, so we couldn’t close this book without discussing it!</p>

  <p class="body"><a id="pgfId-1006462"></a>Given a graph <code class="fm-code-in-text">G=(V,E)</code>, a <i class="calibre17">vertex cover</i><a id="marker-1008279"></a> for <code class="fm-code-in-text">G</code> is any set of its vertices that covers all of its edges; an edge <code class="fm-code-in-text">e=(u,v)</code> <span class="cambria">ϵ</span> <code class="fm-code-in-text">E</code> is covered by a subset of vertices <code class="fm-code-in-text">S</code> <span class="cambria">⊆</span> <code class="fm-code-in-text">V</code> if at least one of the edge’s endpoints, either <code class="fm-code-in-text">u</code> or <code class="fm-code-in-text">v</code>, belongs to <code class="fm-code-in-text">S</code>.</p>

  <p class="body"><a id="pgfId-1006494"></a>Every graph has a trivial vertex cover: the set <code class="fm-code-in-text">V</code> of all vertices. What we are really interested in, though, is finding a minimum vertex cover, the smallest possible subset of vertices that covers all edges. Figure 18.19 shows a few examples of simple graphs and their vertex covers.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F20.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1033043"></a>Figure 18.19 Three examples of minimum vertex covers on graphs. The thicker circles (red vertices) are the only ones needed to cover all edges for these graphs.</p>

  <p class="body"><a id="pgfId-1006530"></a>Notice that some graphs have a single solution (like the first one on the left), while others have multiple minima vertex covers. Can you find the alternative solution for the graph on the right?</p>

  <p class="body"><a id="pgfId-1006546"></a>Figure 18.20, instead, shows examples of what is <i class="calibre17">not</i> a minimum vertex cover.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F21.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1033085"></a>Figure 18.20 Examples of subsets that are not a minimum vertex cover on these graphs. On the left, not all edges are covered by the vertex in the center of the graph, so that vertex by itself is not a vertex cover. On the right, an example of a vertex cover that, however, is not minimal (see figure 18.19).</p>

  <p class="body"><a id="pgfId-1006576"></a>The decision problem for the minimum vertex cover (“Given a subset of vertices, is it a minimum vertex cover for <code class="fm-code-in-text">G</code>?”) has been proved to be NP-Hard.</p>

  <p class="body"><a id="pgfId-1006589"></a>Since there are <code class="fm-code-in-text">2<sup class="superscript1">|V|</sup></code> possible subsets of <code class="fm-code-in-text">V</code>, a brute-force search seems out of the question.</p>

  <p class="body"><a id="pgfId-1006603"></a>There are polynomial-time approximation algorithms for minimum vertex cover, but they can only guarantee a result that is within an approximation factor in the order of magnitude of <code class="fm-code-in-text">2</code> (slightly lower than <code class="fm-code-in-text">2</code>) from the best solution.<a href="#pgfId-1008916"><sup class="footnotenumber">26</sup></a> This problem, in fact, belongs to a computational subclass of NP<a id="marker-1008283"></a> called <i class="calibre17">APX-complete problems</i><a id="marker-1008287"></a>, the set of NP optimization problems for which there exists a polynomial-time constant-factor approximation algorithm.</p>

  <p class="body"><a id="pgfId-1006627"></a>For some categories of graphs, like bipartite graphs and trees, there exist worst-case polynomial-time algorithms to solve minimum vertex cover exactly, but besides these happy cases, we either accept an approximated (non-optimal) solution, or we need an exponential-time algorithm.</p>

  <p class="body"><a id="pgfId-1006636"></a>It goes without saying that we are going for the former by using a genetic algorithm. But first, let’s talk about applications of this problem.</p>

  <h3 class="fm-head2" id="heading_id_21"><a id="pgfId-1006645"></a>18.3.1 Applications of vertex cover</h3>

  <p class="body"><a id="pgfId-1006661"></a>Back <a id="marker-1008291"></a><a id="marker-1008295"></a>to our e-commerce company: as the business grows, so do warehouses. To protect the company from thefts and to guarantee the employees’ safety, we need to install cameras that cover all the alleys. Suppose that our warehouses are shaped like the graphs in figure 18.20, where edges are alleys, and vertices are placed at cross points and dead ends; also suppose that cameras have special wide lenses (or motors, or both) and can cover up to 180° or even 360°. Then, the minimum number of cameras we need is given by the minimum vertex cover for those graphs.</p>

  <p class="body"><a id="pgfId-1006677"></a>Although oversimplified, this is a real, recurring application of the problem, the easiest to adapt to our example scenario, but definitely not the only one.</p>

  <p class="body"><a id="pgfId-1006688"></a>Another field where efficient vertex cover algorithms make a difference is bioinformatics, in particular computational biochemistry. It often happens that DNA sequences in samples present conflicts (the exact definition of conflict depends on the context) that we need to resolve: graphs come to our aid. We can define a conflict graph where vertices are sequences and edges are added when there is a conflict between any pair of sequences. Notice that this graph is potentially disconnected. Since the goal is to resolve all conflicts by removing as few sequences as possible, what we are looking for is the minimum vertex cover of the conflict graph.</p>

  <p class="body"><a id="pgfId-1006711"></a>Going back to computer science applications, vertex cover can be used to aid network security. It has been employed<a href="#pgfId-1008936"><sup class="footnotenumber">27</sup></a> to design optimal strategies, in terms of combinatorial topology of routers, against the diffusion of stealth worms in large computer networks.</p>

  <p class="body"><a id="pgfId-1006726"></a>Finally, among the many other fields where we can find this problem, we’d like to mention one that we have already discussed in this book: minimum vertex cover has been used<a href="#pgfId-1008958"><sup class="footnotenumber">28</sup></a> to develop an efficient nearest neighbor classifier for general metric spaces (not limited to Euclidean spaces or to Hilbert <a id="marker-1008299"></a><a id="marker-1008303"></a>spaces).</p>

  <h3 class="fm-head2" id="heading_id_22"><a id="pgfId-1006744"></a>18.3.2 Implementing a genetic algorithm</h3>

  <p class="body"><a id="pgfId-1006760"></a>Now <a id="marker-1008307"></a><a id="marker-1008311"></a>it’s time to roll up our sleeves and write a decent optimizer for this problem; the good news is that we can reuse most of the work we had done for the 0-1 knapsack!</p>

  <p class="body"><a id="pgfId-1006772"></a>The solutions to the minimum vertex cover problem, in fact, are subsets of the vertices, just like for the knapsack they were subsets of the items available. We can therefore implement chromosomes as bit strings and reuse the same crossover and mutation operators!</p>

  <p class="body"><a id="pgfId-1006791"></a>The only thing that changes, obviously, is the definition of the fitness function. Here the quality of the solution is given by the number of vertices in the chosen subset (the smaller, the better) under the constraint that all edges are covered. We could implement the constraint separately and discard all subsets that are not vertex covers or assign a huge value to their fitness. This way, however, a minimal solution that covers all edges but one would be penalized even with respect to the trivial solution containing all vertices (which is a valid vertex cover). It is important, instead, to keep the former solution in the population, as it could be turned into a valid vertex cover by adding a single vertex.</p>

  <p class="body"><a id="pgfId-1006804"></a>To work around this issue, I suggest we integrate the number of uncovered edges into the fitness function, with a certain multiplicator (by default, we can use <code class="fm-code-in-text">2</code>).</p>

  <p class="body"><a id="pgfId-1006815"></a>Considering the first example in figure 18.20, let’s compare two possible solutions in figure 18.21. The graph on the left has one uncovered edge, so its fitness is <code class="fm-code-in-text">5</code>, the same as the trivial solution (on the right) which is a vertex cover. This example also shows how important it is that the multiplicator for the uncovered edges is greater than 1. If we just added the number of uncovered edges to the number of vertices, the solution on the left would have fitness equal to <code class="fm-code-in-text">4</code>, but any subset with 4 vertices would be a better, and valid, solution.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F22.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1033129"></a>Figure 18.21 Evaluating fitness of possible solutions. We combine the number of vertices with the number of edges not covered, to avoid discarding promising solutions which are not valid vertex covers, like the one on the left.</p>

  <p class="body"><a id="pgfId-1006855"></a>At the same time, using this fitness function, we don’t discard a promising solution that is only two mutations away from the minimum vertex cover (which uses only 3 vertices).</p>

  <p class="body"><a id="pgfId-1006864"></a>Listing 18.13 shows an implementation of the fitness function for this method. The caveat, with respect to the knapsack problem, is that we also need to pass an instance of the graph to this method, because it’s needed in order to check which edges are covered by the solution. In many languages, this can be obtained without changing the template for the genetic algorithm (shown in listings 18.10 and 18.4) by currying the fitness function and bounding it to an instance of the graph before passing it to the genetic algorithm main method.</p>

  <p class="body"><a id="pgfId-1006873"></a>JavaScript, for instance, is one of the languages that allows this: check out the implementation provided with JsGraphs on GitHub.<a href="#pgfId-1008980"><sup class="footnotenumber">29</sup></a></p>

  <p class="fm-code-listing-caption"><a id="pgfId-1018372"></a>Listing 18.13 Vertex cover: fitness function</p>
  <pre class="programlisting"><b class="strong">function</b> vertexCoverFitness(graph, chromosome)       <span class="fm-combinumeral">❶</span>
  fitness ← 0                                        <span class="fm-combinumeral">❷</span>
  <b class="strong">for</b> gene <b class="strong">in</b> chromosome <b class="strong">do</b>                          <span class="fm-combinumeral">❸</span>
    <b class="strong">if</b> gene == 1 <b class="strong">then</b>                                <span class="fm-combinumeral">❹</span>
      fitness ← fitness + 1                            
  <b class="strong">for</b> edge <b class="strong">in</b> graph.edges <b class="strong">do</b>                         <span class="fm-combinumeral">❺</span>
    <b class="strong">if</b> chromosome[edge.source] == 0 
      <b class="strong">and</b> chromosome[edge.destination] == 0 <b class="strong">then</b>     <span class="fm-combinumeral">❻</span>
        fitness ← fitness + 2
  <b class="calibre21">return</b> fitness  </pre>

  <p class="fm-code-annotation"><a id="pgfId-1022209"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">vertexCoverFitness</code> t<a id="marker-1026854"></a>akes a chromosome (a bit string) and returns the fitness value associated with the solution it’s encoding.</p>

  <p class="fm-code-annotation"><a id="pgfId-1022241"></a><span class="fm-combinumeral">❷</span> Initializes the fitness value to <code class="fm-code-in-text2">0</code></p>

  <p class="fm-code-annotation"><a id="pgfId-1022272"></a><span class="fm-combinumeral">❸</span> Cycles through all genes (all bits) in the chromosome</p>

  <p class="fm-code-annotation"><a id="pgfId-1022305"></a><span class="fm-combinumeral">❹</span> If a gene is set to <code class="fm-code-in-text2">1</code>, it means we are using that vertex in the solution, so we have to account for it in the cost. Remember, the goal is minimizing the number of vertices used, while providing a valid vertex cover.</p>

  <p class="fm-code-annotation"><a id="pgfId-1022338"></a><span class="fm-combinumeral">❺</span> Cycles through all edges in the graph</p>

  <p class="fm-code-annotation"><a id="pgfId-1022370"></a><span class="fm-combinumeral">❻</span> If neither of the edge’s endpoints are included in the solution, this edge is not covered (the solution in not a valid vertex cover). Rather than discarding it, we can add a penalty term to the fitness value. In this case, we add <code class="fm-code-in-text2">2</code>, but we could also pass the desired penalty as an argument instead and make this method more flexible.</p>

  <p class="body"><a id="pgfId-1007127"></a>And that was almost all the new code we need to write a solver for vertex cover. There is only a final word of caution.</p>

  <p class="body"><a id="pgfId-1007142"></a>To be certain to return a valid vertex cover, we will need to sort the final population by fitness and, starting from the one with the smallest fitness value, validate solutions: we can return the first one in the sorted list that is also covering <a id="marker-1008319"></a><a id="marker-1008323"></a>all <a id="marker-1008327"></a><a id="marker-1008331"></a><a id="marker-1008335"></a>edges.</p>

  <h2 class="fm-head" id="heading_id_23"><a id="pgfId-1007162"></a>18.4 Other applications of the genetic algorithm</h2>

  <p class="body"><a id="pgfId-1007180"></a>There are countless problems, both on graphs and on generic data structures, that can be formulated as optimization problems, for which a genetic algorithm could be an effective way to get to a near-optimal solution in a reasonable time.</p>

  <p class="body"><a id="pgfId-1007189"></a>We’ll briefly discuss two of them that are particularly relevant for their applications: finding the maximum flow of a graph, and protein folding.</p>

  <h3 class="fm-head2" id="heading_id_24"><a id="pgfId-1007198"></a>18.4.1 Maximum flow</h3>

  <p class="body"><a id="pgfId-1007212"></a>The <a id="marker-1008339"></a><a id="marker-1008343"></a><a id="marker-1008347"></a>maximum flow problem is defined on a specific type of directed graphs called networks (see figure 18.22). A network <code class="fm-code-in-text">N=(V,E)</code> is a directed, weighted graph where there are two special nodes <code class="fm-code-in-text">s,t</code> <span class="cambria">ϵ</span> <code class="fm-code-in-text">V: s</code> is called the source, and <code class="fm-code-in-text">t</code> is called the sink of the network. The peculiarity of these vertices is that the source only has outgoing edges, while the sink only has ingoing edges.</p>

  <p class="body"><a id="pgfId-1007235"></a>In networks, edges’ weights are called <i class="calibre17">capacity</i>. The exact characterization of edges’ capacity depends on the context. For instance, for a hydraulic network, edges model pipes, and their weight is the maximum volume of liquid that can go through the pipe at any time.</p>

  <p class="body"><a id="pgfId-1007255"></a>A network is used to model the flow through its vertices, starting from a source and arriving at a sink, where an edge’s flow is the actual amount (for instance, of water) that goes through the edge. Each vertex <code class="fm-code-in-text">v</code> of the network has an inbound flow (for instance, the total amount of water through its ingoing edges) that can be redistributed to its outgoing edges, with the caveat that the total outbound flow must be equal to the inbound one. Referring to figure 18.22, showing an example of a network, node <code class="fm-code-in-text">D</code> can have a maximum inbound flow (the sum of its ingoing edges’ capacity) of <code class="fm-code-in-text">5</code> units, and a maximum theoretical outbound flow of <code class="fm-code-in-text">6</code>. Therefore, the real flow through <code class="fm-code-in-text">D</code> can never be larger than <code class="fm-code-in-text">5</code>, and that only happens when its inbound flow is at max: if <code class="fm-code-in-text">D</code> at some point had an inbound flow of <code class="fm-code-in-text">4</code>, because some of the flow from <code class="fm-code-in-text">A</code> was diverted to <code class="fm-code-in-text">E</code> instead of <code class="fm-code-in-text">D</code>, its outbound flow would then be limited to <code class="fm-code-in-text">4</code>. When the maximum possible outbound flow is larger than the inbound flow, as in this case, it means that we need to make a choice about how to route the outgoing flow, because we can’t use the capacity of outgoing edges to the fullest. For instance, considering that the max flow from <code class="fm-code-in-text">F</code> to the sink is <code class="fm-code-in-text">1</code>, it makes more sense to route flow from <code class="fm-code-in-text">D</code> toward vertex <code class="fm-code-in-text">G</code>, so that we maximize the total inbound flow of the sink <code class="fm-code-in-text">t</code>: this is also the goal of the maximum flow problem.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F23.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1033173"></a>Figure 18.22 An example of a network, whose maximum flow is 6.</p>

  <p class="body"><a id="pgfId-1007348"></a>More formally, a flow for a network is defined as a mapping between edges and real numbers, an assignment to each edge, such that:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1007357"></a>The flow of an edge is smaller or equal to the edge’s capacity.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1007370"></a>The total flow entering a vertex must be equal to the total flow exiting a vertex (except for <code class="fm-code-in-text">s</code> and <code class="fm-code-in-text">t</code>, which by definition have respectively <i class="calibre15">null inbound</i><a class="calibre14" id="marker-1008351"></a> <i class="calibre15">and null outbound</i> flows<a class="calibre14" id="marker-1008355"></a>).</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1007393"></a>The value of the flow is defined as the sum of the flow exiting the source vertex <code class="fm-code-in-text">s</code> or, equivalently, the sum of the flow entering the sink vertex <code class="fm-code-in-text">t</code>.</p>

  <p class="body"><a id="pgfId-1007406"></a>A maximum flow for a network graph <code class="fm-code-in-text">G</code> is a valid flow whose value is the maximum possible for <code class="fm-code-in-text">G</code>.</p>

  <p class="body"><a id="pgfId-1007419"></a>Notice that a maximum flow problem is not NP-Hard, because there are several polynomial-time algorithms that can solve it exactly. These algorithms, however, are still at least <code class="fm-code-in-text">O(|V|<sup class="superscript1">3</sup>)</code>, which makes them impractical for very large graphs; moreover, there are some variants of this problem that are NP-complete.</p>

  <p class="body"><a id="pgfId-1007436"></a>Now that the problem is clearly stated, how do we design a genetic algorithm for it?</p>

  <p class="body"><a id="pgfId-1007445"></a>First and foremost, as always, we need to decide about the fitness function and chromosome encoding. The latter is easy—we need one gene per edge, and each of them can be assigned any value between <code class="fm-code-in-text">0</code> and the edge’s capacity.</p>

  <p class="body"><a id="pgfId-1007458"></a>This encoding (shown in figure 18.23) automatically validates the first constraint in the formal definition of flow, but it can’t do anything for the second one. We still need to check each solution, because it will only be valid if the second constraint is abided by.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch18_F24.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1033215"></a>Figure 18.23 A solution (aka a chromosome) for the maximum flow of the network in figure 18.22. Is this a valid flow? Is it maximal?</p>

  <p class="body"><a id="pgfId-1007490"></a>We can do this in two ways: either we discard invalid flows (probably not a good choice, for the same reasons as it wasn’t for vertex cover), or we add a term to the fitness function accounting for this.</p>

  <p class="body"><a id="pgfId-1007501"></a>For instance, we can compute the absolute value of the flow difference for all vertices (except <code class="fm-code-in-text">s</code> and <code class="fm-code-in-text">t</code>), and divide the solution’s flow by the cumulative difference, if that’s not zero.</p>

  <p class="body"><a id="pgfId-1007514"></a>Crossover and mutations are easier. We can use single point recombination and a punctiform mutation that randomly increases or decreases an edge’s flow by 1 (within the edge’s capacity).</p>

  <p class="body"><a id="pgfId-1007527"></a>If you’ve read through the chapter so far, you should have all the tools to not only implement this algorithm, but also to design more sophisticated, and possibly better, operators and try different definitions for the fitness function.</p>

  <p class="body"><a id="pgfId-1007536"></a>Instead of delving into the implementation, therefore, I’d like to spend a few words on why this is an important problem.</p>

  <p class="body"><a id="pgfId-1007545"></a>There are several practical applications of maximum flow in the physical world; for example, the scheduling of an airline’s flight crew, or even more interesting to us, the <i class="calibre17">circulation-demand problem</i><a id="marker-1008359"></a>. If you remember, our example e-commerce company was looking for ways to maximize the trucks’ load and minimize the excess of capacity that is needed to face peaks in demand; it turns out that we could write that problem as a maximum flow problem!</p>

  <p class="body"><a id="pgfId-1007566"></a>In the circulation-demand problem, in fact, we have a source of goods, for instance, our warehouse, and a set of destinations where we need to deliver these goods. There are, however, also constraints. One is that the maximum capacity of each truck is an insurmountable limit, because we can’t overload them. And if we have several warehouses/factories, the problem becomes even more challenging.</p>

  <p class="body"><a id="pgfId-1007583"></a>But, besides these practical issues, <i class="calibre17">maximum flow</i><a id="marker-1008363"></a> is a classic problem in computer science and software engineering!</p>

  <p class="body"><a id="pgfId-1007595"></a>A few examples of problems that can be modeled as maximum flow optimization are image segmentation, scheduling, network connectivity, and compilers <a id="marker-1008367"></a><a id="marker-1008371"></a>optimization.</p>

  <h3 class="fm-head2" id="heading_id_25"><a id="pgfId-1007607"></a>18.4.2 Protein folding</h3>

  <p class="body"><a id="pgfId-1007621"></a>In <a id="marker-1008375"></a><a id="marker-1008379"></a>the last few chapters we have mostly discussed graph problems that can be solved through optimization (meta)heuristics. Before wrapping up our review of GAs’ applications, I’d like to mention a different kind of problem, which is terribly important, as we’ve learned in these difficult times.</p>

  <p class="body"><a id="pgfId-1007633"></a>Proteins are large molecules, sequences of amino acids that are encoded in organisms’ DNA and (once DNA is decoded by cells) produced by their cells to perform many different functions within each organism: regulating the response to stimuli, transporting simpler molecules within and across cells, catalyzing metabolic reactions, even DNA replication itself, and many more—they are instrumental to the functioning of any organism.</p>

  <p class="body"><a id="pgfId-1007659"></a>Among other things, the 3-D structure of surface proteins determines the ability of viruses to tie to receptors and infect certain types of cells—something that recently moved protein folding to the top of the list of the most pressing problems to solve.</p>

  <p class="body"><a id="pgfId-1007672"></a>Two proteins differ by their sequence of amino acids, which determines the 3-D structure of the protein, which in turn determines the protein’s activity.</p>

  <p class="body"><a id="pgfId-1007683"></a><i class="calibre17">Protein folding</i><a id="marker-1008383"></a> is the physical process by which a linear protein chain (aka <i class="calibre17">polypeptide</i>) acquires its native 3-D conformation.</p>

  <p class="body"><a id="pgfId-1007703"></a>Now, the sequence of amino acids of a protein is determined by the DNA that encodes it, and is relatively easier to find out experimentally. It is the protein’s 3-D structure, however, that determines its functionality, and that’s much harder to determine in a lab (although new microimaging techniques are being developed).</p>

  <p class="body"><a id="pgfId-1007718"></a>One of the earliest and most successful applications of genetic algorithms was determining the most likely 3-D structure of proteins<a href="#pgfId-1008996"><sup class="footnotenumber">30</sup></a> whose sequence of amino acids was known. The idea is similar to what we discussed in chapters 16 and 17 for force-directed graph drawing (another reason why that was an important topic!). We can design a fitness function taking into account the attraction and repulsion forces between peptides (sequences of amino acids), amino acids, and even down to their atoms, and search for the 3-D configuration of minimal energy for the system.</p>

  <p class="body"><a id="pgfId-1007741"></a>Genetic algorithms and artificial immune systems have been the heuristics of choice for the protein folding problem. It’s worth noting that at the time of writing, the current state of the art in this field is obtained by another biologically inspired algorithm, neural <a id="marker-1008387"></a><a id="marker-1008391"></a><a id="marker-1008395"></a>networks.<a href="#pgfId-1009010"><sup class="footnotenumber">31</sup></a></p>

  <h3 class="fm-head2" id="heading_id_26"><a id="pgfId-1007762"></a>18.4.3 Beyond genetic algorithms</h3>

  <p class="body"><a id="pgfId-1007778"></a>Genetic algorithms are just a branch of the larger field of <i class="calibre17">evolutionary algorithms</i><a id="marker-1008399"></a> (<i class="calibre17">EA</i>), a category including all generic population-based meta-heuristic optimization algorithms.</p>

  <p class="body"><a id="pgfId-1007798"></a>I couldn’t have chosen a better way to close this chapter, and the book, than with a summary of the most interesting EAs in computer science’s literature. Consider it a starting point to deepening your understanding of optimization algorithms.</p>

  <p class="body"><a id="pgfId-1007811"></a><i class="calibre17">Memetic algorithm</i><a id="marker-1024967"></a> <a href="#pgfId-1009029"><sup class="footnotenumber">32</sup></a> is a class of optimization meta-heuristics directly derived from GA. Its peculiarity is that one or more of the mutation operators actually perform a local-search heuristics, something like random sampling over the current solution’s neighborhood, driving each individual toward a local optimum at each iteration (for instance, trying to flip each bit in a chromosome individually, and retaining the best result), and making this algorithm a hybrid between GA and random sampling, or potentially even gradient descent. The key in either situation is leveraging additional domain knowledge for the problem to perform the local optimization.</p>

  <p class="body"><a id="pgfId-1007835"></a><i class="calibre17">Artificial immune system</i><a id="marker-1008407"></a> <a href="#pgfId-1009043"><sup class="footnotenumber">33</sup></a> (<i class="calibre17">AIS</i>) is a class of biologically inspired algorithms modeled after the immune system and its ability to learn and retain memory, that extends the work on genetic algorithms with operators like <i class="calibre17">clonal selection</i>, <i class="calibre17">affinity maturation,</i> and <i class="calibre17">negative selection</i>.</p>

  <p class="body"><a id="pgfId-1007860"></a><i class="calibre17">Particle swarm optimization</i>,<a id="marker-1008411"></a><a href="#pgfId-1009063"><sup class="footnotenumber">34</sup></a> which is instead better used on numerical optimization, uses flock dynamics to explore the cost function landscape. The mechanism is similar to genetic algorithms, with a population (swarm) of solutions that is maintained and moved around, only not through genetic operators, but simpler rules inspired by a mix of kinematics and biology.</p>

  <p class="body"><a id="pgfId-1007877"></a><i class="calibre17">Ant colony optimization</i><a id="marker-1008415"></a> <a href="#pgfId-1009080"><sup class="footnotenumber">35</sup></a> is a probabilistic technique inspired by the way ant colonies use pheromone-based communication to form paths to food sources. It’s particularly well suited for problems on graphs and specifically for those that can be reduced to finding paths in a graph.</p>

  <p class="body"><a id="pgfId-1007901"></a>And these examples are just the tip of the iceberg; there are plenty of evolutionary algorithms inspired by different biological principles, so many that they deserve a book of their own.</p>

  <h3 class="fm-head2" id="heading_id_27"><a id="pgfId-1007911"></a>18.4.4 Algorithms, beyond this book</h3>

  <p class="body"><a id="pgfId-1007929"></a>We are at the end of this book, but this is not the end of your algorithmic journey!</p>

  <p class="body"><a id="pgfId-1007938"></a>First of all, if you enjoyed the topics in this book, here is a list of suggested readings that can help you delve further into some of them:</p>

  <ul class="calibre19">
    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1007947"></a><i class="calibre15">Grokking Machine Learning</i>, by Andrew Trask, talks extensively about machine learning and some of the training algorithms we have briefly described in chapters 12, 13, and 16. If you’d like to learn how <i class="calibre15">gradient descent</i><a class="calibre14" id="marker-1008419"></a> is used in <i class="calibre15">linear regression</i><a class="calibre14" id="marker-1008423"></a> and <i class="calibre15">logistic regression</i><a class="calibre14" id="marker-1008427"></a>, this book is a good starting point.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1007978"></a><i class="calibre15">Grokking Artificial Intelligence Algorithms</i>, by Rishal Hurbans, delves into some of the topics we summarized in this book, such as search and optimization and evolutionary algorithms, and presents advanced concepts we have just mentioned, such as swarm intelligence.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1007997"></a><i class="calibre15">Graph-Powered Machine Learning</i>, by Allesandro Nego, is a good choice if you’d like to put what you’ve learned about graphs to good use and tackle machine learning from a different angle.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1008012"></a><i class="calibre15">Graph Databases in Action</i>, by Dave Bechberger and Josh Perryman, explores another recent application of graphs. With this book, you’ll learn how graphs allow a better modeling of relations in data and why graph databases are the choice of election for highly-intercorrelated data.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1008029"></a><i class="calibre15">Algorithms and Data Structures for Massive Datasets</i>, by Dzejla Medjedovic et al., expands on the topics in this book, focusing on techniques to adapt data structures and algorithms to massive datasets and handle modern big data applications.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1008048"></a>And then, if you’d like to test your understanding of the data structures we have discussed, here is my challenge for you: implement these algorithms in your favorite language, and add them to this repository<span class="fm-hyperlink"><a href="#pgfId-1009099"><sup class="footnotenumber">36</sup></a></span> I created on GitHub! You’ll find detailed instructions on the repo’s README.<span class="fm-hyperlink"><a href="#pgfId-1009115"><sup class="footnotenumber">37</sup></a></span></p>

  <p class="body"><a id="pgfId-1008063"></a>Good luck for the next steps in your learning path, and I hope you will enjoy the coding challenge!</p>

  <h2 class="fm-head" id="heading_id_28"><a id="pgfId-1008072"></a>Summary</h2>

  <ul class="calibre19">
    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1008084"></a>As in populations dynamics, in a genetic algorithm the <i class="calibre15">fitness</i> of organisms is measured, and the ones that best adapt to their environment have a greater chance of surviving and passing their genome to the new generations.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1008100"></a><i class="calibre15">Crossover</i><a class="calibre14" id="marker-1025195"></a> is a new concept, with respect to other optimization algorithms such as simulated annealing that allow the algorithm to randomly recombine features of several organisms.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1008116"></a><i class="calibre15">Mutation</i><a class="calibre14" id="marker-1008435"></a> is the biological analogy to local-search optimization, just like the transition operators for simulated annealing.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1008130"></a>If in the cost function definition many variables are highly coupled, and changing one forces us to change one or more of the others, then the problem (as we modeled it) has high <i class="calibre17">epistasis</i>.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1008144"></a>When there is a high interdependence between variables, simulated annealing can perform better than genetic algorithms. It’s worth trying both on smaller instances and deciding based on the actual data.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1008161"></a><i class="calibre15">Vertex cover</i><a class="calibre14" id="marker-1025226"></a> is a problem with low epistasis where genetic algorithms <a class="calibre14" id="marker-1025227"></a><a class="calibre14" id="marker-1025228"></a>shine.</p>
    </li>
  </ul>
  <hr class="calibre22"/>

  <p class="fm-footnote"><sup class="footnotenumber">1.</sup> <a id="pgfId-1008452"></a>We discussed the travelling salesman problem at length in section 17.2.</p>

  <p class="fm-footnote"><sup class="footnotenumber">2.</sup> <a id="pgfId-1008466"></a>Technically, it follows the path of steepest descent, with a <i class="calibre17">locally-optimal</i><a id="marker-1008488"></a>, greedy choice at each step. These choices usually aren’t <i class="calibre17">globally optimal</i><a id="marker-1008492"></a>, and hence gradient descent is not guaranteed to reach global optimum, unless the cost function has a particular, convex shape with a single minimum point.</p>

  <p class="fm-footnote"><sup class="footnotenumber">3.</sup> <a id="pgfId-1008497"></a>We’ll talk about this in detail in section 18.2.</p>

  <p class="fm-footnote"><sup class="footnotenumber">4.</sup> <a id="pgfId-1008512"></a>With the notable exception of viruses, which are just DNA or RNA encapsulated in a protein coat.</p>

  <p class="fm-footnote"><sup class="footnotenumber">5.</sup> <a id="pgfId-1008527"></a>Approximately the same, as there can be small local variations, for various reasons including copy errors.</p>

  <p class="fm-footnote"><sup class="footnotenumber">6.</sup> <a id="pgfId-1008542"></a>Holland’s book was originally published in 1975. You can currently find the 1992 MIT press edition: <i class="calibre17">Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence</i>, MIT Press, 1992.</p>

  <p class="fm-footnote"><sup class="footnotenumber">7.</sup> <a id="pgfId-1008562"></a><i class="calibre17">Genetic Algorithms in Search, Optimization, and Machine Learning</i>, Addison-Wesley Professional, 1988.</p>

  <p class="fm-footnote"><sup class="footnotenumber">8.</sup> <a id="pgfId-1008576"></a>For example, the Martello-Toth algorithm which is one of the state-of-the-art solutions: Martello, Silvano, and Paolo Toth. <span class="fm-hyperlink">“A bound and bound algorithm for the zero-one multiple knapsack problem.”</span> <i class="calibre17">Discrete Applied Mathematics</i> 3.4 (1981): 275-288.</p>

  <p class="fm-footnote"><sup class="footnotenumber">9.</sup> <a id="pgfId-1008596"></a>Consider, for example, a couple of twins, who share their DNA, but each of them is a different and unique being.</p>

  <p class="fm-footnote"><sup class="footnotenumber">10.</sup> <a id="pgfId-1008614"></a>For example, check out “Evolutionary neurocontrollers for autonomous mobile robots,” <i class="calibre17">Neural Networks</i> Volume 11, Issues 7–8, October–November 1998, pages 1461-1478</p>

  <p class="fm-footnote"><sup class="footnotenumber">11.</sup> <a id="pgfId-1008642"></a>In the rest of the chapter, we’ll talk about “high fitness” as a generic term, decoupling it from the actual implementation. It will mean large values for those problems maximizing a function and small values when the goal of the optimization is to minimize a cost.</p>

  <p class="fm-footnote"><sup class="footnotenumber">12.</sup> <a id="pgfId-1008657"></a>It is possible, and it has been attempted, to include the notions of sex-specific chromosomes and sexual subgroups, but, to the best of my knowledge, the possible improvements in the efficiency or effectiveness of the algorithm are in the order of magnitude of optimization. As an example, the dedicated reader can check Zhang, Ming-ming, Shu-guang Zhao, and Xu Wang. “Sexual Reproduction Adaptive Genetic Algorithm Based on Baldwin Effect and Simulation Study [J].” <i class="calibre17">Journal of System Simulation</i> 10 (2010).</p>

  <p class="fm-footnote"><sup class="footnotenumber">13.</sup> <a id="pgfId-1008681"></a>Because we are trying to maximize the fitness function for the 0-1 knapsack; otherwise, with a function to be minimized, we could have set it to 120%, or 105%, and so on.</p>

  <p class="fm-footnote"><sup class="footnotenumber">14.</sup> <a id="pgfId-1008696"></a>For the predator-prey robots example, for instance, or for any setup where we evolve systems that run tasks in the physical world, we might actually do that and score individuals based on how they perform on a real task.</p>

  <p class="fm-footnote"><sup class="footnotenumber">15.</sup> <a id="pgfId-1008713"></a>Approximately, with a few simplifications: the actual value depends on coding details like if the same individual can be chosen multiple times. This is fine, though, because we are not really interested in the exact values for these probabilities, but in understanding its order of magnitude and getting an idea of how it changes with k.</p>

  <p class="fm-footnote"><sup class="footnotenumber">16.</sup> <a id="pgfId-1008731"></a>For some problems, however, using elitism aggressively can compensate pure randomness in selection.</p>

  <p class="fm-footnote"><sup class="footnotenumber">17.</sup> <a id="pgfId-1008749"></a>This works when higher fitness means larger values, of course. It can be adapted to the other case by using the inverse of the fitness values, for example.</p>

  <p class="fm-footnote"><sup class="footnotenumber">18.</sup> <a id="pgfId-1008764"></a>For each individual, we normalize its fitness by dividing its value by the total sum of the population’s fitness. This way, each normalized fitness is between 0 and 1, and their sum across the whole population is 1.</p>

  <p class="fm-footnote"><sup class="footnotenumber">19.</sup> <a id="pgfId-1008780"></a>Some flowering plants also adopt sexual reproduction through pollination.</p>

  <p class="fm-footnote"><sup class="footnotenumber">20.</sup> <a id="pgfId-1008795"></a>The process is conceptually similar to early phases of <i class="calibre17">meiosis</i>, the mechanism used by cells for sexual reproduction.</p>

  <p class="fm-footnote"><sup class="footnotenumber">21.</sup> <a id="pgfId-1008811"></a>Mutations also happen spontaneously, with on-null probability, during <i class="calibre17">mitosis</i>, the mechanism used for asexual reproduction of all cells (<i class="calibre17">gametes</i> and <i class="calibre17">somatic cells</i>). These mutations can be key to the evolution of the organisms and species, when they happen in gametes, the cells involved in sexual reproduction.</p>

  <p class="fm-footnote"><sup class="footnotenumber">22.</sup> <a id="pgfId-1008833"></a>An <i class="calibre17">unfair</i> coin, where the probability of applying a mutation would be far smaller than ½.</p>

  <p class="fm-footnote"><sup class="footnotenumber">23.</sup> <a id="pgfId-1008850"></a>See <span class="fm-hyperlink"><a href="http://mng.bz/nMMd">http://mng.bz/nMMd</a></span>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">24.</sup> <a id="pgfId-1008867"></a>See <span class="fm-hyperlink"><a href="https://github.com/mlarocca/jsgraphs/blob/master/src/graph/algo/genetic/tsp.mjs">https://github.com/mlarocca/jsgraphs/blob/master/src/graph/algo/genetic/tsp.mjs</a></span>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">25.</sup> <a id="pgfId-1008883"></a>For bit string representations, as we have discussed, this means that none of chromosomes has a certain value for a specific gene; for TSP, which has a different representation, it means that there is no chromosome with a certain subsequence of vertices. For instance, if no chromosomes have vertex <code class="fm-code-in-text1">1</code> before vertex <code class="fm-code-in-text1">0</code>, crossover won’t be able to add this feature. Considering there is a quadratic number of pairs of vertices, it’s unlikely that both possible orderings for all of the pairs appear in the initial population.</p>

  <p class="fm-footnote"><sup class="footnotenumber">26.</sup> <a id="pgfId-1008916"></a>If an algorithm has an approximation factor <code class="fm-code-in-text1">a</code>, it means that on a problem with a solution whose value is <code class="fm-code-in-text1">v</code>, the approximation algorithm will return a solution that is at most <code class="fm-code-in-text1">a*v</code>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">27.</sup> <a id="pgfId-1008936"></a>Filiol, Eric, et al. “Combinatorial optimization of worm propagation on an unknown network.” <i class="calibre17">International Journal of Computer Science</i> 2.2 (2007): 124-130.</p>

  <p class="fm-footnote"><sup class="footnotenumber">28.</sup> <a id="pgfId-1008958"></a>Gottlieb, Lee-Ad, Aryeh Kontorovich, and Robert Krauthgamer. “Efficient classification for metric data.” <i class="calibre17">IEEE Transactions on Information Theory</i> 60.9 (2014): 5750-5759.</p>

  <p class="fm-footnote"><sup class="footnotenumber">29.</sup> <a id="pgfId-1008980"></a>See <span class="fm-hyperlink"><a href="https://github.com/mlarocca/jsgraphs/blob/master/src/graph/algo/genetic/vertex_cover.mjs">https://github.com/mlarocca/jsgraphs/blob/master/src/graph/algo/genetic/vertex_cover.mjs</a></span>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">30.</sup> <a id="pgfId-1008996"></a>Schulze-Kremer, Steffen. “Genetic algorithms and protein folding.” <i class="calibre17">Protein Structure Prediction</i>. Humana Press, 2000. 175-222.</p>

  <p class="fm-footnote"><sup class="footnotenumber">31.</sup> <a id="pgfId-1009010"></a>The AlphaFold project, developed by DeepMind: <span class="fm-hyperlink"><a href="http://mng.bz/Qmm4">http://mng.bz/Qmm4</a></span>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">32.</sup> <a id="pgfId-1009029"></a>Moscato, Pablo. “On evolution, search, optimization, genetic algorithms and martial arts: Towards memetic algorithms.” Caltech concurrent computation program, C3P Report 826 (1989): 1989.</p>

  <p class="fm-footnote"><sup class="footnotenumber">33.</sup> <a id="pgfId-1009043"></a>Kephart, Jeffrey O. “A biologically inspired immune system for computers.” In Proc. <i class="calibre17">Of The Fourth International Workshop On Synthesis And Simulation Of Living Systems</i>, Artificial Life IV. 1994.</p>

  <p class="fm-footnote"><sup class="footnotenumber">34.</sup> <a id="pgfId-1009063"></a>Eberhart, Russell C., Yuhui Shi, and James Kennedy. <i class="calibre17">Swarm Intelligenc</i>e. Elsevier, 2001.</p>

  <p class="fm-footnote"><sup class="footnotenumber">35.</sup> <a id="pgfId-1009080"></a>Dorigo, Marco, and Luca Maria Gambardella. “Ant colony system: a cooperative learning approach to the traveling salesman problem.” <i class="calibre17">IEEE Transactions on evolutionary computation</i> 1.1 (1997): 53-66.</p>

  <p class="fm-footnote"><sup class="footnotenumber">36.</sup> <a id="pgfId-1009099"></a>See <span class="fm-hyperlink"><a href="https://github.com/mlarocca/AlgorithmsAndDataStructuresReadersContributions">https://github.com/mlarocca/AlgorithmsAndDataStructuresReadersContributions</a></span>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">37.</sup> <a id="pgfId-1009115"></a>See <span class="fm-hyperlink"><a href="http://mng.bz/1rjn">http://mng.bz/1rjn</a></span>.</p>
</body>
</html>
