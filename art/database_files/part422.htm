<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>23.5  Extended Concurrency Control Protocols</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part421.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part423.htm">下一个 &gt;</a></p><p class="s65" style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: left;"><a name="bookmark489">23.5  </a><span style=" color: #00AEEF;">Extended Concurrency Control Protocols</span><a name="bookmark534">&zwnj;</a></p><p style="padding-top: 11pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">In this section, we describe further extensions to distributed concurrency control proto- cols. We ﬁrst consider multiversion <span class="s44">2PL </span>and how it can be extended to get globally con- sistent timestamps, in Section 23.5.1. Extensions of snapshot isolation to distributed settings are described in Section 23.5.2. Issues in concurrency control in heteroge- neous distributed databases, where each node may have its own concurrency control technique, are described in Section 23.5.3.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">23.5.1 Multiversion <span class="s81">2PL </span>and Globally Consistent Timestamps</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">The multiversion two-phase locking (<span class="s44">MV2PL</span>) protocol, described in Section 18.7.2, combines the beneﬁts of lock-free read-only transactions with the serializability guar- antees of two-phase locking. Read-only transactions see a snapshot at a point in time, while update transactions use two-phase locking but create new versions of each data item that they update. Recall that with this protocol, each transaction <i>T</i><span class="s97">i </span>gets a</p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 79%;text-align: justify;">unique timestamp CommitTS(<i>T</i><span class="s97">i</span>) (which could be a counter, instead of actual time)</p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">at the time of commit. The transaction sets the timestamp of all items that it up-</p><p style="padding-top: 1pt;padding-left: 119pt;text-indent: 0pt;line-height: 84%;text-align: justify;">dates to CommitTS(<i>T</i><span class="s97">i</span>). Only one transaction performs commit at a point in time; this guarantees that once <i>T</i><span class="s145">i </span>commits, a read-only transaction <i>T</i><span class="s145">j </span>whose StartTS(<i>T</i><span class="s145">j </span>) is set to CommitTS(<i>T</i><span class="s145">i</span>) will see committed values of all versions with timestamp <span class="s86">≤ </span>CommitTS(<i>T</i><span class="s145">i</span>).</p><p class="s42" style="padding-left: 137pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">MV2PL <span class="s43">can be extended to work in a distributed setting by having a central coordi-</span></p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">nator, that assigns start and commit timestamps and ensures that only one transaction can perform commit at a point in time. However, the use of a central coordinator limits scalability in a massively parallel data storage system.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The Google Spanner data storage system pioneered a version of the <span class="s44">MV2PL </span>system that is scalable and uses timestamps based on real clock time. We study the Spanner <span class="s44">MV2PL </span>implementation in the rest of this section.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Suppose every node has a perfectly accurate clock, and that commit processing can happen instantly with no delay between initiation of commit and its completion. Then, when a transaction wants to commit, it gets a commit timestamp by just reading the clock at any one node at any time after getting all locks, but before releasing any lock. All data item versions created by the transaction use this commit timestamp. Transac- tions can be serialized by this commit timestamp. Read-only transactions simply read the clock when they start and use it to get a snapshot of the database as of their start time.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">If the clocks are perfectly accurate, and commit processing is instantaneous, this protocol can be used to implement <span class="s44">MV2PL </span>without any central coordination, making it very scalable.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Unfortunately, in the real world, the above assumptions do not hold, which can lead to the following problems:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-top: 4pt;padding-left: 113pt;text-indent: -16pt;text-align: justify;">1. <span class="p">Clocks are never perfectly accurate, and the clock at each node may be a little fast or a little slow compared to other clocks.</span></p><p class="s13" style="padding-left: 113pt;text-indent: 15pt;line-height: 94%;text-align: justify;"><span class="p">Thus, it is possible to have the following situation. Two update transactions </span>T<span class="s130">1 </span><span class="s94">and </span>T<span class="s130">2</span><span class="s94">, both write a data item </span>x<span class="p">, with </span>T<span class="s130">1 </span><span class="s94">writing it ﬁrst, followed by </span>T<span class="s130">2 </span><span class="s94">but </span>T<span class="s98">2</span><span class="p"> may end up with a lower commit timestamp because it got the timestamp at</span></p><p style="padding-left: 113pt;text-indent: 0pt;text-align: justify;">a diﬀerent node than <i>T</i><span class="s98">1</span>. This situation is not consistent with the serialization ordering of <i>T</i><span class="s98">1</span> and <i>T</i><span class="s98">2</span>, and it cannot happen with <span class="s44">MV2PL </span>in a centralized setting.</p><p style="padding-top: 6pt;padding-left: 113pt;text-indent: -17pt;text-align: justify;"><span class="s63">2. </span>Commit processing takes time, which can cause read-only transactions to miss updates if the protocol is not carefully designed. Consider the following situation. A read-only transaction <i>T</i><span class="s98">1</span> with start timestamp <i>t</i><span class="s98">1</span> reads data item <i>x </i>at node <i>N</i><span class="s98">1</span>, it is possible that soon after the read, another transaction <i>T</i><span class="s98">2</span> with CommitTS(<i>T</i><span class="s98">2</span>) <span class="s86">≤ </span><i>t</i><span class="s98">1</span> (which got the commit timestamp at a diﬀerent node <i>N</i><span class="s98">2</span>) may still perform a</p><p style="padding-left: 113pt;text-indent: 0pt;text-align: justify;">write on <i>x</i>. Then, <i>T</i><span class="s98">1</span> should have read the value written by <i>T</i><span class="s98">2</span>, but did not see it.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: left;">To deal with the ﬁrst problem, namely, the lack of clock synchronization, Spanner uses the following techniques.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s39" style="padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Spanner has a few atomic clocks that are very accurate at each data center and uses the time they provide, along with time information from the Global Positioning system (GPS) satellites, which provides very accurate time information, to get a very good estimate of time at each node. We use the term </span><span class="s13">true time </span><span class="p">to refer to the time that would have been given by an absolutely accurate clock.</span></p><p style="padding-left: 107pt;text-indent: 16pt;text-align: justify;">Each node periodically communicates with time servers to synchronize its clock; if the clock has gone faster it is (logically) slowed down, and if it is slower, it is moved forward to the time from the server. In between synchronizations the local clock continues to tick, advancing the local time. A clock that ticks slower or faster than the correct rate results in local time at the node that is progressively behind or ahead of the true time.</p><p style="padding-top: 4pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><span class="s40">The second key technique is to measure local clock drift each time the node syn- chronizes with a time server and to use it to estimate the rate at which the lo- cal clock loses or gains time. Using this information, the Spanner system main- tains a value </span><span class="s15">ϵ </span>such that if the local clock time is <i>t</i><span class="s181">′</span>, the true time <i>t </i>is bounded by <i>t</i><span class="s181">′</span><span class="s15"> − ϵ </span><span class="s86">≤ </span><i>t </i><span class="s86">≤ </span><i>t</i><span class="s181">′</span><span class="s15"> + ϵ</span>. The Spanner system is able to keep the uncertainty value <span class="s15">ϵ </span>to less than 10 msec typically. The TrueTime <span class="s44">API </span>used by Spanner allows the system</p><p style="padding-left: 107pt;text-indent: 0pt;text-align: justify;">to get the current time value, along with an upper bound on the uncertainty in the time value.</p><p style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><span class="s40">The next piece of the solution is an idea called </span><span class="s63">commit wait</span>. The idea is as follows: After all locks have been acquired at all nodes, the local time <i>t</i><span class="s181">′</span><span class="s15"> </span>is read at a coordi- nator node. We would like to use the true time as timestamp, but we don’t have the exact value. Instead, the highest possible value of true time, namely, <i>t</i><span class="s181">′</span><span class="s15"> + ϵ</span>, is used as a commit timestamp <i>t</i><span class="s97">c</span>. The transaction then waits, <i>while holding locks</i>, until it</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 139pt;text-indent: 0pt;line-height: 85%;text-align: justify;">is sure that the true time <i>t </i>is <span class="s86">≥ </span><i>t</i><span class="s145">c</span>; this just requires waiting for a time interval 2<span class="s15">ϵ</span>, calculated as described earlier.</p><p style="padding-left: 139pt;text-indent: 16pt;text-align: justify;">What the commit wait guarantees is that if a transaction <i>T</i><span class="s98">1</span> has a commit timestamp <i>t</i><span class="s97">c</span>, at the true time <i>t</i><span class="s97">c </span>all locks were held by <i>T</i><span class="s130">1</span><span class="s94">.</span></p><p style="padding-top: 1pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><span class="s40">Given the above, if a version </span><i>x</i><span class="s145">t </span>of a data item <i>x </i>has a timestamp <i>t</i>, we can say that that was indeed the value of <i>x </i>at true time <i>t</i>. This allows us to deﬁne a snapshot of the database at a time <i>t</i>, containing the latest versions of all data items as of time <i>t</i>. A database system is said to provide <span class="s63">external consistency </span>if the serialization order is consistent with the real-world time ordering in which the transactions commit.</p><p style="padding-left: 139pt;text-indent: 0pt;text-align: justify;">Spanner guarantees external consistency by ensuring that the timestamps used to deﬁne the transaction serialization order correspond to the true time when the transactions commit.</p><p class="s13" style="padding-top: 4pt;padding-left: 139pt;text-indent: -16pt;line-height: 91%;text-align: justify;"><span class="s39">• </span><span class="s40">One remaining issue is that transaction commit processing takes time (particularly so when </span><span class="s41">2PC </span><span class="s40">is used). While a transaction with commit timestamp </span>t <span class="p">is committing, a read of </span>x <span class="p">by a read-only transaction with timestamp </span>t<span class="s98">1</span><span class="p"> </span><span class="s86">≥ </span>t <span class="p">may not see the version </span>x<span class="s145">t </span><span class="p">, either because the timestamp has not yet been propagated to the node with the data item </span>x<span class="p">, or the transaction is in prepared state.</span></p><p style="padding-top: 1pt;padding-left: 139pt;text-indent: 14pt;line-height: 84%;text-align: justify;">To deal with this problem, reads that ask for a snapshot as of time <i>t</i><span class="s98">1</span> are made to wait until the system is sure that no transactions with timestamp <span class="s86">≤ </span><i>t</i><span class="s98">1</span> are still in the process of committing. If a transaction with timestamp <i>t </i><span class="s86">≤ </span><i>t</i><span class="s98">1</span> is currently in the prepared phase of <span class="s44">2PC</span>, and we are not sure whether it will commit or abort, a</p><p style="padding-left: 139pt;text-indent: 0pt;text-align: justify;">read with timestamp <i>t</i><span class="s98">1</span> would have to wait until we know the ﬁnal commit status of the transaction.</p><p style="padding-left: 139pt;text-indent: 14pt;text-align: justify;">Read-only transactions can be given a somewhat earlier timestamp, to guaran- tee that they will not have to wait; the trade-oﬀ here is that to avoid waiting, the transaction may not see the latest version of some data items.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">23.5.2 Distributed Snapshot Isolation</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Since snapshot isolation is widely used, extending it to work in a distributed setting is of signiﬁcant practical importance. Recall from Section 18.8 that while snapshot isolation does not guarantee serializability, it avoids a number of concurrency anomalies.</p><p class="s13" style="padding-left: 119pt;text-indent: 17pt;text-align: justify;"><span class="p">If each node implements snapshot isolation independently, the resultant schedules can have anomalies that cannot occur in a centralized system. For example, suppose two transactions, </span>T<span class="s93">1 </span><span class="s94">and </span>T<span class="s93">2 </span><span class="s94">run concurrently on node </span>N<span class="s93">1</span><span class="s94">, where </span>T<span class="s93">1 </span><span class="s94">writes </span>x <span class="p">and </span>T<span class="s93">2 </span><span class="s94">reads </span>x<span class="p">; thus </span>T<span class="s98">2</span><span class="p"> would not see updates made by </span>T<span class="s98">1</span><span class="p"> to </span>x<span class="p">. Suppose also that </span>T<span class="s98">1</span><span class="p"> updates a data item </span>y <span class="p">at node </span>N<span class="s130">2</span><span class="s94">, and commits, and subsequently </span>T<span class="s130">2 </span><span class="s94">reads </span>y <span class="p">at node </span>N<span class="s130">2</span><span class="s94">. Then </span>T<span class="s130">2</span></p><p class="s13" style="padding-left: 119pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="p">would see the value of </span>y <span class="p">updated by </span>T<span class="s130">1 </span><span class="s94">at </span>N<span class="s130">2</span><span class="s94">, but not see </span>T<span class="s130">1</span><span class="s94">’s update to </span>x <span class="p">at </span>N<span class="s130">1</span><span class="s94">. Sucha </span></p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 12pt;text-align: left;">situation could never occur when using snapshot isolation at a single node. Thus, just</p><p style="padding-left: 119pt;text-indent: 0pt;text-align: left;">depending on local enforcement of snapshot isolation at each node is not suﬃcient to enforce snapshot isolation across nodes.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 17pt;text-align: justify;">Several alternative distributed snapshot isolation protocols have been proposed in the literature. Since the protocols are somewhat complicated, we omit details, but references with more details may be found in the bibliographic notes for this chap- ter, available online. Some of these protocols allow local transactions at each node to execute without any global coordination step; an extra cost is paid only by global trans- actions, that is, transactions that execute at more than one node. These protocols have been prototyped on several databases/data storage systems, such as <span class="s44">SAP HANA </span>and HBase.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">There has been some work on extending distributed snapshot isolation protocols to make them serializable. Approaches explored include adding timestamp checks similar to timestamp ordering, creating a transaction dependency graph at a central server, and checking for cycles in the graph, among other approaches.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">23.5.3 Concurrency Control in Federated Database Systems</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Recall from Section 20.5 that in many cases a distributed database has to be con- structed by linking together multiple already-existing database systems, each with its own schema and possibly running diﬀerent database-management software. Recall that such systems are called <i>federated database systems </i>or <i>heterogeneous distributed database systems</i>, and they consist of a layer of software on top of the existing database systems.</p><p style="padding-left: 106pt;text-indent: 0pt;text-align: justify;">Transactions in a federated database may be classiﬁed as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 113pt;text-indent: -16pt;text-align: left;">1. <span style=" color: #231F20;">Local transactions</span><span class="p">. These transactions are executed by each local database system outside of the federated database system’s control.</span></p><p class="s63" style="padding-top: 6pt;padding-left: 113pt;text-indent: -17pt;text-align: left;">2. <span style=" color: #231F20;">Global transactions</span><span class="p">. These transactions are executed under the control of the federated database system.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">The federated database system is aware of the fact that local transactions may run at the local nodes, but it is not aware of what speciﬁc transactions are being executed, or of what data they may access.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Ensuring the local autonomy of each database system requires that no changes be made to its software. A database system at one node thus is not able to communicate directly with one at any other node to synchronize the execution of a global transaction active at several nodes.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Since the federated database system has no control over the execution of local transactions, each local system must use a concurrency-control scheme (e.g., two-phase locking or timestamping) to ensure that its schedule is serializable. In addition, in the case of locking, the local system must be able to guard against the possibility of local deadlocks.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The guarantee of local serializability is not suﬃcient to ensure global serializability. As an illustration, consider two global transactions <i>T</i><span class="s98">1</span> and <i>T</i><span class="s98">2</span>, each of which accesses and updates two data items, <i>A </i>and <i>B</i>, located at nodes <i>N</i><span class="s98">1</span> and <i>N</i><span class="s98">2</span>, respectively. Suppose that the local schedules are serializable. It is still possible to have a situation where, at</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;"><a name="bookmark490"><span class="p">node </span></a>N<span class="s93">1</span><span class="s94">, </span>T<span class="s93">2 </span><span class="s94">follows </span>T<span class="s93">1</span><span class="s94">, whereas, at </span>N<span class="s93">2</span><span class="s94">, </span>T<span class="s93">1 </span><span class="s94">follows </span>T<span class="s93">2</span><span class="s94">, resulting in a nonserializable global schedule. Indeed, even if there is no concurrency among global transactions (i.e., a global transaction is submitted only after the previous one commits or aborts), local serializability is not suﬃcient to ensure global serializability (see Practice Exercise</span><a name="bookmark535">&zwnj;</a></p><p style="padding-left: 119pt;text-indent: 0pt;text-align: left;">23.11).</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Depending on the implementation of the local database systems, a global transac- tion may not be able to control the precise locking behavior of its local subtransactions. Thus, even if all local database systems follow two-phase locking, it may be possible only to ensure that each local transaction follows the rules of the protocol. For exam- ple, one local database system may commit its subtransaction and release locks, while the subtransaction at another local system is still executing. If the local systems permit control of locking behavior and all systems follow two-phase locking, then the feder- ated database system can ensure that global transactions lock in a two-phase manner and the lock points of conﬂicting transactions would then deﬁne their global serializa- tion order. If diﬀerent local systems follow diﬀerent concurrency-control mechanisms, however, this straightforward sort of global control does not work.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">There are many protocols for ensuring consistency despite the concurrent execu- tion of global and local transactions in federated database systems. Some are based on imposing suﬃcient conditions to ensure global serializability. Others ensure only a form of consistency weaker than serializability but achieve this consistency by less restrictive means.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">There are several schemes to ensure global serializability in an environment where update transactions as well as read-only transactions can execute. Several of these schemes are based on the idea of a <span class="s63">ticket</span>. A special data item called a ticket is cre- ated in each local database system. Every global transaction that accesses data at a node must write the ticket at that node. This requirement ensures that global trans- actions conﬂict directly at every node they visit. Furthermore, the global transaction manager can control the order in which global transactions are serialized, by control- ling the order in which the tickets are accessed. References to such schemes appear in the bibliographic notes for this chapter, available online.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part421.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part423.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
