<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>RAID-1 Analysis</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part403.htm">&lt; 上一个</a><span> | </span><a href="../ostep.html">内容</a><span> | </span><a href="part405.htm">下一个 &gt;</a></p><p class="s32" style="padding-top: 3pt;padding-left: 68pt;text-indent: 0pt;text-align: justify;">RAID-1 Analysis</p><p style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Let us assess RAID-1. From a capacity standpoint, RAID-1 is expensive; with the mirroring level = 2, we only obtain half of our peak useful ca- pacity. Thus, with N disks, the useful capacity of mirroring is N/2.</p><p style="padding-left: 68pt;text-indent: 12pt;line-height: 89%;text-align: justify;">From a reliability standpoint, RAID-1 does well. It can tolerate the fail- ure of any one disk. You may also notice RAID-1 can actually do better than this, with a little luck. Imagine, in the figure above, that disk 0 and disk 2 both failed. In such a situation, there is no data loss! More gen- erally, a mirrored system (with mirroring level of 2) can tolerate 1 disk failure for certain, and up to N/2 failures depending on which disks fail. In practice, we generally don’t like to leave things like this to chance; thus most people consider mirroring to be good for handling a single failure.</p><p style="padding-left: 68pt;text-indent: 12pt;line-height: 89%;text-align: justify;">Finally, we analyze performance. From the perspective of the latency of a single read request, we can see it is the same as the latency on a single disk; all the RAID-1 does is direct the read to one of its copies. A write is a little different: it requires two physical writes to complete before it is done. These two writes happen in parallel, and thus the time will be roughly equivalent to the time of a single write; however, because the logical write must wait for both physical writes to complete, it suffers the worst-case seek and rotational delay of the two requests, and thus (on average) will be slightly higher than a write to a single disk.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="408" height="399" alt="image" src="Image_561.png"/></span></p><p class="s27" style="padding-left: 78pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="p">A</span><span class="s7">SIDE</span><span class="p">: </span>T<span class="s45">HE </span>RAID C<span class="s45">ONSISTENT</span>-U<span class="s45">PDATE </span>P<span class="s45">ROBLEM</span></p><p style="padding-left: 41pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Before analyzing RAID-1, let us first discuss a problem that arises in any multi-disk RAID system, known as the <b>consistent-update problem </b>[DAA05]. The problem occurs on a write to any RAID that has to up- date multiple disks during a single logical operation. In this case, let us assume we are considering a mirrored disk array.</p><p style="padding-left: 41pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Imagine the write is issued to the RAID, and then the RAID decides that it must be written to two disks, disk 0 and disk 1. The RAID then issues the write to disk 0, but just before the RAID can issue the request to disk 1, a power loss (or system crash) occurs. In this unfortunate case, let us assume that the request to disk 0 completed (but clearly the request to disk 1 did not, as it was never issued).</p><p style="padding-left: 41pt;text-indent: 0pt;line-height: 89%;text-align: justify;">The result of this untimely power loss is that the two copies of the block are now <b>inconsistent</b>; the copy on disk 0 is the new version, and the copy on disk 1 is the old. What we would like to happen is for the state of both disks to change <b>atomically</b>, i.e., either both should end up as the new version or neither.</p><p style="padding-left: 41pt;text-indent: 0pt;line-height: 89%;text-align: justify;">The general way to solve this problem is to use a <b>write-ahead log </b>of some kind to first record what the RAID is about to do (i.e., update two disks with a certain piece of data) before doing it. By taking this approach, we can ensure that in the presence of a crash, the right thing will happen; by running a <b>recovery </b>procedure that replays all pending transactions to the RAID, we can ensure that no two mirrored copies (in the RAID-1 case) are out of sync.</p><p style="padding-left: 41pt;text-indent: 0pt;line-height: 89%;text-align: justify;">One last note: because logging to disk on every write is prohibitively expensive, most RAID hardware includes a small amount of non-volatile RAM (e.g., battery-backed) where it performs this type of logging. Thus, consistent update is provided without the high cost of logging to disk.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s62" style="text-indent: 0pt;line-height: 6pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s43" style="padding-left: 41pt;text-indent: 11pt;line-height: 89%;text-align: justify;"><span class="p">To analyze steady-state throughput, let us start with the sequential workload. When writing out to disk sequentially, each logical write must result in two physical writes; for example, when we write logical block 0 (in the figure above), the RAID internally would write it to both disk 0 and disk 1. Thus, we can conclude that the maximum bandwidth ob- tained during sequential writing to a mirrored array is ( </span><u>N</u> <span class="s4">· </span>S<span class="p">), or half the</span></p><p style="padding-left: 41pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">peak bandwidth.</p><p style="padding-left: 41pt;text-indent: 11pt;line-height: 89%;text-align: justify;">Unfortunately, we obtain the exact same performance during a se- quential read. One might think that a sequential read could do better, because it only needs to read one copy of the data, not both. However, let’s use an example to illustrate why this doesn’t help much. Imagine we need to read blocks 0, 1, 2, 3, 4, 5, 6, and 7. Let’s say we issue the read of 0 to disk 0, the read of 1 to disk 2, the read of 2 to disk 1, and the read of 3 to disk 3. We continue by issuing reads to 4, 5, 6, and 7 to disks 0, 2, 1, and 3, respectively. One might naively think that because we are utilizing all disks, we are achieving the full bandwidth of the array.</p><p style="padding-left: 52pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">To see that this is not the case, however, consider the requests a single</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s62" style="text-indent: 0pt;line-height: 6pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s43" style="padding-top: 3pt;padding-left: 68pt;text-indent: 0pt;line-height: 91%;text-align: justify;"><span class="p">disk receives (say disk 0). First, it gets a request for block 0; then, it gets a request for block 4 (skipping block 2). In fact, each disk receives a request for every other block. While it is rotating over the skipped block, it is not delivering useful bandwidth to the client. Thus, each disk will only deliver half its peak bandwidth. And thus, the sequential read will only obtain a bandwidth of ( </span><u>N</u> <span class="s4">· </span>S<span class="p">) MB/s.</span></p><p class="s43" style="padding-left: 68pt;text-indent: 12pt;line-height: 89%;text-align: justify;"><span class="p">Random reads are the best case for a mirrored RAID. In this case, we can distribute the reads across all the disks, and thus obtain the full pos- sible bandwidth. Thus, for random reads, RAID-1 delivers </span>N <span class="s4">· </span>R <span class="p">MB/s.</span></p><p class="s62" style="text-indent: 0pt;line-height: 6pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s43" style="padding-left: 68pt;text-indent: 12pt;line-height: 89%;text-align: justify;"><span class="p">Finally, random writes perform as you might expect: </span><u>N</u> <span class="s4">·</span>R <span class="p">MB/s. Each logical write must turn into two physical writes, and thus while all the disks will be in use, the client will only perceive this as half the available bandwidth. Even though a write to logical block X turns into two parallel writes to two different physical disks, the bandwidth of many small re- quests only achieves half of what we saw with striping. As we will soon see, getting half the available bandwidth is actually pretty good!</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part403.htm">&lt; 上一个</a><span> | </span><a href="../ostep.html">内容</a><span> | </span><a href="part405.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
