<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>40.7 Caching and Buffering</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part446.htm">&lt; 上一个</a><span> | </span><a href="../ostep.html">内容</a><span> | </span><a href="part448.htm">下一个 &gt;</a></p><p class="s40" style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;">40.7 Caching and Buffering</p><p style="padding-top: 7pt;padding-left: 68pt;text-indent: 12pt;line-height: 89%;text-align: justify;">As the examples above show, reading and writing files can be expen- sive, incurring many I/Os to the (slow) disk. To remedy what would clearly be a huge performance problem, most file systems aggressively use system memory (DRAM) to cache important blocks.</p><p style="padding-left: 68pt;text-indent: 12pt;line-height: 89%;text-align: justify;">Imagine the open example above: without caching, every file open would require at least two reads for every level in the directory hierarchy (one to read the inode of the directory in question, and at least one to read its data). With a long pathname (e.g., /1/2/3/ ... /100/file.txt), the file system would literally perform hundreds of reads just to open the file!</p><p style="padding-left: 68pt;text-indent: 12pt;line-height: 89%;text-align: justify;">Early file systems thus introduced a <b>fix-sized cache </b>to hold popular blocks. As in our discussion of virtual memory, strategies such as <b>LRU </b>and different variants would decide which blocks to keep in cache. This fix-sized cache would usually be allocated at boot time to be roughly 10% of total memory. Modern systems integrate virtual memory pages and file system pages into a <b>unified page cache </b>[S00]. In this way, memory can be allocated more flexibly across virtual memory and file system, depending on which needs more memory at a given time.</p><p style="padding-left: 68pt;text-indent: 12pt;line-height: 89%;text-align: justify;">Now imagine the file open example with caching. The first open may generate a lot of I/O traffic to read in directory inode and data, but sub- sequent file opens of that same file (or files in the same directory) will mostly hit in the cache and thus no I/O is needed.</p><p style="padding-left: 68pt;text-indent: 12pt;line-height: 89%;text-align: justify;">Let us also consider the effect of caching on writes. Whereas read I/O can be avoided altogether with a sufficiently large cache, write traffic has to go to disk in order to become persistent. Thus, a cache does not serve as the same kind of filter on write traffic that it does for reads. That said, <b>write buffering </b>(as it is sometimes called) certainly has a number of per- formance benefits. First, by delaying writes, the file system can <b>batch </b>some updates into a smaller set of I/Os; for example, if an inode bitmap is updated when one file is created and then updated moments later as another file is created, the file system saves an I/O by delaying the write after the first update. Second, by buffering a number of writes in memory, the system can then <b>schedule </b>the subsequent I/Os and thus increase per- formance. Finally, some writes are avoided altogether by delaying them; for example, if an application creates a file and then deletes it, delaying the writes to reflect the file creation to disk <b>avoids </b>them entirely. In this case, laziness (in writing blocks to disk) is a virtue.</p><p style="padding-left: 68pt;text-indent: 12pt;line-height: 89%;text-align: justify;">For the reasons above, most modern file systems buffer writes in mem- ory for anywhere between five and thirty seconds, representing yet an- other trade-off: if the system crashes before the updates have been prop- agated to disk, the updates are lost; however, by keeping writes in mem- ory longer, performance can be improved by batching, scheduling, and even avoiding writes.</p><p style="padding-left: 68pt;text-indent: 12pt;line-height: 89%;text-align: justify;">Some applications (such as databases) don’t enjoy this trade-off. Thus, to avoid unexpected data loss due to write buffering, they simply force writes to disk, by calling <span class="s41">fsync()</span>, by using <b>direct I/O </b>interfaces that</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 41pt;text-indent: 0pt;line-height: 94%;text-align: justify;">work around the cache, or by using the <b>raw disk </b>interface and avoiding the file system altogether<span class="s35">1</span>. While most applications live with the trade- offs made by the file system, there are enough controls in place to get the system to do what you want it to, should the default not be satisfying.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part446.htm">&lt; 上一个</a><span> | </span><a href="../ostep.html">内容</a><span> | </span><a href="part448.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
