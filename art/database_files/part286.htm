<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>15.7  Evaluation of Expressions</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part285.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part287.htm">下一个 &gt;</a></p><p class="s65" style="padding-top: 4pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a name="bookmark330">15.7  </a><span style=" color: #00AEEF;">Evaluation of Expressions</span><a name="bookmark351">&zwnj;</a></p><p style="padding-top: 11pt;padding-left: 88pt;text-indent: 0pt;text-align: right;">So far, we have studied how individual relational operations are carried out. Now we consider how to evaluate an expression containing multiple operations. The obvious way to evaluate an expression is simply to evaluate one operation at a time, in an ap- propriate order. The result of each evaluation is <span class="s63">materialized </span>in a temporary relation for subsequent use. A disadvantage to this approach is the need to construct the tem- porary relations, which (unless they are small) must be written to disk. An alternative approach is to evaluate several operations simultaneously in a <span class="s63">pipeline</span>, with the results of one operation passed on to the next, without the need to store a temporary relation. In Section 15.7.1 and Section 15.7.2, we consider both the <i>materialization </i>approach and the <i>pipelining </i>approach. We shall see that the costs of these approaches can diﬀer substantially, but also that there are cases where only the materialization approach is</p><p style="padding-left: 88pt;text-indent: 0pt;text-align: left;">feasible.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">15.7.1 Materialization</p><p style="padding-top: 6pt;padding-left: 87pt;text-indent: 0pt;text-align: right;">It is easiest to understand intuitively how to evaluate an expression by looking at a pictorial representation of the expression in an <span class="s63">operator tree</span>. Consider the expression:</p><p style="padding-top: 5pt;padding-left: 59pt;text-indent: 0pt;text-align: center;"><span class="s15">Π</span><i>name</i>(<span class="s15">σ</span><i>building</i><i> </i><span class="s137">=</span><span class="s15"> </span><span class="s98">“Watson”</span>(<i>department</i>) <span class="s86">⋈ </span><i>instructor</i>)</p><p style="padding-top: 5pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">in Figure 15.11.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">If we apply the materialization approach, we start from the lowest-level operations in the expression (at the bottom of the tree). In our example, there is only one such operation: the selection operation on <i>department</i>. The inputs to the lowest-level oper- ations are relations in the database. We execute these operations using the algorithms that we studied earlier, and we store the results in temporary relations. We can use these temporary relations to execute the operations at the next level up in the tree, where the inputs now are either temporary relations or relations stored in the database. In our</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="1" height="38" alt="image" src="Image_2784.png"/></span></p><p class="s382" style="padding-top: 8pt;padding-left: 84pt;text-indent: 0pt;text-align: center;">Π <span class="s111">name</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 274pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span><img width="12" height="11" alt="image" src="Image_2785.png"/></span></p><p class="s258" style="padding-left: 207pt;text-indent: 0pt;text-align: left;"><span><img width="79" height="41" alt="image" src="Image_2786.png"/></span>	<span><img width="80" height="41" alt="image" src="Image_2787.png"/></span></p><p class="s185" style="padding-left: 199pt;text-indent: 0pt;line-height: 9pt;text-align: left;">σ</p><p style="text-indent: 0pt;text-align: left;"><span><img width="1" height="38" alt="image" src="Image_2788.png"/></span></p><p class="s111" style="padding-left: 203pt;text-indent: 0pt;line-height: 7pt;text-align: left;">building <span class="s230">= “Watson”</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s111" style="padding-top: 6pt;padding-left: 182pt;text-indent: 0pt;text-align: left;">department</p><p class="s111" style="padding-left: 49pt;text-indent: 0pt;text-align: left;">instructor</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 59pt;text-indent: 0pt;text-align: center;">Figure 15.11 <span class="s74">Pictorial representation of an expression.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">example, the inputs to the join are the <i>instructor </i>relation and the temporary relation created by the selection on <i>department</i>. The join can now be evaluated, creating another temporary relation.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">By repeating the process, we will eventually evaluate the operation at the root of the tree, giving the ﬁnal result of the expression. In our example, we get the ﬁnal result by executing the projection operation at the root of the tree, using as input the temporary relation created by the join.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Evaluation as just described is called <span class="s63">materialized evaluation</span>, since the results of each intermediate operation are created (materialized) and then are used for evaluation of the next-level operations.</p><p class="s13" style="padding-left: 119pt;text-indent: 17pt;text-align: justify;"><span class="p">The cost of a materialized evaluation is not simply the sum of the costs of the operations involved. When we computed the cost estimates of algorithms, we ignored the cost of writing the result of the operation to disk. To compute the cost of evaluating an expression as done here, we have to add the costs of all the operations, as well as the cost of writing the intermediate results to disk. We assume that the records of the result accumulate in a buﬀer, and, when the buﬀer is full, they are written to disk. The number of blocks written out, </span>b<span class="s97">r </span><span class="p">, can be estimated as </span>n<span class="s97">r </span><span class="s15">∕</span>f<span class="s97">r </span><span class="p">, where </span>n<span class="s97">r </span><span class="p">is the estimated</span></p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 79%;text-align: justify;">number of tuples in the result relation <i>r </i>and <i>f</i><span class="s97">r </span>is the <i>blocking factor </i>of the result relation,</p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">that is, the number of records of <i>r </i>that will ﬁt in a block. In addition to the transfer</p><p class="s15" style="text-indent: 0pt;text-align: left;">⌈</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="text-indent: 0pt;text-align: left;">⌉</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 119pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">time, some disk seeks may be required, since the disk head may have moved between</p><p class="s13" style="padding-top: 1pt;padding-left: 119pt;text-indent: 0pt;line-height: 90%;text-align: left;"><span class="p">successive writes. The number of seeks can be estimated as </span>b<span class="s145">r </span><span class="s15">∕</span>b<span class="s145">b </span><span class="p">where </span>b<span class="s145">b </span><span class="p">is the size of the output buﬀer (measured in blocks).</span></p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;"><span class="s63">Double buﬀering </span>(using two buﬀers, with one continuing execution of the algorithm while the other is being written out) allows the algorithm to execute more quickly by performing <span class="s44">CPU </span>activity in parallel with <span class="s44">I/O </span>activity. The number of seeks can be re- duced by allocating extra blocks to the output buﬀer and writing out multiple blocks at once.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 7pt;padding-left: 119pt;text-indent: 0pt;text-align: left;">15.7.2 Pipelining</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">We can improve query-evaluation eﬃciency by reducing the number of temporary ﬁles that are produced. We achieve this reduction by combining several relational operations into a <i>pipeline </i>of operations, in which the results of one operation are passed along to the next operation in the pipeline. Evaluation as just described is called <span class="s63">pipelined evaluation</span>.</p><p style="padding-left: 119pt;text-indent: 17pt;line-height: 86%;text-align: justify;">For example, consider the expression (<span class="s15">Π</span><i>a</i><span class="s98">1,</span><i>a</i><span class="s98">2</span>(<i>r </i><span class="s86">⋈ </span><i>s</i>)). If materialization were ap- plied, evaluation would involve creating a temporary relation to hold the result of the</p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">join and then reading back in the result to perform the projection. These operations can be combined: When the join operation generates a tuple of its result, it passes that tuple immediately to the project operation for processing. By combining the join and the projection, we avoid creating the intermediate result and instead create the ﬁnal result directly.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 106pt;text-indent: 0pt;text-align: justify;">Creating a pipeline of operations can provide two beneﬁts:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 113pt;text-indent: -16pt;line-height: 13pt;text-align: justify;"><span class="s63">1. </span>It eliminates the cost of reading and writing temporary relations, reducing the cost of query evaluation. Note that the cost formulae that we saw earlier for each operation included the cost of reading the result from disk. If the input to an operator <i>o</i><span class="s97">i </span>is pipelined from a preceding operator <i>o</i><span class="s97">j </span>, the cost of <i>o</i><span class="s97">i </span>should not include the cost of reading the input from disk; the cost formulae that we saw earlier can be modiﬁed accordingly.</p><p class="s63" style="padding-top: 5pt;padding-left: 113pt;text-indent: -17pt;text-align: justify;">2. <span class="p">It can start generating query results quickly, if the root operator of a query- evaluation plan is combined in a pipeline with its inputs. This can be quite useful if the results are displayed to a user as they are generated, since otherwise there may be a long delay before the user sees any query results.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">15.7.2.1 Implementation of Pipelining</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">We can implement a pipeline by constructing a single, complex operation that com- bines the operations that constitute the pipeline. Although this approach may be feasi- ble for some frequently occurring situations, it is desirable in general to reuse the code for individual operations in the construction of a pipeline.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">In the example of Figure 15.11, all three operations can be placed in a pipeline, which passes the results of the selection to the join as they are generated. In turn, it passes the results of the join to the projection as they are generated. The memory requirements are low, since results of an operation are not stored for long. However, as a result of pipelining, the inputs to the operations are not available all at once for processing.</p><p style="padding-left: 106pt;text-indent: 0pt;text-align: justify;">Pipelines can be executed in either of two ways:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 113pt;text-indent: -16pt;text-align: justify;">1. <span class="p">In a </span>demand-driven pipeline<span class="p">, the system makes repeated requests for tuples from the operation at the top of the pipeline. Each time that an operation receives a request for tuples, it computes the next tuple (or tuples) to be returned and then returns that tuple. If the inputs of the operation are not pipelined, the next tuple(s) to be returned can be computed from the input relations, while the sys- tem keeps track of what has been returned so far. If it has some pipelined inputs, the operation also makes requests for tuples from its pipelined inputs. Using the tuples received from its pipelined inputs, the operation computes tuples for its output and passes them up to its parent.</span></p><p class="s63" style="padding-top: 6pt;padding-left: 113pt;text-indent: -17pt;text-align: justify;">2. <span class="p">In a </span>producer-driven pipeline<span class="p">, operations do not wait for requests to produce tu- ples, but instead generate the tuples </span>eagerly<span class="p">. Each operation in a producer-driven pipeline is modeled as a separate process or thread within the system that takes a stream of tuples from its pipelined inputs and generates a stream of tuples for its output.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">We describe next how demand-driven and producer-driven pipelines can be imple- mented.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Each operation in a demand-driven pipeline can be implemented as an <span class="s63">iterator </span>that provides the following functions: <i>open</i>(), <i>next</i>(), and <i>close</i>(). After a call to <i>open</i>(), each call to <i>next</i>() returns the next output tuple of the operation. The implementation of the operation in turn calls <i>open</i>() and <i>next</i>() on its inputs, to get its input tuples when required. The function <i>close</i>() tells an iterator that no more tuples are required. The iterator maintains the <span class="s63">state </span>of its execution in between calls so that successive <i>next</i>() requests receive successive result tuples.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">For example, for an iterator implementing the select operation using linear search, the <i>open</i>() operation starts a ﬁle scan, and the iterator’s state records the point to which the ﬁle has been scanned. When the <i>next</i>() function is called, the ﬁle scan continues from after the previous point; when the next tuple satisfying the selection is found by scanning the ﬁle, the tuple is returned after storing the point where it was found in the iterator state. A merge-join iterator’s <i>open</i>() operation would open its inputs, and if they are not already sorted, it would also sort the inputs. On calls to <i>next</i>(), it would return the next pair of matching tuples. The state information would consist of up to where each input had been scanned. Details of the implementation of iterators are left for you to complete in Practice Exercise 15.7.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Producer-driven pipelines, on the other hand, are implemented in a diﬀerent man- ner. For each pair of adjacent operations in a producer-driven pipeline, the system cre- ates a buﬀer to hold tuples being passed from one operation to the next. The processes or threads corresponding to diﬀerent operations execute concurrently. Each operation at the bottom of a pipeline continually generates output tuples, and puts them in its output buﬀer, until the buﬀer is full. An operation at any other level of a pipeline gen- erates output tuples when it gets input tuples from lower down in the pipeline until its output buﬀer is full. Once the operation uses a tuple from a pipelined input, it removes the tuple from its input buﬀer. In either case, once the output buﬀer is full, the opera- tion waits until its parent operation removes tuples from the buﬀer so that the buﬀer has space for more tuples. At this point, the operation generates more tuples until the buﬀer is full again. The operation repeats this process until all the output tuples have been generated.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">It is necessary for the system to switch between operations only when an output buﬀer is full or when an input buﬀer is empty and more input tuples are needed to gen- erate any more output tuples. In a parallel-processing system, operations in a pipeline may be run concurrently on distinct processors (see Section 22.5.1).</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Using producer-driven pipelining can be thought of as <span class="s63">pushing </span>data up an oper- ation tree from below, whereas using demand-driven pipelining can be thought of as <span class="s63">pulling </span>data up an operation tree from the top. Whereas tuples are generated <i>eagerly </i>in producer-driven pipelining, they are generated <span class="s63">lazily</span>, on demand, in demand-driven pipelining. Demand-driven pipelining is used more commonly than producer-driven pipelining because it is easier to implement. However, producer-driven pipelining is very useful in parallel processing systems. Producer-driven pipelining has also been</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">found to be more eﬃcient than demand-driven pipelining on modern <span class="s44">CPU</span>s since it re- duces the number of function call invocations as compared to demand-driven pipelin- ing. Producer-driven pipelining is increasingly used in systems that generate machine code for high performance query evaluation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">15.7.2.2 Evaluation Algorithms for Pipelining</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Query plans can be annotated to mark edges that are pipelined; such edges are called <span class="s63">pipelined edges</span>. In contrast, non-pipelined edges are referred to as <span class="s63">blocking edges </span>or <span class="s63">materialized edges</span>. The two operators connected by a pipelined edge must be executed concurrently, since one consumes tuples as the other generates them. Since a plan can have multiple pipelined edges, the set of all operators that are connected by pipelined edges must be executed concurrently. A query plan can be divided into subtrees such that each subtree has only pipelined edges, and the edges between the subtrees are non- pipelined. Each such subtree is called a <span class="s63">pipeline stage</span>. The query processor executes the plan one pipeline stage at a time, and concurrently executes all the operators in a single pipeline stage.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Some operations, such as sorting, are inherently <span class="s63">blocking operations</span>, that is, they may not be able to output any results until all tuples from their inputs have been exam- ined.<span class="s76">7</span> But interestingly, blocking operators can consume tuples as they are generated, and can output tuples to their consumers as they are generated; such operations actu- ally execute in two or more stages, and blocking actually happens between two stages of the operation.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">For example, the external sort-merge operation actually has two steps: (i) run- generation, followed by (ii) merging. The run-generation step can accept tuples as they are generated by the input to the sort, and can thus be pipelined with the sort input. The merge step, on the other hand, can send tuples to its consumer as they are gener- ated, and can thus be pipelined with the consumer of the sort operation. But the merge step can start only after the run-generation step has ﬁnished. We can thus model the sort-merge operator as two sub-operators connected to each other by a non-pipelined edge, but each of the sub-operators can be connected by pipelined edges to their input and output respectively.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Other operations, such as join, are not inherently blocking, but speciﬁc evaluation algorithms may be blocking. For example, the indexed nested loops join algorithm can output result tuples as it gets tuples for the outer relation. It is therefore pipelined on its outer (left-hand side) relation; however, it is blocking on its indexed (right-hand side) input, since the index must be fully constructed before the indexed nested-loop join algorithm can execute.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The hash-join algorithm is a blocking operation on both inputs, since it requires both its inputs to be fully retrieved and partitioned before it outputs any tuples. How-</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="95" height="1" alt="image" src="Image_2789.png"/></span></p><p class="s77" style="padding-top: 3pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">7<span class="s78">Blocking operations such as sorting may be able to output tuples early if the input is known to satisfy some special properties such as being sorted, or partially sorted, already. However, in the absence of such information, blocking operations cannot output tuples early.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s383" style="padding-left: 143pt;text-indent: 0pt;text-align: left;">	</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="171" height="79" alt="image" src="Image_2790.png"/></span></p><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">r</p><p style="text-indent: 0pt;text-align: left;"/><p class="s384" style="text-indent: 0pt;text-align: left;"></p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">s</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="188" height="92" alt="image" src="Image_2791.png"/></span></p><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">s</p><p style="text-indent: 0pt;text-align: left;"/><p class="s173" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Part.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">r</p><p style="text-indent: 0pt;text-align: left;"/><p class="s173" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Part.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s173" style="text-indent: 0pt;line-height: 8pt;text-align: left;">HJ-BP</p><p style="text-indent: 0pt;text-align: left;"/><p class="s173" style="text-indent: 0pt;line-height: 8pt;text-align: left;">HA-IM</p><p style="text-indent: 0pt;text-align: left;"/><p class="s230" style="padding-top: 4pt;padding-left: 187pt;text-indent: 0pt;text-align: left;">(a) Logical Query                (b) Pipelined Plan</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 215pt;text-indent: 0pt;text-align: left;">Figure 15.12 <span class="s74">Query plan with pipelining.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">ever, hash-join partitions each of its inputs, and then performs multiple build-probe steps, once per partition. Thus, the hash-join algorithm has 3 steps: (i) partitioning of the ﬁrst input, (ii) partitioning of the second input, and (iii) the build-probe step. The partitioning step for each input can accept tuples as they are generated by the input, and can thus be pipelined with its input. The build-probe step can output tuples to its consumer as the tuples are generated, and can thus be pipelined with its consumer. But the two partitioning steps are connected to the build-probe step by non-pipelined edges, since build-probe can start only after partitioning has been completed on both inputs.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Hybrid hash join can be viewed as partially pipelined on the probe relation, since it can output tuples from the ﬁrst partition as tuples are received for the probe relation. However, tuples that are not in the ﬁrst partition will be output only after the entire pipelined input relation is received. Hybrid hash join thus provides fully pipelined eval- uation on its probe input if the build input ﬁts entirely in memory, or nearly pipelined evaluation if most of the build input ﬁts in memory.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Figure 15.12a shows a query that joins two relations <i>r </i>and <i>s</i>, and then performs an aggregation on the result; details of the join predicate, group by attributes and aggre- gation functions are omitted for simplicity. Figure 15.12b shows a pipelined plan for the query using hash join and in-memory hash aggregation. Pipelined edges are shown using a normal line, while blocking edges are shown using a bold line. Pipeline stages are enclosed in dashed boxes. Note that hash join has been split into three suboper- ators. Two of suboperators, shown abbreviated to <i>Part.</i>, partition <i>r </i>and <i>s </i>respectively. The third, abbreviated to <i>HJ-BP</i>, performs the build and probe phase of the hash join. The <i>HA-IM </i>operator is the in-memory hash aggregation operator. The edges from the partition operators to the <i>HJ-BP </i>operator are blocking edges, since the <i>HJ-BP </i>operator can start execution only after the partition operators have completed execution. The edges from the relations (assumed to be scanned using a relation scan operator) to the partition operators are pipelined, as is the edge from the <i>HJ-BP </i>operator to the <i>HA-IM </i>operator. The resultant pipeline stages are shown enclosed in dashed boxes.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">In general, for each materialized edge we need to add the cost of writing the data to disk, and the cost of the consumer operator should include the cost of reading the data from disk. However, when a materialized edge is between suboperators of a single</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="494" height="1" alt="image" src="Image_2792.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 148pt;text-indent: 0pt;line-height: 88%;text-align: justify;"><i>done</i><span class="s97">r </span>:= <i>false</i>; <i>done</i><span class="s145">s </span>:= <i>false</i>; <i>r </i>:= <span class="s15">∅</span>;</p><p style="padding-left: 148pt;text-indent: 0pt;text-align: justify;"><i>s </i>:= <span class="s15">∅</span>;</p><p style="padding-left: 148pt;text-indent: 0pt;text-align: justify;"><i>result </i>:= <span class="s15">∅</span>;</p><p class="s46" style="padding-top: 1pt;padding-left: 160pt;text-indent: -12pt;line-height: 87%;text-align: justify;">while not <i>done</i><span class="s97">r </span>or not <i>done</i><span class="s97">s </span>do begin</p><p class="s46" style="padding-left: 175pt;text-indent: 0pt;text-align: left;">if <span class="p">queue is empty, </span>then <span class="p">wait until queue is not empty;</span></p><p class="s13" style="padding-left: 175pt;text-indent: 0pt;text-align: left;">t <span class="p">:= top entry in queue;</span></p><p class="s13" style="padding-left: 175pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><b>if </b>t <span class="p">= </span>End<span class="s145">r </span><b>then </b>done<span class="s145">r </span><span class="p">:= </span>true</p><p class="s13" style="padding-left: 184pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><b>else if </b>t <span class="p">= </span>End<span class="s145">s </span><b>then </b>done<span class="s145">s </span><span class="p">:= </span>true</p><p class="s13" style="padding-left: 195pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><b>else if </b>t <span class="p">is from input </span>r</p><p class="s46" style="padding-left: 206pt;text-indent: 0pt;text-align: left;">then</p><p class="s46" style="padding-left: 221pt;text-indent: 0pt;line-height: 13pt;text-align: left;">begin</p><p class="s13" style="padding-left: 236pt;text-indent: 0pt;line-height: 12pt;text-align: left;">r <span class="p">:= </span>r <span class="s15">∪ </span><span class="s95">{</span>t<span class="s95">}</span><span class="p">;</span></p><p class="s13" style="padding-left: 236pt;text-indent: 0pt;line-height: 15pt;text-align: left;">result <span class="p">:= </span>result <span class="s15">∪ </span><span class="p">(</span><span class="s95">{</span>t<span class="s95">} </span><span class="s86">⋈ </span>s<span class="p">);</span></p><p class="s46" style="padding-left: 221pt;text-indent: 0pt;line-height: 12pt;text-align: left;">end</p><p style="padding-left: 206pt;text-indent: 0pt;text-align: left;"><b>else </b>/* <i>t </i>is from input <i>s </i>*/</p><p class="s46" style="padding-left: 221pt;text-indent: 0pt;line-height: 13pt;text-align: left;">begin</p><p class="s13" style="padding-left: 236pt;text-indent: 0pt;line-height: 12pt;text-align: left;">s <span class="p">:= </span>s <span class="s15">∪ </span><span class="s95">{</span>t<span class="s95">}</span><span class="p">;</span></p><p class="s13" style="padding-left: 236pt;text-indent: 0pt;line-height: 15pt;text-align: left;">result <span class="p">:= </span>result <span class="s15">∪ </span><span class="p">(</span>r <span class="s86">⋈ </span><span class="s95">{</span>t<span class="s95">}</span><span class="p">);</span></p><p class="s46" style="padding-left: 59pt;text-indent: 0pt;line-height: 12pt;text-align: center;">end</p><p class="s46" style="padding-left: 34pt;text-indent: 0pt;text-align: center;">end</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="494" height="1" alt="image" src="Image_2793.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 59pt;text-indent: 0pt;text-align: center;">Figure 15.13 <span class="s74">Double-pipelined join algorithm.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">operator, for example between run generation and merge, the materialization cost has already been accounted for in the operators cost, and should not be added again.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">In some applications, a join algorithm that is pipelined on both its inputs and its output is desirable. If both inputs are sorted on the join attribute, and the join condition is an equi-join, merge join can be used, with both its inputs and its output pipelined.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">However, in the more common case that the two inputs that we desire to pipeline into the join are not already sorted, another alternative is the <span class="s63">double-pipelined join </span>tech- nique, shown in Figure 15.13. The algorithm assumes that the input tuples for both input relations, <i>r </i>and <i>s</i>, are pipelined. Tuples made available for both relations are queued for processing in a single queue. Special queue entries, called <i>End</i><span class="s97">r </span>and <i>End</i><span class="s97">s</span>,</p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">which serve as end-of-ﬁle markers, are inserted in the queue after all tuples from <i>r </i>and <i>s</i></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">(respectively) have been generated. For eﬃcient evaluation, appropriate indices should be built on the relations <i>r </i>and <i>s</i>. As tuples are added to <i>r </i>and <i>s</i>, the indices must be kept</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;"><a name="bookmark331">up to date. When hash indices are used on </a><i>r </i>and <i>s</i>, the resultant algorithm is called the<a name="bookmark352">&zwnj;</a></p><p class="s63" style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">double-pipelined hash-join <span class="p">technique.</span></p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The double-pipelined join algorithm in Figure 15.13 assumes that both inputs ﬁt in memory. In case the two inputs are larger than memory, it is still possible to use the double-pipelined join technique as usual until available memory is full. When available memory becomes full, <i>r </i>and <i>s </i>tuples that have arrived up to that point can be treated as being in partition <i>r</i><span class="s98">0</span> and <i>s</i><span class="s98">0</span>, respectively. Tuples for <i>r </i>and <i>s </i>that arrive subsequently are assigned to partitions <i>r</i><span class="s98">1</span> and <i>s</i><span class="s98">1</span>, respectively, which are written to disk, and are not added to the in-memory index. However, tuples assigned to <i>r</i><span class="s98">1</span> and <i>s</i><span class="s98">1</span> are used to probe <i>s</i><span class="s98">0</span> and <i>r</i><span class="s98">0</span>, respectively, before they are written to disk. Thus, the join of <i>r</i><span class="s98">1</span> with <i>s</i><span class="s98">0</span>, and <i>s</i><span class="s98">1</span> with <i>r</i><span class="s98">0</span>, is also carried out in a pipelined fashion. After <i>r </i>and <i>s </i>have been fully processed, the join of <i>r</i><span class="s98">1</span> tuples with <i>s</i><span class="s98">1</span> tuples must be carried out to complete the join; any of the join techniques we have seen earlier can be used to join <i>r</i><span class="s98">1</span> with <i>s</i><span class="s98">1</span>.</p><p class="s68" style="padding-top: 11pt;padding-left: 119pt;text-indent: 0pt;text-align: left;">15.7.3 Pipelines for Continuous-Stream Data</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Pipelining is also applicable in situations where data are entered into the database in a continuous manner, as is the case, for example, for inputs from sensors that are continuously monitoring environmental data. Such data are called <i>data streams</i>, as we saw earlier in Section 10.5. Queries may be written over stream data in order to respond to data as they arrive. Such queries are called <i>continuous queries</i>.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The operations in a continuous query should be implemented using pipelined al- gorithms, so that results from the pipeline can be output without blocking. Producer- driven pipelines (which we discussed earlier in Section 15.7.2.1) are the best suited for continuous query evaluation.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Many such queries perform aggregation with windowing; tumbling windows which divide time into ﬁxed size intervals, such as 1 minute, or 1 hour, are commonly used. Grouping and aggregation is performed separately on each window, as tuples are re- ceived; assuming memory size is large enough, an in-memory hash index is used to perform aggregation.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The result of aggregation on a window can be output once the system knows that no further tuples in that window will be received in future. If tuples are guaranteed to arrive sorted by timestamp, the arrival of a tuple of a following window indicates no more tuples will be received for an earlier window. If tuples may arrive out of order, streams must carry punctuations that indicate that all future tuples will have a timestamp greater than some speciﬁed value. The arrival of a punctuation allows the output of aggregates of windows whose end-timestamp is less than or equal to the timestamp speciﬁed by the punctuation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part285.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part287.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
