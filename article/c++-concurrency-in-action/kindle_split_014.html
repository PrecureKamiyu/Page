<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:mbp="Kindle">
  <head>
    <title>C++ Concurrency in Action, Second Edition</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<h2 class="part" id="ch04">Chapter 4. <a id="ch04__title" class="calibre3"></a>Synchronizing concurrent operations
      </h2>
      
      <p class="noind"><a id="iddle1141" class="calibre4"></a><a id="iddle1190" class="calibre4"></a><i class="calibre6">This chapter covers</i></p>
      
      <p class="calibre19"></p>
      <ul class="calibre21">
         
         <li class="calibre22">Waiting for an event</li>
         
         <li class="calibre22">Waiting for one-off events with futures</li>
         
         <li class="calibre22">Waiting with a time limit</li>
         
         <li class="calibre22">Using the synchronization of operations to simplify code</li>
         
      </ul>
      
      <p class="noind">In the last chapter, we looked at various ways of protecting data that’s shared between threads. But sometimes you don’t just
         need to protect the data, you also need to synchronize actions on separate threads. One thread might need to wait for another
         thread to complete a task before the first thread can complete its own, for example. In general, it’s common to want a thread
         to wait for a specific event to happen or a condition to be <kbd class="calibre17">true</kbd>. Although it would be possible to do this by periodically checking a “task complete” flag or something similar stored in
         shared data, this is far from ideal. The need to synchronize operations between threads like this is such a common scenario
         that the C++ Standard Library provides facilities to handle it, in the form of <i class="calibre6">condition variables</i> and <i class="calibre6">futures</i>. These facilities are extended in the Concurrency Technical Specification (TS), which provides additional operations for
         <i class="calibre6">futures</i>, alongside new synchronization facilities in the form of <i class="calibre6">latches</i> and <i class="calibre6">barriers</i>.
      </p>
      
      <p class="noind"><a id="iddle1143" class="calibre4"></a><a id="iddle1144" class="calibre4"></a><a id="iddle1194" class="calibre4"></a><a id="iddle1329" class="calibre4"></a><a id="iddle2432" class="calibre4"></a><a id="iddle2433" class="calibre4"></a><a id="iddle2637" class="calibre4"></a><a id="iddle2640" class="calibre4"></a>In this chapter, I’ll discuss how to wait for events with condition variables, futures, latches, and barriers, and how to
         use them to simplify the synchronization of operations.
      </p>
      
      
      <h3 id="ch04lev1sec1" class="chapter"><a id="ch04lev1sec1__title" class="calibre3"></a>4.1. Waiting for an event or other condition
      </h3>
      
      <p class="noind">Suppose you’re traveling on an overnight train. One way to ensure you get off at the right station would be to stay awake
         all night and pay attention to where the train stops. You wouldn’t miss your station, but you’d be tired when you got there.
         Alternatively, you could look at the timetable to see when the train is supposed to arrive, set your alarm a bit before, and
         go to sleep. That would be OK; you wouldn’t miss your stop, but if the train got delayed, you’d wake up too early. There’s
         also the possibility that your alarm clock’s batteries would die, and you’d sleep too long and miss your station. What would
         be ideal is if you could go to sleep and have somebody or something wake you up when the train gets to your station, whenever
         that is.
      </p>
      
      <p class="noind">How does that relate to threads? Well, if one thread is waiting for a second thread to complete a task, it has several options.
         First, it could keep checking a flag in shared data (protected by a mutex) and have the second thread set the flag when it
         completes the task. This is wasteful on two counts: the thread consumes valuable processing time repeatedly checking the flag,
         and when the mutex is locked by the waiting thread, it can’t be locked by any other thread. Both of these work against the
         thread doing the waiting: if the waiting thread is running, this limits the execution resources available to run the thread
         being waited for, and while the waiting thread has locked the mutex protecting the flag in order to check it, the thread being
         waited for is unable to lock the mutex to set the flag when it’s done. This is akin to staying awake all night talking to
         the train driver: he has to drive the train more slowly because you keep distracting him, so it takes longer to get there.
         Similarly, the waiting thread is consuming resources that could be used by other threads in the system and may end up waiting
         longer than necessary.
      </p>
      
      <p class="noind">A second option is to have the waiting thread sleep for short periods between the checks using the <kbd class="calibre17">std::this_thread::sleep_for()</kbd> function (see <a href="#ch04lev1sec3" class="calibre4">section 4.3</a>):
      </p>
      
      <pre id="PLd0e7888" class="calibre5">bool flag;
std::mutex m;
void wait_for_flag()
{
    std::unique_lock&lt;std::mutex&gt; lk(m);
    while(!flag)
    {
        lk.unlock();                                                  <b class="calibre24"><i class="calibre6">1</i></b>
        std::this_thread::sleep_for(std::chrono::milliseconds(100));  <b class="calibre24"><i class="calibre6">2</i></b>
        lk.lock();                                                    <b class="calibre24"><i class="calibre6">3</i></b>
    }
}</pre>
      
      <p class="calibre19"></p>
      <ul class="calibre21">
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">1</i> Unlock the mutex.</b></li>
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">2</i> Sleep for 100 ms.</b></li>
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">3</i> Relock the mutex.</b></li>
         
      </ul>
      
      <p class="noind"><a id="iddle1193" class="calibre4"></a><a id="iddle1196" class="calibre4"></a><a id="iddle2097" class="calibre4"></a><a id="iddle2110" class="calibre4"></a><a id="iddle2639" class="calibre4"></a>In the loop, the function unlocks the mutex <b class="calibre24"><i class="calibre6">1</i></b> before the sleep <b class="calibre24"><i class="calibre6">2</i></b>, and locks it again afterward <b class="calibre24"><i class="calibre6">3</i></b> so another thread gets a chance to acquire it and set the flag.
      </p>
      
      <p class="noind">This is an improvement because the thread doesn’t waste processing time while it’s sleeping, but it’s hard to get the sleep
         period right. Too short a sleep in between checks and the thread still wastes processing time checking; too long a sleep and
         the thread will keep on sleeping even when the task it’s waiting for is complete, introducing a delay. It’s rare that this
         oversleeping will have a direct impact on the operation of the program, but it could mean dropped frames in a fast-paced game
         or overrunning a time slice in a real-time application.
      </p>
      
      <p class="noind">The third and preferred option is to use the facilities from the C++ Standard Library to wait for the event itself. The most
         basic mechanism for waiting for an event to be triggered by another thread (such as the presence of additional work in the
         pipeline mentioned previously) is the <i class="calibre6">condition variable</i>. Conceptually, a condition variable is associated with an event or other <i class="calibre6">condition</i>, and one or more threads can <i class="calibre6">wait</i> for that condition to be satisfied. When a thread has determined that the condition is satisfied, it can then <i class="calibre6">notify</i> one or more of the threads waiting on the condition variable in order to wake them up and allow them to continue processing.
      </p>
      
      
      <h4 id="ch04lev2sec1" class="calibre23">4.1.1. <a id="ch04lev2sec1__title" class="calibre4"></a>Waiting for a condition with condition variables
      </h4>
      
      <p class="noind">The Standard C++ Library provides not one but <i class="calibre6">two</i> implementations of a condition variable: <kbd class="calibre17">std::condition_variable</kbd> and <kbd class="calibre17">std::condition_variable_any</kbd>. Both of these are declared in the <kbd class="calibre17">&lt;condition_variable&gt;</kbd> library header. In both cases, they need to work with a mutex in order to provide appropriate synchronization; the former
         is limited to working with <kbd class="calibre17">std::mutex</kbd>, whereas the latter can work with anything that meets the minimal criteria for being mutex-like, hence the <kbd class="calibre17">_any</kbd> suffix. Because <kbd class="calibre17">std::condition_variable_any</kbd> is more general, there’s the potential for additional costs in terms of size, performance, or OS resources, so <kbd class="calibre17">std::condition_variable</kbd> should be preferred unless the additional flexibility is required.
      </p>
      
      <p class="noind">So, how do you use <kbd class="calibre17">std::condition_variable</kbd> to handle the example in the introduction? How do you let the thread that’s waiting for work sleep until there’s data to
         process? The following listing shows one way you could do this with a condition variable.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex01">Listing 4.1. <a id="ch04ex01__title" class="calibre4"></a>Waiting for data to process with <kbd class="calibre17">std::condition_variable</kbd></h5>
      <pre id="PLd0e8053" class="calibre5">std::mutex mut;
std::queue&lt;data_chunk&gt; data_queue;                 <b class="calibre24"><i class="calibre6">1</i></b>
std::condition_variable data_cond;
void data_preparation_thread()
{
    while(more_data_to_prepare())
    {
        data_chunk const data=prepare_data();
        {
            std::lock_guard&lt;std::mutex&gt; lk(mut);
            data_queue.push(data);                 <b class="calibre24"><i class="calibre6">2</i></b>
        }
        data_cond.notify_one();                    <b class="calibre24"><i class="calibre6">3</i></b>
    }
}
void data_processing_thread()
{
    while(true)
    {
        std::unique_lock&lt;std::mutex&gt; lk(mut);      <b class="calibre24"><i class="calibre6">4</i></b>
        data_cond.wait(
            lk,[]{return !data_queue.empty();});   <b class="calibre24"><i class="calibre6">5</i></b>
        data_chunk data=data_queue.front();
        data_queue.pop();
        lk.unlock();                               <b class="calibre24"><i class="calibre6">6</i></b>
        process(data);
        if(is_last_chunk(data))
            break;
    }
}</pre>
      
      <p class="noind"><a id="iddle1698" class="calibre4"></a><a id="iddle2620" class="calibre4"></a>First off, you have a queue <b class="calibre24"><i class="calibre6">1</i></b> that’s used to pass the data between the two threads. When the data is ready, the thread preparing the data locks the mutex
         protecting the queue using a <kbd class="calibre17">std::lock_guard</kbd> and pushes the data onto the queue <b class="calibre24"><i class="calibre6">2</i></b>. It then calls the <kbd class="calibre17">notify_one()</kbd> member function on the <kbd class="calibre17">std::condition_variable</kbd> instance to notify the waiting thread (if there is one) <b class="calibre24"><i class="calibre6">3</i></b>. Note that you put the code to push the data onto the queue in a smaller scope, so you notify the condition variable <i class="calibre6">after</i> unlocking the mutex — this is so that, if the waiting thread wakes immediately, it doesn’t then have to block again, waiting
         for you to unlock the mutex.
      </p>
      
      <p class="noind">On the other side of the fence, you have the processing thread. This thread first locks the mutex, but this time with a <kbd class="calibre17">std::unique_lock</kbd> rather than a <kbd class="calibre17">std::lock_guard</kbd> <b class="calibre24"><i class="calibre6">4</i></b>—you’ll see why in a minute. The thread then calls <kbd class="calibre17">wait()</kbd> on the <kbd class="calibre17">std:: condition_variable</kbd>, passing in the lock object and a lambda function that expresses the condition being waited for <b class="calibre24"><i class="calibre6">5</i></b>. Lambda functions are a new feature in C++11 that allow you to write an anonymous function as part of another expression,
         and they’re ideally suited for specifying predicates for standard library functions such as <kbd class="calibre17">wait()</kbd>. In this case, the simple <kbd class="calibre17">[]{return !data_queue.empty();}</kbd> lambda function checks to see if the <kbd class="calibre17">data_queue</kbd> is not <kbd class="calibre17">empty()</kbd>—that is, there’s some data in the queue ready for processing. Lambda functions are described in more detail in <a href="kindle_split_022.html#app01" class="calibre4">appendix A</a>, <a href="kindle_split_022.html#app01lev1sec5" class="calibre4">section A.5</a>.
      </p>
      
      <p class="noind">The implementation of <kbd class="calibre17">wait()</kbd> then checks the condition (by calling the supplied lambda function) and returns if it’s satisfied (the lambda function returned
         <kbd class="calibre17">true</kbd>). If the condition isn’t satisfied (the lambda function returned <kbd class="calibre17">false</kbd>), <kbd class="calibre17">wait()</kbd> unlocks the mutex and puts the thread in a blocked or waiting state. When the condition variable is notified by a call to
         <kbd class="calibre17">notify_one()</kbd> from the data-preparation thread, the thread wakes from its slumber (unblocks it), reacquires the lock on the mutex, and
         checks the condition again, returning from <kbd class="calibre17">wait()</kbd> with the mutex still locked if the condition has been satisfied. If the condition hasn’t been satisfied, the thread unlocks
         the mutex and resumes waiting. This is why you need the <kbd class="calibre17">std::unique_lock</kbd> rather than <a id="iddle1187" class="calibre4"></a><a id="iddle1195" class="calibre4"></a><a id="iddle1696" class="calibre4"></a><a id="iddle1936" class="calibre4"></a><a id="iddle2541" class="calibre4"></a><a id="iddle2638" class="calibre4"></a>the <kbd class="calibre17">std::lock_guard</kbd>—the waiting thread must unlock the mutex while it’s waiting and lock it again afterward, and <kbd class="calibre17">std::lock_guard</kbd> doesn’t provide that flexibility. If the mutex remained locked while the thread was sleeping, the data-preparation thread
         wouldn’t be able to lock the mutex to add an item to the queue, and the waiting thread would never be able to see its condition
         satisfied.
      </p>
      
      <p class="noind"><a href="#ch04ex01" class="calibre4">Listing 4.1</a> uses a simple lambda function for the wait <b class="calibre24"><i class="calibre6">5</i></b>, which checks to see if the queue is not empty, but any function or callable object could be passed. If you already have
         a function to check the condition (perhaps because it’s more complicated than a simple test like this), then this function
         can be passed in directly; there’s no need to wrap it in a lambda. During a call to <kbd class="calibre17">wait()</kbd>, a condition variable may check the supplied condition any number of times; but it always does so with the mutex locked and
         will return immediately if (and only if) the function provided to test the condition returns <kbd class="calibre17">true</kbd>. When the waiting thread reacquires the mutex and checks the condition, if it isn’t in direct response to a notification
         from another thread, it’s called a <i class="calibre6">spurious wake</i>. Because the number and frequency of any such spurious wakes are by definition indeterminate, it isn’t advisable to use a
         function with side effects for the condition check. If you do so, you must be prepared for the side effects to occur multiple
         times.
      </p>
      
      <p class="noind">Fundamentally, <kbd class="calibre17">std::condition_variable::wait</kbd> is <i class="calibre6">an optimization over a busy-wait</i>. Indeed, a conforming (though less than ideal) implementation technique is just a simple loop:
      </p>
      
      <pre id="PLd0e8267" class="calibre5">template&lt;typename Predicate&gt;
void minimal_wait(std::unique_lock&lt;std::mutex&gt;&amp; lk,Predicate pred){
    while(!pred()){
        lk.unlock();
        lk.lock();
    }
}</pre>
      
      <p class="noind">Your code must be prepared to work with such a minimal implementation of <kbd class="calibre17">wait()</kbd>, as well as an implementation that only wakes up if <kbd class="calibre17">notify_one()</kbd> or <kbd class="calibre17">notify_all()</kbd> is called.
      </p>
      
      <p class="noind">The flexibility to unlock a <kbd class="calibre17">std::unique_lock</kbd> isn’t just used for the call to <kbd class="calibre17">wait()</kbd>; it’s also used once you have the data to process but before processing it <b class="calibre24"><i class="calibre6">6</i></b>. Processing data can potentially be a time-consuming operation, and as you saw in <a href="kindle_split_013.html#ch03" class="calibre4">chapter 3</a>, it’s a bad idea to hold a lock on a mutex for longer than necessary.
      </p>
      
      <p class="noind">Using a queue to transfer data between threads, as in <a href="#ch04ex01" class="calibre4">listing 4.1</a>, is a common scenario. Done well, the synchronization can be limited to the queue itself, which greatly reduces the possible
         number of synchronization problems and race conditions. In view of this, let’s now work on extracting a generic thread-safe
         queue from <a href="#ch04ex01" class="calibre4">listing 4.1</a>.
      </p>
      
      
      
      <h4 id="ch04lev2sec2" class="calibre23">4.1.2. <a id="ch04lev2sec2__title" class="calibre4"></a>Building a thread-safe queue with condition variables
      </h4>
      
      <p class="noind">If you’re going to be designing a generic queue, it’s worth spending a few minutes thinking about the operations that are
         likely to be required, as you did with the <a id="iddle1060" class="calibre4"></a><a id="iddle1314" class="calibre4"></a><a id="iddle1395" class="calibre4"></a><a id="iddle1780" class="calibre4"></a><a id="iddle1819" class="calibre4"></a>thread-safe stack back in <a href="kindle_split_013.html#ch03lev2sec5" class="calibre4">section 3.2.3</a>. Let’s look at the C++ Standard Library for inspiration, in the form of the <kbd class="calibre17">std::queue&lt;&gt;</kbd> container adaptor shown in the following listing.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex02">Listing 4.2. <a id="ch04ex02__title" class="calibre4"></a><kbd class="calibre17">std::queue</kbd> interface
      </h5>
      <pre id="PLd0e8360" class="calibre5">template &lt;class T, class Container = std::deque&lt;T&gt; &gt;
class queue {
public:
    explicit queue(const Container&amp;);
    explicit queue(Container&amp;&amp; = Container());
    template &lt;class Alloc&gt; explicit queue(const Alloc&amp;);
    template &lt;class Alloc&gt; queue(const Container&amp;, const Alloc&amp;);
    template &lt;class Alloc&gt; queue(Container&amp;&amp;, const Alloc&amp;);
    template &lt;class Alloc&gt; queue(queue&amp;&amp;, const Alloc&amp;);
    void swap(queue&amp; q);
    bool empty() const;
    size_type size() const;
    T&amp; front();
    const T&amp; front() const;
    T&amp; back();
    const T&amp; back() const;
    void push(const T&amp; x);
    void push(T&amp;&amp; x);
    void pop();
    template &lt;class... Args&gt; void emplace(Args&amp;&amp;... args);
};</pre>
      
      <p class="noind">If you ignore the construction, assignment, and swap operations, you’re left with three groups of operations: those that query
         the state of the whole queue (<kbd class="calibre17">empty()</kbd> and <kbd class="calibre17">size()</kbd>), those that query the elements of the queue (<kbd class="calibre17">front()</kbd> and <kbd class="calibre17">back()</kbd>), and those that modify the queue (<kbd class="calibre17">push()</kbd>, <kbd class="calibre17">pop()</kbd> and <kbd class="calibre17">emplace()</kbd>). This is the same as you had back in <a href="kindle_split_013.html#ch03lev2sec5" class="calibre4">section 3.2.3</a> for the stack, and therefore you have the same issues regarding race conditions inherent in the interface. Consequently,
         you need to combine <kbd class="calibre17">front()</kbd> and <kbd class="calibre17">pop()</kbd> into a single function call, much as you combined <kbd class="calibre17">top()</kbd> and <kbd class="calibre17">pop()</kbd> for the stack. The code from <a href="#ch04ex01" class="calibre4">listing 4.1</a> adds a new nuance, though: when using a queue to pass data between threads, the receiving thread often needs to wait for
         the data. Let’s provide two variants on <kbd class="calibre17">pop()</kbd>: <kbd class="calibre17">try_pop()</kbd>, which tries to pop the value from the queue but always returns immediately (with an indication of failure) even if there
         wasn’t a value to retrieve; and <kbd class="calibre17">wait_and_pop()</kbd>, which waits until there’s a value to retrieve. If you take your lead for the signatures from the stack example, your interface
         looks like the following.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex03">Listing 4.3. <a id="ch04ex03__title" class="calibre4"></a>The interface of your <kbd class="calibre17">threadsafe_queue</kbd></h5>
      <pre id="PLd0e8424" class="calibre5">#include &lt;memory&gt;                                 <b class="calibre24"><i class="calibre6">1</i></b>
template&lt;typename T&gt;
class threadsafe_queue
{
public:
    threadsafe_queue();
    threadsafe_queue(const threadsafe_queue&amp;);
    threadsafe_queue&amp; operator=(
        const threadsafe_queue&amp;) = delete;       <b class="calibre24"><i class="calibre6">2</i></b>
    void push(T new_value);
    bool try_pop(T&amp; value);                      <b class="calibre24"><i class="calibre6">3</i></b>
    std::shared_ptr&lt;T&gt; try_pop();                <b class="calibre24"><i class="calibre6">4</i></b>
    void wait_and_pop(T&amp; value);
    std::shared_ptr&lt;T&gt; wait_and_pop();
    bool empty() const;
};</pre>
      
      <p class="calibre19"></p>
      <ul class="calibre21">
         
         <li class="calibre22"><a id="iddle2580" class="calibre4"></a><a id="iddle2633" class="calibre4"></a><b class="calibre24"><i class="calibre6">1</i> For std::shared_ptr</b></li>
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">2</i> Disallow assignment for simplicity.</b></li>
         
      </ul>
      
      <p class="noind">As you did for the stack, you’ve cut down on the constructors and eliminated assignment in order to simplify the code. You’ve
         also provided two versions of both <kbd class="calibre17">try_pop()</kbd> and <kbd class="calibre17">wait_for_pop()</kbd>, as before. The first overload of <kbd class="calibre17">try_pop()</kbd> <b class="calibre24"><i class="calibre6">3</i></b> stores the retrieved value in the referenced variable, so it can use the return value for status; it returns <kbd class="calibre17">true</kbd> if it retrieved a value and <kbd class="calibre17">false</kbd> otherwise (see <a href="kindle_split_022.html#app01lev1sec2" class="calibre4">section A.2</a>). The second overload <b class="calibre24"><i class="calibre6">4</i></b> can’t do this, because it returns the retrieved value directly. But the returned pointer can be set to <kbd class="calibre17">NULL</kbd> if there’s no value to retrieve.
      </p>
      
      <p class="noind">So, how does all this relate to <a href="#ch04ex01" class="calibre4">listing 4.1</a>? Well, you can extract the code for <kbd class="calibre17">push()</kbd> and <kbd class="calibre17">wait_and_pop()</kbd> from there, as shown in the next listing.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex04">Listing 4.4. <a id="ch04ex04__title" class="calibre4"></a>Extracting <kbd class="calibre17">push()</kbd> and <kbd class="calibre17">wait_and_pop()</kbd> from <a href="#ch04ex01" class="calibre4">listing 4.1</a></h5>
      <pre id="PLd0e8532" class="calibre5">#include &lt;queue&gt;
#include &lt;mutex&gt;
#include &lt;condition_variable&gt;
template&lt;typename T&gt;
class threadsafe_queue
{
private:
    std::mutex mut;
    std::queue&lt;T&gt; data_queue;
    std::condition_variable data_cond;
public:
    void push(T new_value)
    {
        std::lock_guard&lt;std::mutex&gt; lk(mut);
        data_queue.push(new_value);
        data_cond.notify_one();
    }
    void wait_and_pop(T&amp; value)
    {
        std::unique_lock&lt;std::mutex&gt; lk(mut);
        data_cond.wait(lk,[this]{return !data_queue.empty();});
        value=data_queue.front();
        data_queue.pop();
    }
};
threadsafe_queue&lt;data_chunk&gt; data_queue;      <b class="calibre24"><i class="calibre6">1</i></b>
void data_preparation_thread()
{
    while(more_data_to_prepare())
    {
        data_chunk const data=prepare_data();
        data_queue.push(data);                <b class="calibre24"><i class="calibre6">2</i></b>
    }
}
void data_processing_thread()
{
    while(true)
    {
        data_chunk data;
        data_queue.wait_and_pop(data);        <b class="calibre24"><i class="calibre6">3</i></b>
        process(data);
        if(is_last_chunk(data))
            break;
    }
}</pre>
      
      <p class="noind">The mutex and condition variable are now contained within the <kbd class="calibre17">threadsafe_queue</kbd> instance, so separate variables are no longer required <b class="calibre24"><i class="calibre6">1</i></b>, and no external synchronization is required for the call to <kbd class="calibre17">push()</kbd> <b class="calibre24"><i class="calibre6">2</i></b>. Also, <kbd class="calibre17">wait_and_pop()</kbd> takes care of the condition variable wait <b class="calibre24"><i class="calibre6">3</i></b>.
      </p>
      
      <p class="noind">The other overload of <kbd class="calibre17">wait_and_pop()</kbd> is now trivial to write, and the remaining functions can be copied almost verbatim from the stack example in <a href="kindle_split_013.html#ch03ex05" class="calibre4">listing 3.5</a>. The final queue implementation is shown here.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex05">Listing 4.5. <a id="ch04ex05__title" class="calibre4"></a>Full class definition of a thread-safe queue using condition variables
      </h5>
      <pre id="PLd0e8590" class="calibre5">#include &lt;queue&gt;
#include &lt;memory&gt;
#include &lt;mutex&gt;
#include &lt;condition_variable&gt;
template&lt;typename T&gt;
class threadsafe_queue
{
private:
    mutable std::mutex mut;           <b class="calibre24"><i class="calibre6">1</i></b>
    std::queue&lt;T&gt; data_queue;
    std::condition_variable data_cond;
public:
    threadsafe_queue()
    {}
    threadsafe_queue(threadsafe_queue const&amp; other)
    {
        std::lock_guard&lt;std::mutex&gt; lk(other.mut);
        data_queue=other.data_queue;
    }
    void push(T new_value)
    {
        std::lock_guard&lt;std::mutex&gt; lk(mut);
        data_queue.push(new_value);
        data_cond.notify_one();
    }
    void wait_and_pop(T&amp; value)
    {
        std::unique_lock&lt;std::mutex&gt; lk(mut);
        data_cond.wait(lk,[this]{return !data_queue.empty();});
        value=data_queue.front();
        data_queue.pop();
    }
    std::shared_ptr&lt;T&gt; wait_and_pop()
    {
        std::unique_lock&lt;std::mutex&gt; lk(mut);
        data_cond.wait(lk,[this]{return !data_queue.empty();});
        std::shared_ptr&lt;T&gt; res(std::make_shared&lt;T&gt;(data_queue.front()));
        data_queue.pop();
        return res;
    }
    bool try_pop(T&amp; value)
    {
        std::lock_guard&lt;std::mutex&gt; lk(mut);
        if(data_queue.empty())
            return false;
        value=data_queue.front();
        data_queue.pop();
        return true;
    }
    std::shared_ptr&lt;T&gt; try_pop()
    {
        std::lock_guard&lt;std::mutex&gt; lk(mut);
        if(data_queue.empty())
            return std::shared_ptr&lt;T&gt;();
        std::shared_ptr&lt;T&gt; res(std::make_shared&lt;T&gt;(data_queue.front()));
        data_queue.pop();
        return res;
    }
    bool empty() const
    {
        std::lock_guard&lt;std::mutex&gt; lk(mut);
        return data_queue.empty();
    }
};</pre>
      
      <p class="calibre19"></p>
      <ul class="calibre21">
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">1</i> The mutex must be mutable.</b></li>
         
      </ul>
      
      <p class="noind">Even though <kbd class="calibre17">empty()</kbd> is a <kbd class="calibre17">const</kbd> member function, and the <kbd class="calibre17">other</kbd> parameter to the copy constructor is a <kbd class="calibre17">const</kbd> reference, other threads may have non-<kbd class="calibre17">const</kbd> references to the object, and may be calling mutating member functions, so you still need to lock the mutex. Since locking
         a mutex is a mutating operation, the mutex object must be marked <kbd class="calibre17">mutable</kbd> <b class="calibre24"><i class="calibre6">1</i></b> so it can be locked in <kbd class="calibre17">empty()</kbd> and in the copy constructor.
      </p>
      
      <p class="noind"><a id="iddle1145" class="calibre4"></a><a id="iddle1330" class="calibre4"></a><a id="iddle1416" class="calibre4"></a><a id="iddle1697" class="calibre4"></a><a id="iddle1699" class="calibre4"></a><a id="iddle1915" class="calibre4"></a><a id="iddle2434" class="calibre4"></a><a id="iddle2599" class="calibre4"></a><a id="iddle2621" class="calibre4"></a><a id="iddle2645" class="calibre4"></a>Condition variables are also useful where there’s more than one thread waiting for the same event. If the threads are being
         used to divide the workload, and thus only one thread should respond to a notification, exactly the same structure as shown
         in <a href="#ch04ex01" class="calibre4">listing 4.1</a> can be used; just run multiple instances of the data-processing thread. When new data is ready, the call to <kbd class="calibre17">notify_one()</kbd> will trigger one of the threads currently executing <kbd class="calibre17">wait()</kbd> to check its condition and return from <kbd class="calibre17">wait()</kbd> (because you’ve just added an item to the <kbd class="calibre17">data_queue</kbd>). There’s no guarantee of which thread will be notified or even if there’s a thread waiting to be notified; all the processing
         threads might still be processing data.
      </p>
      
      <p class="noind">Another possibility is that several threads are waiting for the same event, and all of them need to respond. This can happen
         where shared data is being initialized, and the processing threads can all use the same data but need to wait for it to be
         initialized (although there are potentially better mechanisms for this, such as <kbd class="calibre17">std::call_once</kbd>; see <a href="kindle_split_013.html#ch03lev2sec11" class="calibre4">section 3.3.1</a> in <a href="kindle_split_013.html#ch03" class="calibre4">chapter 3</a> for a discussion of the options), or where the threads need to wait for an update to shared data, such as a periodic reinitialization.
         In these cases, the thread preparing the data can call the <kbd class="calibre17">notify_all()</kbd> member function on the condition variable rather than <kbd class="calibre17">notify_one()</kbd>. As the name suggests, this causes <i class="calibre6">all</i> the threads currently executing <kbd class="calibre17">wait()</kbd> to check the condition they’re waiting for.
      </p>
      
      <p class="noind">If the waiting thread is going to wait only once, so when the condition is <kbd class="calibre17">true</kbd> it will never wait on this condition variable again, a condition variable might not be the best choice of synchronization
         mechanisms. This is especially true if the condition being waited for is the availability of a particular piece of data. In
         this scenario, a <i class="calibre6">future</i> might be more appropriate.
      </p>
      
      
      
      
      <h3 id="ch04lev1sec2" class="chapter"><a id="ch04lev1sec2__title" class="calibre3"></a>4.2. Waiting for one-off events with futures
      </h3>
      
      <p class="noind">Suppose you’re going on vacation abroad by plane. Once you get to the airport and clear the various check-in procedures, you
         still have to wait for notification that your flight is ready for boarding, possibly for several hours. Yes, you might be
         able to find some means of passing the time, such as reading a book, surfing the internet, or eating in an overpriced airport
         café, but fundamentally you’re just waiting for one thing: the signal that it’s time to get on the plane. Not only that, but
         a given flight goes only once; the next time you’re going on vacation, you’ll be waiting for a different flight.
      </p>
      
      <p class="noind">The C++ Standard Library models this sort of one-off event with something called a <i class="calibre6">future</i>. If a thread needs to wait for a specific one-off event, it somehow obtains a future representing that event. The thread
         can then periodically wait on the future for short periods of time to see if the event has occurred (check the departures
         board) while performing some other task (eating in the overpriced café) between checks. Alternatively, it can do another task
         until it needs the event to have happened before it can proceed and then just wait for the future to become <i class="calibre6">ready</i>. A future may have data associated with it (such as which gate your flight is boarding at), or it may not. Once an event
         has happened (and the future has become <i class="calibre6">ready</i>), the future can’t be reset.
      </p>
      
      <p class="noind"><a id="iddle1024" class="calibre4"></a><a id="iddle1062" class="calibre4"></a><a id="iddle1333" class="calibre4"></a><a id="iddle1418" class="calibre4"></a><a id="iddle1944" class="calibre4"></a><a id="iddle2448" class="calibre4"></a><a id="iddle2610" class="calibre4"></a><a id="iddle2648" class="calibre4"></a>There are two sorts of futures in the C++ Standard Library, implemented as two class templates declared in the <kbd class="calibre17">&lt;future&gt;</kbd> library header: <i class="calibre6">unique futures</i> (<kbd class="calibre17">std::future&lt;&gt;</kbd>) and <i class="calibre6">shared futures</i> (<kbd class="calibre17">std::shared_future&lt;&gt;</kbd>). These are modeled after <kbd class="calibre17">std::unique_ptr</kbd> and <kbd class="calibre17">std::shared_ptr</kbd>. An instance of <kbd class="calibre17">std::future</kbd> is the one and only instance that refers to its associated event, whereas multiple instances of <kbd class="calibre17">std::shared_future</kbd> may refer to the same event. In the latter case, all the instances will become <i class="calibre6">ready</i> at the same time, and they may all access any data associated with the event. This associated data is the reason these are
         templates; just like <kbd class="calibre17">std::unique_ptr</kbd> and <kbd class="calibre17">std::shared_ptr</kbd>, the template parameter is the type of the associated data. The <kbd class="calibre17">std:future&lt;void&gt;</kbd> and <kbd class="calibre17">std::shared_future&lt;void&gt;</kbd> template specializations should be used where there’s no associated data. Although futures are used to communicate between
         threads, the future objects themselves don’t provide synchronized accesses. If multiple threads need to access a single future
         object, they must protect access via a mutex or other synchronization mechanism, as described in <a href="kindle_split_013.html#ch03" class="calibre4">chapter 3</a>. But as you’ll see in <a href="#ch04lev2sec7" class="calibre4">section 4.2.5</a>, multiple threads may each access their own copy of <kbd class="calibre17">std::shared_future&lt;&gt;</kbd> without further synchronization, even if they all refer to the same asynchronous result.
      </p>
      
      <p class="noind">The Concurrency TS provides extended versions of these class templates in the <kbd class="calibre17">std::experimental</kbd> namespace: <kbd class="calibre17">std::experimental::future&lt;&gt;</kbd> and <kbd class="calibre17">std::experimental::shared_future&lt;&gt;</kbd>. These behave identically to their counterparts in the <kbd class="calibre17">std</kbd> namespace, but they have additional member functions to provide additional facilities. It is important to note that the name
         <kbd class="calibre17">std::experimental</kbd> does not imply anything about the quality of the code (I would hope that the implementation will be the same quality as everything
         else shipped from your library vendor), but highlights that these are non-standard classes and functions, and therefore may
         not have exactly the same syntax and semantics if and when they are finally adopted into a future C++ Standard. To use these
         facilities, you must include the <kbd class="calibre17">&lt;experimental/future&gt;</kbd> header.
      </p>
      
      <p class="noind">The most basic of one-off events is the result of a calculation that has been run in the background. Back in <a href="kindle_split_012.html#ch02" class="calibre4">chapter 2</a> you saw that <kbd class="calibre17">std::thread</kbd> doesn’t provide an easy means of returning a value from such a task, and I promised that this would be addressed in <a href="#ch04" class="calibre4">chapter 4</a> with futures—now it’s time to see how.
      </p>
      
      
      <h4 id="ch04lev2sec3" class="calibre23">4.2.1. <a id="ch04lev2sec3__title" class="calibre4"></a>Returning values from background tasks
      </h4>
      
      <p class="noind">Suppose you have a long-running calculation that you expect will eventually yield a useful result but for which you don’t
         currently need the value. Maybe you’ve found a way to determine the answer to Life, the Universe, and Everything, to pinch
         an example from Douglas Adams.<sup class="calibre18">[<a href="#ch04fn01" class="calibre4">1</a>]</sup> You could start a new thread to perform the calculation, but that means you have to take care of transferring the result
         back, because <kbd class="calibre17">std::thread</kbd> doesn’t provide a direct mechanism for doing so. This is where the <kbd class="calibre17">std::async</kbd> function template (also declared in the <kbd class="calibre17">&lt;future&gt;</kbd> header) comes in.
      </p>
      <blockquote class="smaller">
         <p class="calibre19"><sup class="calibre20"><a id="ch04fn01" class="calibre4">1</a></sup> 
            </p><div class="calibre15">In <i class="calibre6">The Hitchhiker’s Guide to the Galaxy</i>, the computer Deep Thought is built to determine “the answer to Life, the Universe and Everything.” The answer is 42.
            </div>
         <p class="calibre19"></p>
      </blockquote>
      
      <p class="noind"><a id="iddle1880" class="calibre4"></a>You use <kbd class="calibre17">std::async</kbd> to start an <i class="calibre6">asynchronous task</i> for which you don’t need the result right away. Rather than giving you a <kbd class="calibre17">std::thread</kbd> object to wait on, <kbd class="calibre17">std::async</kbd> returns a <kbd class="calibre17">std::future</kbd> object, which will eventually hold the return value of the function. When you need the value, you just call <kbd class="calibre17">get()</kbd> on the future, and the thread blocks until the future is <i class="calibre6">ready</i> and then returns the value. The following listing shows a simple example.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex06">Listing 4.6. <a id="ch04ex06__title" class="calibre4"></a>Using <kbd class="calibre17">std::future</kbd> to get the return value of an asynchronous task
      </h5>
      <pre id="PLd0e9005" class="calibre5">#include &lt;future&gt;
#include &lt;iostream&gt;
int find_the_answer_to_ltuae();
void do_other_stuff();
int main()
{
    std::future&lt;int&gt; the_answer=std::async(find_the_answer_to_ltuae);
    do_other_stuff();
    std::cout&lt;&lt;"The answer is "&lt;&lt;the_answer.get()&lt;&lt;std::endl;
}</pre>
      
      <p class="noind"><kbd class="calibre17">std::async</kbd> allows you to pass additional arguments to the function by adding extra arguments to the call, in the same way that <kbd class="calibre17">std::thread</kbd> does. If the first argument is a pointer to a member function, the second argument provides the object on which to apply
         the member function (either directly, or via a pointer, or wrapped in <kbd class="calibre17">std::ref</kbd>), and the remaining arguments are passed as arguments to the member function. Otherwise, the second and subsequent arguments
         are passed as arguments to the function or callable object specified as the first argument. Just as with <kbd class="calibre17">std::thread</kbd>, if the arguments are rvalues, the copies are created by <i class="calibre6">moving</i> the originals. This allows the use of move-only types as both the function object and the arguments. See the following listing.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex07">Listing 4.7. <a id="ch04ex07__title" class="calibre4"></a>Passing arguments to a function with <kbd class="calibre17">std::async</kbd></h5>
      <pre id="PLd0e9035" class="calibre5">#include &lt;string&gt;
#include &lt;future&gt;
struct X
{
    void foo(int,std::string const&amp;);
    std::string bar(std::string const&amp;);
};
X x;
auto f1=std::async(&amp;X::foo,&amp;x,42,"hello");       <b class="calibre24"><i class="calibre6">1</i></b>
auto f2=std::async(&amp;X::bar,x,"goodbye");         <b class="calibre24"><i class="calibre6">2</i></b>
struct Y
{
    double operator()(double);
};
Y y;
auto f3=std::async(Y(),3.141);                   <b class="calibre24"><i class="calibre6">3</i></b>
auto f4=std::async(std::ref(y),2.718);           <b class="calibre24"><i class="calibre6">4</i></b>
X baz(X&amp;);
std::async(baz,std::ref(x));                     <b class="calibre24"><i class="calibre6">5</i></b>
class move_only
{
public:
    move_only();
    move_only(move_only&amp;&amp;)
    move_only(move_only const&amp;) = delete;
    move_only&amp; operator=(move_only&amp;&amp;);
    move_only&amp; operator=(move_only const&amp;) = delete;
    void operator()();
};
auto f5=std::async(move_only());                 <b class="calibre24"><i class="calibre6">6</i></b></pre>
      
      <p class="calibre19"></p>
      <ul class="calibre21">
         
         <li class="calibre22"><a id="iddle1331" class="calibre4"></a><a id="iddle1410" class="calibre4"></a><a id="iddle2165" class="calibre4"></a><a id="iddle2446" class="calibre4"></a><a id="iddle2619" class="calibre4"></a><a id="iddle2646" class="calibre4"></a><b class="calibre24"><i class="calibre6">1</i> Calls p-&gt;foo(42,“hello”) where p is &amp;x</b></li>
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">2</i> Calls tmpx.bar(“goodbye”) where tmpx is a copy of x</b></li>
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">3</i> Calls tmpy(3.141) where tmpy is move-constructed from Y()</b></li>
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">4</i> Calls y(2.718)</b></li>
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">5</i> Calls baz(x)</b></li>
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">6</i> Calls tmp() where tmp is constructed from std::move(move_only())</b></li>
         
      </ul>
      
      <p class="noind">By default, it’s up to the implementation whether <kbd class="calibre17">std::async</kbd> starts a new thread, or whether the task runs synchronously when the future is waited for. In most cases this is what you
         want, but you can specify which to use with an additional parameter to <kbd class="calibre17">std::async</kbd> before the function to call. This parameter is of the type <kbd class="calibre17">std::launch</kbd>, and can either be <kbd class="calibre17">std::launch::deferred</kbd> to indicate that the function call is to be deferred until either <kbd class="calibre17">wait()</kbd> or <kbd class="calibre17">get()</kbd> is called on the future, <kbd class="calibre17">std::launch::async</kbd> to indicate that the function must be run on its own thread, or <kbd class="calibre17">std::launch::deferred | std::launch::async</kbd> to indicate that the implementation may choose. This last option is the default. If the function call is deferred, it may
         never run. For example:
      </p>
      
      <pre id="PLd0e9186" class="calibre5">auto f6=std::async(std::launch::async,Y(),1.2);                 <b class="calibre24"><i class="calibre6">1</i></b>
auto f7=std::async(std::launch::deferred,baz,std::ref(x));      <b class="calibre24"><i class="calibre6">2</i></b>
auto f8=std::async(                                             <b class="calibre24"><i class="calibre6">3</i></b>
   std::launch::deferred | std::launch::async,
   baz,std::ref(x));
auto f9=std::async(baz,std::ref(x));                            <b class="calibre24"><i class="calibre6">3</i></b>
f7.wait();                                                      <b class="calibre24"><i class="calibre6">4</i></b></pre>
      
      <p class="calibre19"></p>
      <ul class="calibre21">
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">1</i> Run in new thread</b></li>
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">2</i> Run in wait() or get()</b></li>
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">3</i> Implementation chooses</b></li>
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">4</i> Invoke deferred function</b></li>
         
      </ul>
      
      <p class="noind">As you’ll see later in this chapter and again in <a href="kindle_split_018.html#ch08" class="calibre4">chapter 8</a>, using <kbd class="calibre17">std::async</kbd> makes it easy to divide algorithms into tasks that can be run concurrently. However, it’s not the only way to associate a
         <kbd class="calibre17">std::future</kbd> with a task; you can also do it by wrapping the task in an instance of the <kbd class="calibre17">std::packaged_task&lt;&gt;</kbd> class template or by writing code to <a id="iddle1428" class="calibre4"></a><a id="iddle2455" class="calibre4"></a><a id="iddle2525" class="calibre4"></a>explicitly set the values using the <kbd class="calibre17">std::promise&lt;&gt;</kbd> class template. <kbd class="calibre17">std::packaged_task</kbd> is a higher-level abstraction than <kbd class="calibre17">std::promise</kbd>, so I’ll start with that.
      </p>
      
      
      
      <h4 id="ch04lev2sec4" class="calibre23">4.2.2. <a id="ch04lev2sec4__title" class="calibre4"></a>Associating a task with a future
      </h4>
      
      <p class="noind"><kbd class="calibre17">std::packaged_task&lt;&gt;</kbd> ties a future to a function or callable object. When the <kbd class="calibre17">std:: packaged_task&lt;&gt;</kbd> object is invoked, it calls the associated function or callable object and makes the future <i class="calibre6">ready</i>, with the return value stored as the associated data. This can be used as a building block for thread pools (see <a href="kindle_split_019.html#ch09" class="calibre4">chapter 9</a>) or other task management schemes, such as running each task on its own thread, or running them all sequentially on a particular
         background thread. If a large operation can be divided into self-contained sub-tasks, each of these can be wrapped in a <kbd class="calibre17">std::packaged_task&lt;&gt;</kbd> instance, and then that instance passed to the task scheduler or thread pool. This abstracts out the details of the tasks;
         the scheduler just deals with <kbd class="calibre17">std::packaged_task&lt;&gt;</kbd> instances rather than individual functions.
      </p>
      
      <p class="noind">The template parameter for the <kbd class="calibre17">std::packaged_task&lt;&gt;</kbd> class template is a function signature, like <kbd class="calibre17">void()</kbd> for a function taking no parameters with no return value, or <kbd class="calibre17">int(std::string&amp;,double*)</kbd> for a function that takes a non-<kbd class="calibre17">const</kbd> reference to a <kbd class="calibre17">std::string</kbd> and a pointer to a <kbd class="calibre17">double</kbd> and returns an <kbd class="calibre17">int</kbd>. When you construct an instance of <kbd class="calibre17">std::packaged_task</kbd>, you must pass in a function or callable object that can accept the specified parameters and that returns a type that’s convertible
         to the specified return type. The types don’t have to match exactly; you can construct a <kbd class="calibre17">std::packaged_task&lt;double(double)&gt;</kbd> from a function that takes an <kbd class="calibre17">int</kbd> and returns a <kbd class="calibre17">float</kbd> because the types are implicitly convertible.
      </p>
      
      <p class="noind">The return type of the specified function signature identifies the type of the <kbd class="calibre17">std::future&lt;&gt;</kbd> returned from the <kbd class="calibre17">get_future()</kbd> member function, whereas the argument list of the function signature is used to specify the signature of the packaged task’s
         function call operator. For example, a partial class definition for <kbd class="calibre17">std::packaged_task &lt;std::string(std::vector&lt;char&gt;*,int)&gt;</kbd> would be as shown in the following listing.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex08">Listing 4.8. <a id="ch04ex08__title" class="calibre4"></a>Partial class definition for a specialization of <kbd class="calibre17">std::packaged_task&lt; &gt;</kbd></h5>
      <pre id="PLd0e9368" class="calibre5">template&lt;&gt;
class packaged_task&lt;std::string(std::vector&lt;char&gt;*,int)&gt;
{
public:
    template&lt;typename Callable&gt;
    explicit packaged_task(Callable&amp;&amp; f);
    std::future&lt;std::string&gt; get_future();
    void operator()(std::vector&lt;char&gt;*,int);
};</pre>
      
      <p class="noind">The <kbd class="calibre17">std::packaged_task</kbd> object is a callable object, and it can be wrapped in a <kbd class="calibre17">std::function</kbd> object, passed to a <kbd class="calibre17">std::thread</kbd> as the thread function, passed to another function that requires a callable object, or even invoked directly. When the <kbd class="calibre17">std::packaged_task</kbd> is invoked as a function object, the arguments supplied to the function call operator are passed on to the contained function,
         and the return value is stored as the asynchronous result in the <kbd class="calibre17">std::future</kbd> obtained from <kbd class="calibre17">get_future()</kbd>. You can thus wrap a task in a <kbd class="calibre17">std::packaged_task</kbd> and retrieve the future before passing the <kbd class="calibre17">std::packaged_task</kbd> object elsewhere to be invoked in due course. When you need the result, you can wait for the future to become ready. The
         following example shows this in action.
      </p>
      
      
      <h5 class="notetitle" id="ch04lev3sec1"><a id="ch04lev3sec1__title" class="calibre4"></a>Passing tasks between threads
      </h5>
      
      <p class="noind">Many GUI frameworks require that updates to the GUI be done from specific threads, so if another thread needs to update the
         GUI, it must send a message to the right thread in order to do so. <kbd class="calibre17">std:packaged_task</kbd> provides one way of doing this without requiring a custom message for each and every GUI-related activity, as shown here.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex09">Listing 4.9. <a id="ch04ex09__title" class="calibre4"></a>Running code on a GUI thread using <kbd class="calibre17">std::packaged_task</kbd></h5>
      <pre id="PLd0e9418" class="calibre5">#include &lt;deque&gt;
#include &lt;mutex&gt;
#include &lt;future&gt;
#include &lt;thread&gt;
#include &lt;utility&gt;
std::mutex m;
std::deque&lt;std::packaged_task&lt;void()&gt; &gt; tasks;
bool gui_shutdown_message_received();
void get_and_process_gui_message();
void gui_thread()                               <b class="calibre24"><i class="calibre6">1</i></b>
{
    while(!gui_shutdown_message_received())     <b class="calibre24"><i class="calibre6">2</i></b>
    {
        get_and_process_gui_message();          <b class="calibre24"><i class="calibre6">3</i></b>
        std::packaged_task&lt;void()&gt; task;
        {
            std::lock_guard&lt;std::mutex&gt; lk(m);
            if(tasks.empty())                   <b class="calibre24"><i class="calibre6">4</i></b>
                continue;
            task=std::move(tasks.front());      <b class="calibre24"><i class="calibre6">5</i></b>
            tasks.pop_front();
        }
        task();                                 <b class="calibre24"><i class="calibre6">6</i></b>
    }
}
std::thread gui_bg_thread(gui_thread);
template&lt;typename Func&gt;
std::future&lt;void&gt; post_task_for_gui_thread(Func f)
{
    std::packaged_task&lt;void()&gt; task(f);         <b class="calibre24"><i class="calibre6">7</i></b>
    std::future&lt;void&gt; res=task.get_future();    <b class="calibre24"><i class="calibre6">8</i></b>
    std::lock_guard&lt;std::mutex&gt; lk(m);
    tasks.push_back(std::move(task));           <b class="calibre24"><i class="calibre6">9</i></b>
    return res;                                 <b class="calibre24"><i class="calibre6">10</i></b>
}</pre>
      
      <p class="noind"><a id="iddle1332" class="calibre4"></a><a id="iddle1417" class="calibre4"></a><a id="iddle2222" class="calibre4"></a><a id="iddle2647" class="calibre4"></a>This code is simple: the GUI thread <b class="calibre24"><i class="calibre6">1</i></b> loops until a message has been received telling the GUI to shut down <b class="calibre24"><i class="calibre6">2</i></b>, repeatedly polling for GUI messages to handle <b class="calibre24"><i class="calibre6">3</i></b>, such as user clicks, and for tasks on the task queue. If there are no tasks on the queue <b class="calibre24"><i class="calibre6">4</i></b>, it loops again; otherwise, it extracts the task from the queue <b class="calibre24"><i class="calibre6">5</i></b>, releases the lock on the queue, and then runs the task <b class="calibre24"><i class="calibre6">6</i></b>. The future associated with the task will then be made ready when the task completes.
      </p>
      
      <p class="noind">Posting a task on the queue is equally simple: a new packaged task is created from the supplied function <b class="calibre24"><i class="calibre6">7</i></b>, the future is obtained from that task <b class="calibre24"><i class="calibre6">8</i></b> by calling the <kbd class="calibre17">get_future()</kbd> member function, and the task is put on the list <b class="calibre24"><i class="calibre6">9</i></b> before the future is returned to the caller <b class="calibre24"><i class="calibre6">10</i></b>. The code that posted the message to the GUI thread can then wait for the future if it needs to know that the task has been
         completed, or it can discard the future if it doesn’t need to know.
      </p>
      
      <p class="noind">This example uses <kbd class="calibre17">std::packaged_task&lt;void()&gt;</kbd> for the tasks, which wraps a function or other callable object that takes no parameters and returns <kbd class="calibre17">void</kbd> (if it returns anything else, the return value is discarded). This is the simplest possible task, but as you saw earlier,
         <kbd class="calibre17">std::packaged_task</kbd> can also be used in more complex situations—by specifying a different function signature as the template parameter, you can
         change the return type (and thus the type of data stored in the future’s associated state) and also the argument types of
         the function call operator. This example could easily be extended to allow for tasks that are to be run on the GUI thread
         to accept arguments and return a value in the <kbd class="calibre17">std::future</kbd> rather than just a completion indicator.
      </p>
      
      <p class="noind">What about those tasks that can’t be expressed as a simple function call or those tasks where the result may come from more
         than one place? These cases are dealt with by the third way of creating a future: using <kbd class="calibre17">std::promise</kbd> to set the value explicitly.
      </p>
      
      
      
      
      <h4 id="ch04lev2sec5" class="calibre23">4.2.3. <a id="ch04lev2sec5__title" class="calibre4"></a>Making (std::)promises
      </h4>
      
      <p class="noind">When you have an application that needs to handle a lot of network connections, it’s often tempting to handle each connection
         on a separate thread, because this can make the network communication easier to think about and easier to program. This works
         well for low numbers of connections (and thus low numbers of threads). Unfortunately, as the number of connections rises,
         this becomes less suitable; the large numbers of threads consequently consume large amounts of OS resources and potentially
         cause a lot of context switching (when the number of threads exceeds the available hardware concurrency), impacting performance.
         In extreme cases, the OS may run out of resources for running new threads before its capacity for network connections is exhausted.
         In applications with large numbers of network connections, it’s therefore common to have a small number of threads (possibly
         only one) handling the connections, with each thread dealing with multiple connections at once.
      </p>
      
      <p class="noind">Consider one of these threads handling the connections. Data packets will come in from the various connections being handled
         in essentially random order, and likewise, data packets will be queued to be sent in random order. In many cases, other parts
         of the application will be waiting either for data to be successfully sent or for a new batch of data to be successfully received
         via a specific network connection.
      </p>
      
      <p class="noind"><a id="iddle1298" class="calibre4"></a><a id="iddle1805" class="calibre4"></a><kbd class="calibre17">std::promise&lt;T&gt;</kbd> provides a means of setting a value (of type <kbd class="calibre17">T</kbd>) that can later be read through an associated <kbd class="calibre17">std::future&lt;T&gt;</kbd> object. A <kbd class="calibre17">std::promise</kbd>/<kbd class="calibre17">std::future</kbd> pair would provide one possible mechanism for this facility; the waiting thread could block on the future, while the thread
         providing the data could use the promise half of the pairing to set the associated value and make the future <i class="calibre6">ready</i>.
      </p>
      
      <p class="noind">You can obtain the <kbd class="calibre17">std::future</kbd> object associated with a given <kbd class="calibre17">std::promise</kbd> by calling the <kbd class="calibre17">get_future()</kbd> member function, just like with <kbd class="calibre17">std::packaged_task</kbd>. When the value of the promise is set (using the <kbd class="calibre17">set_value()</kbd> member function), the future becomes <i class="calibre6">ready</i> and can be used to retrieve the stored value. If you destroy the <kbd class="calibre17">std::promise</kbd> without setting a value, an exception is stored instead. <a href="#ch04lev2sec6" class="calibre4">Section 4.2.4</a> describes how exceptions are transferred across threads.
      </p>
      
      <p class="noind"><a href="#ch04ex10" class="calibre4">Listing 4.10</a> shows some example code for a thread that’s processing connections as just described. In this example, you use a <kbd class="calibre17">std::promise&lt;bool&gt;</kbd>/<kbd class="calibre17">std::future&lt;bool&gt;</kbd> pair to identify the successful transmission of a block of outgoing data; the value associated with the future is a simple
         success/failure flag. For incoming packets, the data associated with the future is the payload of the data packet.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex10">Listing 4.10. <a id="ch04ex10__title" class="calibre4"></a>Handling multiple connections from a single thread using promises
      </h5>
      <pre id="PLd0e9658" class="calibre5">#include &lt;future&gt;
void process_connections(connection_set&amp; connections)
{
    while(!done(connections))                            <b class="calibre24"><i class="calibre6">1</i></b>
    {
        for(connection_iterator                          <b class="calibre24"><i class="calibre6">2</i></b>
                connection=connections.begin(),end=connections.end();
            connection!=end;
            ++connection)
        {
            if(connection-&gt;has_incoming_data())          <b class="calibre24"><i class="calibre6">3</i></b>
            {
                data_packet data=connection-&gt;incoming();
                std::promise&lt;payload_type&gt;&amp; p=
                    connection-&gt;get_promise(data.id);    <b class="calibre24"><i class="calibre6">4</i></b>
                p.set_value(data.payload);
            }
            if(connection-&gt;has_outgoing_data())          <b class="calibre24"><i class="calibre6">5</i></b>
            {
                outgoing_packet data=
                    connection-&gt;top_of_outgoing_queue();
                connection-&gt;send(data.payload);
                data.promise.set_value(true);            <b class="calibre24"><i class="calibre6">6</i></b>
            }
        }
    }
}</pre>
      
      <p class="noind">The <kbd class="calibre17">process_connections()</kbd> function loops until <kbd class="calibre17">done()</kbd> returns <kbd class="calibre17">true</kbd> <b class="calibre24"><i class="calibre6">1</i></b>. Every time it goes through the loop, it checks each connection in turn <b class="calibre24"><i class="calibre6">2</i></b>, retrieving incoming data if there is any <b class="calibre24"><i class="calibre6">3</i></b> or sending any queued outgoing data <b class="calibre24"><i class="calibre6">5</i></b>. This assumes that an incoming <a id="iddle1334" class="calibre4"></a><a id="iddle1340" class="calibre4"></a><a id="iddle1367" class="calibre4"></a><a id="iddle1419" class="calibre4"></a><a id="iddle1897" class="calibre4"></a><a id="iddle1900" class="calibre4"></a><a id="iddle1937" class="calibre4"></a><a id="iddle2649" class="calibre4"></a>packet has an ID and a payload with the data in it. The ID is mapped to a <kbd class="calibre17">std::promise</kbd> (perhaps by a lookup in an associative container) <b class="calibre24"><i class="calibre6">4</i></b>, and the value is set to the packet’s payload. For outgoing packets, the packet is retrieved from the outgoing queue and
         sent through the connection. Once the send has completed, the promise associated with the outgoing data is set to <kbd class="calibre17">true</kbd> to indicate successful transmission <b class="calibre24"><i class="calibre6">6</i></b>. Whether this maps nicely to the network protocol depends on the protocol; this promise/future style structure may not work
         for a particular scenario, although it does have a similar structure to the asynchronous I/O support of some OSes.
      </p>
      
      <p class="noind">All the code up to now has completely disregarded exceptions. Although it might be nice to imagine a world in which everything
         worked all the time, this isn’t the case. Sometimes disks fill up, sometimes what you’re looking for just isn’t there, sometimes
         the network fails, and sometimes the database goes down. If you were performing the operation in the thread that needed the
         result, the code could just report an error with an exception, so it would be unnecessarily restrictive to require that everything
         goes well just because you wanted to use <kbd class="calibre17">std::packaged_task</kbd> or <kbd class="calibre17">std::promise</kbd>. The C++ Standard Library therefore provides a clean way to deal with exceptions in such a scenario and allows them to be
         saved as part of the associated result.
      </p>
      
      
      
      <h4 id="ch04lev2sec6" class="calibre23">4.2.4. <a id="ch04lev2sec6__title" class="calibre4"></a>Saving an exception for the future
      </h4>
      
      <p class="noind">Consider the following short snippet of code. If you pass in <kbd class="calibre17">-1</kbd> to the <kbd class="calibre17">square_root()</kbd> function, it throws an exception, and this gets seen by the caller:
      </p>
      
      <pre id="PLd0e9814" class="calibre5">double square_root(double x)
{
    if(x&lt;0)
    {
        throw std::out_of_range("x&lt;0");
    }
    return sqrt(x);
}</pre>
      
      <p class="noind">Now suppose that instead of just invoking <kbd class="calibre17">square_root()</kbd> from the current thread
      </p>
      
      <pre id="PLd0e9826" class="calibre5">double y=square_root(-1);</pre>
      
      <p class="noind">you run the call as an asynchronous call:</p>
      
      <pre id="PLd0e9835" class="calibre5">std::future&lt;double&gt; f=std::async(square_root,-1);
double y=f.get();</pre>
      
      <p class="noind">It would be ideal if the behavior was exactly the same; just as <kbd class="calibre17">y</kbd> gets the result of the function call in either case, it would be great if the thread that called <kbd class="calibre17">f.get()</kbd> could see the exception too, just as it would in the single-threaded case.
      </p>
      
      <p class="noind">Well, that’s exactly what happens: if the function call invoked as part of <kbd class="calibre17">std::async</kbd> throws an exception, that exception is stored in the future in place of a stored value, the future becomes <i class="calibre6">ready</i>, and a call to <kbd class="calibre17">get()</kbd> rethrows that stored exception. (Note: the standard leaves it unspecified whether it is the original exception object that’s
         rethrown <a id="iddle1250" class="calibre4"></a><a id="iddle1335" class="calibre4"></a><a id="iddle1420" class="calibre4"></a><a id="iddle2279" class="calibre4"></a><a id="iddle2523" class="calibre4"></a><a id="iddle2644" class="calibre4"></a>or a copy; different compilers and libraries make different choices on this matter.) The same happens if you wrap the function
         in a <kbd class="calibre17">std::packaged_task</kbd>—when the task is invoked, if the wrapped function throws an exception, that exception is stored in the future in place of
         the result, ready to be thrown on a call to <kbd class="calibre17">get()</kbd>.
      </p>
      
      <p class="noind">Naturally, <kbd class="calibre17">std::promise</kbd> provides the same facility, with an explicit function call. If you wish to store an exception rather than a value, you call
         the <kbd class="calibre17">set_exception()</kbd> member function rather than <kbd class="calibre17">set_value()</kbd>. This would typically be used in a <kbd class="calibre17">catch</kbd> block for an exception thrown as part of the algorithm, to populate the promise with that exception:
      </p>
      
      <pre id="PLd0e9939" class="calibre5">extern std::promise&lt;double&gt; some_promise;
try
{
    some_promise.set_value(calculate_value());
}
catch(...)
{
    some_promise.set_exception(std::current_exception());
}</pre>
      
      <p class="noind">This uses <kbd class="calibre17">std::current_exception()</kbd> to retrieve the thrown exception; the alternative here would be to use <kbd class="calibre17">std::make_exception_ptr()</kbd> to store a new exception directly without throwing:
      </p>
      
      <pre id="PLd0e9954" class="calibre5">some_promise.set_exception(std::make_exception_ptr(std::logic_error("foo ")));</pre>
      
      <p class="noind">This is much cleaner than using a <kbd class="calibre17">try</kbd>/<kbd class="calibre17">catch</kbd> block if the type of the exception is known, and it should be used in preference; not only does it simplify the code, but
         it also provides the compiler with greater opportunity to optimize the code.
      </p>
      
      <p class="noind">Another way to store an exception in a future is to destroy the <kbd class="calibre17">std::promise</kbd> or <kbd class="calibre17">std::packaged_task</kbd> associated with the future without calling either of the set functions on the promise or invoking the packaged task. In either
         case, the destructor of <kbd class="calibre17">std::promise</kbd> or <kbd class="calibre17">std::packaged_task</kbd> will store a <kbd class="calibre17">std::future_error</kbd> exception with an error code of <kbd class="calibre17">std::future_errc::broken_promise</kbd> in the associated state if the future isn’t already <i class="calibre6">ready</i>; by creating a future you make a promise to provide a value or exception, and by destroying the source of that value or exception
         without providing one, you break that promise. If the compiler didn’t store anything in the future in this case, waiting threads
         could potentially wait forever.
      </p>
      
      <p class="noind">Up until now, all the examples have used <kbd class="calibre17">std::future</kbd>. However, <kbd class="calibre17">std::future</kbd> has its limitations, not the least of which being that only one thread can wait for the result. If you need to wait for the
         same event from more than one thread, you need to use <kbd class="calibre17">std::shared_future</kbd> instead.
      </p>
      
      
      
      <h4 id="ch04lev2sec7" class="calibre23">4.2.5. <a id="ch04lev2sec7__title" class="calibre4"></a>Waiting from multiple threads
      </h4>
      
      <p class="noind">Although <kbd class="calibre17">std::future</kbd> handles all the synchronization necessary to transfer data from one thread to another, calls to the member functions of a
         particular <kbd class="calibre17">std::future</kbd> instance are not synchronized with each other. If you access a single <kbd class="calibre17">std::future</kbd> object from multiple threads without additional synchronization, you have a <i class="calibre6">data race</i> and undefined behavior. This is by design: <kbd class="calibre17">std::future</kbd> models unique ownership of the asynchronous result, and the one-shot nature of <kbd class="calibre17">get()</kbd> makes such concurrent access pointless anyway—only one thread can retrieve the value, because after the first call to <kbd class="calibre17">get()</kbd> there’s no value left to retrieve.
      </p>
      
      <p class="noind">If your fabulous design for your concurrent code requires that multiple threads can wait for the same event, don’t despair
         just yet; <kbd class="calibre17">std::shared_future</kbd> allows exactly that. Whereas <kbd class="calibre17">std::future</kbd> is only <i class="calibre6">moveable</i> (so ownership can be transferred between instances, but only one instance refers to a particular asynchronous result at a
         time), <kbd class="calibre17">std::shared_future</kbd> instances are <i class="calibre6">copyable</i> (so you can have multiple objects referring to the same associated state).
      </p>
      
      <p class="noind">Now, with <kbd class="calibre17">std::shared_future</kbd>, member functions on an individual object are still unsynchronized, so to avoid data races when accessing a single object
         from multiple threads, you must protect accesses with a lock. The preferred way to use it would be to pass a copy of the <kbd class="calibre17">shared_future</kbd> object to each thread, so each thread can access its own local <kbd class="calibre17">shared_future</kbd> object safely, as the internals are now correctly synchronized by the library. Accessing the shared asynchronous state from
         multiple threads is safe if each thread accesses that state through its own <kbd class="calibre17">std::shared_future</kbd> object. See <a href="#ch04fig01" class="calibre4">figure 4.1</a>.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04fig01">Figure 4.1. <a id="ch04fig01__title" class="calibre4"></a>Using multiple <kbd class="calibre17">std::shared_future</kbd> objects to avoid data races
      </h5>
      
      <p class="center1"><img alt="" src="04fig01_alt.jpg" class="calibre2"/></p>
      
      
      <p class="noind">One potential use of <kbd class="calibre17">std::shared_future</kbd> is for implementing parallel execution of something akin to a complex spreadsheet; each cell has a single final value, which
         may be used by the formulas in multiple other cells. The formulas for calculating the results of the dependent cells can then
         use <kbd class="calibre17">std::shared_future</kbd> to reference the first cell. If all the formulas for the individual cells are then executed in parallel, those tasks that
         can proceed to completion will do so, whereas those that depend on others will block until their dependencies are ready. This
         will allow the system to make maximum use of the available hardware concurrency.
      </p>
      
      <p class="noind">Instances of <kbd class="calibre17">std::shared_future</kbd> that reference some asynchronous state are constructed from instances of <kbd class="calibre17">std::future</kbd> that reference that state. Since <kbd class="calibre17">std::future</kbd> objects don’t share ownership of the asynchronous state with any other object, the ownership must be transferred into the
         <kbd class="calibre17">std::shared_future</kbd> using <kbd class="calibre17">std::move</kbd>, leaving <kbd class="calibre17">std::future</kbd> in an empty state, as if it were a default constructor:
      </p>
      
      <pre id="PLd0e10119" class="calibre5">std::promise&lt;int&gt; p;
std::future&lt;int&gt; f(p.get_future());
assert(f.valid());                            <b class="calibre24"><i class="calibre6">1</i></b>
std::shared_future&lt;int&gt; sf(std::move(f));
assert(!f.valid());                           <b class="calibre24"><i class="calibre6">2</i></b>
assert(sf.valid());                           <b class="calibre24"><i class="calibre6">3</i></b></pre>
      
      <p class="calibre19"></p>
      <ul class="calibre21">
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">1</i> The future f is valid.</b></li>
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">2</i> f is no longer valid.</b></li>
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">3</i> sf is now valid.</b></li>
         
      </ul>
      
      <p class="noind"><a id="iddle1429" class="calibre4"></a>Here, the future <kbd class="calibre17">f</kbd> is initially valid <b class="calibre24"><i class="calibre6">1</i></b> because it refers to the asynchronous state of the promise <kbd class="calibre17">p</kbd>, but after transferring the state to <kbd class="calibre17">sf</kbd>, <kbd class="calibre17">f</kbd> is no longer valid <b class="calibre24"><i class="calibre6">2</i></b>, whereas <kbd class="calibre17">sf</kbd> is <b class="calibre24"><i class="calibre6">3</i></b>.
      </p>
      
      <p class="noind">Just as with other movable objects, the transfer of ownership is implicit for rvalues, so you can construct a <kbd class="calibre17">std::shared_future</kbd> directly from the return value of the <kbd class="calibre17">get_future()</kbd> member function of a <kbd class="calibre17">std::promise</kbd> object, for example:
      </p>
      
      <pre id="PLd0e10208" class="calibre5">std::promise&lt;std::string&gt; p;
std::shared_future&lt;std::string&gt; sf(p.get_future());      <b class="calibre24"><i class="calibre6">1</i></b></pre>
      
      <p class="calibre19"></p>
      <ul class="calibre21">
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">1</i> Implicit transfer of ownership</b></li>
         
      </ul>
      
      <p class="noind"><a id="iddle1110" class="calibre4"></a><a id="iddle1146" class="calibre4"></a><a id="iddle1310" class="calibre4"></a><a id="iddle1701" class="calibre4"></a><a id="iddle2098" class="calibre4"></a><a id="iddle2435" class="calibre4"></a><a id="iddle2559" class="calibre4"></a><a id="iddle2560" class="calibre4"></a><a id="iddle2629" class="calibre4"></a><a id="iddle2654" class="calibre4"></a><a id="iddle2655" class="calibre4"></a><a id="iddle2659" class="calibre4"></a>Here, the transfer of ownership is implicit; <kbd class="calibre17">std::shared_future&lt;&gt;</kbd> is constructed from an rvalue of type <kbd class="calibre17">std::future&lt;std::string&gt;</kbd> <b class="calibre24"><i class="calibre6">1</i></b>.
      </p>
      
      <p class="noind"><kbd class="calibre17">std::future</kbd> also has an additional feature to facilitate the use of <kbd class="calibre17">std::shared_future</kbd>, with the new facility for automatically deducing the type of a variable from its initializer (see <a href="kindle_split_022.html#app01" class="calibre4">appendix A</a>, <a href="kindle_split_022.html#app01lev1sec6" class="calibre4">section A.6</a>). <kbd class="calibre17">std::future</kbd> has a <kbd class="calibre17">share()</kbd> member function that creates a new <kbd class="calibre17">std::shared_future</kbd> and transfers ownership to it directly. This can save a lot of typing and makes code easier to change:
      </p>
      
      <pre id="PLd0e10351" class="calibre5">std::promise&lt; std::map&lt; SomeIndexType, SomeDataType, SomeComparator,
    SomeAllocator&gt;::iterator&gt; p;
auto sf=p.get_future().share();</pre>
      
      <p class="noind">In this case, the type of <kbd class="calibre17">sf</kbd> is deduced to be <kbd class="calibre17">std::shared_future&lt; std::map&lt; SomeIndexType, SomeDataType, SomeComparator, SomeAllocator&gt;::iterator&gt;</kbd>, which is rather a mouthful. If the comparator or allocator is changed, you only need to change the type of the promise;
         the type of the future is automatically updated to match.
      </p>
      
      <p class="noind">Sometimes you want to limit the amount of time you’re waiting for an event, either because you have a hard time limit on how
         long a particular section of code may take, or because there’s other useful work that the thread can be doing if the event
         isn’t going to happen soon. To handle this facility, many of the waiting functions have variants that allow a timeout to be
         specified.
      </p>
      
      
      
      
      <h3 id="ch04lev1sec3" class="chapter"><a id="ch04lev1sec3__title" class="calibre3"></a>4.3. Waiting with a time limit
      </h3>
      
      <p class="noind">All the blocking calls introduced previously will block for an indefinite period of time, suspending the thread until the
         event being waited for occurs. In many cases this is fine, but in some cases you may want to put a limit on how long you wait.
         This might be to allow you to send some form of “I’m still alive” message either to an interactive user, or another process,
         or indeed to allow you to abort the wait if the user has given up waiting and clicked Cancel.
      </p>
      
      <p class="noind">There are two sorts of timeouts you may wish to specify: a <i class="calibre6">duration-based</i> timeout, where you wait for a specific amount of time (for example, 30 milliseconds); or an <i class="calibre6">absolute</i> timeout, where you wait until a specific point in time (for example, 17:30:15.045987023 UTC on November 30, 2011). Most of
         the waiting functions provide variants that handle both forms of timeouts. The variants that handle the duration-based timeouts
         have a <kbd class="calibre17">_for</kbd> suffix, and those that handle the absolute timeouts have an <kbd class="calibre17">_until</kbd> suffix.
      </p>
      
      <p class="noind">So, for example, <kbd class="calibre17">std::condition_variable</kbd> has two overloads of the <kbd class="calibre17">wait_for()</kbd> member function and two overloads of the <kbd class="calibre17">wait_until()</kbd> member function that correspond to the two overloads of <kbd class="calibre17">wait()</kbd>—one overload that just waits until signaled, or the timeout expires, or a spurious wakeup occurs; and another that will check
         the supplied predicate when woken and will return only when the supplied predicate is <kbd class="calibre17">true</kbd> (and the condition variable has been signaled) or the timeout expires.
      </p>
      
      <p class="noind">Before we look at the details of the functions that use the timeouts, let’s examine the way that times are specified in C++,
         starting with clocks.
      </p>
      
      
      
      <h4 id="ch04lev2sec8" class="calibre23">4.3.1. <a id="ch04lev2sec8__title" class="calibre4"></a>Clocks
      </h4>
      
      <p class="noind"><a id="iddle1311" class="calibre4"></a><a id="iddle2067" class="calibre4"></a><a id="iddle2075" class="calibre4"></a><a id="iddle2561" class="calibre4"></a><a id="iddle2593" class="calibre4"></a><a id="iddle2598" class="calibre4"></a><a id="iddle2656" class="calibre4"></a>As far as the C++ Standard Library is concerned, a clock is a source of time information. Specifically, a clock is a class
         that provides four distinct pieces of information:
      </p>
      
      <p class="calibre19"></p>
      <ul class="calibre21">
         
         <li class="calibre22">The time <i class="calibre6">now</i></li>
         
         <li class="calibre22">The type of the value used to represent the times obtained from the clock</li>
         
         <li class="calibre22">The tick period of the clock</li>
         
         <li class="calibre22">Whether or not the clock ticks at a uniform rate and is therefore considered to be a <i class="calibre6">steady</i> clock
         </li>
         
      </ul>
      
      <p class="noind">The current time of a clock can be obtained by calling the <kbd class="calibre17">now()</kbd> static member function for that <kbd class="calibre17">clock</kbd> class; for example, <kbd class="calibre17">std::chrono::system_clock::now()</kbd> will return the current time of the system clock. The type of the time points for a particular clock is specified by the
         <kbd class="calibre17">time_point</kbd> member <kbd class="calibre17">typedef</kbd>, so the return type of <kbd class="calibre17">some_clock::now()</kbd> is <kbd class="calibre17">some_clock::time_point</kbd>.
      </p>
      
      <p class="noind">The tick period of the clock is specified as a fractional number of seconds, which is given by the <kbd class="calibre17">period</kbd> member <kbd class="calibre17">typedef</kbd> of the clock—a clock that ticks 25 times per second has a <kbd class="calibre17">period</kbd> of <kbd class="calibre17">std::ratio&lt;1,25&gt;</kbd>, whereas a clock that ticks every 2.5 seconds has a <kbd class="calibre17">period</kbd> of <kbd class="calibre17">std::ratio&lt;5,2&gt;</kbd>. If the tick period of a clock can’t be known until runtime, or it may vary during a given run of the application, the <kbd class="calibre17">period</kbd> may be specified as the average tick period, smallest possible tick period, or some other value that the library writer deems
         appropriate. There’s no guarantee that the observed tick period in a given run of the program matches the specified <kbd class="calibre17">period</kbd> for that clock.
      </p>
      
      <p class="noind">If a clock <i class="calibre6">ticks at a uniform rate</i> (whether or not that rate matches the <kbd class="calibre17">period</kbd>) and <i class="calibre6">can’t be adjusted</i>, the clock is said to be a <i class="calibre6">steady</i> clock. The <kbd class="calibre17">is_steady</kbd> static data member of the clock class is <kbd class="calibre17">true</kbd> if the clock is steady, and <kbd class="calibre17">false</kbd> otherwise. Typically, <kbd class="calibre17">std::chrono::system_clock</kbd> will <i class="calibre6">not</i> be steady, because the clock can be adjusted, even if such adjustment is done automatically to take account of local clock
         drift. Such an adjustment may cause a call to <kbd class="calibre17">now()</kbd> to return a value earlier than that returned by a prior call to <kbd class="calibre17">now()</kbd>, which is in violation of the requirement for a uniform tick rate. Steady clocks are important for timeout calculations,
         as you’ll see shortly, so the C++ Standard Library provides one in the form of <kbd class="calibre17">std::chrono::steady_clock</kbd>. The other clocks provided by the C++ Standard Library are <kbd class="calibre17">std::chrono::system_clock</kbd> (mentioned earlier), which represents the “real-time” clock of the system and provides functions for converting its time
         points to and from <kbd class="calibre17">time_t</kbd> values, and <kbd class="calibre17">std::chrono::high_resolution_clock</kbd>, which provides the smallest possible tick period (and thus the highest possible resolution) of all the library-supplied
         clocks. It may be a <kbd class="calibre17">typedef</kbd> to one of the other clocks. These clocks are defined in the <kbd class="calibre17">&lt;chrono&gt;</kbd> library header, along with the other time facilities.
      </p>
      
      <p class="noind">We’ll look at the representation of time points shortly, but first let’s look at how durations are represented.</p>
      
      
      
      
      <h4 id="ch04lev2sec9" class="calibre23">4.3.2. <a id="ch04lev2sec9__title" class="calibre4"></a>Durations
      </h4>
      
      <p class="noind"><a id="iddle2093" class="calibre4"></a>Durations are the simplest part of the time support; they’re handled by the <kbd class="calibre17">std:: chrono::duration&lt;&gt;</kbd> class template (all the C++ time-handling facilities used by the Thread Library are in the <kbd class="calibre17">std::chrono</kbd> namespace). The first template parameter is the type of the representation (such as <kbd class="calibre17">int</kbd>, <kbd class="calibre17">long</kbd>, or <kbd class="calibre17">double</kbd>), and the second is a fraction specifying how many seconds each unit of the duration represents. For example, a number of
         minutes stored in a <kbd class="calibre17">short</kbd> is <kbd class="calibre17">std::chrono::duration&lt;short,std:: ratio&lt;60,1&gt;&gt;</kbd>, because there are 60 seconds in a minute. On the other hand, a count of milliseconds stored in a <kbd class="calibre17">double</kbd> is <kbd class="calibre17">std::chrono::duration&lt;double,std::ratio &lt;1,1000&gt;&gt;</kbd>, because each millisecond is 1/1000th of a second.
      </p>
      
      <p class="noind">The Standard Library provides a set of predefined <kbd class="calibre17">typedef</kbd>s in the <kbd class="calibre17">std::chrono</kbd> namespace for various durations: <kbd class="calibre17">nanoseconds</kbd>, <kbd class="calibre17">microseconds</kbd>, <kbd class="calibre17">milliseconds</kbd>, <kbd class="calibre17">seconds</kbd>, <kbd class="calibre17">minutes</kbd>, and <kbd class="calibre17">hours</kbd>. They all use a sufficiently large integral type for the representation chosen such that you can represent a duration of
         over 500 <i class="calibre6">years</i> in the appropriate units if you so desire. There are also <kbd class="calibre17">typedef</kbd>s for all the SI ratios from <kbd class="calibre17">std::atto</kbd> (10–18) to <kbd class="calibre17">std::exa</kbd> (1018) (and beyond, if your platform has 128-bit integer types) for use when specifying custom durations such as <kbd class="calibre17">std::duration&lt;double,std::centi&gt;</kbd> for a count of 1/100th of a second represented in a <kbd class="calibre17">double</kbd>.
      </p>
      
      <p class="noind">For convenience, there are a number of predefined literal suffix operators for durations in the <kbd class="calibre17">std::chrono_literals</kbd> namespace, introduced with C++14. This can simplify code that uses hard-coded duration values, such as
      </p>
      
      <pre id="PLd0e10696" class="calibre5">using namespace std::chrono_literals;
auto one_day=24h;
auto half_an_hour=30min;
auto max_time_between_messages=30ms;</pre>
      
      <p class="noind">When used with integer literals, these suffixes are equivalent to using the predefined duration <kbd class="calibre17">typedef</kbd>s, so <kbd class="calibre17">15ns</kbd> and <kbd class="calibre17">std::chrono::nanoseconds(15)</kbd> are identical values. However, when used with floating-point literals, these suffixes create a suitably-scaled floating-point
         duration with an unspecified representation type. Therefore, <kbd class="calibre17">2.5min</kbd> will be <kbd class="calibre17">std::chrono::duration&lt;</kbd><i class="calibre6"><kbd class="calibre17">some-floating-point-type</kbd></i><kbd class="calibre17">,std::ratio&lt;60,1&gt;&gt;</kbd>. If you are concerned about the range or precision of the implementation’s chosen floating point type, then you will need
         to construct an object with a suitable representation yourself, rather than using the convenience of the literal suffixes.
      </p>
      
      <p class="noind">Conversion between durations is implicit where it does not require truncation of the value (so converting hours to seconds
         is OK, but converting seconds to hours is not). Explicit conversions can be done with <kbd class="calibre17">std::chrono::duration_cast&lt;&gt;</kbd>:
      </p>
      
      <pre id="PLd0e10731" class="calibre5">std::chrono::milliseconds ms(54802);
std::chrono::seconds s=
    std::chrono::duration_cast&lt;std::chrono::seconds&gt;(ms);</pre>
      
      <p class="noind">The result is truncated rather than rounded, so <kbd class="calibre17">s</kbd> will have a value of 54 in this example.
      </p>
      
      <p class="noind"><a id="iddle1695" class="calibre4"></a><a id="iddle2563" class="calibre4"></a><a id="iddle2564" class="calibre4"></a><a id="iddle2568" class="calibre4"></a><a id="iddle2658" class="calibre4"></a>Durations support arithmetic, so you can add and subtract durations to get new durations or multiply or divide by a constant
         of the underlying representation type (the first template parameter). Thus <kbd class="calibre17">5*seconds(1)</kbd> is the same as <kbd class="calibre17">seconds(5)</kbd> or <kbd class="calibre17">minutes(1) – seconds(55)</kbd>. The count of the number of units in the duration can be obtained with the <kbd class="calibre17">count()</kbd> member function. Thus <kbd class="calibre17">std::chrono::milliseconds(1234).count()</kbd> is 1234.
      </p>
      
      <p class="noind">Duration-based waits are done with instances of <kbd class="calibre17">std::chrono::duration&lt;&gt;</kbd>. For example, you can wait for up to 35 milliseconds for a future to be ready:
      </p>
      
      <pre id="PLd0e10802" class="calibre5">std::future&lt;int&gt; f=std::async(some_task);
if(f.wait_for(std::chrono::milliseconds(35))==std::future_status::ready)
    do_something_with(f.get());</pre>
      
      <p class="noind">The <kbd class="calibre17">wait</kbd> functions all return a status to indicate whether the wait timed out or the waited-for event occurred. In this case, you’re
         waiting for a future, so the function returns <kbd class="calibre17">std::future_status::timeout</kbd> if the wait times out, <kbd class="calibre17">std::future_status:: ready</kbd> if the future is ready, or <kbd class="calibre17">std::future_status::deferred</kbd> if the future’s task is deferred. The time for a duration-based wait is measured using a steady clock internal to the library,
         so 35 milliseconds means 35 milliseconds of elapsed time, even if the system clock was adjusted (forward or back) during the
         wait. Of course, the vagaries of system scheduling and the varying precisions of OS clocks means that the time between the
         thread issuing the call and returning from it may be much longer than 35 ms.
      </p>
      
      <p class="noind">With durations under our belt, we can now move on to time points.</p>
      
      
      
      <h4 id="ch04lev2sec10" class="calibre23">4.3.3. <a id="ch04lev2sec10__title" class="calibre4"></a>Time points
      </h4>
      
      <p class="noind">The time point for a clock is represented by an instance of the <kbd class="calibre17">std::chrono::time_point&lt;&gt;</kbd> class template, which specifies which clock it refers to as the first template parameter and the units of measurement (a
         specialization of <kbd class="calibre17">std::chrono::duration&lt;&gt;</kbd>) as the second template parameter. The value of a time point is the length of time (in multiples of the specified duration)
         since a specific point in time called the <i class="calibre6">epoch</i> of the clock. The epoch of a clock is a basic property but not something that’s directly available to query or specified
         by the C++ Standard. Typical epochs include 00:00 on January 1, 1970 and the instant when the computer running the application
         booted up. Clocks may share an epoch or have independent epochs. If two clocks share an epoch, the <kbd class="calibre17">time_point typedef</kbd> in one class may specify the other as the clock type associated with the <kbd class="calibre17">time_point</kbd>. Although you can’t find out when the epoch is, you <i class="calibre6">can</i> get the <kbd class="calibre17">time_since_epoch()</kbd> for a given <kbd class="calibre17">time_point</kbd>. This member function returns a duration value specifying the length of time since the clock epoch to that particular time
         point.
      </p>
      
      <p class="noind">For example, you might specify a time point as <kbd class="calibre17">std::chrono::time_point&lt;std:: chrono::system_clock, std::chrono::minutes&gt;</kbd>. This would hold the time relative to the system clock but measured in minutes as opposed to the native precision of the
         system clock (which is typically seconds or less).
      </p>
      
      <p class="noind"><a id="iddle2630" class="calibre4"></a>You can add durations and subtract durations from instances of <kbd class="calibre17">std::chrono:: time_point&lt;&gt;</kbd> to produce new time points, so <kbd class="calibre17">std::chrono::high_resolution_clock:: now() + std::chrono::nanoseconds(500)</kbd> will give you a time 500 nanoseconds in the future. This is good for calculating an absolute timeout when you know the maximum
         duration of a block of code, but there are multiple calls to waiting functions within it or nonwaiting functions that precede
         a waiting function but take up some of the time budget.
      </p>
      
      <p class="noind">You can also subtract one time point from another that shares the same clock. The result is a duration specifying the length
         of time between the two time points. This is useful for timing blocks of code, for example:
      </p>
      
      <pre id="PLd0e10884" class="calibre5">auto start=std::chrono::high_resolution_clock::now();
do_something();
auto stop=std::chrono::high_resolution_clock::now();
std::cout&lt;&lt;"do_something() took "
  &lt;&lt;std::chrono::duration&lt;double,std::chrono::seconds&gt;(stop-start).count()
  &lt;&lt;" seconds"&lt;&lt;std::endl;</pre>
      
      <p class="noind">The clock parameter of a <kbd class="calibre17">std::chrono::time_point&lt;&gt;</kbd> instance does more than just specify the epoch, though. When you pass the time point to a <kbd class="calibre17">wait</kbd> function that takes an absolute timeout, the clock parameter of the time point is used to measure the time. This has important
         consequences when the clock is changed, because the <kbd class="calibre17">wait</kbd> tracks the clock change and won’t return until the clock’s <kbd class="calibre17">now()</kbd> function returns a value later than the specified timeout. If the clock is adjusted forward, this may reduce the total length
         of the wait (as measured by a steady clock), and if it’s adjusted backward, this may increase the total length of the wait.
      </p>
      
      <p class="noind">As you may expect, time points are used with the <kbd class="calibre17">_until</kbd> variants of the <kbd class="calibre17">wait</kbd> functions. The typical use case is as an offset from <i class="calibre6">some-clock</i><kbd class="calibre17">::now()</kbd> at a fixed point in the program, although time points associated with the system clock can be obtained by converting from
         <kbd class="calibre17">time_t</kbd> using the <kbd class="calibre17">std::chrono::system_clock::to_time_point()</kbd> static member function to schedule operations at a user-visible time. For example, if you have a maximum of 500 milliseconds
         to wait for an event associated with a condition variable, you might do something like in the following listing.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex11">Listing 4.11. <a id="ch04ex11__title" class="calibre4"></a>Waiting for a condition variable with a timeout
      </h5>
      <pre id="PLd0e10930" class="calibre5">#include &lt;condition_variable&gt;
#include &lt;mutex&gt;
#include &lt;chrono&gt;
std::condition_variable cv;
bool done;
std::mutex m;
bool wait_loop()
{
    auto const timeout= std::chrono::steady_clock::now()+
        std::chrono::milliseconds(500);
    std::unique_lock&lt;std::mutex&gt; lk(m);
    while(!done)
    {
        if(cv.wait_until(lk,timeout)==std::cv_status::timeout)
            break;
    }
    return done;
}</pre>
      
      <p class="noind"><a id="iddle1398" class="calibre4"></a><a id="iddle1925" class="calibre4"></a><a id="iddle1926" class="calibre4"></a><a id="iddle2258" class="calibre4"></a><a id="iddle2562" class="calibre4"></a><a id="iddle2565" class="calibre4"></a><a id="iddle2578" class="calibre4"></a><a id="iddle2579" class="calibre4"></a><a id="iddle2657" class="calibre4"></a>This is the recommended way to wait for condition variables with a time limit if you’re not passing a predicate to <kbd class="calibre17">wait</kbd>. This way, the overall length of the loop is bounded. As you saw in <a href="#ch04lev2sec1" class="calibre4">section 4.1.1</a>, you need to loop when using condition variables if you don’t pass in the predicate, in order to handle spurious wakeups.
         If you use <kbd class="calibre17">wait_for()</kbd> in a loop, you might end up waiting almost the full length of time before a spurious wake-up, and the next time through the
         wait time starts again. This may repeat any number of times, making the total wait time unbounded.
      </p>
      
      <p class="noind">With the basics of specifying timeouts under your belt, let’s look at the functions that you can use <kbd class="calibre17">timeout</kbd> with.
      </p>
      
      
      
      <h4 id="ch04lev2sec11" class="calibre23">4.3.4. <a id="ch04lev2sec11__title" class="calibre4"></a>Functions that accept timeouts
      </h4>
      
      <p class="noind">The simplest use for a <kbd class="calibre17">timeout</kbd> is to add a delay to the processing of a particular thread so that it doesn’t take processing time away from other threads
         when it has nothing to do. You saw an example of this in <a href="#ch04lev1sec1" class="calibre4">section 4.1</a>, where you polled a “done” flag in a loop. The two functions that handle this are <kbd class="calibre17">std::this_thread::sleep_for()</kbd> and <kbd class="calibre17">std::this_thread::sleep_until()</kbd>. They work like a basic alarm clock: the thread goes to sleep either for the specified duration (with <kbd class="calibre17">sleep_for()</kbd>) or until the specified point in time (with <kbd class="calibre17">sleep_until()</kbd>). <kbd class="calibre17">sleep_for()</kbd> makes sense for examples like those in <a href="#ch04lev1sec1" class="calibre4">section 4.1</a>, where something must be done periodically, and the elapsed time is what matters. On the other hand, <kbd class="calibre17">sleep_until()</kbd> allows you to schedule the thread to wake at a particular point in time. This could be used to trigger the backups at midnight,
         or the payroll print run at 6:00 a.m., or to suspend the thread until the next frame refresh when doing a video playback.
      </p>
      
      <p class="noind">Sleeping isn’t the only facility that takes a <kbd class="calibre17">timeout</kbd>; you already saw that you can use <kbd class="calibre17">timeout</kbd>s with condition variables and futures. You can even use <kbd class="calibre17">timeout</kbd>s when trying to acquire a lock on a mutex if the mutex supports it. Plain <kbd class="calibre17">std::mutex</kbd> and <kbd class="calibre17">std::recursive_mutex</kbd> don’t support <kbd class="calibre17">timeout</kbd>s on locking, but <kbd class="calibre17">std::timed_mutex</kbd> does, as does <kbd class="calibre17">std::recursive_timed_mutex</kbd>. Both these types support <kbd class="calibre17">try_lock_for()</kbd> and <kbd class="calibre17">try_lock_until()</kbd> member functions that try to obtain the lock within a specified time period or before a specified time point. <a href="#ch04table01" class="calibre4">Table 4.1</a> shows the functions from the C++ Standard Library that can accept <kbd class="calibre17">timeout</kbd>s, their parameters, and their return values. Parameters listed as <kbd class="calibre17">duration</kbd> must be an instance of <kbd class="calibre17">std::duration&lt;&gt;</kbd>, and those listed as <kbd class="calibre17">time_point</kbd> must be an instance of <kbd class="calibre17">std::time_point&lt;&gt;</kbd>.
      </p>
      
      <h5 class="notetitle" id="ch04table01">Table 4.1. <a id="ch04table01__title" class="calibre4"></a></h5>
      <table cellspacing="5" frame="hsides" rules="groups" cellpadding="8" width="100%" class="calibre25">
         <colgroup span="3" class="calibre8">
            <col width="200" class="calibre9"/>
            <col width="200" class="calibre9"/>
            <col width="200" class="calibre9"/>
         </colgroup>
         <thead class="calibre26">
            <tr class="calibre11">
               <th class="doctablecell1" scope="col" valign="top">
                  <p class="noind"><a id="iddle1113" class="calibre4"></a><a id="iddle1117" class="calibre4"></a><a id="iddle1142" class="calibre4"></a><a id="iddle1390" class="calibre4"></a><a id="iddle1411" class="calibre4"></a><a id="iddle1579" class="calibre4"></a><a id="iddle2430" class="calibre4"></a><a id="iddle2431" class="calibre4"></a>Class/Namespace
                  </p>
               </th>
               <th class="doctablecell1" scope="col" valign="top">
                  <p class="noind">Functions</p>
               </th>
               <th class="doctablecell1" scope="col" valign="top">
                  <p class="noind">Return Values</p>
               </th>
            </tr>
         </thead>
         <tbody class="calibre10">
            <tr class="calibre11">
               <td class="doctablecell">std::this_thread namespace</td>
               <td class="doctablecell">sleep_for(<i class="calibre6">duration</i>) sleep_until(<i class="calibre6">time_point</i>)
               </td>
               <td class="doctablecell">N/A</td>
            </tr>
            <tr class="calibre11">
               <td class="doctablecell" rowspan="2">std::condition_variable or std::condition_variable_anywait_for(<i class="calibre6">lock,duration</i>)
                  wait_until(<i class="calibre6">lock,time_point</i>)
               </td>
               <td class="doctablecell">std::cv_status::timeout or std::cv_status::no_timeout</td>
               <td class="doctablecell"> </td>
            </tr>
            <tr class="calibre11">
               <td class="doctablecell">wait_for(<i class="calibre6">lock,duration, predicate</i>) <i class="calibre6">wait_until(lock,time_point, predicate</i>)
               </td>
               <td class="doctablecell">bool—the return value of the predicate when woken</td>
            </tr>
            <tr class="calibre11">
               <td class="doctablecell">std::timed_mutex, std::recursive_timed_mutex or std::shared_timed_mutextry_lock_for(<i class="calibre6">duration</i>)
                  try_lock_until(<i class="calibre6">time_point</i>)
               </td>
               <td class="doctablecell">bool—true if the lock was acquired, false otherwise</td>
               <td class="doctablecell"> </td>
            </tr>
            <tr class="calibre11">
               <td class="doctablecell">std::shared_timed_mutex</td>
               <td class="doctablecell">try_lock_shared_for(<i class="calibre6">duration</i>) try_lock_shared_until(<i class="calibre6">time_point</i>)
               </td>
               <td class="doctablecell">bool—true if the lock was acquired, false otherwise</td>
            </tr>
            <tr class="calibre11">
               <td class="doctablecell" rowspan="2">std::unique_lock&lt;TimedLockable&gt;unique_lock(<i class="calibre6">lockable,duration</i>)
                  unique_lock(<i class="calibre6">lockable,time_point</i>)
               </td>
               <td class="doctablecell">N/A—owns_lock() on the newly-constructed object returns true if the lock was acquired, false otherwise</td>
               <td class="doctablecell"> </td>
            </tr>
            <tr class="calibre11">
               <td class="doctablecell">try_lock_for(<i class="calibre6">duration</i>) try_lock_until(<i class="calibre6">time_point</i>)
               </td>
               <td class="doctablecell">bool—true if the lock was acquired, false otherwise</td>
            </tr>
            <tr class="calibre11">
               <td class="doctablecell" rowspan="2">std::shared_lock&lt;<i class="calibre6">SharedTimedLockable</i>&gt;shared_lock (<i class="calibre6">lockable,duration</i>)
                  shared_lock(<i class="calibre6">lockable,time_point</i>)
               </td>
               <td class="doctablecell">N/A—owns_lock() on the newly-constructed object returns true if the lock was acquired, false otherwise</td>
               <td class="doctablecell"> </td>
            </tr>
            <tr class="calibre11">
               <td class="doctablecell">try_lock_for(<i class="calibre6">duration</i>) try_lock_until(<i class="calibre6">time_point</i>)
               </td>
               <td class="doctablecell">bool—true if the lock was acquired, false otherwise</td>
            </tr>
            <tr class="calibre11">
               <td class="doctablecell">std::future&lt;<i class="calibre6">ValueType</i>&gt; or std::shared_future&lt;<i class="calibre6">ValueType</i>&gt;wait_for(<i class="calibre6">duration</i>)
                  wait_until(<i class="calibre6">time_point</i>)
               </td>
               <td class="doctablecell">std::future_status::timeout if the wait timed out, std::future_status::ready if the future is ready, or std::future_status::deferred
                  if the future holds a deferred function that hasn’t yet started
               </td>
               <td class="doctablecell"> </td>
            </tr>
         </tbody>
      </table>
      
      <p class="noind">Now that I’ve covered the mechanics of condition variables, futures, promises, and packaged tasks, it’s time to look at the
         wider picture and how they can be used to simplify the synchronization of operations between threads.
      </p>
      
      
      
      
      
      <h3 id="ch04lev1sec4" class="chapter"><a id="ch04lev1sec4__title" class="calibre3"></a>4.4. Using synchronization of operations to simplify code
      </h3>
      
      <p class="noind">Using the synchronization facilities described so far in this chapter as building blocks allows you to focus on the operations
         that need synchronizing rather than the mechanics. One way this can help simplify your code is that it accommodates a much
         more <i class="calibre6">functional</i> (in the sense of <i class="calibre6">functional programming</i>) approach to programming <a id="iddle1392" class="calibre4"></a><a id="iddle1413" class="calibre4"></a><a id="iddle1444" class="calibre4"></a><a id="iddle1457" class="calibre4"></a><a id="iddle1815" class="calibre4"></a><a id="iddle1829" class="calibre4"></a>concurrency. Rather than sharing data directly between threads, each task can be provided with the data it needs, and the
         result can be disseminated to any other threads that need it through the use of futures.
      </p>
      
      
      <h4 id="ch04lev2sec12" class="calibre23">4.4.1. <a id="ch04lev2sec12__title" class="calibre4"></a>Functional programming with futures
      </h4>
      
      <p class="noind">The term <i class="calibre6">functional programming</i> (FP) refers to a style of programming where the result of a function call depends solely on the parameters to that function
         and doesn’t depend on any external state. This is related to the mathematical concept of a function, and it means that if
         you invoke a function twice with the same parameters, the result is exactly the same. This is a property of many of the mathematical
         functions in the C++ Standard Library, such as <kbd class="calibre17">sin</kbd>, <kbd class="calibre17">cos</kbd>, and <kbd class="calibre17">sqrt</kbd>, and simple operations on basic types, such as <kbd class="calibre17">3+3</kbd>, <kbd class="calibre17">6*9</kbd>, or <kbd class="calibre17">1.3/4.7</kbd>. A <i class="calibre6">pure</i> function doesn’t <i class="calibre6">modify</i> any external state either; the effects of the function are entirely limited to the return value.
      </p>
      
      <p class="noind">This makes things easy to think about, especially when concurrency is involved, because many of the problems associated with
         shared memory discussed in <a href="kindle_split_013.html#ch03" class="calibre4">chapter 3</a> disappear. If there are no modifications to shared data, there can be no race conditions and thus no need to protect shared
         data with mutexes either. This is such a powerful simplification that programming languages such as Haskell (<a href="http://www.haskell.org/" class="calibre4">http://www.haskell.org/</a>), where all functions are pure by default, are becoming increasingly popular for programming concurrent systems. Because
         most things are pure, the <i class="calibre6">impure</i> functions that actually <i class="calibre6">do</i> modify the shared state stand out all the more, and it’s therefore easier to reason about how they fit into the overall structure
         of the application.
      </p>
      
      <p class="noind">The benefits of FP aren’t limited to those languages where it’s the default paradigm, however. C++ is a multiparadigm language,
         and it’s entirely possible to write programs in the FP style. This is even easier in C++11 than it was in C++98, with the
         advent of lambda functions (see <a href="kindle_split_022.html#app01" class="calibre4">appendix A</a>, <a href="kindle_split_022.html#app01lev1sec6" class="calibre4">section A.6</a>), the incorporation of <kbd class="calibre17">std::bind</kbd> from Boost and TR1, and the introduction of automatic type deduction for variables (see <a href="kindle_split_022.html#app01" class="calibre4">appendix A</a>, <a href="kindle_split_022.html#app01lev1sec7" class="calibre4">section A.7</a>). Futures are the final piece of the puzzle that makes FP-style concurrency viable in C++; a future can be passed around
         between threads to allow the result of one computation to depend on the result of another, <i class="calibre6">without any explicit access to shared data</i>.
      </p>
      
      
      <h5 class="notetitle" id="ch04lev3sec2"><a id="ch04lev3sec2__title" class="calibre4"></a>FP-style Quicksort
      </h5>
      
      <p class="noind">To illustrate the use of futures for FP-style concurrency, let’s look at a simple implementation of the Quicksort algorithm.
         The basic idea of the algorithm is simple: given a list of values, take an element to be the pivot element, and then partition
         the list into two sets—those less than the pivot and those greater than or equal to the pivot. A sorted copy of the list is
         obtained by sorting the two sets and returning the sorted list of values less than the pivot, followed by the pivot, followed
         by the sorted list of values greater than or equal to the pivot. <a href="#ch04fig02" class="calibre4">Figure 4.2</a> shows how a list of 10 integers is sorted under this scheme. An FP-style sequential implementation is shown in the following
         listing; it takes and returns a list by value rather than sorting in place like <kbd class="calibre17">std::sort()</kbd> does.<a id="iddle1932" class="calibre4"></a><a id="iddle2221" class="calibre4"></a><a id="iddle2335" class="calibre4"></a></p>
      
      
      <p class="noind"></p>
      
      
      <h5 class="notetitle" id="ch04fig02">Figure 4.2. <a id="ch04fig02__title" class="calibre4"></a>FP-style recursive sorting
      </h5>
      
      <p class="center1"><img alt="" src="04fig02_alt.jpg" class="calibre2"/></p>
      
      
      
      
      <h5 class="notetitle" id="ch04ex12">Listing 4.12. <a id="ch04ex12__title" class="calibre4"></a>A sequential implementation of Quicksort
      </h5>
      <pre id="PLd0e11738" class="calibre5">template&lt;typename T&gt;
std::list&lt;T&gt; sequential_quick_sort(std::list&lt;T&gt; input)
{
    if(input.empty())
    {
        return input;
    }
    std::list&lt;T&gt; result;
    result.splice(result.begin(),input,input.begin());           <b class="calibre24"><i class="calibre6">1</i></b>
    T const&amp; pivot=*result.begin();                              <b class="calibre24"><i class="calibre6">2</i></b>

    auto divide_point=std::partition(input.begin(),input.end(),
            [&amp;](T const&amp; t){return t&lt;pivot;});                   <b class="calibre24"><i class="calibre6">3</i></b>
    std::list&lt;T&gt; lower_part;
    lower_part.splice(lower_part.end(),input,input.begin(),
        divide_point);                                           <b class="calibre24"><i class="calibre6">4</i></b>
    auto new_lower(
        sequential_quick_sort(std::move(lower_part)));           <b class="calibre24"><i class="calibre6">5</i></b>
    auto new_higher(
        sequential_quick_sort(std::move(input)));                <b class="calibre24"><i class="calibre6">6</i></b>
    result.splice(result.end(),new_higher);                      <b class="calibre24"><i class="calibre6">7</i></b>
    result.splice(result.begin(),new_lower);                     <b class="calibre24"><i class="calibre6">8</i></b>
    return result;
}</pre>
      
      <p class="noind">Although the interface is FP-style, if you used FP style throughout, you’d do a lot of copying, so you use “normal” imperative
         style for the internals. You take the first element as the pivot by slicing it off the front of the list using <kbd class="calibre17">splice()</kbd> <b class="calibre24"><i class="calibre6">1</i></b>. Although this can potentially result in a suboptimal sort (in terms of numbers of comparisons and exchanges), doing anything
         else with a <kbd class="calibre17">std::list</kbd> can add quite a bit of time because of the list traversal. You know you’re going to want it in the result, so you can splice
         it <a id="iddle1391" class="calibre4"></a><a id="iddle1412" class="calibre4"></a><a id="iddle1680" class="calibre4"></a><a id="iddle1681" class="calibre4"></a><a id="iddle1756" class="calibre4"></a><a id="iddle1757" class="calibre4"></a><a id="iddle1830" class="calibre4"></a>directly into the list you’ll be using for that. Now, you’re also going to want to use it for comparisons, so let’s take a
         reference to it to avoid copying <b class="calibre24"><i class="calibre6">2</i></b>. You can then use <kbd class="calibre17">std::partition</kbd> to divide the sequence into those values <i class="calibre6">less than</i> the pivot and those <i class="calibre6">not less than</i> the pivot <b class="calibre24"><i class="calibre6">3</i></b>. The easiest way to specify the partition criteria is to use a lambda function; you use a reference capture to avoid copying
         the <kbd class="calibre17">pivot</kbd> value (see <a href="kindle_split_022.html#app01" class="calibre4">appendix A</a>, <a href="kindle_split_022.html#app01lev1sec5" class="calibre4">section A.5</a> for more on lambda functions).
      </p>
      
      <p class="noind"><kbd class="calibre17">std::partition()</kbd> rearranges the list in place and returns an iterator marking the first element that’s <i class="calibre6">not</i> less than the pivot value. The full type for an iterator can be quite long-winded, so you just use the <kbd class="calibre17">auto</kbd> type specifier to force the compiler to work it out for you (see <a href="kindle_split_022.html#app01" class="calibre4">appendix A</a>, <a href="kindle_split_022.html#app01lev1sec7" class="calibre4">section A.7</a>).
      </p>
      
      <p class="noind">Now, you’ve opted for an FP-style interface, so if you’re going to use recursion to sort the two “halves,” you’ll need to
         create two lists. You can do this by using <kbd class="calibre17">splice()</kbd> again to move the values from <kbd class="calibre17">input</kbd> up to the <kbd class="calibre17">divide_point</kbd> into a new list: <kbd class="calibre17">lower_part</kbd> <b class="calibre24"><i class="calibre6">4</i></b>. This leaves the remaining values alone in <kbd class="calibre17">input</kbd>. You can then sort the two lists with recursive calls, <b class="calibre24"><i class="calibre6">5</i></b> and <b class="calibre24"><i class="calibre6">6</i></b>. By using <kbd class="calibre17">std::move()</kbd> to pass the lists in, you can avoid copying here too—the result is implicitly moved out anyway. Finally, you can use <kbd class="calibre17">splice()</kbd> yet again to piece the <kbd class="calibre17">result</kbd> together in the right order. The <kbd class="calibre17">new_higher</kbd> values go on the end <b class="calibre24"><i class="calibre6">7</i></b>, after the pivot, and the <kbd class="calibre17">new_lower</kbd> values go at the beginning, before the pivot <b class="calibre24"><i class="calibre6">8</i></b>.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04lev3sec3"><a id="ch04lev3sec3__title" class="calibre4"></a>FP-style parallel Quicksort
      </h5>
      
      <p class="noind">Because this uses a functional style already, it’s now easy to convert this to a parallel version using futures, as shown
         in the next listing. The set of operations is the same as before, except that some of them now run in parallel. This version
         uses an implementation of the Quicksort algorithm using futures and a functional style.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex13">Listing 4.13. <a id="ch04ex13__title" class="calibre4"></a>Parallel Quicksort using futures
      </h5>
      <pre id="PLd0e11950" class="calibre5">template&lt;typename T&gt;
std::list&lt;T&gt; parallel_quick_sort(std::list&lt;T&gt; input)
{
    if(input.empty())
    {
        return input;
    }
    std::list&lt;T&gt; result;
    result.splice(result.begin(),input,input.begin());
    T const&amp; pivot=*result.begin();
    auto divide_point=std::partition(input.begin(),input.end(),
            [&amp;](T const&amp; t){return t&lt;pivot;});
    std::list&lt;T&gt; lower_part;
    lower_part.splice(lower_part.end(),input,input.begin(),
        divide_point);
    std::future&lt;std::list&lt;T&gt; &gt; new_lower(                           <b class="calibre24"><i class="calibre6">1</i></b>
        std::async(&amp;parallel_quick_sort&lt;T&gt;,std::move(lower_part)));
    auto new_higher(
        parallel_quick_sort(std::move(input)));                     <b class="calibre24"><i class="calibre6">2</i></b>
    result.splice(result.end(),new_higher);                         <b class="calibre24"><i class="calibre6">3</i></b>
    result.splice(result.begin(),new_lower.get());                  <b class="calibre24"><i class="calibre6">4</i></b>
    return result;
}</pre>
      
      <p class="noind"><a id="iddle1931" class="calibre4"></a><a id="iddle2166" class="calibre4"></a><a id="iddle2167" class="calibre4"></a>The big change here is that rather than sorting the lower portion on the current thread, you sort it on another thread using
         <kbd class="calibre17">std::async()</kbd> <b class="calibre24"><i class="calibre6">1</i></b>. The upper portion of the list is sorted with direct recursion as before <b class="calibre24"><i class="calibre6">2</i></b>. By recursively calling <kbd class="calibre17">parallel_quick_sort()</kbd>, you can take advantage of the available hardware concurrency. If <kbd class="calibre17">std::async()</kbd> starts a new thread every time, then if you recurse down three times, you’ll have eight threads running; if you recurse down
         10 times (for ~1000 elements), you’ll have 1,024 threads running if the hardware can handle it. If the library decides there
         are too many spawned tasks (perhaps because the number of tasks has exceeded the available hardware concurrency), it may switch
         to spawning the new tasks synchronously. They will run in the thread that calls <kbd class="calibre17">get()</kbd> rather than on a new thread, thus avoiding the overhead of passing the task to another thread when this won’t help the performance.
         It’s worth noting that it’s perfectly conforming for an implementation of <kbd class="calibre17">std::async</kbd> to start a new thread for each task (even in the face of massive oversubscription) unless <kbd class="calibre17">std::launch::deferred</kbd> is explicitly specified, or to run all tasks synchronously unless <kbd class="calibre17">std::launch::async</kbd> is explicitly specified. If you’re relying on the library for automatic scaling, you’re advised to check the documentation
         for your implementation to see what behavior it exhibits.
      </p>
      
      <p class="noind">Rather than using <kbd class="calibre17">std::async()</kbd>, you could write your own <kbd class="calibre17">spawn_task()</kbd> function as a simple wrapper around <kbd class="calibre17">std::packaged_task</kbd> and <kbd class="calibre17">std::thread</kbd>, as shown in <a href="#ch04ex14" class="calibre4">listing 4.14</a>; you’d create a <kbd class="calibre17">std::packaged_task</kbd> for the result of the function call, get the future from it, run it on a thread, and return the future. This wouldn’t offer
         much of an advantage (and indeed would likely lead to massive oversubscription), but it would pave the way to migrate to a
         more sophisticated implementation that adds the task to a queue to be run by a pool of worker threads. We’ll look at thread
         pools in <a href="kindle_split_019.html#ch09" class="calibre4">chapter 9</a>. It’s probably worth going this way in preference to using <kbd class="calibre17">std::async</kbd> only if you know what you’re doing and want complete control over the way the thread pool is built and executes tasks.
      </p>
      
      <p class="noind">Anyway, back to <kbd class="calibre17">parallel_quick_sort</kbd>. Because you just used direct recursion to get <kbd class="calibre17">new_higher</kbd>, you can splice it into place as before <b class="calibre24"><i class="calibre6">3</i></b>. But <kbd class="calibre17">new_lower</kbd> is now <kbd class="calibre17">std::future&lt;std::list&lt;T&gt;&gt;</kbd> rather than a list, so you need to call <kbd class="calibre17">get()</kbd> to retrieve the value before you can call <kbd class="calibre17">splice()</kbd> <b class="calibre24"><i class="calibre6">4</i></b>. This then waits for the background task to complete and <i class="calibre6">moves</i> the result into the <kbd class="calibre17">splice()</kbd> call; <kbd class="calibre17">get()</kbd> returns an rvalue reference to the contained result, so it can be moved out (see <a href="kindle_split_022.html#app01" class="calibre4">appendix A</a>, <a href="kindle_split_022.html#app01lev2sec1" class="calibre4">section A.1.1</a> for more on rvalue references and move semantics).
      </p>
      
      <p class="noind">Even assuming that <kbd class="calibre17">std::async()</kbd> makes optimal use of the available hardware concurrency, this still isn’t an ideal parallel implementation of Quicksort.
         For one thing, <kbd class="calibre17">std::partition</kbd> does a lot of the work, and that’s still a sequential call, but it’s good enough for now. If you’re interested in the fastest
         possible parallel implementation, check the academic literature. Alternatively, you could use the parallel overload from the
         C++17 Standard Library (see <a href="kindle_split_020.html#ch10" class="calibre4">chapter 10</a>).
      </p>
      
      
      <p class="noind"></p>
      
      
      <h5 class="notetitle" id="ch04ex14">Listing 4.14. <a id="ch04ex14__title" class="calibre4"></a>A sample implementation of <kbd class="calibre17">spawn_task</kbd></h5>
      <pre id="PLd0e12111" class="calibre5">template&lt;typename F,typename A&gt;
std::future&lt;std::result_of&lt;F(A&amp;&amp;)&gt;::type&gt;
    spawn_task(F&amp;&amp; f,A&amp;&amp; a)
{
    typedef std::result_of&lt;F(A&amp;&amp;)&gt;::type result_type;
    std::packaged_task&lt;result_type(A&amp;&amp;)&gt;
        task(std::move(f)));
    std::future&lt;result_type&gt; res(task.get_future());
    std::thread t(std::move(task),std::move(a));
    t.detach();
    return res;
}</pre>
      
      <p class="noind"><a id="iddle1122" class="calibre4"></a><a id="iddle1327" class="calibre4"></a><a id="iddle1381" class="calibre4"></a><a id="iddle1621" class="calibre4"></a><a id="iddle1622" class="calibre4"></a><a id="iddle1632" class="calibre4"></a><a id="iddle1720" class="calibre4"></a><a id="iddle2440" class="calibre4"></a>FP isn’t the only concurrent programming paradigm that eschews shared mutable data; another paradigm is CSP (Communicating
         Sequential Processes),<sup class="calibre18">[<a href="#ch04fn02" class="calibre4">2</a>]</sup> where threads are conceptually entirely separate, with no shared data but with communication channels that allow messages
         to be passed between them. This is the paradigm adopted by the programming language Erlang (<a href="http://www.erlang.org/" class="calibre4">http://www.erlang.org/</a>) and by the MPI (Message Passing Interface; <a href="http://www.mpi-forum.org/" class="calibre4">http://www.mpi-forum.org/</a>) environment commonly used for high-performance computing in C and C++. I’m sure that by now you’ll be unsurprised to learn
         that this can also be supported in C++ with a bit of discipline; the following section discusses one way to achieve this.
      </p>
      <blockquote class="smaller">
         <p class="calibre19"><sup class="calibre20"><a id="ch04fn02" class="calibre4">2</a></sup> 
            </p><div class="calibre15">Communicating Sequential Processes, C.A.R. Hoare, Prentice Hall, 1985. Available free online at <a href="http://www.usingcsp.com/cspbook.pdf" class="calibre4">http://www.usingcsp.com/cspbook.pdf</a>.
            </div>
         <p class="calibre19"></p>
      </blockquote>
      
      
      
      
      <h4 id="ch04lev2sec13" class="calibre23">4.4.2. <a id="ch04lev2sec13__title" class="calibre4"></a>Synchronizing operations with message passing
      </h4>
      
      <p class="noind">The idea of CSP is simple: if there’s no shared data, each thread can be reasoned about entirely independently, purely on
         the basis of how it behaves in response to the messages that it received. Each thread is therefore effectively a state machine:
         when it receives a message, it updates its state in some manner and maybe sends one or more messages to other threads, with
         the processing performed depending on the initial state. One way to write such threads would be to formalize this and implement
         a Finite State Machine model, but this isn’t the only way; the state machine can be implicit in the structure of the application.
         Which method works better in any given scenario depends on the exact behavioral requirements of the situation and the expertise
         of the programming team. However you choose to implement each thread, the separation into independent processes has the potential
         to remove much of the complication from shared-data concurrency and therefore make programming easier, lowering the bug rate.
      </p>
      
      <p class="noind">True communicating sequential processes have no shared data, with all communication passed through the message queues, but
         because C++ threads share an address space, it’s not possible to enforce this requirement. This is where the discipline comes
         in: as application or library authors, it’s our responsibility to ensure that we don’t <a id="iddle1918" class="calibre4"></a>share data between the threads. Of course, the message queues must be shared in order for the threads to communicate, but
         the details can be wrapped in the library.
      </p>
      
      <p class="noind">Imagine for a moment that you’re implementing the code for an ATM. This code needs to handle interaction with the person trying
         to withdraw money and interaction with the relevant bank, as well as control the physical machinery to accept the person’s
         card, display appropriate messages, handle key presses, issue money, and return their card.
      </p>
      
      <p class="noind">One way to handle everything would be to split the code into three independent threads: one to handle the physical machinery,
         one to handle the ATM logic, and one to communicate with the bank. These threads could communicate purely by passing messages
         rather than sharing any data. For example, the thread handling the machinery would send a message to the logic thread when
         the person at the machine entered their card or pressed a button, and the logic thread would send a message to the machinery
         thread indicating how much money to dispense, and so forth.
      </p>
      
      <p class="noind">One way to model the ATM logic would be as a state machine. In each state, the thread waits for an acceptable message, which
         it then processes. This may result in transitioning to a new state, and the cycle continues. The states involved in a simple
         implementation are shown in <a href="#ch04fig03" class="calibre4">figure 4.3</a>. In this simplified implementation, the system waits for a card to be inserted. Once the card is inserted, it then waits
         for the user to enter their PIN, one digit at a time. They can delete the last digit entered. Once enough digits have been
         entered, the PIN is verified. If the PIN is not OK, you’re finished, so you return the card to the customer and resume waiting
         for someone to enter their card. If the PIN is OK, you wait for them to either cancel the transaction or select an amount
         to withdraw. If they cancel, you’re finished, and you return their card. If they select an amount, you wait for confirmation
         from the bank before issuing the cash and returning the card or displaying an “insufficient funds” message and returning their
         card. Obviously, a real ATM is considerably more complex, but this is enough to illustrate the idea.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04fig03">Figure 4.3. <a id="ch04fig03__title" class="calibre4"></a>A simple state machine model for an ATM
      </h5>
      
      <p class="center1"><img alt="" src="04fig03_alt.jpg" class="calibre2"/></p>
      
      
      <p class="noind">Having designed a state machine for your ATM logic, you can implement it with a class that has a member function to represent
         each state. Each member function can then wait for specific sets of incoming messages and handle them when they arrive, possibly
         triggering a switch to another state. Each distinct message type is represented by a separate <kbd class="calibre17">struct</kbd>. <a href="#ch04ex15" class="calibre4">Listing 4.15</a> shows part of a simple implementation of the ATM logic in such a system, with the main loop and the implementation of the
         first state, waiting for the card to be inserted.
      </p>
      
      <p class="noind">As you can see, all the necessary synchronization for the message passing is entirely hidden inside the message-passing library
         (a basic implementation of which is given in <a href="kindle_split_024.html#app03" class="calibre4">appendix C</a>, along with the full code for this example).
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex15">Listing 4.15. <a id="ch04ex15__title" class="calibre4"></a>A simple implementation of an ATM logic class
      </h5>
      <pre id="PLd0e12263" class="calibre5">struct card_inserted
{
    std::string account;
};
class atm
{
    messaging::receiver incoming;
    messaging::sender bank;
    messaging::sender interface_hardware;
    void (atm::*state)();
    std::string account;
    std::string pin;
    void waiting_for_card()                              <b class="calibre24"><i class="calibre6">1</i></b>
    {
        interface_hardware.send(display_enter_card());   <b class="calibre24"><i class="calibre6">2</i></b>
        incoming.wait()                                  <b class="calibre24"><i class="calibre6">3</i></b>
            .handle&lt;card_inserted&gt;(
                [&amp;](card_inserted const&amp; msg)            <b class="calibre24"><i class="calibre6">4</i></b>
                {
                    account=msg.account;
                    pin="";
                    interface_hardware.send(display_enter_pin());
                    state=&amp;atm::getting_pin;
                }
                );
    }
    void getting_pin();
public:
    void run()                                           <b class="calibre24"><i class="calibre6">5</i></b>
    {
        state=&amp;atm::waiting_for_card;                    <b class="calibre24"><i class="calibre6">6</i></b>
        try
        {
            for(;;)
            {
                (this-&gt;*state)();                        <b class="calibre24"><i class="calibre6">7</i></b>
            }
        }
        catch(messaging::close_queue const&amp;)
        {
        }
    }
};</pre>
      
      <p class="noind"><a id="iddle1009" class="calibre4"></a><a id="iddle1438" class="calibre4"></a><a id="iddle1870" class="calibre4"></a>As already mentioned, the implementation described here is grossly simplified from the real logic that would be required in
         an ATM, but it does give you a feel for the message-passing style of programming. There’s no need to think about synchronization
         and concurrency issues, just which messages may be received at any given point and which messages to send. The state machine
         for this ATM logic runs on a single thread, with other parts of the system such as the interface to the bank and the terminal
         interface running on separate threads. This style of program design is called the <i class="calibre6">Actor model</i>—there are several discrete <i class="calibre6">actors</i> in the system (each running on a separate thread), which send messages to each other to perform the task at hand, and there’s
         no shared state except that which is directly passed via messages.
      </p>
      
      <p class="noind">Execution starts with the <kbd class="calibre17">run()</kbd> member function <b class="calibre24"><i class="calibre6">5</i></b>, which sets the initial state to <kbd class="calibre17">waiting_for_card</kbd> <b class="calibre24"><i class="calibre6">6</i></b> and then repeatedly executes the member function representing the current state (whatever it is) <b class="calibre24"><i class="calibre6">7</i></b>. The state functions are simple member functions of the <kbd class="calibre17">atm</kbd> class. The <kbd class="calibre17">waiting_for_card</kbd> state function <b class="calibre24"><i class="calibre6">1</i></b> is also simple: it sends a message to the interface to display a “waiting for card” message <b class="calibre24"><i class="calibre6">2</i></b>, and then waits for a message to handle <b class="calibre24"><i class="calibre6">3</i></b>. The only type of message that can be handled here is a <kbd class="calibre17">card_inserted</kbd> message, which you handle with a lambda function <b class="calibre24"><i class="calibre6">4</i></b>. You could pass any function or function object to the <kbd class="calibre17">handle</kbd> function, but for a simple case like this, it’s easiest to use a lambda. Note that the <kbd class="calibre17">handle()</kbd> function call is chained onto the <kbd class="calibre17">wait()</kbd> function; if a message is received that doesn’t match the specified type, it’s discarded, and the thread continues to wait
         until a matching message is received.
      </p>
      
      <p class="noind">The lambda function itself caches the account number from the card in a member variable, clears the current PIN, sends a message
         to the interface hardware to display something asking the user to enter their PIN, and changes to the “getting PIN” state.
         Once the message handler has completed, the <kbd class="calibre17">state</kbd> function returns, and the main loop then calls the new state function <b class="calibre24"><i class="calibre6">7</i></b>.
      </p>
      
      <p class="noind">The <kbd class="calibre17">getting_pin</kbd> state function is a bit more complex in that it can handle three distinct types of message, as in <a href="#ch04fig03" class="calibre4">figure 4.3</a>. This is shown in the following listing.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex16">Listing 4.16. <a id="ch04ex16__title" class="calibre4"></a>The <kbd class="calibre17">getting_pin</kbd> state function for the simple ATM implementation
      </h5>
      <pre id="PLd0e12405" class="calibre5">void atm::getting_pin()
{
    incoming.wait()
        .handle&lt;digit_pressed&gt;(                    <b class="calibre24"><i class="calibre6">1</i></b>
            [&amp;](digit_pressed const&amp; msg)
            {
                unsigned const pin_length=4;
                pin+=msg.digit;
                if(pin.length()==pin_length)
                {
                    bank.send(verify_pin(account,pin,incoming));
                    state=&amp;atm::verifying_pin;
                }
            }
            )
        .handle&lt;clear_last_pressed&gt;(               <b class="calibre24"><i class="calibre6">2</i></b>
            [&amp;](clear_last_pressed const&amp; msg)
            {
                if(!pin.empty())
                {
                    pin.resize(pin.length()-1);
                }
            }
            )
        .handle&lt;cancel_pressed&gt;(                   <b class="calibre24"><i class="calibre6">3</i></b>
            [&amp;](cancel_pressed const&amp; msg)
            {
                state=&amp;atm::done_processing;
            }
            );
}</pre>
      
      <p class="noind"><a id="iddle1116" class="calibre4"></a><a id="iddle1147" class="calibre4"></a><a id="iddle1185" class="calibre4"></a><a id="iddle1212" class="calibre4"></a><a id="iddle2135" class="calibre4"></a><a id="iddle2622" class="calibre4"></a>This time, there are three message types you can process, so the <kbd class="calibre17">wait()</kbd> function has three <kbd class="calibre17">handle()</kbd> calls chained on the end, <b class="calibre24"><i class="calibre6">1</i></b>, <b class="calibre24"><i class="calibre6">2</i></b>, and <b class="calibre24"><i class="calibre6">3</i></b>. Each call to <kbd class="calibre17">handle()</kbd> specifies the message type as the template parameter and then passes in a lambda function that takes that particular message
         type as a parameter. Because the calls are chained together in this way, the <kbd class="calibre17">wait()</kbd> implementation knows that it’s waiting for a <kbd class="calibre17">digit_pressed</kbd> message, a <kbd class="calibre17">clear_last_pressed</kbd> message, or a <kbd class="calibre17">cancel_pressed</kbd> message. Messages of any other type are again discarded.
      </p>
      
      <p class="noind">This time, you don’t necessarily change state when you get a message. For example, if you get a <kbd class="calibre17">digit_pressed</kbd> message, you add it to the <kbd class="calibre17">pin</kbd> unless it’s the final digit. The main loop <b class="calibre24"><i class="calibre6">7</i></b> in <a href="#ch04ex15" class="calibre4">listing 4.15</a> will then call <kbd class="calibre17">getting_pin()</kbd> again to wait for the next digit (or clear or cancel).
      </p>
      
      <p class="noind">This corresponds to the behavior shown in <a href="#ch04fig03" class="calibre4">figure 4.3</a>. Each state box is implemented by a distinct member function, which waits for the relevant messages and updates the state
         as appropriate.
      </p>
      
      <p class="noind">As you can see, this style of programming can greatly simplify the task of designing a concurrent system, because each thread
         can be treated entirely independently. It is an example of using multiple threads to separate concerns and as such requires
         you to explicitly decide how to divide the tasks between threads.
      </p>
      
      <p class="noind">Back in <a href="#ch04lev1sec2" class="calibre4">section 4.2</a>, I mentioned that the Concurrency TS provides extended versions of futures. The core part of the extensions is the ability
         to specify <i class="calibre6">continuations</i>—<a id="iddle1372" class="calibre4"></a><a id="iddle1373" class="calibre4"></a><a id="iddle1402" class="calibre4"></a><a id="iddle1403" class="calibre4"></a><a id="iddle2631" class="calibre4"></a><a id="iddle2660" class="calibre4"></a>additional functions that are run automatically when the future becomes <i class="calibre6">ready</i>. Let’s take the opportunity to explore how this can simplify our code.
      </p>
      
      
      
      <h4 id="ch04lev2sec14" class="calibre23">4.4.3. <a id="ch04lev2sec14__title" class="calibre4"></a>Continuation-style concurrency with the Concurrency TS
      </h4>
      
      <p class="noind">The Concurrency TS provides new versions of <kbd class="calibre17">std::promise</kbd> and <kbd class="calibre17">std::packaged_task</kbd> in the <kbd class="calibre17">std::experimental</kbd> namespace that all differ from their <kbd class="calibre17">std</kbd> originals in the same way: they return instances of <kbd class="calibre17">std::experimental::future</kbd> rather than <kbd class="calibre17">std::future</kbd>. This enables users to take advantage of the key new feature in <kbd class="calibre17">std::experimental::future</kbd>—<i class="calibre6">continuations</i>.
      </p>
      
      <p class="noind">Suppose you have a task running that will produce a result, and a future that will hold the result when it becomes available.
         You then have some code that needs to run in order to process that result. With <kbd class="calibre17">std::future</kbd> you would have to wait for the future to become ready, either with the fully-blocking <kbd class="calibre17">wait()</kbd> member function or either of the <kbd class="calibre17">wait_for()</kbd> or <kbd class="calibre17">wait_until()</kbd> member functions to allow a wait with a <kbd class="calibre17">timeout</kbd>. This can be inconvenient, and can complicate the code. What you want is a means of saying “When the data is ready, <i class="calibre6">then</i> do this processing”. This is exactly what continuations give us; unsurprisingly, the member function to add a continuation
         to a future is called <kbd class="calibre17">then()</kbd>. Given a future <kbd class="calibre17">fut</kbd>, a continuation is added with the call <kbd class="calibre17">fut.then(continuation)</kbd>.
      </p>
      
      <p class="noind">Just like <kbd class="calibre17">std::future</kbd>, <kbd class="calibre17">std::experimental::future</kbd> only allows the stored value to be retrieved once. If that value is being consumed by a continuation, this means it cannot
         be accessed by other code. Consequently, when a continuation is added with <kbd class="calibre17">fut.then()</kbd>, the original future, <kbd class="calibre17">fut</kbd>, becomes <i class="calibre6">invalid</i>. Instead, the call to <kbd class="calibre17">fut.then()</kbd> returns a new future to hold the result of the continuation call. This is shown in the following code:
      </p>
      
      <pre id="PLd0e12659" class="calibre5">std::experimental::future&lt;int&gt; find_the_answer;
auto fut=find_the_answer();
auto fut2=fut.then(find_the_question);
assert(!fut.valid());
assert(fut2.valid());</pre>
      
      <p class="noind">The <kbd class="calibre17">find_the_question</kbd> continuation function is scheduled to run “on an unspecified thread” when the original future is <i class="calibre6">ready</i>. This gives the implementation freedom to run it on a thread pool or another library-managed thread. As it stands, this gives
         the implementation a lot of freedom; this is deliberate, with the intention that when continuations are added to a future
         C++ Standard, the implementers will be able to draw on their experience to better specify the choice of threads and provide
         users with suitable mechanisms for controlling the choice of threads.
      </p>
      
      <p class="noind">Unlike direct calls to <kbd class="calibre17">std::async</kbd> or <kbd class="calibre17">std::thread</kbd>, you cannot pass arguments to a continuation function, because the argument is already defined by the library—the continuation
         is passed a <i class="calibre6">ready</i> future that holds the result that triggered the continuation. Assuming your <kbd class="calibre17">find_the_answer</kbd> function returns an <kbd class="calibre17">int</kbd>, the <kbd class="calibre17">find_the_question</kbd> <a id="iddle1099" class="calibre4"></a><a id="iddle1115" class="calibre4"></a><a id="iddle1211" class="calibre4"></a><a id="iddle1898" class="calibre4"></a><a id="iddle1901" class="calibre4"></a><a id="iddle2473" class="calibre4"></a><a id="iddle2492" class="calibre4"></a>function referenced in the previous example must take a <kbd class="calibre17">std::experimental:: future&lt;int&gt;</kbd> as its sole parameter; for example:
      </p>
      
      <pre id="PLd0e12741" class="calibre5">std::string find_the_question(std::experimental::future&lt;int&gt; the_answer);</pre>
      
      <p class="noind">The reason for this is that the future on which the continuation was chained may end up holding a value or an exception. If
         the future was implicitly dereferenced to pass the value directly to the continuation, then the library would have to decide
         how to handle the exception, whereas by passing the future to the continuation, the continuation can handle the exception.
         In simple cases, this may be done by calling <kbd class="calibre17">fut.get()</kbd> and allowing the re-thrown exception to propagate out of the continuation function. Just as for functions passed to <kbd class="calibre17">std::async</kbd>, exceptions that escape a continuation are stored in the future that holds the continuation result.
      </p>
      
      <p class="noind">Note that the Concurrency TS doesn’t specify that there is an equivalent to <kbd class="calibre17">std::async</kbd>, though implementations may provide one as an extension. Writing such a function is fairly straightforward: use <kbd class="calibre17">std::experimental::promise</kbd> to obtain a future, and then spawn a new thread running a lambda that sets the promise’s value to the return value of the
         supplied function, as in the next listing.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex17">Listing 4.17. <a id="ch04ex17__title" class="calibre4"></a>A simple equivalent to <kbd class="calibre17">std::async</kbd> for Concurrency TS futures
      </h5>
      <pre id="PLd0e12771" class="calibre5">template&lt;typename Func&gt;
std::experimental::future&lt;decltype(std::declval&lt;Func&gt;()())&gt;
spawn_async(Func&amp;&amp; func){
    std::experimental::promise&lt;
        decltype(std::declval&lt;Func&gt;()())&gt; p;
    auto res=p.get_future();
    std::thread t(
        [p=std::move(p),f=std::decay_t&lt;Func&gt;(func)]()
            mutable{
            try{
                p.set_value_at_thread_exit(f());
            } catch(...){
                p.set_exception_at_thread_exit(std::current_exception());
            }
    });
    t.detach();
    return res;
}</pre>
      
      <p class="noind">This stores the result of the function in the future, or catches the exception thrown from the function and stores that in
         the future, just as <kbd class="calibre17">std::async</kbd> does. Also, it uses <kbd class="calibre17">set_value_at_thread_exit</kbd> and <kbd class="calibre17">set_exception_at_thread_exit</kbd> to ensure that <kbd class="calibre17">thread_local</kbd> variables have been properly cleaned up before the future becomes ready.
      </p>
      
      <p class="noind">The value returned from a <kbd class="calibre17">then()</kbd> call is a fully-fledged future itself. This means that you can chain continuations.
      </p>
      
      
      
      
      <h4 id="ch04lev2sec15" class="calibre23">4.4.4. <a id="ch04lev2sec15__title" class="calibre4"></a>Chaining continuations
      </h4>
      
      <p class="noind">Suppose you have a series of time-consuming tasks to do, and you want to do them asynchronously in order to free up the main
         thread for other tasks. For example, when the user logs in to your application, you might need to send the credentials to
         the backend for authentication; then, when the details have been authenticated, make a further request to the backend for
         information about the user’s account; and finally, when that information has been retrieved, update the display with the relevant
         information. As sequential code, you might write something like the following listing.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex18">Listing 4.18. <a id="ch04ex18__title" class="calibre4"></a>A simple sequential function to process user login
      </h5>
      <pre id="PLd0e12812" class="calibre5">void process_login(std::string const&amp; username,std::string const&amp; password)
{
    try {
        user_id const id=backend.authenticate_user(username,password);
        user_data const info_to_display=backend.request_current_info(id);
        update_display(info_to_display);
    } catch(std::exception&amp; e){
        display_error(e);
    }
}</pre>
      
      <p class="noind">However, you don’t want sequential code; you want asynchronous code so you’re not blocking the UI thread. With plain <kbd class="calibre17">std::async</kbd>, you could punt it all to a background thread like the next listing, but that would still block that thread, consuming resources
         while waiting for the tasks to complete. If you have many such tasks, then you can end up with a large number of threads that
         are doing nothing except waiting.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex19">Listing 4.19. <a id="ch04ex19__title" class="calibre4"></a>Processing user login with a single async task
      </h5>
      <pre id="PLd0e12827" class="calibre5">std::future&lt;void&gt; process_login(
    std::string const&amp; username,std::string const&amp; password)
{
    return std::async(std::launch::async,[=](){
        try {
            user_id const id=backend.authenticate_user(username,password);
            user_data const info_to_display=
                backend.request_current_info(id);
            update_display(info_to_display);
        } catch(std::exception&amp; e){
            display_error(e);
        }
    });
}</pre>
      
      <p class="noind">In order to avoid all these blocked threads, you need some mechanism for chaining tasks as they each complete: continuations.
         The following listing shows the same overall process, but this time split into a series of tasks, each of which is then chained
         on the previous one as a continuation.
      </p>
      
      
      <p class="noind"></p>
      
      
      <h5 class="notetitle" id="ch04ex20">Listing 4.20. <a id="ch04ex20__title" class="calibre4"></a>A function to process user login with continuations
      </h5>
      <pre id="PLd0e12842" class="calibre5">std::experimental::future&lt;void&gt; process_login(
    std::string const&amp; username,std::string const&amp; password)
{
    return spawn_async([=](){
        return backend.authenticate_user(username,password);
    }).then([](std::experimental::future&lt;user_id&gt; id){
        return backend.request_current_info(id.get());
    }).then([](std::experimental::future&lt;user_data&gt; info_to_display){
        try{
            update_display(info_to_display.get());
        } catch(std::exception&amp; e){
            display_error(e);
        }
    });
}</pre>
      
      <p class="noind"><a id="iddle1462" class="calibre4"></a>Note how each continuation takes a <kbd class="calibre17">std::experimental::future</kbd> as the sole parameter, and then uses <kbd class="calibre17">.get()</kbd> to retrieve the contained value. This means that exceptions get propagated all the way down the chain, so the call to <kbd class="calibre17">info_to_display.get()</kbd> in the final continuation will throw if any of the functions in the chain threw an exception, and the catch block here can
         handle all the exceptions, just like the <kbd class="calibre17">catch</kbd> block in <a href="#ch04ex18" class="calibre4">listing 4.18</a> did.
      </p>
      
      <p class="noind">If the function calls to the backend block internally because they’re waiting for messages to cross the network or for a database
         operation to complete, then you’re not done yet. You may have split the task into its individual parts, but they’re still
         blocking calls, so you still get blocked threads. What you need is for the backend calls to return futures that become ready
         when the data is ready, without blocking any threads. In this case, <kbd class="calibre17">backend.async_authenticate_user(username,password)</kbd> will now return a <kbd class="calibre17">std::experimental::future&lt;user_id&gt;</kbd> rather than a plain <kbd class="calibre17">user_id</kbd>.
      </p>
      
      <p class="noind">You might think this would complicate the code, because returning a future from a continuation would give you <kbd class="calibre17">future&lt;future&lt;some_value&gt;&gt;</kbd>, or else you’d have to put the <kbd class="calibre17">.then</kbd> calls inside the continuations. Thankfully, if you thought that, then you’d be mistaken: the continuation support has a nifty
         feature called future-unwrapping. If the continuation function you pass to a <kbd class="calibre17">.then()</kbd> call returns a <kbd class="calibre17">future&lt;some_type&gt;</kbd>, then the <kbd class="calibre17">.then()</kbd> call will return a <kbd class="calibre17">future&lt;some_type&gt;</kbd> in turn. This means your final code looks like the next listing, and there is no blocking in your asynchronous function chain.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex21">Listing 4.21. <a id="ch04ex21__title" class="calibre4"></a>A function to process user login with fully asynchronous operations
      </h5>
      <pre id="PLd0e12909" class="calibre5">std::experimental::future&lt;void&gt; process_login(
    std::string const&amp; username,std::string const&amp; password)
{
    return backend.async_authenticate_user(username,password).then(
        [](std::experimental::future&lt;user_id&gt; id){
            return backend.async_request_current_info(id.get());
        }).then([](std::experimental::future&lt;user_data&gt; info_to_display){
            try{
                update_display(info_to_display.get());
            } catch(std::exception&amp; e){
                display_error(e);
            }
        });
}</pre>
      
      <p class="noind"><a id="iddle2136" class="calibre4"></a><a id="iddle2140" class="calibre4"></a>This is almost as straightforward as the sequential code from <a href="#ch04ex18" class="calibre4">listing 4.18</a>, with a little bit more boilerplate around the <kbd class="calibre17">.then</kbd> calls and the lambda declarations. If your compiler supports C++14 generic lambdas, then the types of the futures in the
         lambda parameters can be replaced with <kbd class="calibre17">auto</kbd>, which simplifies the code even further:
      </p>
      
      <pre id="PLd0e12939" class="calibre5">return backend.async_authenticate_user(username,password).then(
        [](<b class="calibre24">auto</b> id){
            return backend.async_request_current_info(id.get());
        });</pre>
      
      <p class="noind">If you need anything more complex than simple linear control flow, then you can implement this by putting the logic in one
         of the lambdas; for truly complex control flow you probably need to write a separate function.
      </p>
      
      <p class="noind">So far, we’ve focused on the continuation support in <kbd class="calibre17">std::experimental::future</kbd>. As you might expect, <kbd class="calibre17">std::experimental::shared_future</kbd> also supports continuations. The difference here is that <kbd class="calibre17">std::experimental::shared_future</kbd> objects can have more than one continuation, and the continuation parameter is a <kbd class="calibre17">std::experimental:: shared_future</kbd> rather than a <kbd class="calibre17">std::experimental::future</kbd>. This naturally falls out of the shared nature of <kbd class="calibre17">std::experimental::shared_future</kbd>—because multiple objects can refer to the same shared state, if only one continuation was allowed, there would be a race
         condition between two threads that were each trying to add continuations to their own <kbd class="calibre17">std::experimental::shared_future</kbd> objects. This is obviously undesirable, so multiple continuations are permitted. Once you have multiple continuations permitted,
         you may as well allow them to be added via the same <kbd class="calibre17">std::experimental:: shared_future</kbd> instance, rather than only allowing one continuation per object. In addition, you can’t package the shared state in a one-shot
         <kbd class="calibre17">std::experimental:: future</kbd> passed to the first continuation, when you’re going to want to also pass it to the second continuation. Thus the parameter
         passed to the continuation function must also be a <kbd class="calibre17">std::experimental::shared_future</kbd>:
      </p>
      
      <pre id="PLd0e12984" class="calibre5">auto fut=spawn_async(some_function).share();
auto fut2=fut.then([](std::experimental::shared_future&lt;some_data&gt; data){
    do_stuff(data);
    });
auto fut3=fut.then([](std::experimental::shared_future&lt;some_data&gt; data){
    return do_other_stuff(data);
    });</pre>
      
      <p class="noind"><kbd class="calibre17">fut</kbd> is a <kbd class="calibre17">std::experimental::shared_future</kbd> due to the <kbd class="calibre17">share()</kbd> call, so the continuation function must take a <kbd class="calibre17">std::experimental::shared_future</kbd> as its parameter. <a id="iddle1124" class="calibre4"></a><a id="iddle1415" class="calibre4"></a><a id="iddle2142" class="calibre4"></a><a id="iddle2643" class="calibre4"></a>However, the return value from the continuation is a plain <kbd class="calibre17">std::experimental:: future</kbd>—that value isn’t currently shared until you do something to share it—so both <kbd class="calibre17">fut2</kbd> and <kbd class="calibre17">fut3</kbd> are <kbd class="calibre17">std::experimental::future</kbd>s.
      </p>
      
      <p class="noind">Continuations aren’t the only enhancement to futures in the Concurrency TS, though they are probably the most important. Also
         provided are two overloaded functions that allow you to wait for either <i class="calibre6">any one</i> of a bunch of futures to become ready, or <i class="calibre6">all</i> of a bunch of futures to become ready.
      </p>
      
      
      
      <h4 id="ch04lev2sec16" class="calibre23">4.4.5. <a id="ch04lev2sec16__title" class="calibre4"></a>Waiting for more than one future
      </h4>
      
      <p class="noind">Suppose you have a large volume of data to process, and each item can be processed independently. This is a prime opportunity
         to make use of the available hardware by spawning a set of asynchronous tasks to process the data items, each of them returning
         the processed data via a future. However, if you need to wait for all the tasks to finish and then gather all the results
         for some final processing, this can be inconvenient—you have to wait for each future in turn, and then gather the results.
         If you want to do the result gathering with another asynchronous task, then you either have to spawn it up front so it is
         occupying a thread that’s waiting, or you have to keep polling the futures and spawn the new task when all the futures are
         <i class="calibre6">ready</i>. An example of such code is shown in the following listing.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex22">Listing 4.22. <a id="ch04ex22__title" class="calibre4"></a>Gathering results from futures using <kbd class="calibre17">std::async</kbd></h5>
      <pre id="PLd0e13076" class="calibre5">std::future&lt;FinalResult&gt; process_data(std::vector&lt;MyData&gt;&amp; vec)
{
    size_t const chunk_size=<i class="calibre6">whatever</i>;
    std::vector&lt;std::future&lt;ChunkResult&gt;&gt; results;
    for(auto begin=vec.begin(),end=vec.end();beg!=end;){
        size_t const remaining_size=end-begin;
        size_t const this_chunk_size=std::min(remaining_size,chunk_size);
        results.push_back(
            std::async(process_chunk,begin,begin+this_chunk_size));
        begin+=this_chunk_size;
    }
    return std::async([all_results=std::move(results)](){
        std::vector&lt;ChunkResult&gt; v;
        v.reserve(all_results.size());
        for(auto&amp; f: all_results)
        {
            v.push_back(f.get());       <b class="calibre24"><i class="calibre6">1</i></b>
        }
        return gather_results(v);
    });
}</pre>
      
      <p class="noind">This code spawns a new asynchronous task to wait for the results, and then processes them when they are all available. However,
         because it waits for each task individually, it will repeatedly be woken by the scheduler at <b class="calibre24"><i class="calibre6">1</i></b> as each result becomes available, and then go back to sleep again when it finds another result that is not yet ready. Not
         only <a id="iddle1123" class="calibre4"></a><a id="iddle1414" class="calibre4"></a><a id="iddle2641" class="calibre4"></a><a id="iddle2661" class="calibre4"></a><a id="iddle2662" class="calibre4"></a>does this occupy the thread doing the waiting, but it adds additional context switches as each future becomes ready, which
         adds additional overhead.
      </p>
      
      <p class="noind">With <kbd class="calibre17">std::experimental::when_all</kbd>, this waiting and switching can be avoided. You pass the set of futures to be waited on to <kbd class="calibre17">when_all</kbd>, and it returns a new future that becomes ready when all the futures in the set are ready. This future can then be used with
         continuations to schedule additional work when the all the futures are ready. See, for example, the next listing.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex23">Listing 4.23. <a id="ch04ex23__title" class="calibre4"></a>Gathering results from futures using <kbd class="calibre17">std::experimental::when_all</kbd></h5>
      <pre id="PLd0e13149" class="calibre5">std::experimental::future&lt;FinalResult&gt; process_data(
    std::vector&lt;MyData&gt;&amp; vec)
{
    size_t const chunk_size=<i class="calibre6">whatever</i>;
    std::vector&lt;std::experimental::future&lt;ChunkResult&gt;&gt; results;
    for(auto begin=vec.begin(),end=vec.end();beg!=end;){
        size_t const remaining_size=end-begin;
        size_t const this_chunk_size=std::min(remaining_size,chunk_size);
        results.push_back(
            spawn_async(
            process_chunk,begin,begin+this_chunk_size));
        begin+=this_chunk_size;
    }
    return std::experimental::when_all(
        results.begin(),results.end()).then(     <b class="calibre24"><i class="calibre6">1</i></b>
        [](std::future&lt;std::vector&lt;
             std::experimental::future&lt;ChunkResult&gt;&gt;&gt; ready_results)
        {
            std::vector&lt;std::experimental::future&lt;ChunkResult&gt;&gt;
                all_results=ready_results .get();
            std::vector&lt;ChunkResult&gt; v;
            v.reserve(all_results.size());
            for(auto&amp; f: all_results)
            {
                v.push_back(f.get());            <b class="calibre24"><i class="calibre6">2</i></b>
            }
            return gather_results(v);
        });
}</pre>
      
      <p class="noind">In this case, you use <kbd class="calibre17">when_all</kbd> to wait for all the futures to become ready, and then schedule the function using <kbd class="calibre17">.then</kbd> rather than <kbd class="calibre17">async</kbd> <b class="calibre24"><i class="calibre6">1</i></b>. Though the lambda is superficially the same, it takes the <kbd class="calibre17">results</kbd> vector as a parameter (wrapped in a future) rather than as a capture, and the calls to <kbd class="calibre17">get</kbd> on the futures at <b class="calibre24"><i class="calibre6">2</i></b> do not block, as all the values are ready by the time execution gets there. This has the potential to reduce the load on
         the system for little change to the code.
      </p>
      
      <p class="noind">To complement <kbd class="calibre17">when_all</kbd>, we also have <kbd class="calibre17">when_any</kbd>. This creates a future that becomes ready when <i class="calibre6">any</i> of the supplied futures becomes ready. This works well for scenarios where you’ve spawned multiple tasks to take advantage
         of the available concurrency, but need to do something when the first one becomes ready.
      </p>
      
      
      
      
      <h4 id="ch04lev2sec17" class="calibre23">4.4.6. <a id="ch04lev2sec17__title" class="calibre4"></a>Waiting for the first future in a set with when_any
      </h4>
      
      <p class="noind"><a id="iddle2143" class="calibre4"></a>Suppose you are searching a large dataset for a value that meets particular criteria, but if there are multiple such values,
         then any will do. This is a prime target for parallelism—you can spawn multiple threads, each of which checks a subset of
         the data; if a given thread finds a suitable value, then it sets a flag indicating that the other threads should stop their
         search, and then sets the final return value. In this case, you want to do the further processing when the first task completes
         its search, even if the other tasks haven’t finished cleaning up yet.
      </p>
      
      <p class="noind">Here, you can use <kbd class="calibre17">std::experimental::when_any</kbd> to gather the futures together, and provide a new future that is ready when at least one of the original set is ready. Whereas
         <kbd class="calibre17">when_all</kbd> gave you a future that wrapped the collection of futures you passed in, <kbd class="calibre17">when_any</kbd> adds a further layer, combining the collection with an index value that indicates which future triggered the combined future
         to be ready into an instance of the <kbd class="calibre17">std::experimental::when_any_result</kbd> class template.
      </p>
      
      <p class="noind">An example of using <kbd class="calibre17">when_any</kbd> as described here is shown in the next listing.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex24">Listing 4.24. <a id="ch04ex24__title" class="calibre4"></a>Using <kbd class="calibre17">std::experimental::when_any</kbd> to process the first value found
      </h5>
      <pre id="PLd0e13250" class="calibre5">std::experimental::future&lt;FinalResult&gt;
find_and_process_value(std::vector&lt;MyData&gt; &amp;data)
{
    unsigned const concurrency = std::thread::hardware_concurrency();
    unsigned const num_tasks = (concurrency &gt; 0) ? concurrency : 2;
    std::vector&lt;std::experimental::future&lt;MyData *&gt;&gt; results;
    auto const chunk_size = (data.size() + num_tasks - 1) / num_tasks;
    auto chunk_begin = data.begin();
    std::shared_ptr&lt;std::atomic&lt;bool&gt;&gt; done_flag =
        std::make_shared&lt;std::atomic&lt;bool&gt;&gt;(false);
    for (unsigned i = 0; i &lt; num_tasks; ++i) {                            <b class="calibre24"><i class="calibre6">1</i></b>
        auto chunk_end =
            (i &lt; (num_tasks - 1)) ? chunk_begin + chunk_size : data.end();
        results.push_back(spawn_async([=] {                               <b class="calibre24"><i class="calibre6">2</i></b>
            for (auto entry = chunk_begin;
                !*done_flag &amp;&amp; (entry != chunk_end);
                 ++entry) {
                if (matches_find_criteria(*entry)) {
                    *done_flag = true;
                    return &amp;*entry;
                }
            }
            return (MyData *)nullptr;
        }));
        chunk_begin = chunk_end;
    }
    std::shared_ptr&lt;std::experimental::promise&lt;FinalResult&gt;&gt; final_result =
        std::make_shared&lt;std::experimental::promise&lt;FinalResult&gt;&gt;();
    struct DoneCheck {
        std::shared_ptr&lt;std::experimental::promise&lt;FinalResult&gt;&gt;
            final_result;

        DoneCheck(
            std::shared_ptr&lt;std::experimental::promise&lt;FinalResult&gt;&gt;
                final_result_)
            : final_result(std::move(final_result_)) {}

        void operator()(                                                  <b class="calibre24"><i class="calibre6">4</i></b>
            std::experimental::future&lt;std::experimental::when_any_result&lt;
                std::vector&lt;std::experimental::future&lt;MyData *&gt;&gt;&gt;&gt;
                results_param) {
            auto results = results_param.get();
            MyData *const ready_result =
                results.futures[results.index].get();                     <b class="calibre24"><i class="calibre6">5</i></b>
            if (ready_result)
                final_result-&gt;set_value(                                  <b class="calibre24"><i class="calibre6">6</i></b>
                    process_found_value(*ready_result));
            else {
                results.futures.erase(
                    results.futures.begin() + results.index);             <b class="calibre24"><i class="calibre6">7</i></b>
                if (!results.futures.empty()) {
                    std::experimental::when_any(                          <b class="calibre24"><i class="calibre6">8</i></b>
                        results.futures.begin(), results.futures.end())
                        .then(std::move(*this));
            } else {
                final_result-&gt;set_exception(
                    std::make_exception_ptr(                              <b class="calibre24"><i class="calibre6">9</i></b>
                        std::runtime_error("Not found")));
            }
        }
    };

    std::experimental::when_any(results.begin(), results.end())
        .then(DoneCheck(final_result));                                   <b class="calibre24"><i class="calibre6">3</i></b>
    return final_result-&gt;get_future();                                    <b class="calibre24"><i class="calibre6">10</i></b>
}</pre>
      
      <p class="noind"><a id="iddle1299" class="calibre4"></a>The initial loop <b class="calibre24"><i class="calibre6">1</i></b> spawns off <kbd class="calibre17">num_tasks</kbd> asynchronous tasks, each running the lambda function from <b class="calibre24"><i class="calibre6">2</i></b>. This lambda captures by copying, so each task will have its own values for <kbd class="calibre17">chunk_begin</kbd> and <kbd class="calibre17">chunk_end</kbd>, as well as a copy of the shared pointer, <kbd class="calibre17">done_flag</kbd>. This avoids any concerns over lifetime issues.
      </p>
      
      <p class="noind">Once all the tasks have been spawned, you want to handle the case that a task returned. This is done by chaining a continuation
         on the <kbd class="calibre17">when_any</kbd> call <b class="calibre24"><i class="calibre6">3</i></b>. This time you write the continuation as a class because you want to reuse it recursively. When one of the initial tasks
         is ready, the <kbd class="calibre17">DoneCheck</kbd> function call operator is invoked <b class="calibre24"><i class="calibre6">4</i></b>. First, it extracts the value from the future that is ready <b class="calibre24"><i class="calibre6">5</i></b>, and then if the value was found, you process it and set the final result <b class="calibre24"><i class="calibre6">6</i></b>. Otherwise, you drop the ready future from the collection <b class="calibre24"><i class="calibre6">7</i></b>, and if there are still more futures to check, issue a new call to <kbd class="calibre17">when_any</kbd> <b class="calibre24"><i class="calibre6">8</i></b>, that will trigger its continuation when the next future is ready. If there are no futures left, then none of them found
         the value, so store an exception instead <b class="calibre24"><i class="calibre6">9</i></b>. The return value of the function is the future for the final result <b class="calibre24"><i class="calibre6">10</i></b>. There are alternative ways to solve this problem, but I hope this shows how one might use <kbd class="calibre17">when_any</kbd>.
      </p>
      
      <p class="noind"><a id="iddle1064" class="calibre4"></a><a id="iddle1114" class="calibre4"></a><a id="iddle1118" class="calibre4"></a><a id="iddle1121" class="calibre4"></a><a id="iddle1184" class="calibre4"></a><a id="iddle1186" class="calibre4"></a><a id="iddle1223" class="calibre4"></a><a id="iddle1507" class="calibre4"></a><a id="iddle2138" class="calibre4"></a>Both these examples of using <kbd class="calibre17">when_all</kbd> and <kbd class="calibre17">when_any</kbd> have used the iterator-range overloads, which take a pair of iterators denoting the beginning and end of a set of futures
         to wait for. Both functions also come in variadic forms, where they accept a number of futures directly as parameters to the
         function. In this case, the result is a future holding a tuple (or a <kbd class="calibre17">when_any_result</kbd> holding a tuple) rather than a vector:
      </p>
      
      <pre id="PLd0e13455" class="calibre5">std::experimental::future&lt;int&gt; f1=spawn_async(func1);
std::experimental::future&lt;std::string&gt; f2=spawn_async(func2);
std::experimental::future&lt;double&gt; f3=spawn_async(func3);
std::experimental::future&lt;
    std::tuple&lt;
        std::experimental::future&lt;int&gt;,
        std::experimental::future&lt;std::string&gt;,
        std::experimental::future&lt;double&gt;&gt;&gt; result=
    std::experimental::when_all(std::move(f1),std::move(f2),std::move(f3));</pre>
      
      <p class="noind">This example highlights something important about all the uses of <kbd class="calibre17">when_any</kbd> and <kbd class="calibre17">when_all</kbd>—they always move from any <kbd class="calibre17">std::experimental::future</kbd>s passed in via a container, and they take their parameters by value, so you have to explicitly move the futures in, or pass
         temporaries.
      </p>
      
      <p class="noind">Sometimes the event that you’re waiting for is for a set of threads to reach a particular point in the code, or to have processed
         a certain number of data items between them. In these cases, you might be better served using a latch or a barrier rather
         than a future. Let’s look at the latches and barriers that are provided by the Concurrency TS.
      </p>
      
      
      
      <h4 id="ch04lev2sec18" class="calibre23">4.4.7. <a id="ch04lev2sec18__title" class="calibre4"></a>Latches and barriers in the Concurrency TS
      </h4>
      
      <p class="noind">First off, let’s consider what is meant when we talk of a <i class="calibre6">latch</i> or a <i class="calibre6">barrier</i>. A <i class="calibre6">latch</i> is a synchronization object that becomes ready when its counter is decremented to zero. Its name comes from the fact that
         it <i class="calibre6">latches</i> the output—once it is ready, it stays ready until it is destroyed. A latch is thus a lightweight facility for waiting for
         a series of events to occur.
      </p>
      
      <p class="noind">On the other hand, a <i class="calibre6">barrier</i> is a reusable synchronization component used for internal synchronization between a set of threads. Whereas a latch doesn’t
         care which threads decrement the counter—the same thread can decrement the counter multiple times, or multiple threads can
         each decrement the counter once, or some combination of the two—with barriers, each thread can only arrive at the barrier
         once per cycle. When threads arrive at the barrier, they block until all of the threads involved have arrived at the barrier,
         at which point they are all released. The barrier can then be reused—the threads can then arrive at the barrier again to wait
         for all the threads for the next cycle.
      </p>
      
      <p class="noind">Latches are inherently simpler than barriers, so let’s start with the latch type from the Concurrency TS: <kbd class="calibre17">std::experimental::latch</kbd>.
      </p>
      
      
      
      
      <h4 id="ch04lev2sec19" class="calibre23">4.4.8. <a id="ch04lev2sec19__title" class="calibre4"></a>A basic latch type: std::experimental::latch
      </h4>
      
      <p class="noind"><a id="iddle1224" class="calibre4"></a><kbd class="calibre17">std::experimental::latch</kbd> comes from the <kbd class="calibre17">&lt;experimental/latch&gt;</kbd> header. When you construct a <kbd class="calibre17">std::experimental::latch</kbd>, you specify the initial counter value as the one and only argument to the constructor. Then, as the events that you are
         waiting for occur, you call <kbd class="calibre17">count_down</kbd> on your latch object, and the latch becomes ready when that count reaches zero. If you need to wait for the latch to become
         ready, then you can call <kbd class="calibre17">wait</kbd> on the latch; if you only need to check if it is ready, then you can call <kbd class="calibre17">is_ready</kbd>. Finally, if you need to both count down the counter and then wait for the counter to reach zero, you can call <kbd class="calibre17">count_down_and_wait</kbd>. A basic example is shown in the following listing.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex25">Listing 4.25. <a id="ch04ex25__title" class="calibre4"></a>Waiting for events with <kbd class="calibre17">std::experimental::latch</kbd></h5>
      <pre id="PLd0e13550" class="calibre5">void foo(){
    unsigned const thread_count=...;
    latch done(thread_count);                                    <b class="calibre24"><i class="calibre6">1</i></b>
    my_data data[thread_count];
    std::vector&lt;std::future&lt;void&gt; &gt; threads;
    for(unsigned i=0;i&lt;thread_count;++i)
        threads.push_back(std::async(std::launch::async,[&amp;,i]{   <b class="calibre24"><i class="calibre6">2</i></b>
            data[i]=make_data(i);
            done.count_down();                                   <b class="calibre24"><i class="calibre6">3</i></b>
            do_more_stuff();                                     <b class="calibre24"><i class="calibre6">4</i></b>
        }));
    done.wait();                                                 <b class="calibre24"><i class="calibre6">5</i></b>
    process_data(data,thread_count);                             <b class="calibre24"><i class="calibre6">6</i></b>
}                                                                <b class="calibre24"><i class="calibre6">7</i></b></pre>
      
      <p class="noind">This constructs <kbd class="calibre17">done</kbd> with the number of events that you need to wait for <b class="calibre24"><i class="calibre6">1</i></b>, and then spawns the appropriate number of threads using <kbd class="calibre17">std::async</kbd> <b class="calibre24"><i class="calibre6">2</i></b>. Each thread then counts down the latch when it has generated the relevant chunk of data <b class="calibre24"><i class="calibre6">3</i></b> before continuing on with further processing <b class="calibre24"><i class="calibre6">4</i></b>. The main thread can wait for all the data to be ready by waiting on the latch <b class="calibre24"><i class="calibre6">5</i></b> before processing the generated data <b class="calibre24"><i class="calibre6">6</i></b>. The data processing at <b class="calibre24"><i class="calibre6">6</i></b> will potentially run concurrently with the final processing steps of each thread <b class="calibre24"><i class="calibre6">4</i></b>—there is no guarantee that the threads have all completed until the <kbd class="calibre17">std::future</kbd> destructors run at the end of the function <b class="calibre24"><i class="calibre6">7</i></b>.
      </p>
      
      <p class="noind">One thing to note is that the lambda passed to <kbd class="calibre17">std::async</kbd> at <b class="calibre24"><i class="calibre6">2</i></b> captures everything by reference except <kbd class="calibre17">i</kbd>, which is captured by value. This is because <kbd class="calibre17">i</kbd> is the loop counter, and capturing that by reference would cause a data race and undefined behavior, whereas <kbd class="calibre17">data</kbd> and <kbd class="calibre17">done</kbd> are things you need to share access to. Also, you only need a latch at all in this scenario because the threads have additional
         processing to do after the data is ready; otherwise you could wait for all the futures to ensure the tasks were complete before
         processing the data.
      </p>
      
      <p class="noind">It is safe to access <kbd class="calibre17">data</kbd> in the <kbd class="calibre17">process_data</kbd> call <b class="calibre24"><i class="calibre6">6</i></b>, even though it is stored by tasks running in other threads, because the latch is a synchronization object, so changes visible
         to a thread that call <kbd class="calibre17">count_down</kbd> are guaranteed to be visible to a <a id="iddle1023" class="calibre4"></a><a id="iddle1119" class="calibre4"></a><a id="iddle2130" class="calibre4"></a><a id="iddle2133" class="calibre4"></a>thread that returns from a call to <kbd class="calibre17">wait</kbd> on the same latch object. Formally, the call to <kbd class="calibre17">count_down</kbd> <i class="calibre6">synchronizes</i> with the call to <kbd class="calibre17">wait</kbd>—we’ll see what that means when we look at the low-level memory ordering and synchronization constraints in <a href="kindle_split_015.html#ch05" class="calibre4">chapter 5</a>.
      </p>
      
      <p class="noind">Alongside latches, the Concurrency TS gives us barriers—reusable synchronization objects for synchronizing a group of threads.
         Let’s look at those next.
      </p>
      
      
      
      <h4 id="ch04lev2sec20" class="calibre23">4.4.9. <a id="ch04lev2sec20__title" class="calibre4"></a>std::experimental::barrier: a basic barrier
      </h4>
      
      <p class="noind">The Concurrency TS provides two types of barriers in the <kbd class="calibre17">&lt;experimental/barrier&gt;</kbd> header: <kbd class="calibre17">std::experimental::barrier</kbd> and <kbd class="calibre17">std::experimental::flex_barrier</kbd>. The former is more basic, and potentially therefore has lower overhead, whereas the latter is more flexible, but potentially
         has more overhead.
      </p>
      
      <p class="noind">Suppose you have a group of threads that are operating on some data. Each thread can do its processing on the data independently
         of the others, so no synchronization is needed during the processing, but all the threads must have completed their processing
         before the next data item can be processed, or before the subsequent processing can be done. <kbd class="calibre17">std::experimental::barrier</kbd> is targeted at precisely this scenario. You construct a barrier with a count specifying the number of threads involved in
         the synchronization group. As each thread is done with its processing, it arrives at the barrier and waits for the rest of
         the group by calling <kbd class="calibre17">arrive_and_wait</kbd> on the barrier object. When the last thread in the group arrives, all the threads are released, and the barrier is reset.
         The threads in the group can then resume their processing and either process the next data item or proceed with the next stage
         of processing, as appropriate.
      </p>
      
      <p class="noind">Whereas latches latch, so once they are ready they stay ready, barriers do not—barriers release the waiting threads and then
         reset so they can be used again. They also only synchronize <i class="calibre6">within</i> a group of threads—a thread cannot wait for a barrier to be ready unless it is one of the threads in the synchronization
         group. Threads can explicitly drop out of the group by calling <kbd class="calibre17">arrive_and_drop</kbd> on the barrier, in which case that thread cannot wait for the barrier to be ready anymore, and the count of threads that
         must arrive in the next cycle is one less than the number of threads that had to arrive in the current cycle.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex26">Listing 4.26. <a id="ch04ex26__title" class="calibre4"></a>Using <kbd class="calibre17">std::experimental::barrier</kbd></h5>
      <pre id="PLd0e13759" class="calibre5">result_chunk process(data_chunk);
std::vector&lt;data_chunk&gt;
divide_into_chunks(data_block data, unsigned num_threads);

void process_data(data_source &amp;source, data_sink &amp;sink) {
    unsigned const concurrency = std::thread::hardware_concurrency();
    unsigned const num_threads = (concurrency &gt; 0) ? concurrency : 2;

    std::experimental::barrier sync(num_threads);
    std::vector&lt;joining_thread&gt; threads(num_threads);

    std::vector&lt;data_chunk&gt; chunks;
    result_block result;

    for (unsigned i = 0; i &lt; num_threads; ++i) {
        threads[i] = joining_thread([&amp;, i] {
            while (!source.done()) {                                   <b class="calibre24"><i class="calibre6">6</i></b>
                if (!i) {                                              <b class="calibre24"><i class="calibre6">1</i></b>
                    data_block current_block =
                        source.get_next_data_block();
                    chunks = divide_into_chunks(
                        current_block, num_threads);
                }
                sync.arrive_and_wait();                                <b class="calibre24"><i class="calibre6">2</i></b>
                result.set_chunk(i, num_threads, process(chunks[i]));  <b class="calibre24"><i class="calibre6">3</i></b>
                sync.arrive_and_wait();                                <b class="calibre24"><i class="calibre6">4</i></b>
                if (!i) {                                              <b class="calibre24"><i class="calibre6">5</i></b>
                    sink.write_data(std::move(result));
                }
            }
        });
    }
}                                                                      <b class="calibre24"><i class="calibre6">7</i></b></pre>
      
      <p class="noind"><a id="iddle1120" class="calibre4"></a><a id="iddle1502" class="calibre4"></a><a href="#ch04ex26" class="calibre4">Listing 4.26</a> shows an example of using a barrier to synchronize a group of threads. You have data coming from <kbd class="calibre17">source</kbd>, and you’re writing the output to <kbd class="calibre17">sink</kbd>, but in order to make use of the available concurrency in the system, you’re splitting each block of data into <kbd class="calibre17">num_threads</kbd> chunks. This has to be done serially, so you have an initial block <b class="calibre24"><i class="calibre6">1</i></b> that only runs on the thread for which <kbd class="calibre17">i==0</kbd>. All threads then wait on the barrier for that serial code to complete <b class="calibre24"><i class="calibre6">2</i></b> before you reach the parallel region, where each thread processes its individual chunk and updates the <kbd class="calibre17">result</kbd> with that <b class="calibre24"><i class="calibre6">3</i></b> before synchronizing again <b class="calibre24"><i class="calibre6">4</i></b>. You then have a second serial region where only thread 0 writes the result out to the <kbd class="calibre17">sink</kbd> <b class="calibre24"><i class="calibre6">5</i></b>. All threads then keep looping until the <kbd class="calibre17">source</kbd> reports that everything is <kbd class="calibre17">done</kbd> <b class="calibre24"><i class="calibre6">6</i></b>. Note that as each thread loops round, the serial section at the bottom of the loop combines with the section at the top;
         because only thread 0 has anything to do in either of these sections, this is OK, and all the threads will synchronize together
         at the first use of the barrier <b class="calibre24"><i class="calibre6">2</i></b>. When all the processing is done, then all the threads will exit the loop, and the destructors for the <kbd class="calibre17">joining_thread</kbd> objects will wait for them all to finish at the end of the outer function <b class="calibre24"><i class="calibre6">7</i></b> (<kbd class="calibre17">joining_thread</kbd> was introduced in <a href="kindle_split_012.html#ch02" class="calibre4">chapter 2</a>, <a href="kindle_split_012.html#ch02ex07" class="calibre4">listing 2.7</a>).
      </p>
      
      <p class="noind">The key thing to note here is that the calls to <kbd class="calibre17">arrive_and_wait</kbd> are at the points in the code where it is important that no threads proceed until all threads are ready. At the first synchronization
         point, all the threads are waiting for thread 0 to arrive, but the use of the barrier provides you with a clean line in the
         sand. At the second synchronization point, you have the reverse situation: it is thread 0 that is waiting for all the other
         threads to arrive before it can write out the completed <kbd class="calibre17">result</kbd> to the <kbd class="calibre17">sink</kbd>.
      </p>
      
      <p class="noind">The Concurrency TS doesn’t just give you one barrier type; as well as <kbd class="calibre17">std::experimental::barrier</kbd>, you also get <kbd class="calibre17">std::experimental::flex_barrier</kbd>, which is more flexible. One of the ways that it is more flexible is that it allows for a final serial region to be run when
         all threads have arrived at the barrier, before they are all released again.
      </p>
      
      
      
      
      <h4 id="ch04lev2sec21" class="calibre23">4.4.10. <a id="ch04lev2sec21__title" class="calibre4"></a>std::experimental::flex_barrier—std::experimental::barrier’s flexible friend
      </h4>
      
      <p class="noind">The interface to <kbd class="calibre17">std::experimental::flex_barrier</kbd> differs from that of <kbd class="calibre17">std:: experimental::barrier</kbd> in only one way: there is an additional constructor that takes a completion function, as well as a thread count. This function
         is run on exactly one of the threads that arrived at the barrier, once all the threads have arrived at the barrier. Not only
         does it provide a means of specifying a chunk of code that must be run serially, it also provides a means of changing the
         number of threads that must arrive at the barrier for the next cycle. The thread count can be changed to any number, whether
         higher or lower than the previous count; it is up to the programmer who uses this facility to ensure that the correct number
         of threads will arrive at the barrier the next time round.
      </p>
      
      <p class="noind">The following listing shows how <a href="#ch04ex26" class="calibre4">listing 4.26</a> could be rewritten to use <kbd class="calibre17">std:: experimental::flex_barrier</kbd> to manage the serial region.
      </p>
      
      
      
      <h5 class="notetitle" id="ch04ex27">Listing 4.27. <a id="ch04ex27__title" class="calibre4"></a>Using <kbd class="calibre17">std::flex_barrier</kbd> to provide a serial region
      </h5>
      <pre id="PLd0e13936" class="calibre5">void process_data(data_source &amp;source, data_sink &amp;sink) {
    unsigned const concurrency = std::thread::hardware_concurrency();
    unsigned const num_threads = (concurrency &gt; 0) ? concurrency : 2;

    std::vector&lt;data_chunk&gt; chunks;

    auto split_source = [&amp;] {                                  <b class="calibre24"><i class="calibre6">1</i></b>
        if (!source.done()) {
            data_block current_block = source.get_next_data_block();
            chunks = divide_into_chunks(current_block, num_threads);
        }
    };

    split_source();                                            <b class="calibre24"><i class="calibre6">2</i></b>

    result_block result;

    std::experimental::flex_barrier sync(num_threads, [&amp;] {    <b class="calibre24"><i class="calibre6">3</i></b>
        sink.write_data(std::move(result));
        split_source();                                        <b class="calibre24"><i class="calibre6">4</i></b>
        return -1;                                             <b class="calibre24"><i class="calibre6">5</i></b>
    });
    std::vector&lt;joining_thread&gt; threads(num_threads);

    for (unsigned i = 0; i &lt; num_threads; ++i) {
        threads[i] = joining_thread([&amp;, i] {
            while (!source.done()) {                           <b class="calibre24"><i class="calibre6">6</i></b>
                result.set_chunk(i, num_threads, process(chunks[i]));
                sync.arrive_and_wait();                        <b class="calibre24"><i class="calibre6">7</i></b>
            }
        });
    }
}</pre>
      
      <p class="noind">The first difference between this code and <a href="#ch04ex26" class="calibre4">listing 4.26</a> is that you’ve extracted a lambda that splits the next data block into chunks <b class="calibre24"><i class="calibre6">1</i></b>. This is called before you start <b class="calibre24"><i class="calibre6">2</i></b>, and encapsulates the code that was run on thread 0 at the start of each iteration.
      </p>
      
      <p class="noind">The second difference is that your <kbd class="calibre17">sync</kbd> object is now a <kbd class="calibre17">std::experimental::flex_barrier</kbd>, and you are passing a completion function as well as a thread count <b class="calibre24"><i class="calibre6">3</i></b>. This completion function is run on one thread after each thread has arrived, and so can encapsulate the code that was to
         be run on thread 0 at the end of each iteration, and then there’s a call to your newly-extracted <kbd class="calibre17">split_source</kbd> lambda that would have been called at the start of the next iteration <b class="calibre24"><i class="calibre6">4</i></b>. The return value of -1 <b class="calibre24"><i class="calibre6">5</i></b> indicates that the number of participating threads is to remain unchanged; a return value of zero or more would specify the
         number of participating threads in the next cycle.
      </p>
      
      <p class="noind">The main loop <b class="calibre24"><i class="calibre6">6</i></b> is now simplified: it only contains the parallel portion of the code, and thus only needs a single synchronization point
         <b class="calibre24"><i class="calibre6">7</i></b>. The use of <kbd class="calibre17">std:: experimental::flex_barrier</kbd> has thus simplified the code.
      </p>
      
      <p class="noind">The use of the completion function to provide a serial section is quite powerful, as is the ability to change the number of
         participating threads. For example, this could be used by pipeline style code where the number of threads is less during the
         initial priming of the pipeline and the final draining of the pipeline than it is during the main processing, when all the
         stages of the pipeline are operating.
      </p>
      
      
      
      
      <h3 id="ch04lev1sec5" class="chapter"><a id="ch04lev1sec5__title" class="calibre3"></a>Summary
      </h3>
      
      <p class="noind">Synchronizing operations between threads is an important part of writing an application that uses concurrency: if there’s
         no synchronization, the threads are essentially independent and might as well be written as separate applications that are
         run as a group because of their related activities. In this chapter, I’ve covered various ways of synchronizing operations
         from the basic condition variables, through futures, promises, packaged tasks, latches, and barriers. I’ve also discussed
         ways of approaching the synchronization issues: functional-style programming, where each task produces a result entirely dependent
         on its input rather than on the external environment; message passing, where communication between threads is via asynchronous
         messages sent through a messaging subsystem that acts as an intermediary; and continuation style, where the follow-on tasks
         for each operation are specified, and the system takes care of the scheduling.
      </p>
      
      <p class="noind">Having discussed many of the high-level facilities available in C++, it’s now time to look at the low-level facilities that
         make it all work: the C++ memory model and atomic operations.
      </p>
      
      
      
      
      <div class="calibre13" id="calibre_pb_20"></div>
</body></html>
