<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>15.2  Measures of Query Cost</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part280.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part282.htm">下一个 &gt;</a></p><p class="s65" style="padding-left: 40pt;text-indent: 0pt;text-align: left;">15.2  <span style=" color: #00AEEF;">Measures of Query Cost</span></p><p style="padding-top: 12pt;padding-left: 88pt;text-indent: 0pt;text-align: right;">There are multiple possible evaluation plans for a query, and it is important to be able to compare the alternatives in terms of their (estimated) cost, and choose the best plan. To do so, we must estimate the cost of individual operations and combine them to get the cost of a query evaluation plan. Thus, as we study evaluation algorithms for each operation later in this chapter, we also outline how to estimate the cost of the operation. The cost of query evaluation can be measured in terms of a number of diﬀerent resources, including disk accesses, <span class="s44">CPU </span>time to execute a query, and, in parallel and distributed database systems, the cost of communication. (We discuss parallel and dis-</p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">tributed database systems in Chapter 21 through Chapter 23.)</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">For large databases resident on magnetic disk, the <span class="s44">I/O </span>cost to access data from disk usually dominates the other costs; thus, early cost models focused on the <span class="s44">I/O </span>cost when estimating the cost of query operations. However, with ﬂash storage becoming larger and less expensive, most organizational data today can be stored on solid-state drives (<span class="s44">SSD</span>s) in a cost eﬀective manner. In addition, main memory sizes have increased signiﬁcantly, and the cost of main memory has decreased enough in recent years that for many organizations, organizational data can be stored cost-eﬀectively in main memory for querying, although it must of course be stored on ﬂash or magnetic storage to ensure persistence.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">With data resident in-memory or on <span class="s44">SSDs</span>, <span class="s44">I/O </span>cost does not dominate the overall cost, and we must include <span class="s44">CPU </span>costs when computing the cost of query evaluation. We do not include <span class="s44">CPU </span>costs in our model to simplify our presentation, but note that they can be approximated by simple estimators. For example, the cost model used by <span class="s44">P</span>ostgre<span class="s44">SQL </span>(as of 2018) includes (i) a <span class="s44">CPU </span>cost per tuple, (ii) a <span class="s44">CPU </span>cost for processing each index entry (in addition to the <span class="s44">I/O </span>cost), and (iii) a <span class="s44">CPU </span>cost per operator or function (such as arithmetic operators, comparison operators, and related functions). The database has default values for each of these costs, which are multiplied by the number of tuples processed, the number of index entries processed, and the number of operators and functions executed, respectively. The defaults can be changed as a conﬁguration parameter.</p><p class="s13" style="padding-left: 88pt;text-indent: 17pt;line-height: 13pt;text-align: justify;"><span class="p">We use the </span>number of blocks transferred <span class="p">from storage and the </span>number of random <span class="s101">I/O </span>accesses<span class="p">, each of which will require a disk seek on magnetic storage, as two important factors in estimating the cost of a query-evaluation plan. If the disk subsystem takes an average of </span>t<span class="s97">T </span><span class="p">seconds to transfer a block of data and has an average block-access time (disk seek time plus rotational latency) of </span>t<span class="s97">S </span><span class="p">seconds, then an operation that transfers </span>b <span class="p">blocks and performs </span>S <span class="p">random </span><span class="s44">I/O </span><span class="p">accesses would take </span>b <span class="s15">∗ </span>t<span class="s145">T </span><span class="s15">+ </span>S <span class="s15">∗ </span>t<span class="s145">S </span><span class="p">seconds.</span></p><p style="padding-top: 1pt;padding-left: 88pt;text-indent: 17pt;line-height: 87%;text-align: justify;">The values of <i>t</i><span class="s145">T </span>and <i>t</i><span class="s145">S </span>must be calibrated for the disk system used. We summarize performance data here; see Chapter 12 for full details on storage systems. Typical values for high-end magnetic disks in the year 2018 would be <i>t</i><span class="s97">S </span><span class="s15">= </span>4 milliseconds and <i>t</i><span class="s97">T </span><span class="s15">= </span>0<span class="s83">.</span>1</p><p class="s66" style="padding-top: 3pt;padding-left: 338pt;text-indent: 0pt;text-align: left;">15.2 <span style=" color: #00AEEF;">Measures of Query Cost  </span><span class="s164">693</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">milliseconds, assuming a 4-kilobyte block size and a transfer rate of 40 megabytes per second.<span class="s76">2</span></p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Although <span class="s44">SSDs </span>do not perform a physical seek operation, they have an overhead for initiating an <span class="s44">I/O </span>operation; we refer to the latency from the time an <span class="s44">I/O </span>request</p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 93%;text-align: justify;">is made to the time when the ﬁrst byte of data is returned as <i>t</i><span class="s97">S</span>. For mid-range <span class="s44">SSD</span>s in 2018 using the <span class="s44">SATA </span>interface, <i>t</i><span class="s145">S </span>is around 90 microseconds, while the transfer time <i>t</i><span class="s145">T </span>is about 10 microseconds for a 4-kilobyte block. Thus, <span class="s44">SSD</span>s can support about 10,000 random 4-kilobyte reads per second, and they support 400 megabytes/second throughput on sequential reads using the standard <span class="s44">SATA </span>interface. <span class="s44">SSD</span>s using the <span class="s44">PCI</span>e 3.0x4 interface have smaller <i>t</i><span class="s97">S </span>, of 20 to 60 microseconds, and much higher transfer rates of around 2 gigabytes/second, corresponding to <i>t</i><span class="s97">T </span>of 2 microseconds, allowing around 50,000 to 15,000 random 4-kilobyte block reads per second, depending on the model.<span class="s76">3</span></p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">For data that are already present in main memory, reads happen at the unit of cache lines, instead of disk blocks. But assuming entire blocks of data are read, the time to transfer <i>t</i><span class="s97">T </span>for a 4-kilobyte block is less than 1 microsecond for data in memory.</p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 79%;text-align: justify;">The latency to fetch data from memory, <i>t</i><span class="s97">S </span>, is less than 100 nanoseconds.</p><p style="padding-left: 137pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">Given the wide diversity of speeds of diﬀerent storage devices, database systems</p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">must ideally perform test seeks and block transfers to estimate <i>t</i><span class="s97">S </span>and <i>t</i><span class="s97">T </span>for speciﬁc systems/storage devices, as part of the software installation process. Databases that do not automatically infer these numbers often allow users to specify the numbers as part of conﬁguration ﬁles.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">We can reﬁne our cost estimates further by distinguishing block reads from block writes. Block writes are typically about twice as expensive as reads on magnetic disks, since disk systems read sectors back after they are written to verify that the write was successful. On <span class="s44">PCI</span>e ﬂash, write throughput may be about 50 percent less than read throughput, but the diﬀerence is almost completely masked by the limited speed of <span class="s44">SATA </span>interfaces, leading to write throughput matching read throughput. However, the throughput numbers do not reﬂect the cost of erases that are required if blocks are overwritten. For simplicity, we ignore this detail.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The cost estimates we give do not include the cost of writing the ﬁnal result of an operation back to disk. These are taken into account separately where required.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="95" height="1" alt="image" src="Image_2761.png"/></span></p><p class="s80" style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;line-height: 92%;text-align: justify;"><span class="s77">2</span><span class="s78">Storage device speciﬁcations often mention the transfer rate, and the number of random </span><span class="s153">I/O </span><span class="s78">operations that can be carried out in 1 second. The values </span><i>t</i><span class="s363">T </span>can be computed as block size divided by transfer rate, while <i>t</i><span class="s363">S </span>can be computed as (1<span class="s230">∕</span><i>N </i>) <span class="s230">− </span><i>t</i><span class="s363">T </span>, where <i>N </i>is the number of random <span class="s161">I/O </span>operations per second that the device supports, since a random <span class="s161">I/O </span>operation performs a random <span class="s161">I/O </span>access, followed by data transfer of 1 block.</p><p class="s78" style="padding-left: 119pt;text-indent: 0pt;line-height: 10pt;text-align: justify;"><span class="s77">3</span>The <span class="s153">I/O </span>operations per second number used here are for the case of sequential <span class="s153">I/O </span>requests, usually denoted as QD-1 in the <span class="s153">SSD </span>speciﬁcations. <span class="s153">SSD</span>s can support multiple random requests in parallel, with 32 to 64 parallel requests being commonly supported; an <span class="s153">SSD </span>with <span class="s153">SATA </span>interface supports nearly 100,000 random 4-kilobyte block reads in a second</p><p class="s80" style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">if multiple requests are sent in parallel, while <span class="s161">PCI</span>e disks can support over 350,000 random 4-kilobyte block reads per second; these numbers are referred to as the QD-32 or QD-64 numbers depending on how many requests are sent in parallel. We do not explore parallel requests in our cost model, since we only consider sequential query processing algorithms in this chapter. Shared-memory parallel query processing techniques, discussed in Section 22.6, can be used to exploit the parallel request capabilities of <span class="s161">SSDs</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">The costs of all the algorithms that we consider depend on the size of the buﬀer in main memory. In the best case, if data ﬁts in the buﬀer, the data can be read into the buﬀers, and the disk does not need to be accessed again. In the worst case, we may assume that the buﬀer can hold only a few blocks of data — approximately one block per relation. However, with large main memories available today, such worst-case assumptions are overly pessimistic. In fact, a good deal of main memory is typically available for processing a query, and our cost estimates use the amount of memory available to an operator, <i>M </i>, asa parameter. In <span class="s44">P</span>ostgre<span class="s44">SQL </span>the total memory available to a query, called the eﬀective cache size, is assumed by default to be 4 gigabytes, for the purpose of cost estimation; if a query has several operators that run concurrently, the available memory has to be divided amongst the operators.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">In addition, although we assume that data must be read from disk initially, it is possible that a block that is accessed is already present in the in-memory buﬀer. Again, for simplicity, we ignore this eﬀect; as a result, the actual disk-access cost during the execution of a plan may be less than the estimated cost. To account (at least partially) for buﬀer residence, <span class="s44">P</span>ostgre<span class="s44">SQL </span>uses the following “hack”: the cost of a random page read is assumed to be 1/10<span class="s76">th</span> of the actual random page read cost, to model the situation that 90% of reads are found to be resident in cache. Further, to model the situation that internal nodes of B<span class="s181">+</span>-tree indices are traversed often, most database systems assume that all internal nodes are present in the in-memory buﬀer, and assume that a traversal of an index only incurs a single random <span class="s44">I/O </span>cost for the leaf node.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The <span class="s63">response time </span>for a query-evaluation plan (that is, the wall-clock time required to execute the plan), assuming no other activity is going on in the computer, would account for all these costs, and could be used as a measure of the cost of the plan. Unfortunately, the response time of a plan is very hard to estimate without actually executing the plan, for the following two reasons:</p><p class="s63" style="padding-top: 10pt;padding-left: 113pt;text-indent: -16pt;text-align: justify;">1. <span class="p">The response time depends on the contents of the buﬀer when the query begins execution; this information is not available when the query is optimized and is hard to account for even if it were available.</span></p><p class="s63" style="padding-top: 6pt;padding-left: 113pt;text-indent: -17pt;text-align: justify;">2. <span class="p">In a system with multiple disks, the response time depends on how accesses are distributed among disks, which is hard to estimate without detailed knowledge of data layout on disk.</span></p><p style="padding-top: 10pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Interestingly, a plan may get a better response time at the cost of extra resource con- sumption. For example, if a system has multiple disks, a plan <i>A </i>that requires extra disk reads, but performs the reads in parallel across multiple disks may, ﬁnish faster than another plan <i>B </i>that has fewer disk reads, but performs reads from only one disk at a time. However, if many instances of a query using plan <i>A </i>run concurrently, the overall response time may actually be more than if the same instances are executed using plan <i>B</i>, since plan <i>A </i>generates more load on the disks.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">As a result, instead of trying to minimize the response time, optimizers generally try to minimize the total <span class="s63">resource consumption </span>of a query plan. Our model of estimating</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;"><a name="bookmark326">the total disk access time (including seek and data transfer) is an example of such a resource consumption–based model of query cost.</a><a name="bookmark347">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part280.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part282.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
