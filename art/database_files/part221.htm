<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>11.4  Data Mining</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part220.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part222.htm">下一个 &gt;</a></p><p class="s65" style="padding-left: 40pt;text-indent: 0pt;text-align: left;">11.4  <span style=" color: #00AEEF;">Data Mining</span></p><p style="padding-top: 12pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">The term <span class="s63">data mining </span>refers loosely to the process of analyzing large databases to ﬁnd useful patterns. Like knowledge discovery in artiﬁcial intelligence (also called machine learning) or statistical analysis, data mining attempts to discover rules and patterns from data. However, data mining diﬀers from traditional machine learning and statis- tics in that it deals with large volumes of data, stored primarily on disk. Today, many machine-learning algorithms also work on very large volumes of data, blurring the dis- tinction between data mining and machine learning. Data-mining techniques form part of the process of <span class="s63">knowledge discovery in databases </span>(<span class="s64">KDD</span>).</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: right;">Some types of knowledge discovered from a database can be represented by a set of <span class="s63">rules</span>. The following is an example of a rule, stated informally: “Young women with annual incomes greater than $50,000 are the most likely people to buy small sports cars.” Of course such rules are not universally true and have degrees of “support” and “conﬁdence,” as we shall see. Other types of knowledge are represented by equations relating diﬀerent variables to each other. More generally, knowledge discovered by ap- plying machine-learning techniques on past instances in a database is represented by a <span class="s63">model</span>, which is then used for predicting outcomes for new instances. Features or at- tributes of instances are inputs to the model, and the output of a model is a prediction. There are a variety of possible types of patterns that may be useful, and diﬀerent techniques are used to ﬁnd diﬀerent types of patterns. We shall study a few examples</p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">of patterns and see how they may be automatically derived from a database.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Usually there is a manual component to data mining, consisting of preprocessing data to a form acceptable to the algorithms and post-processing of discovered patterns to ﬁnd novel ones that could be useful. There may also be more than one type of pattern that can be discovered from a given database, and manual interaction may be needed to pick useful types of patterns. For this reason, data mining is really a semiautomatic process in real life. However, in our description we concentrate on the automatic aspect of mining.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: left;">11.4.1 Types of Data-Mining Tasks</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">The most widely used applications of data mining are those that require some sort of <span class="s63">prediction</span>. For instance, when a person applies for a credit card, the credit-card company wants to predict if the person is a good credit risk. The prediction is to be based on known attributes of the person, such as age, income, debts, and past debt- repayment history. Rules for making the prediction are derived from the same attributes of past and current credit-card holders, along with their observed behavior, such as whether they defaulted on their credit-card dues. Other types of prediction include predicting which customers may switch over to a competitor (these customers may be oﬀered special discounts to tempt them not to switch), predicting which people are likely to respond to promotional mail (“junk mail”), or predicting what types of phone calling-card usage are likely to be fraudulent.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Another class of applications looks for <span class="s63">associations</span>, for instance, books that tend to be bought together. If a customer buys a book, an online bookstore may suggest other associated books. If a person buys a camera, the system may suggest accessories that tend to be bought along with cameras. A good salesperson is aware of such patterns and exploits them to make additional sales. The challenge is to automate the process. Other types of associations may lead to discovery of causation. For instance, discovery of unexpected associations between a newly introduced medicine and cardiac problems led to the ﬁnding that the medicine may cause cardiac problems in some people. The medicine was then withdrawn from the market.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Associations are an example of <span class="s63">descriptive patterns</span>. <span class="s63">Clusters </span>are another example of such patterns. For example, over a century ago a cluster of typhoid cases was found around a well, which led to the discovery that the water in the well was contaminated and was spreading typhoid. Detection of clusters of disease remains important even today.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">11.4.2 Classification</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Abstractly, the <span class="s63">classification </span>problem is this: Given that items belong to one of several classes, and given past instances (called <span class="s63">training instances</span>) of items along with the classes to which they belong, the problem is to predict the class to which a new item belongs. The class of the new instance is not known, so other attributes of the instance must be used to predict the class.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">As an example, suppose that a credit-card company wants to decide whether or not to give a credit card to an applicant. The company has a variety of information about the person, such as her age, educational background, annual income, and current debts, that it can use for making a decision.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">To make the decision, the company assigns a credit worthiness level of excellent, good, average, or bad to each of a sample set of current or past customers according to each customer’s payment history. These instances form the set of training instances.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Then, the company attempts to learn rules or models that classify the credit- worthiness of a new applicant as excellent, good, average, or bad, on the basis of the</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">information about the person, other than the actual payment history (which is unavail- able for new customers). There are a number of techniques for classiﬁcation, and we outline a few of them in this section.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-top: 7pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">11.4.2.1 Decision-Tree Classifiers</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Decision-tree classiﬁers are a widely used technique for classiﬁcation. As the name suggests, <span class="s63">decision-tree classifiers </span>use a tree; each leaf node has an associated class, and each internal node has a predicate (or more generally, a function) associated with it. Figure 11.11 shows an example of a decision tree. To keep the example simple, we use just two attributes: education level (highest degree earned) and income.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">To classify a new instance, we start at the root and traverse the tree to reach a leaf; at an internal node we evaluate the predicate (or function) on the data instance to ﬁnd which child to go to. The process continues until we reach a leaf node. For example, if the degree level of a person is masters, and the person’s income is 40K, starting from the root we follow the edge labeled “masters,” and from there the edge labeled “25K to 75K,” to reach a leaf. The class at the leaf is “good,” so we predict that the credit risk of that person is good.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">There are a number of techniques for building decision-tree classiﬁers from a given training set. We omit details, but you can learn more details from the references pro- vided in the Further Reading section.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="438" height="204" alt="image" src="Image_2508.png"/></span></p><p class="s33" style="text-indent: 0pt;line-height: 10pt;text-align: left;">degree</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="text-indent: 0pt;line-height: 10pt;text-align: left;">none</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="text-indent: 0pt;line-height: 10pt;text-align: left;">bachelors</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="text-indent: 0pt;line-height: 11pt;text-align: left;">income    <span class="s285">income</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="padding-left: 14pt;text-indent: 0pt;line-height: 10pt;text-align: left;">masters</p><p class="s33" style="padding-top: 6pt;text-indent: 0pt;text-align: left;">income</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="padding-left: 7pt;text-indent: 0pt;line-height: 10pt;text-align: left;">doctorate</p><p class="s33" style="padding-top: 6pt;text-indent: 0pt;text-align: left;">income</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="text-indent: 0pt;line-height: 10pt;text-align: left;">&lt;50K</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="text-indent: 0pt;line-height: 10pt;text-align: left;">&gt;100K</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="padding-left: 19pt;text-indent: 0pt;line-height: 10pt;text-align: left;">&lt;25K</p><p class="s33" style="padding-top: 6pt;text-indent: 0pt;text-align: left;">&gt;=50K</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">&gt;75K <span class="s286">&lt;25K</span></p><p class="s33" style="padding-top: 4pt;text-indent: 0pt;text-align: left;">25 to 75K</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="text-indent: 0pt;line-height: 10pt;text-align: left;">&gt;=25K</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="text-indent: 0pt;line-height: 10pt;text-align: left;">50 to 100K</p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="text-indent: 0pt;line-height: 10pt;text-align: left;">&lt;50K</p><p style="text-indent: 0pt;text-align: left;"/><p class="s287" style="padding-left: 114pt;text-indent: 0pt;line-height: 12pt;text-align: left;">bad  <span class="s285">average  </span><span class="s33">good</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s140" style="padding-left: 61pt;text-indent: 0pt;text-align: center;"><span><img width="28" height="28" alt="image" src="Image_2509.png"/></span><span class="s258"> </span>bad    <span><img width="28" height="28" alt="image" src="Image_2510.png"/></span> average    <span><img width="28" height="28" alt="image" src="Image_2511.png"/></span> good      <span><img width="28" height="28" alt="image" src="Image_2512.png"/></span> excellent</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 59pt;text-indent: 0pt;text-align: center;">Figure 11.11 <span class="s74">Classification tree.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: left;">11.4.2.2 Bayesian Classifiers</p><p class="s15" style="text-indent: 0pt;text-align: left;">|</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;"><span class="s63">Bayesian classifiers </span><span class="p">ﬁnd the distribution of attribute values for each class in the training data; when given a new instance </span>d<span class="p">, they use the distribution information to estimate, for each class </span>c<span class="s145">j </span><span class="p">, the probability that instance </span>d <span class="p">belongs to class </span>c<span class="s145">j </span><span class="p">, denoted by </span>p<span class="p">(</span>c<span class="s145">j </span>d<span class="p">), in a manner outlined here. The class with maximum probability becomes the predicted</span></p><p class="s15" style="text-indent: 0pt;text-align: left;">|</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">class for instance <i>d</i>.</p><p style="padding-top: 1pt;padding-left: 119pt;text-indent: 17pt;line-height: 87%;text-align: justify;">To ﬁnd the probability <i>p</i>(<i>c</i><span class="s97">j </span><i>d</i>) of instance <i>d </i>being in class <i>c</i><span class="s97">j </span>, Bayesian classiﬁers use <span class="s63">Bayes’ theorem</span>, which says:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="65" height="1" alt="image" src="Image_2513.png"/></span></p><p class="s109" style="text-indent: 0pt;line-height: 8pt;text-align: left;">j</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="text-indent: 0pt;line-height: 12pt;text-align: left;">p<span class="p">(</span>d<span class="p">)</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-top: 5pt;padding-left: 84pt;text-indent: 0pt;text-align: center;"><span class="s288">p</span><span class="s289">(</span><span class="s288">c </span><span class="s290">|</span><span class="s288">d</span><span class="s289">) </span><span class="s290">= </span>p<span class="p">(</span>d<span class="s15">|</span>c<span class="s97">j </span><span class="p">)</span>p<span class="p">(</span>c<span class="s97">j </span><span class="p">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="text-indent: 0pt;text-align: left;">|</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 5pt;padding-left: 119pt;text-indent: 0pt;line-height: 88%;text-align: justify;">where <i>p</i>(<i>d c</i><span class="s97">j </span>) is the probability of generating instance <i>d </i>given class <i>c</i><span class="s97">j </span>, <i>p</i>(<i>c</i><span class="s97">j </span>) is the prob- ability of occurrence of class <i>c</i><span class="s97">j </span>, and <i>p</i>(<i>d</i>) is the probability of instance <i>d </i>occurring. Of these, <i>p</i>(<i>d</i>) can be ignored since it is the same for all classes. <i>p</i>(<i>c</i><span class="s145">j </span>) is simply the fraction of training instances that belong to class <i>c</i><span class="s145">j </span>.</p><p style="padding-left: 137pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">For example, let us consider a special case where only one attribute, <i>income</i>, is</p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">used for classiﬁcation, and suppose we need to classify a person whose income is 76,000. We assume that income values are broken up into buckets, and we assume that the bucket containing 76,000 contains values in the range (75,000, 80,000). Sup- pose among instances of class <i>excellent</i>, the probability of income being in (75,000,</p><p class="s15" style="text-indent: 0pt;text-align: left;">|</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 2pt;padding-left: 119pt;text-indent: 0pt;line-height: 63%;text-align: right;">80,000) is 0<span class="s83">.</span>1, while among instances of class <i>good</i>, the probability of income being in (75,000, 80,000) is 0<span class="s83">.</span>05. Suppose also that overall 0<span class="s83">.</span>1 fraction of people are classiﬁed as <i>excellent</i>, and 0<span class="s83">.</span>3 are classiﬁed as <i>good</i>. Then, <i>p</i>(<i>d c</i><span class="s97">j </span>)<i>p</i>(<i>c</i><span class="s97">j </span>) for class <i>excellent </i>is <span class="s83">.</span>01, while for class <i>good</i>, it is 0<span class="s83">.</span>015. The person would therefore be classiﬁed in class <i>good</i>. In general, multiple attributes need to be considered for classiﬁcation. Then, ﬁnd-</p><p class="s15" style="text-indent: 0pt;text-align: left;">|</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 1pt;padding-left: 119pt;text-indent: 0pt;line-height: 94%;text-align: justify;">ing <i>p</i>(<i>d c</i><span class="s97">j </span>) exactly is diﬃcult, since it requires the distribution of instances of <i>c</i><span class="s97">j </span>, across all combinations of values for the attributes used for classiﬁcation. The number of such combinations (for example of income buckets, with degree values and other attributes)</p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">can be very large. With a limited training set used to ﬁnd the distribution, most combi- nations would not have even a single training set matching them, leading to incorrect classiﬁcation decisions. To avoid this problem, as well as to simplify the task of classiﬁ- cation, <span class="s63">naive Bayesian classifiers </span>assume attributes have independent distributions and thereby estimate:</p><p class="s13" style="text-indent: 0pt;text-align: left;"><span class="s15">∗ </span>p<span class="p">(</span>d</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="text-indent: 0pt;text-align: left;">|<span class="s13">c </span><span class="p">)</span><span class="s291">2 </span><span class="s149">j</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-top: 6pt;padding-left: 154pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><span class="s15">= </span>p<span class="p">(</span>d<span class="s130">1</span><span class="s15">|</span>c<span class="s97">j </span><span class="p">)     </span><span class="s15">∗ </span><span class="s86">⋯ </span><span class="s15">∗ </span>p<span class="p">(</span>d<span class="s97">n</span><span class="s15">|</span>c<span class="s97">j </span><span class="p">)</span></p><p class="s13" style="padding-left: 53pt;text-indent: 0pt;line-height: 26%;text-align: center;">p<span class="p">(</span>d<span class="s15">|</span>c<span class="s97">j </span><span class="p">)</span></p><p class="s15" style="text-indent: 0pt;text-align: left;">|</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">That is, the probability of the instance <i>d </i>occurring is the product of the probability of occurrence of each of the attribute values <i>d</i><span class="s145">i </span>of <i>d</i>, given the class is <i>c</i><span class="s145">j </span>.</p><p class="s13" style="padding-left: 119pt;text-indent: 17pt;line-height: 87%;text-align: justify;"><span class="p">The probabilities </span>p<span class="p">(</span>d<span class="s97">i </span>c<span class="s97">j </span><span class="p">) derive from the distribution of values for each attribute </span>i<span class="p">, for each class </span>c<span class="s97">j </span><span class="p">. This distribution is computed from the training instances that belong to each class </span>c<span class="s97">j </span><span class="p">; the distribution is usually approximated by a histogram. For instance, we may divide the range of values of attribute </span>i <span class="p">into equal intervals, and store the frac-</span></p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">tion of instances of class <i>c</i><span class="s97">j </span>that fall in each interval. Given a value <i>d</i><span class="s97">i </span>for attribute <i>i</i>, the</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="text-indent: 0pt;text-align: left;">|</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-top: 5pt;padding-left: 88pt;text-indent: 0pt;line-height: 87%;text-align: justify;"><span class="p">value of </span>p<span class="p">(</span>d<span class="s97">i </span>c<span class="s97">j </span><span class="p">) is simply the fraction of instances belonging to class </span>c<span class="s97">j </span><span class="p">that fall in the interval to which </span>d<span class="s97">i </span><span class="p">belongs.</span></p><p class="s183" style="padding-top: 12pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">11.4.2.3 Support Vector Machine Classifiers</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">The <span class="s63">Support Vector Machine </span>(<span class="s292">SVM</span>) is a type of classiﬁer that has been found to give very accurate classiﬁcation across a range of applications. We provide some basic in- formation about Support Vector Machine classiﬁers here; see the references in the bibliographical notes for further information.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Support Vector Machine classiﬁers can best be understood geometrically. In the simplest case, consider a set of points in a two-dimensional plane, some belonging to class <i>A</i>, and some belonging to class <i>B</i>. We are given a training set of points whose class (<i>A </i>or <i>B</i>) is known, and we need to build a classiﬁer of points using these training points. This situation is illustrated in Figure 11.12, where the points in class <i>A </i>are denoted by <span class="s49">X </span>marks, while those in class <i>B </i>are denoted by <span class="s49">O </span>marks.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Suppose we can draw a line on the plane, such that all points in class <i>A </i>lie to one side and all points in line <i>B </i>lie to the other. Then, the line can be used to classify new points, whose class we don’t already know. But there may be many possible such lines that can separate points in class <i>A </i>from points in class <i>B</i>. A few such lines are shown in Figure 11.12. The Support Vector Machine classiﬁer chooses the line whose distance from the nearest point in either class (from the points in the training dataset) is maximum. This line (called the <i>maximum margin line</i>) is then used to classify other points into class <i>A </i>or <i>B</i>, depending on which side of the line they lie on. In Figure 11.12, the maximum margin line is shown in bold, while the other lines are shown as dashed lines.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 174pt;text-indent: 0pt;text-align: left;"><span><img width="265" height="235" alt="image" src="Image_2514.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 59pt;text-indent: 0pt;text-align: center;">Figure 11.12 <span class="s74">Example of a Support Vector Machine classifier.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 17pt;text-align: justify;">The preceding intuition can be generalized to more than two dimensions, allow- ing multiple attributes to be used for classiﬁcation; in this case, the classiﬁer ﬁnds a dividing plane, not a line. Further, by ﬁrst transforming the input points using certain functions, called <i>kernel functions</i>, Support Vector Machine classiﬁers can ﬁnd nonlin- ear curves separating the sets of points. This is important for cases where the points are not separable by a line or plane. In the presence of noise, some points of one class may lie in the midst of points of the other class. In such cases, there may not be any line or meaningful curve that separates the points in the two classes; then, the line or curve that most accurately divides the points into the two classes is chosen.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Although the basic formulation of Support Vector Machines is for binary classi- ﬁers, i.e., those with only two classes, they can be used for classiﬁcation into multiple classes as follows: If there are <i>N </i>classes, we build <i>N </i>classiﬁers, with classiﬁer <i>i </i>perform- ing a binary classiﬁcation, classifying a point either as in class <i>i </i>or not in class <i>i</i>. Given a point, each classiﬁer <i>i </i>also outputs a value indicating how related a given point is to class <i>i</i>. We then apply all <i>N </i>classiﬁers on a given point and choose the class for which the relatedness value is the highest.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-top: 7pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">11.4.2.4 Neural Network Classifiers</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Neural-net classiﬁers use the training data to train artiﬁcial neural nets. There is a large body of literature on neural nets; we do not provide details here, but we outline a few key properties of neural network classiﬁers.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Neural networks consist of several layers of “neurons,” each of which are connected to neurons in the preceding layer. An input instance of the problem is fed to the ﬁrst layer; neurons at each layer are “activated” based on some function applied to the inputs at the preceding layer. The function applied at each neuron computes a weighted combination of the activations of the input neurons and generates an output based on the weighted combination. The activation of a neuron in one layer thus aﬀects the activation of neurons in the next layer. The ﬁnal output layer typically has one neuron corresponding to each class of the classiﬁcation problem being addressed. The neuron with maximum activation for a given input decides the predicted class for that input.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Key to the success of a neural network is the weights used in the computation described above. These weights are learned, based on training data. They are initially set to some default value, and then training data are used to learn the weights. Training is typically done by applying each input to the current state of the neural network and checking if the prediction is correct. If not, a <i>backpropagation algorithm </i>is used to tweak the weights of the neurons in the network, to bring the prediction closer to the correct one for the current input. Repeating this process results in a trained neural network, which can then be used for classiﬁcation on new inputs.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">In recent years, neural networks have achieved a great degree of success for tasks which were earlier considered very hard, such as vision (e.g., recognition of objects in images), speech recognition, and natural language translation. A simple example of a vision task is that of identifying the species, such as cat or dog, given an image of</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">an animal; such problems are basically classiﬁcation problems. Other examples include identifying object occurrences in an image and assigning a class label to each identiﬁed object.</p><p class="s63" style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Deep neural networks<span class="p">, which are neural networks with a large number of layers, have proven very successful at such tasks, if given a very large number of training in- stances. The term </span>deep learning <span class="p">refers to the machine-learning techniques that create such deep neural networks, and train them on very large numbers of training instances.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">11.4.3 Regression</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;"><span class="s63">Regression </span>deals with the prediction of a value, rather than a class. Given values for a set of variables, <i>X</i><span class="s98">1</span>, <i>X</i><span class="s98">2</span>, <span class="s15">… </span>, <i>X</i><span class="s145">n</span>, we wish to predict the value of a variable <i>Y </i>. For instance, we could treat the level of education as a number and income as another number, and, on the basis of these two variables, we wish to predict the likelihood of default, which could be a percentage chance of defaulting, or the amount involved in the default.</p><p class="s13" style="padding-left: 106pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><span class="p">One way is to infer coeﬃcients </span>a<span class="s130">0</span><span class="s94">, </span>a<span class="s130">1</span><span class="s94">, </span>a<span class="s130">2</span><span class="s94">, </span><span class="s15">… </span><span class="p">, </span>a<span class="s97">n </span><span class="p">such that:</span></p><p class="s15" style="padding-top: 10pt;padding-left: 59pt;text-indent: 0pt;text-align: center;"><span class="s13">Y </span>= <span class="s13">a</span><span class="s130">0 </span>+ <span class="s13">a</span><span class="s130">1 </span>∗ <span class="s13">X</span><span class="s130">1 </span>+ <span class="s13">a</span><span class="s130">2 </span>∗ <span class="s13">X</span><span class="s130">2 </span>+ <span class="s86">⋯ </span>+ <span class="s13">a</span><span class="s97">n </span>∗ <span class="s13">X</span><span class="s97">n</span></p><p style="padding-top: 11pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Finding such a linear polynomial is called <span class="s63">linear regression</span>. In general, we wish to ﬁnd a curve (deﬁned by a polynomial or other formula) that ﬁts the data; the process is also called <span class="s63">curve fitting</span>.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The ﬁt may be only approximate, because of noise in the data or because the re- lationship is not exactly a polynomial, so regression aims to ﬁnd coeﬃcients that give the best possible ﬁt. There are standard techniques in statistics for ﬁnding regression coeﬃcients. We do not discuss these techniques here, but the bibliographical notes provide references.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">11.4.4 Association Rules</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Retail shops are often interested in associations between diﬀerent items that people buy. Examples of such associations are:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s39" style="padding-left: 91pt;text-indent: 0pt;text-align: left;">• <span class="s40">Someone who buys bread is quite likely also to buy milk.</span></p><p class="s13" style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: left;"><span class="s39">• </span><span class="s40">A person who bought the book </span>Database System Concepts <span class="p">is quite likely also to buy the book </span>Operating System Concepts<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">Association information can be used in several ways. When a customer buys a particu- lar book, an online shop may suggest associated books. A grocery shop may decide to place bread close to milk, since they are often bought together, to help shoppers ﬁnish their task faster. Or, the shop may place them at opposite ends of a row and place other associated items in between to tempt people to buy those items as well as the shoppers walk from one end of the row to the other. A shop that oﬀers discounts on one associ-</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">ated item may not oﬀer a discount on the other, since the customer will probably buy the other anyway.</p><p style="padding-left: 137pt;text-indent: 0pt;text-align: justify;">An example of an association rule is:</p><p class="s13" style="padding-top: 8pt;padding-left: 275pt;text-indent: 0pt;text-align: justify;">bread <span class="s86">⇒ </span>milk</p><p style="padding-top: 7pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">In the context of grocery-store purchases, the rule says that customers who buy bread also tend to buy milk with a high probability. An association rule must have an as- sociated <span class="s63">population</span>: The population consists of a set of <span class="s63">instances</span>. In the grocery-store example, the population may consist of all grocery-store purchases; each purchase is an instance. In the case of a bookstore, the population may consist of all people who made purchases, regardless of when they made a purchase. Each customer is an instance. In the bookstore example, the analyst has decided that when a purchase is made is not signiﬁcant, whereas for the grocery-store example, the analyst may have decided to concentrate on single purchases, ignoring multiple visits by the same customer.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Rules have an associated <i>support</i>, as well as an associated <i>confidence</i>. These are deﬁned in the context of the population:</p><p class="s39" style="padding-top: 9pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s63">Support </span><span class="p">is a measure of what fraction of the population satisﬁes both the an- tecedent and the consequent of the rule.</span></p><p style="padding-top: 1pt;padding-left: 139pt;text-indent: 17pt;line-height: 70%;text-align: justify;">For instance, suppose only 0<span class="s83">.</span>001 percent of all purchases include milk and screwdrivers. The support for the rule:</p><p class="s13" style="padding-top: 8pt;padding-left: 272pt;text-indent: 0pt;text-align: justify;">milk <span class="s86">⇒ </span>screwdrivers</p><p style="padding-top: 7pt;padding-left: 139pt;text-indent: 0pt;text-align: justify;">is low. The rule may not even be statistically signiﬁcant—perhaps there was only a single purchase that included both milk and screwdrivers. Businesses are usually not interested in rules that have low support, since they involve few customers and are not worth bothering about.</p><p style="padding-left: 139pt;text-indent: 17pt;text-align: justify;">On the other hand, if 50 percent of all purchases involve milk and bread, then support for rules involving bread and milk (and no other item) is relatively high, and such rules may be worth attention. Exactly what minimum degree of support is considered desirable depends on the application.</p><p class="s39" style="padding-top: 4pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s63">Confidence </span><span class="p">is a measure of how often the consequent is true when the antecedent is true. For instance, the rule:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-top: 2pt;padding-left: 285pt;text-indent: 0pt;text-align: justify;">bread <span class="s86">⇒ </span>milk</p><p style="padding-top: 7pt;padding-left: 139pt;text-indent: 0pt;text-align: justify;">has a conﬁdence of 80 percent if 80 percent of the purchases that include bread also include milk. A rule with a low conﬁdence is not meaningful. In business applications, rules usually have conﬁdences signiﬁcantly less than 100 percent, whereas in other domains, such as in physics, rules may have high conﬁdences.</p><p class="s13" style="padding-top: 1pt;padding-left: 139pt;text-indent: 17pt;line-height: 77%;text-align: justify;"><span class="p">Note that the conﬁdence of </span>bread <span class="s86">⇒ </span>milk <span class="p">may be very diﬀerent from the conﬁdence of </span>milk <span class="s86">⇒ </span>bread<span class="p">, although both have the same support.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">11.4.5 Clustering</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Intuitively, clustering refers to the problem of ﬁnding clusters of points in the given data. The problem of <span class="s63">clustering </span>can be formalized from distance metrics in several ways. One way is to phrase it as the problem of grouping points into <i>k </i>sets (for a given <i>k</i>) so that the average distance of points from the <i>centroid </i>of their assigned cluster is minimized. <span class="s76">3</span> Another way is to group points so that the average distance between every pair of points in each cluster is minimized. There are other deﬁnitions too; see the bibliographical notes for details. But the intuition behind all these deﬁnitions is to group similar points together in a single set.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Another type of clustering appears in classiﬁcation systems in biology. (Such clas- siﬁcation systems do not attempt to <i>predict </i>classes; rather they attempt to cluster re- lated items together.) For instance, leopards and humans are clustered under the class mammalia, while crocodiles and snakes are clustered under reptilia. Both mammalia and reptilia come under the common class chordata. The clustering of mammalia has further subclusters, such as carnivora and primates. We thus have <span class="s63">hierarchical cluster- ing</span>. Given characteristics of diﬀerent species, biologists have created a complex hier- archical clustering scheme grouping related species together at diﬀerent levels of the hierarchy.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The statistics community has studied clustering extensively. Database research has provided scalable clustering algorithms that can cluster very large datasets (that may not ﬁt in memory).</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">An interesting application of clustering is to predict what new movies (or books or music) a person is likely to be interested in on the basis of:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 97pt;text-indent: 0pt;text-align: left;">1. <span class="p">The person’s past preferences in movies.</span></p><p class="s63" style="padding-top: 6pt;padding-left: 96pt;text-indent: 0pt;text-align: left;">2. <span class="p">Other people with similar past preferences.</span></p><p class="s63" style="padding-top: 6pt;padding-left: 96pt;text-indent: 0pt;text-align: left;">3. <span class="p">The preferences of such people for new movies.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">One approach to this problem is as follows: To ﬁnd people with similar past preferences we create clusters of people based on their preferences for movies. The accuracy of clustering can be improved by previously clustering movies by their similarity, so even if people have not seen the same movies, if they have seen similar movies they would be clustered together. We can repeat the clustering, alternately clustering people, then movies, then people, and so on until we reach an equilibrium. Given a new user, we ﬁnd a cluster of users most similar to that user, on the basis of the user’s preferences for movies already seen. We then predict movies in movie clusters that are popular with that user’s cluster as likely to be interesting to the new user. In fact, this problem</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="95" height="1" alt="image" src="Image_2515.png"/></span></p><p class="s293" style="text-indent: 0pt;line-height: 6pt;text-align: center;"> i<span class="s294">=</span><span class="s295">1 </span></p><p class="s223" style="text-indent: 0pt;line-height: 7pt;text-align: center;">n</p><p style="text-indent: 0pt;text-align: left;"/><p class="s77" style="padding-top: 2pt;padding-left: 88pt;text-indent: 0pt;line-height: 10pt;text-align: left;">3<span class="s78">The centroid of a set of points is deﬁned as a point whose coordinate on each dimension is the average of the coor- dinates of all the points of that set on that dimension. For example, in two dimensions, the centroid of a set of points</span></p><p class="s296" style="text-indent: 0pt;line-height: 7pt;text-align: left;">1</p><p style="text-indent: 0pt;text-align: left;"/><p class="s296" style="text-indent: 0pt;line-height: 7pt;text-align: left;">1</p><p style="text-indent: 0pt;text-align: left;"/><p class="s296" style="text-indent: 0pt;line-height: 7pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s296" style="text-indent: 0pt;line-height: 7pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s223" style="text-indent: 0pt;line-height: 7pt;text-align: left;">n</p><p style="text-indent: 0pt;text-align: left;"/><p class="s223" style="text-indent: 0pt;line-height: 7pt;text-align: left;">n</p><p style="text-indent: 0pt;text-align: left;"/><p class="s293" style="text-indent: 0pt;line-height: 6pt;text-align: center;"> i<span class="s294">=</span><span class="s295">1  </span></p><p class="s223" style="text-indent: 0pt;line-height: 7pt;text-align: center;">n</p><p style="text-indent: 0pt;text-align: left;"/><p class="s80" style="padding-left: 88pt;text-indent: 0pt;text-align: left;"><span class="s297">{ </span>(<i>x </i>, <i>y </i>), (<i>x </i>, <i>y </i>), <span class="s230">… </span>, (<i>x </i>, <i>y </i>) <span class="s297">} </span>is given by <span class="s298">( </span><span class="s299">∑</span><span class="s300">n</span></p><p class="s301" style="padding-left: 4pt;text-indent: 0pt;line-height: 30%;text-align: left;">x<span class="s302">i </span><span class="s303">, </span><span class="s304">∑</span><span class="s305">n</span></p><p class="s306" style="padding-left: 4pt;text-indent: 0pt;line-height: 35%;text-align: left;">y<span class="s302">i </span><span class="s230">)</span><span class="s303">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">is an instance of <i>collaborative filtering</i>, where users collaborate in the task of ﬁltering information to ﬁnd information of interest.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">11.4.6 Text Mining</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;"><span class="s63">Text mining </span>applies data-mining techniques to textual documents. There are a number of diﬀerent text mining tasks. One such task is <span class="s63">sentiment analysis</span>. For example, sup- pose a company wishes to ﬁnd out how users have reacted to a new product. There are typically a large number of product reviews on the web — for example, reviews by diﬀerent users on e-commerce platforms. Reading each review to ﬁnd out reactions is not practical for a human. Instead, the company may analyze reviews to ﬁnd the <i>sentiment </i>of the reviews of the product; the sentiment could be positive, negative, or neutral. The occurrence of speciﬁc words such as excellent, good, awesome, beautiful, and so on are correlated with a positive sentiment, while words such as awful, average, worthless, poor quality, and so on are correlated with a negative sentiment. Sentiment analysis techniques can be used to analyze the reviews and come up with an overall score reﬂecting the broad sense of the reviews.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Another task is <span class="s63">information extraction</span>, which creates structured information from unstructured textual descriptions, or semi-structured data such as tabular displays of data in documents. A key subtask of this process is <span class="s63">entity recognition</span>, that is, the task of identifying mentions of entities in text and disambiguating them. For example, an arti- cle may mention the name Michael Jordan. There are at least two famous people named Michael Jordan: one was a basketball player, while the other is a professor who is a well known machine-learning expert. <span class="s63">Disambiguation </span>is the process of ﬁguring out which of these two is being referred to in a particular article, and it can be done based on the article context; in this case, an occurrence of the name Michael Jordan in a sports arti- cle probably refers to the basketball player, while an occurrence in a machine-learning paper probably refers to the professor. After entity recognition, other techniques may be used to learn attributes of entities and to learn relationships between entities.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Information extraction can be used in many ways. For example, it can be used to analyze customer support conversations or reviews posted on social media, to judge customer satisfaction, and to decide when intervention is needed to retain customers. Service providers may want to know what aspect of the service such as pricing, quality, hygiene, or behavior of the person providing the service, a review was positive or nega- tive about; information extraction techniques can be used to infer what aspect an article or a part of an article is about, and to infer the associated sentiment. Attributes such as the location of service can also be extracted and are important for taking corrective action.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Information extracted from the enormous collection of documents and other re- sources on the web can be valuable for many tasks. Such extracted information can be represented in a graph, called a <span class="s63">knowledge graph</span>, which we outlined in Section 8.1.4. Such knowledge graphs are used by web search engines to generate more meaningful answers to user queries.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part220.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part222.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
