<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Linux Kernel Development, Third Edition</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<div id="filepos575425" style="height:0pt"></div><h2 class="calibre_4" id="calibre_pb_50"><span class="bold">9. An Introduction to Kernel Synchronization</span></h2><div class="calibre_5"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos575611"> </div>
<p class="calibre_2">In a shared memory application, developers must ensure that shared resources are protected from concurrent access. The kernel is no exception. Shared resources require protection from concurrent access because if multiple threads of execution<sup class="calibre8"><a id="filepos575909" href="#filepos576325">1</a></sup> access and manipulate the data at the same time, the threads may overwrite each other’s changes or access data while it is in an inconsistent state. Concurrent access of shared data is a recipe for instability that often proves hard to track down and debug—getting it right at the start is important.</p><div class="calibre_3"> </div>
<p class="calibre_2"><sup class="calibre8"><a id="filepos576325" href="#filepos575909">1</a></sup>
<em class="calibre4">The term threads of execution implies any instance of executing code. This includes, for example, a task in the kernel, an interrupt handler, a bottom half, or a kernel thread. This chapter may shorten</em> threads of execution to <em class="calibre4">simply</em> threads. <em class="calibre4">Keep in mind that this term describes any executing code.</em></p><div class="calibre_3"> </div>
<p class="calibre_2">Properly protecting shared resources can be tough. Years ago, before Linux supported symmetrical multiprocessing, preventing concurrent access of data was simple. Because only a single processor was supported, the only way data could be concurrently accessed was if an interrupt occurred or if kernel code explicitly rescheduled and enabled another task to run. With earlier kernels, development was simple.</p><div class="calibre_3"> </div>
<p class="calibre_2">Those halcyon days are over. Symmetrical multiprocessing support was introduced in the 2.0 kernel and has been continually enhanced ever since. Multiprocessing support implies that kernel code can simultaneously run on two or more processors. Consequently, without protection, code in the kernel, running on two different processors, can simultaneously access shared data at exactly the same time. With the introduction of the 2.6 kernel, the Linux kernel is preemptive. This implies that (again, in the absence of protection) the scheduler can preempt kernel code at virtually any point and reschedule another task. Today, a number of scenarios enable for concurrency inside the kernel, and they all require protection.</p><div class="calibre_3"> </div>
<p class="calibre_2"><a id="filepos577983"></a>This chapter discusses the issues of concurrency and synchronization in the abstract, as they exist in any operating system kernel. The next chapter details the specific mechanisms and interfaces that the Linux kernel provides to solve synchronization issues and prevent race conditions.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos578321"> </div>
<h3 class="calibre_21"><span class="bold">Critical Regions and Race Conditions</span></h3><div class="calibre_22"> </div>
<p class="calibre_2">Code paths that access and manipulate shared data are called <em class="calibre4">critical regions</em> (also called <em class="calibre4">critical sections</em>). It is usually unsafe for multiple threads of execution to access the same resource simultaneously. To prevent concurrent access during critical regions, the programmer must ensure that code executes <em class="calibre4">atomically</em>—that is, operations complete without interruption as if the entire critical region were one indivisible instruction. It is a bug if it is possible for two threads of execution to be simultaneously executing within the same critical region. When this does occur, we call it a <em class="calibre4">race condition</em>, so-named because the threads <em class="calibre4">raced</em> to get there first. Note how rare a race condition in your code might manifest itself—debugging race conditions is often difficult because they are not easily reproducible. Ensuring that unsafe concurrency is prevented and that race conditions do not occur is called <em class="calibre4">synchronization</em>.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos579507"> </div>
<h4 class="calibre_27"><span class="calibre3">Why Do We Need Protection?</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">To best understand the need for synchronization, let’s look at the ubiquity of race conditions. For a first example, let’s consider a real-world case: an ATM (Automated Teller Machine, called a cash machine, cashpoint, or ABM outside of the United States).</p><div class="calibre_3"> </div>
<p class="calibre_2">One of the most common functions performed by cash machines is withdrawing money from an individual’s personal bank account. A person walks up to the machine, inserts an ATM card, types in a PIN, selects <em class="calibre4">Withdrawal</em>, inputs a pecuniary amount, hits OK, takes the money, and mails it to me.</p><div class="calibre_3"> </div>
<p class="calibre_2">After the user has asked for a specific amount of money, the cash machine needs to ensure that the money actually exists in that user’s account. If the money exists, it then needs to deduct the withdrawal from the total funds available. The code to implement this would look something like</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00104.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2"><a id="filepos580742"></a>Now, let’s presume that another deduction in the user’s funds is happening at the same time. It does not matter <em class="calibre4">how</em> the simultaneous deduction is happening: Assume that the user’s spouse is initiating another withdrawal at another ATM, a payee is electronically transferring funds out of the account, or the bank is deducting a fee from the account (as banks these days are so wont to do). Any of these scenarios fits our example.</p><div class="calibre_3"> </div>
<p class="calibre_2">Both systems performing the withdrawal would have code similar to what we just looked at: First check whether the deduction is possible, then compute the new total funds, and finally execute the physical deduction. Now let’s make up some numbers. Presume that the first deduction is a withdrawal from an ATM for $100 and that the second deduction is the bank applying a fee of $10 because the customer walked into the bank. Assume the customer has a total of $105 in the bank. Obviously, one of these transactions <em class="calibre4">cannot</em> correctly complete without sending the account into the red.</p><div class="calibre_3"> </div>
<p class="calibre_2">What you would expect is something like this: The fee transaction happens first. Ten dollars is less than $105, so 10 is subtracted from 105 to get a new total of 95, and $10 is pocketed by the bank. Then the ATM withdrawal comes along and fails because $95 is less than $100.</p><div class="calibre_3"> </div>
<p class="calibre_2">With race conditions, life can be much more interesting. Assume that the two transactions are initiated at roughly the same time. Both transactions verify that sufficient funds exist: $105 is more than both $100 and $10, so all is good. Then the withdrawal process subtracts $100 from $105, yielding $5. The fee transaction then does the same, subtracting $10 from $105 and getting $95. The withdrawal process then updates the user’s new total available funds to $5. Now the fee transaction also updates the new total, resulting in $95. Free money!</p><div class="calibre_3"> </div>
<p class="calibre_2">Clearly, financial institutions must ensure that this can never happen. They must lock the account during certain operations, making each transaction atomic with respect to any other transaction. Such transactions must occur in their entirety, without interruption, or not occur at all.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos583166"> </div>
<h4 class="calibre_27"><span class="calibre3">The Single Variable</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">Now, let’s look at a specific computing example. Consider a simple shared resource, a single global integer, and a simple critical region, the operation of merely incrementing it:</p><div class="calibre_3"> </div>
<p class="calibre_28"><tt class="calibre6"><span class="calibre12">i++;</span></tt></p><div class="calibre_22"> </div>
<p class="calibre_2">This might translate into machine instructions to the computer’s processor that resemble the following:</p><div class="calibre_3"> </div>
<p class="calibre_28"><tt class="calibre6"><span class="calibre12">get the current value of i and copy it into a register<br class="calibre1"/>add one to the value stored in the register<br class="calibre1"/>write back to memory the new value of i</span></tt></p><div class="calibre_22"> </div>
<p class="calibre_2">Now, assume that there are two threads of execution, both enter this critical region, and the initial value of <code class="calibre6"><span class="calibre7">i</span></code> is 7. The desired outcome is then similar to the following (with each row representing a unit of time):</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00105.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2">As expected, 7 incremented twice is 9. A possible outcome, however, is the following:</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00106.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2">If both threads of execution read the initial value of <code class="calibre6"><span class="calibre7">i</span></code> before it is incremented, both threads increment and save the same value. As a result, the variable <code class="calibre6"><span class="calibre7">i</span></code> contains the value 8 when, in fact, it should now contain 9. This is one of the simplest examples of a critical region. Thankfully, the solution is equally as simple: We merely need a way to perform these operations in one indivisible step. Most processors provide an instruction to atomically read, increment, and write back a single variable. Using this <em class="calibre4">atomic instruction</em>, the only possible outcome is</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00107.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2">Or conversely</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00108.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2"><a id="filepos585586"></a>It would never be possible for the two atomic operations to interleave. The processor would physically ensure that it was impossible. Using such an instruction would alleviate the problem. The kernel provides a set of interfaces that implement these atomic instructions; they are discussed in the next chapter.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos585947"> </div>
<h3 class="calibre_21"><span class="bold">Locking</span></h3><div class="calibre_22"> </div>
<p class="calibre_2">Now, let’s consider a more complicated race condition that requires a more complicated solution. Assume you have a queue of requests that needs to be serviced. For this exercise, let’s assume the implementation is a linked list, in which each node represents a request. Two functions manipulate the queue. One function adds a new request to the tail of the queue. Another function removes a request from the head of the queue and does something useful with the request. Various parts of the kernel invoke these two functions; thus, requests are continually being added, removed, and serviced. Manipulating the request queues certainly requires multiple instructions. If one thread attempts to read from the queue while another is in the middle of manipulating it, the reading thread will find the queue in an inconsistent state. It should be apparent the sort of damage that could occur if access to the queue could occur concurrently. Often, when the shared resource is a complex data structure, the result of a race condition is corruption of the data structure.</p><div class="calibre_3"> </div>
<p class="calibre_2">The previous scenario, at first, might not have a clear solution. How can you prevent one processor from reading from the queue while another processor is updating it? Although it is feasible for a particular architecture to implement simple instructions, such as arithmetic and comparison, atomically it is ludicrous for architectures to provide instructions to support the indefinitely sized critical regions that would exist in the previous example. What is needed is a way of making sure that only one thread manipulates the data structure at a time—a mechanism for preventing access to a resource while another thread of execution is in the marked region.</p><div class="calibre_3"> </div>
<p class="calibre_2">A <em class="calibre4">lock</em> provides such a mechanism; it works much like a lock on a door. Imagine the room beyond the door as the critical region. Inside the room, only one thread of execution can be present at a given time. When a thread enters the room, it locks the door behind it. When the thread is finished manipulating the shared data, it leaves the room and unlocks the door. If another thread reaches the door while it is locked, it must wait for the thread inside to exit the room and unlock the door before it can enter. Threads hold locks; locks protect data.</p><div class="calibre_3"> </div>
<p class="calibre_2">In the previous request queue example, a single lock could have been used to protect the queue. Whenever there was a new request to add to the queue, the thread would first obtain the lock. Then it could safely add the request to the queue and ultimately release the lock. When a thread wanted to remove a request from the queue, it too would obtain the lock. Then it could read the request and remove it from the queue. Finally, it would release the lock. Any other access to the queue would similarly need to obtain the lock. Because the lock can be held by only one thread at a time, only a single thread can manipulate the queue at a time. If a thread comes along while another thread is already <a id="filepos589213"></a>updating it, the second thread has to wait for the first to release the lock before it can continue. The lock prevents concurrency and protects the queue from race conditions.</p><div class="calibre_3"> </div>
<p class="calibre_2">Any code that accesses the queue first needs to obtain the relevant lock. If another thread of execution comes along, the lock prevents concurrency:</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00109.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2">Notice that locks are <em class="calibre4">advisory</em> and <em class="calibre4">voluntary</em>. Locks are entirely a programming construct that the programmer must take advantage of. Nothing prevents you from writing code that manipulates the fictional queue without the appropriate lock. Such a practice, of course, would eventually result in a race condition and corruption.</p><div class="calibre_3"> </div>
<p class="calibre_2">Locks come in various shapes and sizes—Linux alone implements a handful of different locking mechanisms. The most significant difference between the various mechanisms is the behavior when the lock is unavailable because another thread already holds it—some lock variants <em class="calibre4">busy wait</em>,<sup class="calibre8"><a id="filepos590429" href="#filepos590710">2</a></sup> whereas other locks put the current task to sleep until the lock becomes available. The next chapter discusses the behavior of the different locks in Linux and their interfaces.</p><div class="calibre_3"> </div>
<p class="calibre_2"><sup class="calibre8"><a id="filepos590710" href="#filepos590429">2</a></sup>
<em class="calibre4">That is, spin in a tight loop, checking the status of the lock over and over, waiting for the lock to become available.</em></p><div class="calibre_3"> </div>
<p class="calibre_2">Astute readers are now screaming. The lock does not solve the problem; it simply shrinks the critical region down to just the lock and unlock code: probably much smaller, sure, but still a potential race! Fortunately, locks are implemented using atomic operations that ensure no race exists. A single instruction can verify whether the key is taken and, if not, seize it. How this is done is architecture-specific, but almost all processors implement an atomic <em class="calibre4">test and set</em> instruction that tests the value of an integer and sets it to a new value only if it is zero. A value of zero means unlocked. On the popular x86 architecture, locks are implemented using such a similar instruction called <em class="calibre4">compare and exchange</em>.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos591726"> </div>
<h4 class="calibre_27"><span class="calibre3">Causes of Concurrency</span></h4><div class="calibre_24"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos591849"> </div>
<p class="calibre_2">In user-space, the need for synchronization stems from the fact that programs are scheduled preemptively at the will of the scheduler. Because a process can be preempted at any time and another process can be scheduled onto the processor, a process can be involuntarily preempted in the middle of accessing a critical region. If the newly scheduled process then enters the same critical region (say, if the two processes manipulate the same shared memory or write to the same file descriptor), a race can occur. The same problem can occur with multiple single-threaded processes sharing files, or within a single program with signals, because signals can occur asynchronously. This type of concurrency—in which two things do not actually happen at the same time but interleave with each other such that they might as well—is called <em class="calibre4">pseudo-concurrency</em>.</p><div class="calibre_3"> </div>
<p class="calibre_2">If you have a symmetrical multiprocessing machine, two processes can actually be executed in a critical region at the exact same time. That is called <em class="calibre4">true concurrency</em>. Although the causes and semantics of true versus pseudo concurrency are different, they both result in the same race conditions and require the same sort of protection.</p><div class="calibre_3"> </div>
<p class="calibre_2">The kernel has similar causes of concurrency:</p><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• <strong class="calibre3">Interrupts—</strong> An interrupt can occur asynchronously at almost any time, interrupting the currently executing code.</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• <strong class="calibre3">Softirqs and tasklets—</strong> The kernel can raise or schedule a softirq or tasklet at almost any time, interrupting the currently executing code.</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• <strong class="calibre3">Kernel preemption—</strong> Because the kernel is preemptive, one task in the kernel can preempt another.</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• <strong class="calibre3">Sleeping and synchronization with user-space—</strong> A task in the kernel can sleep and thus invoke the scheduler, resulting in the running of a new process.</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• <strong class="calibre3">Symmetrical multiprocessing—</strong> Two or more processors can execute kernel code at exactly the same time.</p></blockquote><div class="calibre_3"> </div>
<p class="calibre_2">Kernel developers need to understand and prepare for these causes of concurrency. It is a major bug if an interrupt occurs in the middle of code that is manipulating a resource and the interrupt handler can access the same resource. Similarly, it is a bug if kernel code is preemptive while it is accessing a shared resource. Likewise, it is a bug if code in the kernel sleeps while in the middle of a critical section. Finally, two processors should never simultaneously access the same piece of data. With a clear picture of what data needs protection, it is not hard to provide the locking to keep the system stable. Rather, the hard part is identifying these conditions and realizing that to prevent concurrency, you need some form of protection.</p><div class="calibre_3"> </div>
<p class="calibre_2">Let us reiterate this point, because it is important. Implementing the actual locking in your code to protect shared data is not difficult, especially when done early on during the design phase of development. The tricky part is identifying the actual shared data and the corresponding critical sections. This is why designing locking into your code from the get-go, and not as an afterthought, is of paramount importance. It can be difficult to go <a id="filepos595708"></a>in, <em class="calibre4">ex post</em>, and identify critical regions and retrofit locking into the existing code. The resulting code is often not pretty, either. The takeaway from this is to <em class="calibre4">always</em> design proper locking into your code from the beginning.</p><div class="calibre_3"> </div>
<p class="calibre_2">Code that is safe from concurrent access from an interrupt handler is said to be <em class="calibre4">interrupt-safe</em>. Code that is safe from concurrency on symmetrical multiprocessing machines is <em class="calibre4">SMP-safe</em>. Code that is safe from concurrency with kernel preemption is <em class="calibre4">preempt-safe</em>.<sup class="calibre8"><a id="filepos596296" href="#filepos596540">3</a></sup> The actual mechanisms used to provide synchronization and protect against race conditions in all these cases is covered in the next chapter.</p><div class="calibre_3"> </div>
<p class="calibre_2"><sup class="calibre8"><a id="filepos596540" href="#filepos596296">3</a></sup>
<em class="calibre4">You will also see that, barring a few exceptions, being SMP-safe implies being preempt-safe.</em></p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos596744"> </div>
<h4 class="calibre_27"><span class="calibre3">Knowing What to Protect</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">Identifying what data specifically needs protection is vital. Because any data that can be accessed concurrently almost assuredly needs protection, it is often easier to identify what data does <em class="calibre4">not</em> need protection and work from there. Obviously, any data that is local to one particular thread of execution does not need protection, because only that thread can access the data. For example, local automatic variables (and dynamically allocated data structures whose address is stored only on the stack) do not need any sort of locking because they exist solely on the stack of the executing thread. Likewise, data that is accessed by only a specific task does not require locking (because a process can execute on only one processor at a time).</p><div class="calibre_3"> </div>
<p class="calibre_2">What <em class="calibre4">does</em> need locking? Most global kernel data structures do. A good rule of thumb is that if another thread of execution can access the data, the data needs some sort of locking; if anyone else can see it, lock it. Remember to lock <em class="calibre4">data</em>, not <em class="calibre4">code</em>.</p><div class="calibre_3"> </div>
<div border="1" class="calibre_26"><blockquote class="calibre10"><div class="calibre11">
<p class="calibre_2"></p><div class="calibre_3"> </div>
<p class="calibre_23"><span class="calibre9"><span class="calibre3">CONFIG Options: SMP Versus UP</span></span></p><div class="calibre_24"> </div>
<p class="calibre_2">Because the Linux kernel is configurable at compile time, it makes sense that you can tailor the kernel specifically for a given machine. Most important, the <code class="calibre6"><span class="calibre7">CONFIG_SMP</span></code> configure option controls whether the kernel supports SMP. Many locking issues disappear on uniprocessor machines; consequently, when <code class="calibre6"><span class="calibre7">CONFIG_SMP</span></code> is unset, unnecessary code is not compiled into the kernel image. For example, such configuration enables uniprocessor machines to forego the overhead of spin locks. The same trick applies to <code class="calibre6"><span class="calibre7">CONFIG_PREEMPT</span></code> (the configure option enabling kernel preemption). This was an excellent design decision—the kernel maintains one clean source base, and the various locking mechanisms are used as needed. Different combinations of <code class="calibre6"><span class="calibre7">CONFIG_SMP</span></code> and <code class="calibre6"><span class="calibre7">CONFIG_PREEMPT</span></code> on different architectures compile in varying lock support.</p><div class="calibre_3"> </div>
<p class="calibre_2">In your code, provide appropriate protection for the most pessimistic case, SMP with kernel preemption, and all scenarios will be covered.</p><div class="calibre_3"> </div>
</div></blockquote></div><div class="calibre_7"> </div>
<p class="calibre_2"><a id="filepos599545"></a>Whenever you write kernel code, you should ask yourself these questions:</p><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• Is the data global? Can a thread of execution other than the current one access it?</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• Is the data shared between process context and interrupt context? Is it shared between two different interrupt handlers?</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• If a process is preempted while accessing this data, can the newly scheduled process access the same data?</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• Can the current process sleep (block) on anything? If it does, in what state does that leave any shared data?</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• What prevents the data from being freed out from under me?</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• What happens if this function is called again on another processor?</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• Given the proceeding points, how am I going to ensure that my code is safe from concurrency?</p></blockquote><div class="calibre_3"> </div>
<p class="calibre_2">In short, nearly all global and shared data in the kernel requires some form of the synchronization methods, discussed in the next chapter.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos601095"> </div>
<h3 class="calibre_21"><span class="bold">Deadlocks</span></h3><div class="calibre_22"> </div>
<p class="calibre_2">A <em class="calibre4">deadlock</em> is a condition involving one or more threads of execution and one or more resources, such that each thread waits for one of the resources, but all the resources are already held. The threads all wait for each other, but they never make any progress toward releasing the resources that they already hold. Therefore, none of the threads can continue, which results in a deadlock.</p><div class="calibre_3"> </div>
<p class="calibre_2">A good analogy is a four-way traffic stop. If each car at the stop decides to wait for the other cars before going, no car will ever proceed, and we have a traffic deadlock.</p><div class="calibre_3"> </div>
<p class="calibre_2">The simplest example of a deadlock is the self-deadlock:<sup class="calibre8"><a id="filepos601940" href="#filepos602262">4</a></sup> If a thread of execution attempts to acquire a lock it already holds, it has to wait for the lock to be released. But it will never release the lock, because it is busy waiting for the lock, and the result is deadlock:</p><div class="calibre_3"> </div>
<p class="calibre_2"><sup class="calibre8"><a id="filepos602262" href="#filepos601940">4</a></sup>
<em class="calibre4">Some kernels prevent this type of deadlock by providing recursive locks. These are locks that a single thread of execution may acquire multiple times. Linux, thankfully, does not provide recursive locks. This is widely considered a good thing. Although recursive locks might alleviate the self-deadlock problem, they very readily lead to sloppy locking semantics.</em></p><div class="calibre_3"> </div>
<p class="calibre_28"><tt class="calibre6"><span class="calibre12">acquire lock<br class="calibre1"/>acquire lock, again<br class="calibre1"/>wait for lock to become available<br class="calibre1"/>...</span></tt></p><div class="calibre_22"> </div>
<p class="calibre_2"><a id="filepos602922"></a>Similarly, consider <em class="calibre4">n</em> threads and <em class="calibre4">n</em> locks. If each thread holds a lock that the other thread wants, all threads block while waiting for their respective locks to become available. The most common example is with two threads and two locks, which is often called the <em class="calibre4">deadly embrace</em> or the <em class="calibre4">ABBA deadlock</em>:</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00110.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2">Each thread is waiting for the other, and neither thread will ever release its original lock; therefore, neither lock will become available.</p><div class="calibre_3"> </div>
<p class="calibre_2">Prevention of deadlock scenarios is important. Although it is difficult to prove that code is free of deadlocks, you <em class="calibre4">can</em> write deadlock-free code. A few simple rules go a long way:</p><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• Implement lock ordering. Nested locks must <em class="calibre4">always</em> be obtained in the same order. This prevents the deadly embrace deadlock. Document the lock ordering so others will follow it.</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• Prevent starvation. Ask yourself, <em class="calibre4">does this code always finish? If</em> foo <em class="calibre4">does not occur, will</em> bar <em class="calibre4">wait forever?</em></p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• Do not double acquire the same lock.</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• Design for simplicity. Complexity in your locking scheme invites deadlocks.</p></blockquote><div class="calibre_3"> </div>
<p class="calibre_2">The first point is most important and worth stressing. If two or more locks are acquired at the same time, they must <em class="calibre4">always</em> be acquired in the same order. Let’s assume you have the <em class="calibre4">cat</em>, <em class="calibre4">dog</em>, and <em class="calibre4">fox</em> locks that protect data structures of the same name. Now assume you have a function that needs to work on all three of these data structures simultaneously—perhaps to copy data between them. Whatever the case, the data structures require locking to ensure safe access. If one function acquires the locks in the order <em class="calibre4">cat</em>, <em class="calibre4">dog</em>, and then <em class="calibre4">fox</em>, then <em class="calibre4">every</em> other function must obtain these locks (or a subset of them) in this same order. For example, it is a potential deadlock (and hence a bug) to first obtain the <em class="calibre4">fox</em> lock and then obtain the <em class="calibre4">dog</em> lock because the <em class="calibre4">dog</em> lock must always be acquired prior to the <em class="calibre4">fox</em> lock. Here is an example in which this would cause a deadlock:</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00111.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2"><a id="filepos605736"></a>Thread one is waiting for the fox lock, which thread two holds, while thread two is waiting for the dog lock, which thread one holds. Neither ever releases its lock and hence both wait forever—bam, deadlock. If the locks were always obtained in the same order, a deadlock in this manner would not be possible.</p><div class="calibre_3"> </div>
<p class="calibre_2">Whenever locks are nested within other locks, a specific ordering must be obeyed. It is good practice to place the ordering in a comment above the lock. Something like the following is a good idea:</p><div class="calibre_3"> </div>
<p class="calibre_28"><tt class="calibre6"><span class="calibre12">/*<br class="calibre1"/> * cat_lock – locks access to the cat structure<br class="calibre1"/> * always obtain before the dog lock!<br class="calibre1"/> */</span></tt></p><div class="calibre_22"> </div>
<p class="calibre_2">The order of <em class="calibre4">unlock</em> does not matter with respect to deadlock, although it is common practice to release the locks in an order inverse to that in which they were acquired.</p><div class="calibre_3"> </div>
<p class="calibre_2">Preventing deadlocks is important. The Linux kernel has some basic debugging facilities for detecting deadlock scenarios in a running kernel. These features are discussed in the next chapter.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos607050"> </div>
<h3 class="calibre_21"><span class="bold">Contention and Scalability</span></h3><div class="calibre_22"> </div>
<p class="calibre_2">The term <em class="calibre4">lock contention</em>, or simply <em class="calibre4">contention</em>, describes a lock currently in use but that another thread is trying to acquire. A lock that is <em class="calibre4">highly contended</em> often has threads waiting to acquire it. High contention can occur because a lock is frequently obtained, held for a long time after it is obtained, or both. Because a lock’s job is to serialize access to a resource, it comes as no surprise that locks can slow down a system’s performance. A highly contended lock can become a bottleneck in the system, quickly limiting its performance. Of course, the locks are also required to prevent the system from tearing itself to shreds, so a solution to high contention must continue to provide the necessary concurrency protection.</p><div class="calibre_3"> </div>
<p class="calibre_2"><em class="calibre4">Scalability</em> is a measurement of how well a system can be expanded. In operating systems, we talk of the scalability with a large number of processes, a large number of processors, or large amounts of memory. We can discuss scalability in relation to virtually any component of a computer to which we can attach a quantity. Ideally, doubling the number of processors should result in a doubling of the system’s processor performance. This, of course, is never the case.</p><div class="calibre_3"> </div>
<p class="calibre_2">The scalability of Linux on a large number of processors has increased dramatically in the time since multiprocessing support was introduced in the 2.0 kernel. In the early days of Linux multiprocessing support, only one task could execute in the kernel at a time. During 2.2, this limitation was removed as the locking mechanisms grew more fine-grained. Through 2.4 and onward, kernel locking became even finer grained. Today, in the 2.6 Linux kernel, kernel locking is very fine-grained and scalability is good.</p><div class="calibre_3"> </div>
<p class="calibre_2">The granularity of locking is a description of the size or amount of data that a lock protects. A very coarse lock protects a large amount of data—for example, an entire <a id="filepos609277"></a>subsystem’s set of data structures. On the other hand, a very fine-grained lock protects a small amount of data—say, only a single element in a larger structure. In reality, most locks fall somewhere in between these two extremes, protecting neither an entire subsystem nor an individual element, but perhaps a single structure or list of structures. Most locks start off fairly coarse and are made more fine-grained as lock contention proves to be a problem.</p><div class="calibre_3"> </div>
<p class="calibre_2">One example of evolving to finer-grained locking is the scheduler runqueues, discussed in <a href="index_split_013.html#filepos223287">Chapter 4</a>, “Process Scheduling.” In 2.4 and prior kernels, the scheduler had a single runqueue. (Recall that a runqueue is the list of runnable processes.) Early in the 2.6 series, the O(1) scheduler introduced per-processor runqueues, each with a unique lock. The locking evolved from a single global lock to separate locks for each processor. This was an important optimization, because the runqueue lock was highly contended on large machines, essentially serializing the entire scheduling process down to a single processor executing in the scheduler at a time. Later in the 2.6 series, the <em class="calibre4">CFS Scheduler</em> improved scalability further.</p><div class="calibre_3"> </div>
<p class="calibre_2">Generally, this scalability improvement is a good thing because it improves Linux’s performance on larger and more powerful systems. Rampant scalability “improvements” can lead to a decrease in performance on smaller SMP and UP machines, however, because smaller machines may not need such fine-grained locking but will nonetheless need to put up with the increased complexity and overhead. Consider a linked list. An initial locking scheme would provide a single lock for the entire list. In time, this single lock might prove to be a scalability bottleneck on large multiprocessor machines that frequently access this linked list. In response, the single lock could be broken up into one lock per node in the linked list. For each node that you wanted to read or write, you obtained the node’s unique lock. Now there is only lock contention when multiple processors are accessing the same exact node. What if there is still lock contention, however? Do you provide a lock for each element in each node? Each bit of each element? The answer is <em class="calibre4">no</em>. Even though this fine-grained locking might ensure excellent scalability on large SMP machines, how does it perform on dual processor machines? The overhead of all those extra locks is wasted if a dual processor machine does not see significant lock contention to begin with.</p><div class="calibre_3"> </div>
<p class="calibre_2">Nonetheless, scalability is an important consideration. Designing your locking from the beginning to scale well is important. Coarse locking of major resources can easily become a bottleneck on even small machines. There is a thin line between too-coarse locking and too-fine locking. Locking that is too coarse results in poor scalability if there is high lock contention, whereas locking that is too fine results in wasteful overhead if there is little lock contention. Both scenarios equate to poor performance. <em class="calibre4">Start simple and grow in complexity only as needed. Simplicity is key.</em></p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos612679"> </div>
<h3 class="calibre_21"><span class="bold">Conclusion</span></h3><div class="calibre_22"> </div>
<p class="calibre_2">Making your code SMP-safe is not something that can be added as an afterthought. Proper synchronization—locking that is free of deadlocks, scalable, and clean—requires design decisions from start through finish. Whenever you write kernel code, whether it is a new system call or a rewritten driver, protecting data from concurrent access needs to be a primary concern.</p><div class="calibre_3"> </div>
<p class="calibre_2">Provide sufficient protection for every scenario—SMP, kernel preemption, and so on—and rest assured the data will be safe on any given machine and configuration. The next chapter discusses just how to do this.</p><div class="calibre_3"> </div>
<p class="calibre_2">With the fundamentals and the theories of synchronization, concurrency, and locking behind us, let’s now dive into the actual tools that the Linux kernel provides to ensure that your code is race- and deadlock-free.</p><div class="calibre_3"> </div>  <div class="mbp_pagebreak" id="calibre_pb_51"></div>
</body></html>
