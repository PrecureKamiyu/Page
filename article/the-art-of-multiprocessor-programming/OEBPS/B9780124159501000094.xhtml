<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML" lang="EN" xml:lang="en"><head><meta content="text/html; charset=utf-8" http-equiv="default-style"/><title>The Art of Multiprocessor Programming</title><link href="Elsevier_eBook.css" rel="stylesheet" type="text/css"/><link href="math.css" rel="stylesheet" type="text/css"/><link href="media.css" media="only screen" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4f1c4a5b-a3e2-48ff-98f3-ff17812cd57a" name="Adept.expected.resource"/></head><body><section epub:type="chapter" role="doc-chapter"><div aria-label="Page 1" epub:type="pagebreak" id="page_1" role="doc-pagebreak"/><div id="CN"><a id="c0010tit1"/></div><header><hgroup><h1 class="chaptitle" id="c0010tit">Chapter 1: Introduction</h1></hgroup><section epub:type="preamble"><div class="abstract"><h2 class="h1hd" id="ab0010"><a id="st0010"/>Abstract</h2><p class="abspara">This chapter introduces and motivates the study of shared-memory multiprocessor programming, or concurrent programming. It describes the overall plan of the book, and then presents some basic concepts of concurrent computation, and presents some of the fundamental problems—mutual exclusion, the producer–consumer problem, and the readers–writers problem—and some simple approaches to solve these problems. It ends with a brief discussion of Amdahl's law.</p></div></section><section id="ks0010"><h3 class="h2hd" id="st0015">Keywords</h3><p class="keywords">parallelism; concurrent programming; shared-memory multiprocessors; safety; liveness; mutual exclusion; coordination protocol; producer—consumer problem; readers–writers problem; deadlock-freedom; starvation-freedom; Amdahl's law</p></section></header><p class="textfl" id="p0010">At the dawn of the twenty-first century, the computer industry underwent yet another revolution. The major chip manufacturers had increasingly been unable to make processor chips both smaller and faster. As Moore's law approached the end of its 50-year reign, manufacturers turned to “multicore” architectures, in which multiple processors (cores) on a single chip communicate directly through shared hardware caches. Multicore chips make computing more effective by exploiting <i>parallelism</i>: harnessing multiple circuits to work on a single task.</p><p class="text" id="p0015">The spread of multiprocessor architectures has had a pervasive effect on how we develop software. During the twentieth century, advances in technology brought regular increases in clock speed, so software would effectively “speed up” by itself over time. In this century, however, that “free ride” has come to an end. Today, advances in technology bring regular increases in parallelism, but only minor increases in clock speed. Exploiting that parallelism is one of the outstanding challenges of modern computer science.</p><p class="text" id="p0020">This book focuses on how to program multiprocessors that communicate via a shared memory. Such systems are often called <i>shared-memory multiprocessors</i> or, more recently, <i>multicores</i>. Programming challenges arise at all scales of multiprocessor systems—at a very small scale, processors within a single chip need to coordinate access to a shared memory location, and on a large scale, processors in a supercomputer need to coordinate the routing of data. Multiprocessor programming is challenging because modern computer systems are inherently <i>asynchronous</i>: activities can be halted or delayed without warning by interrupts, preemption, cache misses, failures, and other events. These delays are inherently unpredictable, and can vary enormously in scale: a cache miss might delay a processor for fewer than ten instructions, a page fault for a few million instructions, and operating system preemption for hundreds of millions of instructions.</p><p class="text" id="p0025">We approach multiprocessor programming from two complementary directions: principles and practice. In the <i>principles</i> part of this book, we focus on <i>computability</i>: figuring out what can be computed in an asynchronous concurrent environment. We use an idealized model of computation in which multiple concurrent <i>threads</i> manipulate a set of shared <i>objects</i>. The sequence of the thread operations on the objects is called the <i>concurrent program</i> or <i>concurrent algorithm</i>. This model is essentially the model presented by threads in Java, C#, and C++.</p><p class="text" id="p0030">Surprisingly, there are easy-to-specify shared objects that cannot be implemented by any concurrent algorithm. It is therefore important to understand what <span aria-label="Page 2" epub:type="pagebreak" id="page_2" role="doc-pagebreak"/>not to try, before proceeding to write multiprocessor programs. Many of the issues that will land multiprocessor programmers in trouble are consequences of fundamental limitations of the computational model, so we view the acquisition of a basic understanding of concurrent shared-memory computability as a necessary step. The chapters dealing with principles take the reader through a quick tour of asynchronous computability, attempting to expose various computability issues, and how they are addressed through the use of hardware and software mechanisms.</p><p class="text" id="p0035">An important step in the understanding of computability is the specification and verification of what a given program actually does. This is perhaps best described as <i>program correctness</i>. The correctness of multiprocessor programs, by their very nature, is more complex than that of their sequential counterparts, and requires a different set of tools, even for the purpose of “informal reasoning” (which, of course, is what most programmers actually do).</p><p class="text" id="p0040">Sequential correctness is mostly concerned with safety properties. A <i>safety</i> property states that some “bad thing” never happens. For example, a traffic light never displays green in all directions, even if the power fails. Naturally, concurrent correctness is also concerned with safety, but the problem is much, much harder, because safety must be ensured despite the vast number of ways that the steps of concurrent threads can be interleaved. Equally important, concurrent correctness encompasses a variety of <i>liveness</i> properties that have no counterparts in the sequential world. A <i>liveness</i> property states that a particular good thing will happen. For example, a red traffic light will eventually turn green.</p><p class="text" id="p0045">A final goal of the part of the book dealing with principles is to introduce a variety of metrics and approaches for reasoning about concurrent programs, which will later serve us when discussing the correctness of real-world objects and programs.</p><p class="text" id="p0050">The second part of the book deals with the <i>practice</i> of multiprocessor programming, and focuses on performance. Analyzing the performance of multiprocessor algorithms is also different in flavor from analyzing the performance of sequential programs. Sequential programming is based on a collection of well-established and well-understood abstractions. When we write a sequential program, we can often ignore that underneath it all, pages are being swapped from disk to memory, and smaller units of memory are being moved in and out of a hierarchy of processor caches. This complex memory hierarchy is essentially invisible, hiding behind a simple programming abstraction.</p><p class="text" id="p0055">In the multiprocessor context, this abstraction breaks down, at least from a performance perspective. To achieve adequate performance, programmers must sometimes “outwit” the underlying memory system, writing programs that would seem bizarre to someone unfamiliar with multiprocessor architectures. Someday, perhaps, concurrent architectures will provide the same degree of efficient abstraction as sequential architectures, but in the meantime, programmers should beware.</p><p class="text" id="p0060">The practice part of the book presents a progressive collection of shared objects and programming tools. Every object and tool is interesting in its own right, and we use each one to expose the reader to higher-level issues: spin locks illustrate contention, linked lists illustrate the role <span aria-label="Page 3" epub:type="pagebreak" id="page_3" role="doc-pagebreak"/>of locking in data structure design, and so on. Each of these issues has important consequences for program performance. We hope that readers will understand the issue in a way that will later allow them to apply the lessons learned to specific multiprocessor systems. We culminate with a discussion of state-of-the-art technologies such as <i>transactional memory</i>.</p><p class="text" id="p0065">For most of this book, we present code in the Java programming language, which provides automatic memory management. However, memory management is an important aspect of programming, especially concurrent programming. So, in the last two chapters, we switch to C++. In some cases, the code presented is simplified by omitting nonessential details. Complete code for all the examples is available on the book's companion website at <a href="https://textbooks.elsevier.com/web/product_details.aspx?isbn=978124159501"><i>https://textbooks.elsevier.com/web/product_details.aspx?isbn=978124159501</i></a>.</p><p class="text" id="p0070">There are, of course, other languages which would have worked as well. In the appendix, we explain how the concepts expressed here in Java or C++ can be expressed in some other popular languages or libraries. We also provide a primer on multiprocessor hardware.</p><p class="text" id="p0075">Throughout the book, we avoid presenting specific performance numbers for programs and algorithms, instead focusing on general trends. There is a good reason why: multiprocessors vary greatly, and what works well on one machine may work significantly less well on another. We focus on general trends to ensure that observations are not tied to specific platforms at specific times.</p><p class="text" id="p0080">Each chapter has suggestions for further reading, along with exercises suitable for Sunday morning entertainment.</p><section><h2 class="h1hd" id="s0010"><a id="st0020"/>1.1 Shared objects and synchronization</h2><p class="textfl" id="p0085">On the first day of your new job, your boss asks you to find all primes between 1 and 10<sup>10</sup> (never mind why) using a parallel machine that supports ten concurrent threads. This machine is rented by the minute, so the longer your program takes, the more it costs. You want to make a good impression. What do you do?</p><p class="text" id="p0090">As a first attempt, you might consider giving each thread an equal share of the input domain. Each thread might check 10<sup>9</sup> numbers, as shown <span aria-label="Page 4" epub:type="pagebreak" id="page_4" role="doc-pagebreak"/>in <a href="#f0010" id="cf0010">Fig. 1.1</a>. This approach fails to distribute the work evenly for an elementary but important reason: Equal ranges of inputs do not produce equal amounts of work. Primes do not occur uniformly; there are more primes between 1 and 10<sup>9</sup> than between <span class="hiddenClass"><mml:math><mml:mn>9</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000094/si1.png" style="vertical-align:middle" width="43"/></span> and 10<sup>10</sup>. To make matters worse, the computation time per prime is not the same in all ranges: it usually takes longer to test whether a large number is prime than a small number. In short, there is no reason to believe that the work will be divided equally among the threads, and it is not clear even which threads will have the most work.</p><div class="pageavoid"><figure class="fig" id="f0010"><img alt="Image" height="125" src="images/B9780124159501000094/gr001.jpg" width="459"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 1.1</span> Balancing the work load by dividing up the input domain. Each thread in {0..9} gets an equal subset of the range.</div></figcaption></figure></div><p class="text" id="p0095">A more promising way to split the work among the threads is to assign each thread one integer at a time (<a href="#f0015" id="cf0015">Fig. 1.2</a>). When a thread is finished testing an integer, it asks for another. To this end, we introduce a <i>shared counter</i>, an object that encapsulates an integer value, and that provides a <img alt="Image" height="11" src="images/B9780124159501000094/fx001.jpg" width="99"/>() method, which increments the counter's value and returns the counter's prior value.</p><div class="pageavoid"><figure class="fig" id="f0015"><img alt="Image" height="159" src="images/B9780124159501000094/gr002.jpg" width="514"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 1.2</span> Balancing the work load using a shared counter. Each thread is given a dynamically determined number of numbers to test.</div></figcaption></figure></div><p class="text" id="p0100"><a href="#f0020" id="cf0020">Fig. 1.3</a> shows a naïve implementation of <img alt="Image" height="9" src="images/B9780124159501000094/fx002.jpg" width="46"/> in Java. This counter implementation works well when used by a single thread, but it fails when shared by multiple threads. The problem is that the expression</p><div class="pageavoid"><figure class="fig" id="f0025"><img alt="Image" class="img" height="10" src="images/B9780124159501000094/fx003.jpg" width="104"/></figure></div><p class="textfl"><span aria-label="Page 5" epub:type="pagebreak" id="page_5" role="doc-pagebreak"/></p><div class="pageavoid"><figure class="fig" id="f0020"><img alt="Image" height="142" src="images/B9780124159501000094/gr003.jpg" width="518"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 1.3</span> An implementation of the shared counter.</div></figcaption></figure></div><p class="text" id="p0105">is in effect an abbreviation of the following, more complex code:</p><div class="pageavoid"><figure class="fig" id="f0030"><img alt="Image" class="img" height="43" src="images/B9780124159501000094/fx004.jpg" width="126"/></figure></div><p class="textfl"/><p class="text" id="p0110">In this code fragment, <img alt="Image" height="9" src="images/B9780124159501000094/fx005.jpg" width="32"/> is a field of the <img alt="Image" height="9" src="images/B9780124159501000094/fx002.jpg" width="46"/> object, and is shared among all the threads. Each thread, however, has its own copy of <img alt="Image" height="11" src="images/B9780124159501000094/fx006.jpg" width="25"/>, which is a local variable to each thread.</p><p class="text" id="p0115">Now imagine that two threads call the counter's <img alt="Image" height="11" src="images/B9780124159501000094/fx001.jpg" width="99"/>() method at about the same time, so that they both read 1 from <img alt="Image" height="9" src="images/B9780124159501000094/fx005.jpg" width="32"/>. In this case, each thread would set its local <img alt="Image" height="11" src="images/B9780124159501000094/fx006.jpg" width="25"/> variables to 1, set <img alt="Image" height="9" src="images/B9780124159501000094/fx005.jpg" width="32"/> to 2, and return 1. This behavior is not what we intended: we expect concurrent calls to the counter's <img alt="Image" height="11" src="images/B9780124159501000094/fx001.jpg" width="99"/>() to return distinct values. It could be worse: after one thread reads 1 from <img alt="Image" height="9" src="images/B9780124159501000094/fx005.jpg" width="32"/>, but before it sets <img alt="Image" height="9" src="images/B9780124159501000094/fx005.jpg" width="32"/> to 2, another thread could go through the increment loop several times, reading 1 and writing 2, then reading 2 and writing 3. When the first thread finally completes its operation and sets <img alt="Image" height="9" src="images/B9780124159501000094/fx005.jpg" width="32"/> to 2, it will actually be setting the counter back from 3 to 2.</p><p class="text" id="p0120">The heart of the problem is that incrementing the counter's value requires two distinct operations on the shared variable: reading the <img alt="Image" height="9" src="images/B9780124159501000094/fx005.jpg" width="32"/> field into a temporary variable and writing it back to the <img alt="Image" height="9" src="images/B9780124159501000094/fx002.jpg" width="46"/> object.</p><p class="text" id="p0125">Something similar happens when you try to pass someone approaching you head-on in a corridor. You may find yourself veering right and then left several times to avoid the other person doing exactly the same thing. Sometimes you manage to avoid bumping into them and sometimes you do not. In fact, as we will see in the later chapters, such collisions are provably unavoidable.<sup><a epub:type="noteref" href="#fn001" id="cf0025" role="doc-noteref">1</a></sup> On an intuitive level, what is going on is that each of you is performing two distinct steps: looking at (“reading”) the other's current position, and moving (“writing”) to one side or the other. The problem is, when you read the other's position, you have no way of knowing whether they have decided to stay or move. In the same way that you and the annoying stranger must decide on which side to pass each other, threads accessing a shared <img alt="Image" height="9" src="images/B9780124159501000094/fx002.jpg" width="46"/> must decide who goes first and who goes second.</p><p class="text" id="p0130">As we discuss in Chapter <a href="B9780124159501000148.xhtml">5</a>, modern multiprocessor hardware provides special <i>read–modify–write</i> instructions that allow threads to read, modify, and write a value to memory in one <i>atomic</i> (that is, indivisible) hardware step. For the <img alt="Image" height="9" src="images/B9780124159501000094/fx002.jpg" width="46"/> object, we can use such hardware to increment the counter atomically.</p><p class="text" id="p0135">We can also ensure atomic behavior by guaranteeing in software (using only read and write instructions) that only one thread executes the read-and-write sequence at a time. The problem of ensuring that only one thread can execute a particular block of code at a time, called the <i>mutual exclusion</i> problem, is one of the classic coordination problems in multiprocessor programming.</p><p class="text" id="p0140"><span aria-label="Page 6" epub:type="pagebreak" id="page_6" role="doc-pagebreak"/>As a practical matter, you are unlikely ever to find yourself having to design your own mutual exclusion algorithm (you would probably call on a library). Nevertheless, understanding how to implement mutual exclusion from the basics is an essential condition for understanding concurrent computation in general. There is no more effective way to learn how to reason about essential and ubiquitous issues such as mutual exclusion, deadlock, bounded fairness, and blocking versus nonblocking synchronization.</p></section><section><h2 class="h1hd" id="s0015"><a id="st0025"/>1.2 A fable</h2><p class="textfl" id="p0145">Instead of treating coordination problems (such as mutual exclusion) as programming exercises, we prefer to frame concurrent coordination problems as interpersonal problems. In the next few sections, we present a sequence of fables, illustrating some of the basic problems. Like most authors of fables, we retell stories mostly invented by others (see the chapter notes at the end of this chapter).</p><p class="text" id="p0150">Alice and Bob are neighbors, and they share a yard. Alice owns a cat and Bob owns a dog. Both pets like to run around in the yard, but (naturally) they do not get along. After some unfortunate experiences, Alice and Bob agree that they should coordinate to make sure that both pets are never in the yard at the same time. Of course, they rule out trivial solutions that do not allow either pet into an empty yard, or that reserve the yard exclusively to one pet or the other.</p><p class="text" id="p0155">How should they do it? Alice and Bob need to agree on mutually compatible procedures for deciding what to do. We call such an agreement a <i>coordination protocol</i> (or just a <i>protocol</i>, for short).</p><p class="text" id="p0160">The yard is large, so Alice cannot simply look out of the window to check whether Bob's dog is present. She could perhaps walk over to Bob's house and knock on the door, but that takes a long time, and what if it rains? Alice might lean out the window and shout “Hey Bob! Can I let the cat out?” The problem is that Bob might not hear her. He could be watching TV, visiting his girlfriend, or out shopping for dog food. They could try to coordinate by cell phone, but the same difficulties arise if Bob is in the shower, driving through a tunnel, or recharging his phone's batteries.</p><p class="text" id="p0165">Alice has a clever idea. She sets up one or more empty beer cans on Bob's windowsill (<a href="#f0035" id="cf0030">Fig. 1.4</a>), ties a string around each one, and runs the string back to her house. Bob does the same. When she wants to send a signal to Bob, she yanks the string to knock over one of the cans. When Bob notices a can has been knocked over, he resets the can.</p><div class="pageavoid"><figure class="fig" id="f0035"><img alt="Image" height="199" src="images/B9780124159501000094/gr004.jpg" width="495"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 1.4</span> Communicating with cans.</div></figcaption></figure></div><p class="text" id="p0170">Up-ending beer cans by remote control may seem like a creative idea, but it does not solve this problem. The problem is that Alice can place only a limited number of cans on Bob's windowsill, and sooner or later, she is going to run out of cans to knock over. Granted, Bob resets a can as soon as he notices it has been knocked over, but what if he goes to Cancún for spring break? As long as Alice relies on Bob to reset the beer cans, sooner or later, she might run out.</p><p class="text" id="p0175"><span aria-label="Page 7" epub:type="pagebreak" id="page_7" role="doc-pagebreak"/>So Alice and Bob try a different approach. Each one sets up a flagpole, easily visible to the other. When Alice wants to release her cat, she does the following:</p><div><ol><li class="numlist" id="o0010">1.  She raises her flag.</li><li class="numlist" id="o0015">2.  When Bob's flag is lowered, she releases her cat.</li><li class="numlist" id="o0020">3.  When her cat comes back, she lowers her flag.</li></ol></div><p class="textfl"/><p class="text" id="p0195">When Bob wants to release his dog, his behavior is a little more complicated:</p><div><ol><li class="numlist" id="o0025">1.  He raises his flag.</li><li class="numlist" id="o0030">2.  While Alice's flag is raised<ol><li class="numlist1" id="o0035">a.  Bob lowers his flag,</li><li class="numlist1" id="o0040">b.  Bob waits until Alice's flag is lowered,</li><li class="numlist1" id="o0045">c.  Bob raises his flag.</li></ol></li><li class="numlist" id="o0050">3.  As soon as his flag is raised and hers is down, he releases his dog.</li><li class="numlist" id="o0055">4.  When his dog comes back, he lowers his flag.</li></ol></div><p class="textfl"/><p class="text" id="p0235">This protocol rewards further study as a solution to Alice and Bob's problem. On an intuitive level, it works because of the following <i>flag principle</i>: If Alice and Bob each</p><div><ol><li class="numlist" id="o0060">1.  raises his or her own flag, and then</li><li class="numlist" id="o0065">2.  looks at the other's flag,</li></ol></div><p class="textfl"> then at least one will see the other's flag raised (clearly, the last one to look will see the other's flag raised) and will not let his or her pet enter the yard. However, this observation does not <i>prove</i> that the pets will never be in the yard together. What if, for example, Alice lets her cat in and out of the yard several times while Bob is looking?</p><p class="text" id="p0250">To prove that the pets will never be in the yard together, assume by way of contradiction that there is a way the pets could end up in the yard <span aria-label="Page 8" epub:type="pagebreak" id="page_8" role="doc-pagebreak"/>together. Consider the last time Alice and Bob each raised their flag and looked at the other's flag before sending the pet into the yard. When Alice last looked, her flag was already fully raised. She must have not seen Bob's flag, or she would not have released the cat, so Bob must have not completed raising his flag before Alice started looking. It follows that when Bob looked for the last time, after raising his flag for the last time, it must have been after Alice started looking, so he must have seen Alice's flag raised and would not have released his dog, a contradiction.</p><p class="text" id="p0255">This kind of argument by contradiction shows up over and over again, and it is worth spending some time understanding why this claim is true. It is important to note that we never assumed that “raising my flag” or “looking at your flag” happens instantaneously, nor did we make any assumptions about how long such activities take. All we care about is when these activities start or end.</p><section><h3 class="h2hd" id="s0020"><a id="st0030"/>1.2.1 Properties of a mutual exclusion protocol</h3><p class="textfl" id="p0260">To show that the flag protocol is a correct solution to Alice and Bob's problem, we must understand what properties a solution requires, and then show that the protocol meets them.</p><p class="text" id="p0265">We already proved that the pets are excluded from being in the yard at the same time, a property we call <i>mutual exclusion</i>.</p><p class="text" id="p0270">Mutual exclusion is only one of several properties of interest. After all, a protocol in which Alice and Bob never release a pet satisfies the mutual exclusion property, but it is unlikely to satisfy their pets.</p><p class="text" id="p0275">Here is another property of central importance: If only one pet wants to enter the yard, then it eventually succeeds. In addition, if both pets want to enter the yard, then eventually at least one of them succeeds. We consider this <i>deadlock-freedom</i> property to be essential. Note that whereas mutual exclusion is a safety property, deadlock-freedom is a liveness property.</p><p class="text" id="p0280">We claim that Alice and Bob's protocol is deadlock-free. Suppose both pets want to use the yard. Alice and Bob both raise their flags. Bob eventually notices that Alice's flag is raised, and defers to her by lowering his flag, allowing Alice's cat into the yard.</p><p class="text" id="p0285">Another property of interest is <i>starvation-freedom</i> (sometimes called <i>lockout-freedom</i>): If a pet wants to enter the yard, will it eventually succeed? Here, Alice and Bob's protocol performs poorly. Whenever Alice and Bob are in conflict, Bob defers to Alice, so it is possible that Alice's cat can use the yard over and over again, while Bob's dog becomes increasingly uncomfortable. Later on, we consider protocols that prevent starvation.</p><p class="text" id="p0290">The last property of interest concerns <i>waiting</i>. Imagine that Alice raises her flag, and is then suddenly stricken with appendicitis. She (and the cat) are taken to the hospital, and after a successful operation, she spends the next week under observation at the hospital. Although Bob is relieved that Alice is well, his dog cannot use the yard for an entire week until Alice returns. The problem is that the protocol states that Bob (and his dog) must <i>wait</i> for Alice to lower her flag. If Alice is delayed (even for a good reason), then Bob is also delayed (for no apparently good reason).</p><p class="text" id="p0295"><span aria-label="Page 9" epub:type="pagebreak" id="page_9" role="doc-pagebreak"/>The question of waiting is important as an example of <i>fault-tolerance</i>. Normally, we expect Alice and Bob to respond to each other in a reasonable amount of time, but what if they do not do so? The mutual exclusion problem, by its very essence, requires waiting: No mutual exclusion protocol, no matter how clever, can avoid it. Nevertheless, we will see that many other coordination problems can be solved without waiting, sometimes in unexpected ways.</p></section><section><h3 class="h2hd" id="s0025"><a id="st0035"/>1.2.2 The moral</h3><p class="textfl" id="p0300">Having reviewed the strengths and weaknesses of Alice and Bob's protocol, we now turn our attention back to computer science.</p><p class="text" id="p0305">First, we examine why shouting across the yard and placing cell phone calls does not work. Two kinds of communication occur naturally in concurrent systems:</p><div><ul><li class="bulllist" id="u0010">•  <i>Transient</i> communication requires both parties to participate at the same time. Shouting, gestures, or cell phone calls are examples of transient communication.</li><li class="bulllist" id="u0015">•  <i>Persistent</i> communication allows the sender and receiver to participate at different times. Posting letters, sending email, or leaving notes under rocks are all examples of persistent communication.</li></ul></div><p class="textfl"> Mutual exclusion requires persistent communication. The problem with shouting across the yard or placing cell phone calls is that it may or may not be okay for Bob to release his dog, but if Alice does not respond to messages, he will never know.</p><p class="text" id="p0320">The can-and-string protocol might seem somewhat contrived, but it corresponds accurately to a common communication protocol in concurrent systems: <i>interrupts</i>. In modern operating systems, one common way for one thread to get the attention of another is to send it an interrupt. More precisely, thread <i>A</i> interrupts thread <i>B</i> by setting a bit at a location periodically checked by <i>B</i>. Sooner or later, <i>B</i> notices the bit has been set and reacts. After reacting, <i>B</i> typically resets the bit (<i>A</i> cannot reset the bit). Even though interrupts cannot solve the mutual exclusion problem, they can still be very useful. For example, interrupt communication is the basis of the Java language's <img alt="Image" height="9" src="images/B9780124159501000094/fx007.jpg" width="26"/>() and <img alt="Image" height="11" src="images/B9780124159501000094/fx008.jpg" width="56"/>() calls.</p><p class="text" id="p0325">On a more positive note, the fable shows that mutual exclusion between two threads can be solved (however imperfectly) using only two one-bit variables, each of which can be written by one thread and read by the other.<span aria-label="Page 10" epub:type="pagebreak" id="page_10" role="doc-pagebreak"/></p></section></section><section><h2 class="h1hd" id="s0030"><a id="st0040"/>1.3 The producer–consumer problem</h2><p class="textfl" id="p0330">Mutual exclusion is not the only problem worth investigating. Eventually, Alice and Bob fall in love and marry. Eventually, they divorce. (What were they thinking?) The judge gives Alice custody of the pets, and tells Bob to feed them. The pets now get along with one another, but they side with Alice, and attack Bob whenever they see him. As a result, Alice and Bob need to devise a protocol for Bob to deliver food to the pets without Bob and the pets being in the yard together. Moreover, the protocol should not waste anyone's time: Alice does not want to release her pets into the yard unless there is food there, and Bob does not want to enter the yard unless the pets have consumed all the food. This problem is known as the <i>producer–consumer</i> problem.</p><p class="text" id="p0335">Surprisingly perhaps, the can-and-string protocol we rejected for mutual exclusion does exactly what we need for the producer–consumer problem. Bob places a can <i>standing up</i> on Alice's windowsill, ties one end of his string around the can, and puts the other end of the string in his living room. He then puts food in the yard and knocks the can down. When Alice wants to release the pets, she does the following:</p><div><ol><li class="numlist" id="o0070">1.  She waits until the can is down.</li><li class="numlist" id="o0075">2.  She releases the pets.</li><li class="numlist" id="o0080">3.  When the pets return, Alice checks whether they finished the food. If so, she resets the can.</li></ol></div><p class="textfl"/><p class="text" id="p0355">Bob does the following:</p><div><ol><li class="numlist" id="o0085">1.  He waits until the can is up.</li><li class="numlist" id="o0090">2.  He puts food in the yard.</li><li class="numlist" id="o0095">3.  He pulls the string and knocks the can down.</li></ol></div><p class="textfl"> The state of the can thus reflects the state of the yard. If the can is down, it means there is food and the pets can eat, and if the can is up, it means the food is gone and Bob can put some more out. We check the following three properties:</p><div><ul><li class="bulllist" id="u0020">•  <i>Mutual exclusion</i>: Bob and the pets are never in the yard together.</li><li class="bulllist" id="u0025">•  <i>Starvation-freedom</i>: If Bob is always willing to feed, and the pets are hungry, then the pets will eventually eat.</li><li class="bulllist" id="u0030">•  <i>Producer–consumer</i>: The pets will not enter the yard unless there is food, and Bob will never provide more food if there is unconsumed food.</li></ul></div><p class="textfl"/><p class="text" id="p0390">Both this producer–consumer protocol and the earlier mutual exclusion protocol ensure that Alice and Bob are never in the yard at the same time. However, Alice and Bob cannot use this producer–consumer protocol for mutual exclusion, and it is important to understand why: The mutual exclusion problem requires deadlock-freedom: Each person must be able to enter the yard if it is empty (and the other is not trying to enter). By contrast, the producer–consumer protocol's starvation-freedom property assumes continuous cooperation from both parties.</p><p class="text" id="p0395">Here is how we reason about this protocol:</p><div><ul><li class="bulllist" id="u0035">•  <i>Mutual exclusion</i>: Instead of a proof by contradiction, as we used earlier, we use an inductive “state machine”-based proof. Think of the stringed can as a machine that repeatedly transitions between two states, <i>up</i> and <i>down</i>. To show that mutual exclusion always holds, we must check that it holds initially, and continues to hold when transitioning from one state to the other.</li><li class="bulllist">Initially, the yard is empty, so mutual exclusion holds whether the can is up or down. Next, we check that mutual exclusion, once established, continues to hold when the state changes. Suppose the can is down. <span aria-label="Page 11" epub:type="pagebreak" id="page_11" role="doc-pagebreak"/>Bob is not in the yard, and from the protocol we can see that he does not enter the yard while the can is down, so only the pets may be present. The can is not raised until the pets have left the yard, so when the can is raised, the pets are not present. While the can is up, from the protocol we can see that the pets do not enter the yard, so only Bob may be present. The can is not knocked down until Bob has left the yard. These are all the possible transitions, so our protocol satisfies mutual exclusion.</li><li class="bulllist" id="u0040">•  <i>Starvation-freedom</i>: Suppose the protocol is not starvation-free: it happens that the pets are hungry, there is no food, and Bob is trying to provide food, but he does not succeed. If the can is up, then Bob will provide food and knock over the can, allowing the pets to eat. If the can is down, then since the pets are hungry, Alice will eventually raise the can, bringing us back to the previous case.</li><li class="bulllist" id="u0045">•  <i>Producer–consumer</i>: The mutual exclusion property implies that the pets and Bob will never be in the yard together. Bob will not enter the yard until Alice raises the can, which she will do only when there is no more food. Similarly, the pets will not enter the yard until Bob lowers the can, which he will do only after placing the food.</li></ul></div><p class="textfl"/><p class="text" id="p0420">Like the earlier mutual exclusion protocol, this protocol exhibits <i>waiting</i>: If Bob deposits food in the yard and then goes on vacation without resetting the can, then the pets may starve despite the presence of food.</p><p class="text" id="p0425">Turning our attention back to computer science, the producer–consumer problem appears in almost all parallel and distributed systems. It is the way in which threads place data in communication buffers to be read or transmitted across a network interconnect or shared bus.</p></section><section><h2 class="h1hd" id="s0035"><a id="st0045"/>1.4 The readers–writers problem</h2><p class="textfl" id="p0430">Bob and Alice decide they love their pets so much they need to communicate simple messages about them. Bob puts up a billboard in front of his house. The billboard holds a sequence of large tiles, each tile holding a single letter. Bob, at his leisure, posts a message on the bulletin board by lifting one tile at a time. Alice, whose eyesight is poor, reads the message at her leisure by looking at the billboard through a telescope, one tile at a time.</p><p class="text" id="p0435">This may sound like a workable system, but it is not. Imagine that Bob posts the message</p><pre>sell the cat</pre><p class="text" id="p0445">Alice, looking through her telescope, transcribes the message</p><pre>sell the</pre><p class="text" id="p0455">At this point Bob takes down the tiles and writes out a new message</p><pre>wash the dog</pre><p class="text" id="p0465"><span aria-label="Page 12" epub:type="pagebreak" id="page_12" role="doc-pagebreak"/> Alice, continuing to scan across the billboard, transcribes the message</p><pre>sell the dog</pre><p class="text" id="p0475">You can imagine the rest.</p><p class="text" id="p0480">This <i>readers–writers problem</i> has some straightforward solutions:</p><div><ul><li class="bulllist" id="u0050">•  Alice and Bob can use the mutual exclusion protocol to make sure that Alice reads only complete sentences. She might still miss a sentence, however.</li><li class="bulllist" id="u0055">•  They can use the can-and-string protocol, with Bob producing sentences and Alice consuming them.</li></ul></div><p class="textfl"/><p class="text" id="p0495">If this problem is so easy to solve, then why do we bring it up? Both the mutual exclusion and producer–consumer protocols require <i>waiting</i>: If one participant is subjected to an unexpected delay, then so is the other. In the context of shared-memory multiprocessors, a solution to the readers–writers problem is a way of allowing a thread to capture an instantaneous view of several memory locations. Capturing such a view without waiting, that is, without preventing other threads from modifying these locations while they are being read, is a powerful tool that can be used for backups, debugging, and in many other situations. Surprisingly, the readers–writers problem does have solutions that do <i>not</i> require waiting. We examine several such solutions in later chapters.</p></section><section><h2 class="h1hd" id="s0040"><a id="st0050"/>1.5 The harsh realities of parallelization</h2><p class="textfl" id="p0500">Here is why multiprocessor programming is so much fun. In an ideal world, upgrading from a uniprocessor to an <i>n</i>-way multiprocessor should provide about an <i>n</i>-fold increase in computational power. In practice, sadly, this (almost) never happens. The primary reason is that most real-world computational problems cannot be effectively parallelized without incurring the costs of interprocessor communication and coordination.</p><p class="text" id="p0505">Consider five friends who decide to paint a five-room house. If all the rooms are the same size, then it makes sense to assign each friend to paint one room. As long as everyone paints at about the same rate, we would get a five-fold speedup over the single-painter case. The task becomes more complicated if the rooms are of different sizes. For example, if one room is twice the size of the others, then the five painters will not achieve a five-fold speedup because the overall completion time is dominated by the one room that takes the longest to paint.</p><p class="text" id="p0510">This kind of analysis is very important for concurrent computation. The formula we need is called <i>Amdahl's law</i>. It captures the notion that the extent to which we can speed up any complex job (not just painting) is limited by how much of the job must be executed sequentially.</p><p class="text" id="p0515">Define the <i>speedup S</i> of a job to be the ratio between the time it takes one processor to complete the job (as measured by a wall <span aria-label="Page 13" epub:type="pagebreak" id="page_13" role="doc-pagebreak"/>clock) versus the time it takes <i>n</i> concurrent processors to complete the same job. <i>Amdahl's law</i> characterizes the maximum speedup <i>S</i> that can be achieved by <i>n</i> processors collaborating on an application, where <i>p</i> is the fraction of the job that can be executed in parallel. Assume, for simplicity, that it takes (normalized) time 1 for a single processor to complete the job. With <i>n</i> concurrent processors, the parallel part takes time <span class="hiddenClass"><mml:math><mml:mi>p</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>n</mml:mi></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000094/si2.png" style="vertical-align:middle" width="29"/></span> and the sequential part takes time <span class="hiddenClass"><mml:math><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mi>p</mml:mi></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000094/si3.png" style="vertical-align:middle" width="38"/></span>. Overall, the parallelized computation takes time</p><p class="hiddenClass"><mml:math><mml:mn>1</mml:mn><mml:mo linebreak="badbreak" linebreakstyle="after">−</mml:mo><mml:mi>p</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">+</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></p><div class="showClass"><p class="fig"><img alt="Image" height="41" src="images/B9780124159501000094/si4.png" width="100"/><a id="deq1"/></p></div><p class="textfl"> Amdahl's law says that the maximum speedup, that is, the ratio between the sequential (single-processor) time and the parallel time, is</p><p class="hiddenClass"><mml:math><mml:mi>S</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak" linebreakstyle="after">−</mml:mo><mml:mi>p</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">+</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></p><div class="showClass"><p class="fig"><img alt="Image" height="69" src="images/B9780124159501000094/si5.png" width="145"/><a id="deq2"/></p></div><p class="textfl"> To illustrate the implications of Amdahl's law, consider our room painting example. Assume that each small room is one unit, and the single large room is two units. Assigning one painter (processor) per room means that five of six units can be painted in parallel, implying that <span class="hiddenClass"><mml:math><mml:mi>p</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy="false">/</mml:mo><mml:mn>6</mml:mn></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000094/si6.png" style="vertical-align:middle" width="55"/></span>, and <span class="hiddenClass"><mml:math><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mi>p</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">/</mml:mo><mml:mn>6</mml:mn></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000094/si7.png" style="vertical-align:middle" width="81"/></span>. Amdahl's law states that the resulting speedup is</p><p class="hiddenClass"><mml:math><mml:mi>S</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak" linebreakstyle="after">−</mml:mo><mml:mi>p</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">+</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">/</mml:mo><mml:mn>6</mml:mn><mml:mo linebreak="badbreak" linebreakstyle="after">+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">/</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:mfrac><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>3</mml:mn><mml:mo>.</mml:mo></mml:math></p><div class="showClass"><p class="fig"><img alt="Image" height="69" src="images/B9780124159501000094/si8.png" width="304"/><a id="deq3"/></p></div><p class="textfl"> Alarmingly, five painters working on five rooms where one room is twice the size of the others yields only a three-fold speedup.</p><p class="text" id="p0520">It can get worse. Imagine we have 10 rooms and 10 painters, where each painter is assigned to a room, but one room (out of 10) is twice the size of the others. Here is the resulting speedup:</p><p class="hiddenClass"><mml:math><mml:mi>S</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">/</mml:mo><mml:mn>11</mml:mn><mml:mo linebreak="badbreak" linebreakstyle="after">+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">/</mml:mo><mml:mn>11</mml:mn></mml:mrow></mml:mfrac><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>5.5</mml:mn><mml:mo>.</mml:mo></mml:math></p><div class="showClass"><p class="fig"><img alt="Image" height="50" src="images/B9780124159501000094/si9.png" width="216"/><a id="deq4"/></p></div><p class="textfl"> With even a small imbalance, applying ten painters to a job yields only a five-fold speedup, roughly half of what one might naïvely expect.</p><p class="text" id="p0525">The solution, therefore, as with our earlier prime printing problem, seems to be that as soon as one painter's work on a room is done, he/she helps others to paint the remaining room. The issue, of course, is that this shared painting of the room will require coordination among painters. But can we afford to avoid it?</p><p class="text" id="p0530">Here is what Amdahl's law tells us about the utilization of multiprocessor machines. Some computational problems are “embarrassingly parallel”: they can easily be divided into components that can be executed concurrently. Such problems sometimes arise in scientific computing or in graphics, but rarely in systems. In general, however, for a given problem and a 10-processor machine, Amdahl's law says that even if we manage to parallelize 90% of the solution, but not the remaining <span aria-label="Page 14" epub:type="pagebreak" id="page_14" role="doc-pagebreak"/>10%, then we end up with a five-fold speedup, not a 10-fold speedup. In other words, the remaining 10% that we did not parallelize cut our utilization of the machine in half. It seems worthwhile to invest effort to derive as much parallelism from the remaining 10% as possible, even if it is difficult. Typically, it is hard because these additional parallel parts involve substantial communication and coordination. A major focus of this book is understanding the tools and techniques that allow programmers to effectively program the parts of the code that require coordination and synchronization, because the gains made on these parts may have a profound impact on performance.</p><p class="text" id="p0535">Returning to the prime number printing program of <a href="#f0015" id="cf0035">Fig. 1.2</a>, let us revisit the three main lines of code:</p><p class="text" id="p0540"/><div class="pageavoid"><figure class="fig" id="f0040"><img alt="Image" class="img" height="44" src="images/B9780124159501000094/fx009.jpg" width="418"/></figure></div><p class="textfl"/><p class="text" id="p0545">It would have been simpler to have threads perform these three lines atomically, that is, in a single mutually exclusive block. Instead, only the call to <img alt="Image" height="11" src="images/B9780124159501000094/fx001.jpg" width="99"/>() is atomic. This approach makes sense when we consider the implications of Amdahl's law: It is important to minimize the granularity of sequential code, in this case, the code accessed using mutual exclusion. Moreover, it is important to implement mutual exclusion in an effective way, since the communication and coordination around the mutually exclusive shared counter can substantially affect the performance of our program as a whole.</p></section><section><h2 class="h1hd" id="s0045"><a id="st0055"/>1.6 Parallel programming</h2><p class="textfl" id="p0550">For many of the applications we wish to parallelize, significant parts can easily be determined as executable in parallel because they do not require any form of coordination or communication. However, at the time this book is being written, there is no cookbook recipe for identifying these parts. This is where the application designer must use his or her accumulated understanding of the algorithm being parallelized. Luckily, in many cases it is obvious how to identify such parts. The more substantial problem, the one which this book addresses, is how to deal with the remaining parts of the program. As noted earlier, these are the parts that cannot be parallelized easily because the program must access shared data and requires interprocess coordination and communication in an essential way.</p><p class="text" id="p0555">The goal of this text is to expose the reader to core ideas behind modern coordination paradigms and concurrent data structures. We present the reader with a unified, comprehensive picture of the elements that are key to effective multiprocessor programming, ranging from basic principles to best-practice engineering techniques.</p><p class="text" id="p0560">Multiprocessor programming poses many challenges, ranging from grand intellectual issues to subtle engineering tricks. We tackle these challenges using successive refinement, starting with an idealized model in which mathematical concerns are paramount, and gradually moving on to more pragmatic models, where we increasingly focus on basic engineering principles.</p><p class="text" id="p0565"><span aria-label="Page 15" epub:type="pagebreak" id="page_15" role="doc-pagebreak"/>For example, the first problem we consider is mutual exclusion, the oldest and still one of the fundamental problems in the field. We begin with a mathematical perspective, analyzing the computability and correctness properties of various algorithms on an idealized architecture. The algorithms themselves, while classical, are not practical for modern multicore architectures. Nevertheless, learning how to reason about such idealized algorithms is an important step toward learning how to reason about more realistic (and more complex) algorithms. It is particularly important to learn how to reason about subtle liveness issues such as starvation and deadlock.</p><p class="text" id="p0570">Once we understand how to reason about such algorithms in general, we turn our attention to more realistic contexts. We explore a variety of algorithms and data structures using different multiprocessor architectures with the goal of understanding which are effective, and why.</p></section><section><h2 class="h1hd" id="s0050"><a id="st0060"/>1.7 Chapter notes</h2><p class="textfl" id="p0575">Most of the parable of Alice and Bob is adapted from Leslie Lamport's invited lecture at the 1984 ACM Symposium on Principles of Distributed Computing <a epub:type="noteref" href="#br0520" id="cf0040" role="doc-noteref">[104]</a>. The readers–writers problem is a classical synchronization problem that has received attention in numerous papers over the past 20 years. Amdahl's law is due to Gene Amdahl, a parallel processing pioneer <a epub:type="noteref" href="#br0045" id="cf0045" role="doc-noteref">[9]</a>.</p></section><section><h2 class="h1hd" id="s0055"><a id="st0065"/>1.8 Exercises</h2><p class="textfl" id="p0580"/><div class="boxg1" id="enun0010"><p class="b1num">Exercise 1.1 </p><div><p class="b1textfl" id="p0585">The <i>dining philosophers problem</i> was invented by E.W. Dijkstra, a concurrency pioneer, to clarify the notions of deadlock- and starvation-freedom. Imagine five philosophers who spend their lives just thinking and feasting on rice. They sit around a circular table, illustrated in <a href="#f0045" id="cf0050">Fig. 1.5</a>. However, there are only five chopsticks (forks, in the original formulation). Each philosopher thinks. When he gets hungry, he picks up the two chopsticks closest to him. If he can pick up both chopsticks, he can eat for a while. After a philosopher finishes eating, he puts down the chopsticks and again starts to think.</p><div><ol><li class="b1numlist" id="o0100">1.  Write a program to simulate the behavior of the philosophers, where each philosopher is a thread and the chopsticks are shared objects. Note that you must prevent a situation where two philosophers hold the same chopstick at the same time.</li><li class="b1numlist" id="o0105">2.  Amend your program so that it never reaches a state where philosophers are deadlocked, that is, it is never the case that every philosopher holds one chopstick and is stuck waiting for another to get the second chopstick.</li><li class="b1numlist" id="o0110">3.  Amend your program so that no philosopher ever starves.</li><li class="b1numlist" id="o0115">4.  Write a program to provide a starvation-free solution for <i>n</i> philosophers for any natural number <i>n</i>.</li></ol></div><p class="b1textfl"/><div class="pageavoid"><figure class="fig" id="f0045"><img alt="Image" height="176" src="images/B9780124159501000094/gr005.jpg" width="184"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 1.5</span> Traditional dining table arrangement according to Dijkstra.</div></figcaption></figure></div></div></div><p class="textfl"/><p class="text" id="p0610"><span aria-label="Page 16" epub:type="pagebreak" id="page_16" role="doc-pagebreak"/></p><div class="boxg1" id="enun0015"><p class="b1num">Exercise 1.2 </p><div><p class="b1textfl" id="p0615">For each of the following, state whether it is a safety or liveness property. Identify the bad or good thing of interest.</p><div><ol><li class="b1numlist" id="o0120">1.  Patrons are served in the order they arrive.</li><li class="b1numlist" id="o0125">2.  Anything that can go wrong, will go wrong.</li><li class="b1numlist" id="o0130">3.  No one wants to die.</li><li class="b1numlist" id="o0135">4.  Two things are certain: death and taxes.</li><li class="b1numlist" id="o0140">5.  As soon as one is born, one starts dying.</li><li class="b1numlist" id="o0145">6.  If an interrupt occurs, then a message is printed within one second.</li><li class="b1numlist" id="o0150">7.  If an interrupt occurs, then a message is printed.</li><li class="b1numlist" id="o0155">8.  I will finish what Darth Vader has started.</li><li class="b1numlist" id="o0160">9.  The cost of living never decreases.</li><li class="b1numlista" id="o0165">10.  You can always tell a Harvard man.</li></ol></div><p class="b1textfl"/></div></div><p class="textfl"/><p class="text" id="p0670"/><div class="boxg1" id="enun0020"><p class="b1num">Exercise 1.3 </p><div><p class="b1textfl" id="p0675">In the producer–consumer fable, we assumed that Bob can see whether the can on Alice's windowsill is up or down. Design a producer–consumer protocol using cans and strings that works even if Bob cannot see the state of Alice's can (this is how real-world interrupt bits work).</p></div></div><p class="textfl"/><p class="text" id="p0680"/><div class="boxg1" id="enun0025"><p class="b1num">Exercise 1.4 </p><div><p class="b1textfl" id="p0685">You are one of <i>P</i> recently arrested prisoners. The warden, a deranged computer scientist, makes the following announcement: </p><p class="quote" id="sp0040"><i>You may meet together today and plan a strategy, but after today you will be in isolated cells and have no communication with one another.</i></p><p class="quote" id="sp0045"><i>I have set up a “switch room” which contains a light switch, which is either</i> on <i>or</i> off<i>. The switch is not connected to anything.</i></p><p class="quote" id="sp0050"><i>Every now and then, I will select one prisoner at random to enter the “switch room.” This prisoner may throw the switch (from</i> on <i>to</i> off<i>, or vice versa), or may leave the switch unchanged. Nobody else will ever enter this room.</i></p><p class="quote" id="sp0055"><i>Each prisoner will visit the switch room arbitrarily often. More precisely, for any N, eventually each of you will visit the switch room at least N</i> <span aria-label="Page 17" epub:type="pagebreak" id="page_17" role="doc-pagebreak"/><i>times.</i></p><p class="quote" id="sp0060"><i>At any time, any of you may declare: “We have all visited the switch room at least once.” If the claim is correct, I will set you free. If the claim is incorrect, I will feed all of you to the crocodiles. Choose wisely!</i></p><p class="textfl"/><div><ul><li class="b1bulllist" id="u0060">•  Devise a winning strategy when you know that the initial state of the switch is <i>off</i>.</li><li class="b1bulllist" id="u0065">•  Devise a winning strategy when you do not know whether the initial state of the switch is <i>on</i> or <i>off</i>.</li></ul></div><p class="b1textfl"> Hint: The prisoners need not all do the same thing.</p></div></div><p class="textfl"/><p class="text" id="p0700"/><div class="boxg1" id="enun0030"><p class="b1num">Exercise 1.5 </p><div><p class="b1textfl" id="p0705">The same warden has a different idea. He orders the prisoners to stand in line, and places red and blue hats on each of their heads. No prisoner knows the color of his own hat, or the color of any hat behind him, but he can see the hats of the prisoners in front. The warden starts at the back of the line and asks each prisoner to guess the color of his own hat. The prisoner can answer only “red” or “blue.” If he gives the wrong answer, he is fed to the crocodiles. If he answers correctly, he is freed. Each prisoner can hear the answer of the prisoners behind him, but cannot tell whether that prisoner was correct.</p><p class="b1text" id="p0710">The prisoners are allowed to consult and agree on a strategy beforehand (while the warden listens in) but after being lined up, they cannot communicate any other way besides their answer of “red” or “blue.”</p><p class="b1text" id="p0715">Devise a strategy that allows at least <span class="hiddenClass"><mml:math><mml:mi>P</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>1</mml:mn></mml:math></span><span><img alt="Image" height="11" src="images/B9780124159501000094/si10.png" style="vertical-align:middle" width="39"/></span> of <i>P</i> prisoners to be freed.</p></div></div><p class="textfl"/><p class="text" id="p0720"/><div class="boxg1" id="enun0035"><p class="b1num">Exercise 1.6 </p><div><p class="b1textfl" id="p0725">A financial risk management program is sped up by making 85% of the application concurrent, while 15% remains sequential. However, it turns out that during a concurrent execution the number of cache misses grows in a way dependent on <i>N</i>, the number of cores used. The dependency is <span class="hiddenClass"><mml:math><mml:mrow><mml:mi mathvariant="italic">CacheMiss</mml:mi></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">+</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mfrac></mml:math></span><span><img alt="Image" height="21" src="images/B9780124159501000094/si11.png" style="vertical-align:middle" width="126"/></span>. Profiling the program reveals that 20% of the operations performed are memory accesses for both the sequential and parallel parts. The cost of other operations, including cache accesses, is 1 unit, and accessing memory has a cost of <span class="hiddenClass"><mml:math><mml:mn>3</mml:mn><mml:mi>N</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">+</mml:mo><mml:mn>11</mml:mn></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000094/si12.png" style="vertical-align:middle" width="57"/></span> units for the parallel part and a cost of 14 for the sequential part. Compute the optimal number of processors on which the program should run.</p></div></div><p class="textfl"/><p class="text" id="p0730"/><div class="boxg1" id="enun0040"><p class="b1num">Exercise 1.7 </p><div><p class="b1textfl" id="p0735">You are given a program that includes a method <i>M</i> that executes sequentially. Use Amdahl's law to resolve the following questions.</p><div><ul><li class="b1bulllist" id="u0070">•  Suppose <i>M</i> accounts for 30% of the program's execution time. What is the limit for the overall speedup that can be achieved on an <i>n</i>-processor machine?</li><li class="b1bulllist" id="u0075">•  Suppose <i>M</i> accounts for 40% of the program's execution time. You hire a programmer to replace <i>M</i> with <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000094/si13.png" style="vertical-align:middle" width="21"/></span>, which has a <i>k</i>-fold speedup over <i>M</i>. What value of <i>k</i> yields an overall speedup of 2 for the whole program?</li><li class="b1bulllist" id="u0080">•  Suppose <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000094/si13.png" style="vertical-align:middle" width="21"/></span>, the parallel replacement for <i>M</i>, has a four-fold speedup. What fraction of the overall execution time must <i>M</i> account for if replacing it with <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000094/si13.png" style="vertical-align:middle" width="21"/></span> doubles the program's speedup?</li></ul></div><p class="b1textfl"> You may assume that the program, when executed sequentially, takes unit time.</p></div></div><p class="textfl"/><p class="text" id="p0755"><span aria-label="Page 18" epub:type="pagebreak" id="page_18" role="doc-pagebreak"/></p><div class="boxg1" id="enun0045"><p class="b1num">Exercise 1.8 </p><div><p class="b1textfl" id="p0760">Running your application on two processors yields a speedup of <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000094/si14.png" style="vertical-align:middle" width="17"/></span>. Use Amdahl's law to derive a formula for <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000094/si15.png" style="vertical-align:middle" width="17"/></span>, the speedup on <i>n</i> processors, in terms of <i>n</i> and <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="13" src="images/B9780124159501000094/si14.png" style="vertical-align:middle" width="17"/></span>.</p></div></div><p class="textfl"/><p class="text" id="p0765"/><div class="boxg1" id="enun0050"><p class="b1num">Exercise 1.9 </p><div><p class="b1textfl" id="p0770">You have a choice between buying one uniprocessor that executes five zillion instructions per second or a 10-processor multiprocessor where each processor executes one zillion instructions per second. Using Amdahl's law, explain how you would decide which to buy for a particular application.</p></div></div><p class="textfl"/></section><footer><section epub:type="bibliography" role="doc-bibliography"><div id="bl0355"><h2 class="reftitle" id="st0070">Bibliography</h2><p class="reflist" epub:type="biblioentry footnote" id="br0045" role="doc-biblioentry">[9] G.M. Amdahl,  Validity of the single-processor approach to achieving large scale computing capabilities,   <i>AFIPS Conference Proceedings</i>.  <i>Atlantic City, NJ</i>.  Reston, VA: AFIPS Press; April 1967:483–485.</p><p class="reflist2" epub:type="biblioentry footnote" id="br0520" role="doc-biblioentry">[104] Leslie Lamport,  Invited address: solved problems, unsolved problems and non-problems in concurrency,   <i>Proceedings of the Third Annual ACM Symposium on Principles of Distributed Computing</i>.  1984:1–11.</p></div></section><section epub:type="rearnotes"><div class="ftnote"><hr/><p class="ftnote1" epub:type="footnote" id="fn001" role="doc-footnote"><sup><a epub:type="noteref" href="#cf0025" role="doc-noteref">1 </a></sup> <a id="np0010"/>“A preventive approach such as “always sidestep to the right” does not work because the approaching person may be British.”</p></div></section></footer></section></body></html>