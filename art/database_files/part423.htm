<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>23.6  Replication with Weak Degrees of Consistency</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part422.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part424.htm">下一个 &gt;</a></p><p class="s65" style="padding-left: 72pt;text-indent: 0pt;text-align: left;">23.6  <span style=" color: #00AEEF;">Replication with Weak Degrees of Consistency</span></p><p style="padding-top: 12pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">The replication protocols we have seen so far guarantee consistency, even if there are node and network failures. However, these protocols have a nontrivial cost, and further they may block if a signiﬁcant number of nodes fail or get disconnected due to a network partition. Further, in the case of a network partition, a node that is not in the majority partition would not only be unable to perform writes, but it would also be unable to perform even reads.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 106pt;text-indent: 0pt;text-align: left;">Many applications wish to have higher availability, even at the cost of consistency.</p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">We study the trade-oﬀs between consistency and availability in this section.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">23.6.1 Trading Off Consistency for Availability</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">The protocols we have seen so far require a (weighted) majority of nodes be in a par- tition for updates to proceed. Nodes that are in a minority partition cannot process updates; if a network failure results in more than two partitions, no partition may have a majority of nodes. Under such a situation, the system would be completely unavail- able for updates, and depending on the read-quorum, may even become unavailable for reads. The write-all-available protocol which we saw earlier provides availability but not consistency.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Ideally, we would like to have consistency and availability, even in the face of par- titions. Unfortunately, this is not possible, a fact that is crystallized in the so-called <span class="s64">CAP </span><span class="s84">theorem</span>, which states that any distributed database can have at most two of the following three properties:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s39" style="padding-left: 91pt;text-indent: 0pt;text-align: left;">• <span class="s40">Consistency.</span></p><p class="s39" style="padding-top: 3pt;padding-left: 91pt;text-indent: 0pt;text-align: left;">• <span class="s40">Availability.</span></p><p class="s39" style="padding-top: 3pt;padding-left: 91pt;text-indent: 0pt;text-align: left;">• <span class="s40">Partition-tolerance.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">The proof of the <span class="s44">CAP </span>theorem uses the following deﬁnition of consistency, with repli- cated data: an execution of a set of operations (reads and writes) on replicated data is said to be <span class="s63">consistent </span>if its result is the same as if the operations were executed on a single node, in a sequential order that is consistent with the ordering of operations issued by each process (transaction). The notion of consistency is similar to atomicity of transactions, but with each operation treated as a transaction, and is weaker than the atomicity property of transactions.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">In any large-scale distributed system, partitions cannot be prevented, and as a re- sult, either availability or consistency has to be sacriﬁced. The schemes we have seen earlier sacriﬁce availability for consistency in the face of partitions.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Consider a web-based social-networking system that replicates its data on three servers, and a network partition occurs that prevents the servers from communicating with each other. Since none of the partitions has a majority, it would not be possible to execute updates on any of the partitions. If one of these servers is in the same partition as a user, the user actually has access to data, but would be unable to update the data, since another user may be concurrently updating the same object in another partition, which could potentially lead to inconsistency. Inconsistency is not as great a risk in a social-networking system as in a banking database. A designer of such a system may decide that a user who can access the system should be allowed to perform updates on whatever replicas are accessible, even at the risk of inconsistency.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 17pt;text-align: justify;">In contrast to systems such as banking databases that require the <span class="s44">ACID </span>properties, systems such as the social-networking system mentioned above are said to require the <span class="s64">BASE </span>properties:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s39" style="padding-left: 123pt;text-indent: 0pt;text-align: left;">• <span class="s40">Basically available.</span></p><p class="s39" style="padding-top: 3pt;padding-left: 123pt;text-indent: 0pt;text-align: left;">• <span class="s40">Soft state.</span></p><p class="s39" style="padding-top: 3pt;padding-left: 123pt;text-indent: 0pt;text-align: left;">• <span class="s40">Eventually consistent.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">The primary requirement is availability, even at the cost of consistency. Updates should be allowed, even in the event of partitioning, following, for example, the write-all- available protocol (which is similar to multimaster replication described in Section 23.6). Soft state refers to the property that the state of the database may not be precisely deﬁned, with each replica possibly having a somewhat diﬀerent state due to partition- ing of the network. Eventually consistent is the requirement that once a partitioning is resolved, eventually all replicas will become consistent with each other.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">This last step requires that inconsistent copies of data items be identiﬁed; if one is an earlier version of the other, the earlier version can be replaced by the later version. It is possible, however, that the two copies were the result of independent updates to a common base copy. A scheme for detecting such inconsistent updates, called the version-vector scheme, is described in Section 23.6.4.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Restoring consistency in the face of inconsistent updates requires that the updates be merged in some way that is meaningful to the application. We discuss possible so- lutions for resolution of conﬂicting updates, in Section 23.6.5.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">In general, no system designer wants to deal with the possibility of inconsistent updates and the resultant problems of detection and resolution. Where possible, the system should be kept consistent. Inconsistent updates are allowed only when a node is disconnected from the network, in applications that can tolerate inconsistency.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Some key-value stores such as Apache Cassandra and MongoDB allow an applica- tion to specify how many replicas need to be accessible to carry out a write operation or a read operation. As long as a majority of replicas are accessible, there is no problem with consistency for writes. However, if the application sets the required number at less than a majority, and many replicas are inaccessible, updates are allowed to go ahead; there is, however, a risk of inconsistent updates, which must be resolved later.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">For applications where inconsistency can cause signiﬁcant problems, or is harder to resolve, system designers prefer to build fault-tolerant systems using replication and distributed consensus that avoid inconsistencies, even at the cost of potential non- availability.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">23.6.2 Asynchronous Replication</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Many relational database systems support replication with weak consistency, which can take one of several forms.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 17pt;text-align: justify;">With <span class="s63">asynchronous replication </span>the database allows updates at a <i>primary </i>node (also referred to as the <span class="s63">master </span>node) and propagates updates to replicas at other nodes sub- sequently; the transaction that performs the update can commit once the update is performed at the primary, even before replicas are updated. Propagation of updates after commit is also referred to as <span class="s63">lazy propagation</span>. In contrast, the term <span class="s63">synchronous replication </span>refers to the case where updates are propagated to other replicas as part of a single transaction.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">With asynchronous replication, the system must guarantee that once the trans- action commits at the primary, the updates are eventually propagated to all replicas, even if there are system failures in between. Later in this section, we shall see how this property is guaranteed using persistent messaging.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Since propagation of updates is done asynchronously, a read at a replica may not get the latest version of a data item. Asynchronous propagation of updates is commonly used to allow update transactions to commit quickly, even at the cost of consistency. A system designer may choose to use replicas only for fault tolerance. However, if the replica is available on a local machine, or another machine that can be accessed with low latency, it may be much cheaper to read the data item at the replica instead of reading it from the primary, as long as the application is willing to accept potentially stale data values.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Data storage systems based on asynchronous replication may allow data items to have versions, with associated timestamps. A transaction may then request a version with required freshness properties, for example not more than 10 minutes old. If a local replica has a version of the data item satisfying the freshness criterion, it can be used; otherwise, the read may have to be sent to the primary node.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Consider, for example, an airline reservation site that shows the prices of multiple ﬂight options. Prices may vary frequently, and the system does not guarantee that a user will actually be able to book a ticket at the price shown initially. Thus, it is quite acceptable to show a price that is a few minutes old. Asynchronous replication is a good solution for this application: price data can be replicated to a large number of servers, which share the load of user queries; and price data are updated at a primary node and replicated asynchronously to all other replicas.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Multiversion concurrency control schemes can be used to give a <i>transaction- consistent snapshot </i>of the database to read-only transactions that execute at a replica; that is, the transaction should see all updates of all transactions up to some transaction in the serialization order and should not see any updates of transactions later in the serialization order. The multiversion <span class="s44">2PL </span>scheme, described in Section 23.5.1, can be extended to allow a read-only transaction to access a replica that may not have up-to- date versions of some data items, but still get a transaction-consistent snapshot view of the database. To do so, replicas must be aware of what is the latest timestamp <i>t</i><span class="s97">safe</span></p><p class="s13" style="padding-left: 88pt;text-indent: 0pt;line-height: 76%;text-align: justify;"><span class="p">such that they have received all updates with commit timestamp before </span>t<span class="p">. Any read of a snapshot with timestamp </span>t <span class="s83">&lt; </span>t<span class="s97">safe </span><span class="p">can be processed by that replica. Such a scheme is used in the Google Spanner database,</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 17pt;text-align: justify;">Asynchronous replication is used in traditional (centralized) databases to create one or more replicas of the database, on which large queries can be executed, without interfering with transactions running on a primary node. Such replication is referred to <span class="s63">master-slave replication</span>, since the replicas cannot perform any updates on their own but must only perform updates that the master node asks them to perform.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">In such systems, asynchronous propagation of updates is typically done in a con- tinuous fashion to minimize delays until an update is seen at a replica. However, in data warehouses, updates may be propagated periodically— every night, for example— so that update propagation does not interfere with query processing.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: right;">Some database systems support <span class="s63">multimaster replication </span>(also called <span class="s63">update- anywhere replication</span>); updates are permitted at any replica of a data item and are propa- gated to all replicas either synchronously, using two-phase commit, or asynchronously. Asynchronous replication is also used in some distributed storage systems. Such systems partition data, as we have seen earlier, but replicate each partition. There is a primary node for each partition, and updates are typically sent to the primary node, which commits the updates locally, and propagates them asynchronously to the other replicas of the partition. Some systems such as <span class="s44">PNUTS </span>even allow each data item in a partition to specify which node should act as the primary node for that data item; that node is responsible for committing updates to the data item, and propagating the update to the other replicas. The motivation is to allow a node that is geographically</p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">close to a user to act as the primary node for data items corresponding to that user.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">In any system supporting asynchronous propagation of updates, it is important that once an update is committed at the primary, it must deﬁnitely be delivered to the other replicas. If there are multiple updates at a primary node, they must be delivered in the same order to the replicas; out-of-order delivery can cause an earlier update to arrive late and overwrite a later update.</p><p class="s13" style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Persistent messaging<span class="p">, which we saw in Section 23.2.3, provides guaranteed deliv- ery of messages and is widely used for asynchronous replication. The implementation techniques for persistent messages described in Section 23.2.3 can be easily modiﬁed to ensure that messages are delivered in the order in which they were sent. With persistent messaging, each primary node needs to be aware of the location of all the replicas.</span></p><p class="s13" style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Publish-subscribe systems<span class="p">, which we saw in Section 22.8.1, oﬀer a more ﬂexible way of ensuring reliable message delivery. Recall that publish-subscribe systems allow messages to be published with an associated topic, and subscribers can subscribe to any desired topic. To implement asynchronous replication, a topic is created corresponding to each partition. All replicas of a partition subscribe to the topic corresponding to the partition. Any update (including inserts, deletes, and data item updates) to a partition is published as a message with the topic corresponding to the partition. The publish- subscribe system ensures that once such a message is published, it will be delivered to all subscribers in the order in which it was published.</span></p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Publish-subscribe systems designed for parallel systems, such as the Apache Kafka system, or the Yahoo Message Bus service used for asynchronous replication in the <span class="s44">PNUTS </span>distributed data storage system, allow a large number of topics, and use mul-</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">tiple servers to handle messages to diﬀerent topics in parallel. Thus, asynchronous replication can be made scalable.</p><p class="s13" style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Fault tolerance <span class="p">is an issue with asynchronous propagation of updates. If a primary node fails, a new node must take over as primary; this can be done either using an election algorithm, as we saw earlier or by having a master node (which is itself chosen by election) decide which node takes over the job of a failed primary node.</span></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Consider what happens if a primary copy records an update but fails before the update is sent to the replicas. The new primary node has no way of ﬁnding out what was the last update committed at the primary copy. It can either wait for the primary to recover, which is unacceptable, or it can proceed without knowing what updates were committed just before failure. In the latter case, there is a risk that a transaction on the new primary may read an old value of a data item or perform an update that conﬂicts with an earlier update on the old primary.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">To reduce the chance of such problems, some systems replicate the log records of the primary node to a backup node and allow the transaction to commit at the primary only after the log record has been successfully replicated at the backup node; if the primary node fails, the backup node takes over as the primary. Recall that this is the two-safe protocol from Section 19.7. This protocol is resilient to failure of one node, but not to the failure of two nodes.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">If an application is built on top of a storage system using asynchronous replication, applications may potentially see some anomalous behaviors such as a read not seeing the eﬀect of an earlier write done by the same application, or a later read seeing an earlier version of a data item than an earlier read, if diﬀerent reads and writes are sent to diﬀerent replicas. While such anomalies cannot be completely prevented in the event of failures, they can be avoided during normal operation by taking some precautions. For example, if read and write requests for a data item from a particular node are always sent to the same replica, the application will see any writes it has performed, and if two reads are performed on the same data item, the later read will see a version at least as new as the earlier read. This property is guaranteed if a primary replica is used to perform all actions on a data item.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 8pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">23.6.3 Asynchronous View Maintenance</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Indices and materialized views are forms of data derived from underlying data, and can thus be viewed as forms of replicated data. Just like replicas, indices and materi- alized views could be updated (maintained) as part of each transaction that updates the underlying data; doing so would ensure consistency of the derived data with the underlying data.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">However, many systems prefer to perform index and view maintenance in an asyn- chronous manner, to reduce the overhead on transactions that update the underlying data. As a result, the indices and materialized views could be out of date. Any transac-</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: left;">tion that uses such indices or materialized views must be aware that these structures may be out of date.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: left;">We now consider how to maintain indices and materialized views in the face of concurrent updates.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s39" style="padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s40">The ﬁrst requirement for view maintenance is for the subsystem that performs maintenance to receive information about updates to the underlying data in such a way that each update is delivered exactly once, despite failures.</span></p><p style="padding-left: 139pt;text-indent: 15pt;text-align: justify;">Publish-subscribe systems are a good match for the ﬁrst requirement above. All updates to any underlying relation are published to the pub-sub system with the relation name as the topic; the view maintenance subsystem subscribes to the topics corresponding to its underlying relations and received all relevant updates. As we saw in Section 22.8.1, we can have topics corresponding to each tablet of a stored relation. For a nonmaterialized intermediate relation that is partitioned, we can have a topic corresponding to each partition.</p><p class="s39" style="padding-top: 4pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s40">The second requirement is for the subsystem to update the derived data in such a way that the derived data will be consistent with the underlying data, despite concurrent updates to the underlying data.</span></p><p style="padding-left: 139pt;text-indent: 16pt;text-align: justify;">Since the underlying data may receive further updates as an earlier update is being processed, no asynchronous view maintenance technique can guarantee that the view state is consistent with the state of the underlying data at all times. However, the consistency requirement can be formalized as follows: if there are no updates to the underlying data for a suﬃcient amount of time, asynchronous maintenance must ensure that the derived data is consistent with the underlying data; such a requirement is known as an <span class="s63">eventual consistency </span>requirement.</p><p style="padding-left: 139pt;text-indent: 14pt;text-align: justify;">The technique for parallel maintenance of materialized views which we saw in Section 22.7.5 uses the exchange operator model to send updates to nodes and allows view maintenance to be done locally. Techniques designed for view main- tenance in a centralized setting can be used at each node, on locally materialized input data. Recall from Section 16.5.1 that view maintenance may be deferred, that is, it may be done after the transaction commits. Techniques for deferred view maintenance in a centralized setting already need to deal with concurrent updates; such techniques can be used locally at each node.</p><p class="s13" style="padding-top: 4pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><span class="s40">A third requirement is for reads to get a consistent view of data. In general, a query that reads data from multiple nodes may not observe the updates of a transaction </span>T <span class="p">on node </span>N<span class="s98">1</span><span class="p">, but may see the updates that </span>T <span class="p">performed on node </span>N<span class="s98">2</span><span class="p">, thus seeing a transactionally inconsistent view of data. Systems that use asynchronous repli- cation typically do not support transactionally consistent views of the database.</span></p><p style="padding-left: 139pt;text-indent: 14pt;text-align: justify;">Further, scans of the database may not see an operation-consistent view of the database. (Recall the notion of operation consistency from Section 18.9, which requires that any operation should not see a database state that reﬂects only some of the updates of another operation. In Section 18.9 we saw an example of a scan</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 107pt;text-indent: 0pt;text-align: justify;">using an index that could see two versions, or neither version, of a record updated by a concurrent transaction, if the relation scan does not follow two-phase locking. A similar problem occurs with asynchronous propagation of updates, even if both the relation scan and the update transaction follow two-phase locking.</p><p class="s13" style="padding-top: 1pt;padding-left: 107pt;text-indent: 14pt;line-height: 92%;text-align: justify;"><span class="p">For example, consider a relation </span>r<span class="p">(</span>A<span class="p">, </span>B<span class="p">, </span>C<span class="p">), with primary key </span>A<span class="p">, which is parti- tioned on attribute </span>B<span class="p">. Now consider a query that is scanning the relation </span>r<span class="p">. Suppose there is a concurrent update to a tuple </span>t<span class="s98">1</span><span class="p"> </span><span class="s15">∈ </span>r<span class="p">, which updates attribute </span>t<span class="s98">1</span><span class="s83">.</span>B <span class="p">from </span>v<span class="s98">1</span></p><p style="padding-left: 107pt;text-indent: 0pt;line-height: 8pt;text-align: justify;">to <i>v</i><span class="s98">2</span>. Such an update requires deletion of the old tuple from the partition corre-</p><p style="padding-left: 107pt;text-indent: 0pt;text-align: justify;">sponding to value <i>v</i><span class="s98">1</span>, and insertion of the new tuple in the partition corresponding to <i>v</i><span class="s98">2</span>. These updates are propagated asynchronously.</p><p style="padding-left: 107pt;text-indent: 16pt;text-align: justify;">Now, the scan of <i>r </i>could possibly scan the node corresponding to <i>v</i><span class="s98">1</span> after the old tuple is deleted there but visit the node corresponding to <i>v</i><span class="s98">2</span> before the asynchronous propagation inserts the updated tuple in that node. Then, the scan would completely miss the tuple, even though it should have seen either the old value or the new value of <i>t</i><span class="s98">1</span>. Further, the scan could visit the node corresponding to <i>v</i><span class="s98">1</span> before the delete is propagated to that node, and the node corresponding to <i>v</i><span class="s98">2</span> after the insert is propagated to that node, and thereby see two versions of <i>t</i><span class="s98">1</span>, one from before the update and one from after the update. Neither case would be possible with two-phase locking, if updates are propagated synchronously to all copies.</p><p style="padding-left: 107pt;text-indent: 14pt;text-align: justify;">If a multiversion concurrency control technique is used, where data items have timestamps, snapshot reads are a good way to get a consistent scan of a relation; the snapshot timestamp should be set a suﬃciently old value that all updates as of that timestamp have reached all replicas.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 7pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">23.6.4 Detecting Inconsistent Updates</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Many applications developed for such high availability are designed to continue func- tioning locally even when the node running the application is disconnected from the other nodes.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">As an example, when data are replicated, and the network gets partitioned, if a sys- tem chooses to trade oﬀ consistency to get availability, updates may be done concur- rently at multiple replicas. Such conﬂicting updates need to be detected and resolved. When a connection is re-established, the application needs to communicate with a stor- age system to send any updates done locally and fetch updates performed elsewhere. There is a potential for conﬂicting updates from diﬀerent nodes. For example, node <i>N</i><span class="s98">1</span> may update a locally cached copy of a data item while it is disconnected; concurrently another node may have updated the data item on the storage system, or may have up- dated its own local copy of the data item. Such conﬂicting updates must be detected, and resolved.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">As another example, consider an application on a mobile device that supports oﬄine updates (i.e., permits updates even if the mobile device is not connected to the</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">network). To give the user a seamless usage experience, such applications perform the updates on a locally cached copy, and then apply the update to the data store when the device goes back online. If the same data item may be updated from multiple devices, the problem of conﬂicting updates arises here, too. The schemes described below can be used in this context too, with nodes understood to also refer to mobile devices.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">A mechanism for detecting conﬂicting updates is described in this section. How to resolve conﬂicting updates once they are detected is application dependent, and there is no general technique for doing so. However, some commonly used approaches are discussed in Section 23.6.5.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">For data items updated by only one node, it is a simple matter to propagate the updates when the node gets reconnected to the storage system. If the node only caches read-only copies of data that may be updated by other nodes, the cached data may be- come inconsistent. When the node gets reconnected, it can be sent <span class="s63">invalidation reports </span>that inform it of out-of-date cache entries.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">However, if updates can occur at more than one node, detecting conﬂicting up- dates is more diﬃcult. Schemes based on <span class="s63">version numbering </span>allow updates of shared data from multiple nodes. These schemes do not guarantee that the updates will be consistent. Rather, they guarantee that, if two nodes independently update the same version of a data item, the clash will be detected eventually, when the nodes exchange information either directly or through a common node.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The <span class="s63">version-vector scheme </span>detects inconsistencies when replicas of a data item are independently updated. This scheme allows copies of a data item to be stored at mul- tiple nodes.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The basic idea is for each node <i>i </i>to store, with its copy of each data item <i>d</i>,a <span class="s63">version vector </span>— that is, a set of version numbers <span class="s15">{</span><i>V </i>[<i>j</i>]<span class="s15">}</span>, with one entry for each other node <i>j </i>on which the data item could potentially be updated. When a node <i>i </i>updates a data item <i>d</i>, it increments the version number <i>V </i>[<i>i</i>] by one.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">For example, suppose a data item is replicated at nodes <i>N</i><span class="s98">1</span>, <i>N</i><span class="s98">2</span> and <i>N</i><span class="s98">3</span>. If the item is initially created at <i>N</i><span class="s98">1</span>, the version vector could be [1, 0, 0]. If it is then replicated at <i>N</i><span class="s98">2</span>, and then updated at node <i>N</i><span class="s98">2</span>, the resultant version vector would be [1, 1, 0]. Suppose now that this version of the data item is replicated to <i>N</i><span class="s98">3</span>, and then both <i>N</i><span class="s98">2</span> and <i>N</i><span class="s98">3</span> concurrently update the data item. Then, the version vector of the data item at <i>N</i><span class="s98">2</span> would be [1, 2, 0], while the version vector at <i>N</i><span class="s98">3</span> would be [1, 1, 1].</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Whenever two nodes <i>i </i>and <i>j </i>connect with each other, they exchange updated data items, so that both obtain new versions of the data items. However, before exchanging data items, the nodes have to discover whether the copies are consistent:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 145pt;text-indent: -16pt;line-height: 88%;text-align: justify;"><span class="s63">1. </span>If the version vectors <i>V</i><span class="s145">i </span>and <i>V</i><span class="s145">j </span>of the copy of the data item at nodes <i>i </i>and <i>j </i>are the same— that is, for each <i>k</i>, <i>V</i><span class="s97">i</span>[<i>k</i>] <span class="s15">= </span><i>V</i><span class="s97">j </span>[<i>k</i>]— then the copies of data item <i>d </i>are identical.</p><p style="padding-top: 6pt;padding-left: 145pt;text-indent: -17pt;line-height: 86%;text-align: justify;"><span class="s63">2. </span>If, for each <i>k</i>, <i>V</i><span class="s145">i</span>[<i>k</i>] <span class="s86">≤ </span><i>V</i><span class="s145">j </span>[<i>k</i>] and the version vectors are not identical, then the copy of data item <i>d </i>at node <i>i </i>is older than the one at node <i>j</i>. That is, the copy of</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 113pt;text-indent: 0pt;text-align: justify;">data item <i>d </i>at node <i>j </i>was obtained by one or more modiﬁcations of the copy of the data item at node <i>i</i>. Node <i>i </i>replaces its copy of <i>d</i>, as well as its copy of the version vector for <i>d</i>, with the copies from node <i>j</i>.</p><p style="padding-left: 128pt;text-indent: 0pt;text-align: justify;">In our example above, if <i>N</i><span class="s98">1</span> had the vector [1, 0, 0] for a data item, while <i>N</i><span class="s98">2</span></p><p style="padding-left: 113pt;text-indent: 0pt;text-align: justify;">had the vector [1, 1, 0], then the version at <i>N</i><span class="s98">2</span> is newer than the version at <i>N</i><span class="s98">1</span>.</p><p style="padding-top: 7pt;padding-left: 113pt;text-indent: -17pt;line-height: 70%;text-align: justify;"><span class="s63">3. </span>If there are a pair of values <i>k </i>and <i>m </i>such that <i>V</i><span class="s97">i</span>[<i>k</i>] <span class="s83">&lt; </span><i>V</i><span class="s97">j </span>[<i>k</i>] and <i>V</i><span class="s97">i</span>[<i>m</i>] <span class="s83">&gt; </span><i>V</i><span class="s97">j </span>[<i>m</i>], then the copies are <i>inconsistent</i>; that is, the copy of <i>d </i>at <i>i </i>contains updates per-</p><p style="padding-top: 1pt;padding-left: 113pt;text-indent: 0pt;text-align: justify;">formed by node <i>k </i>that have not been propagated to node <i>j</i>, and, similarly, the copy of <i>d </i>at <i>j </i>contains updates performed by node <i>m </i>that have not been propa- gated to node <i>i</i>. Then, the copies of <i>d </i>are inconsistent, since two or more updates have been performed on <i>d </i>independently.</p><p class="s13" style="padding-left: 113pt;text-indent: 15pt;text-align: justify;"><span class="p">In our example, after the concurrent updates at </span>N<span class="s98">2</span><span class="p"> and </span>N<span class="s98">3</span><span class="p">, the two version vectors show the updates are inconsistent. Let </span>V<span class="s98">2</span><span class="p"> and </span>V<span class="s98">3</span><span class="p"> denote the version vec- tors at </span>N<span class="s130">2 </span><span class="s94">and </span>N<span class="s130">3</span><span class="s94">. Then </span>V<span class="s130">2</span><span class="s94">[2] </span><span class="s15">= </span><span class="p">2 while </span>V<span class="s130">3</span><span class="s94">[2] </span><span class="s15">= </span><span class="p">1, whereas </span>V<span class="s130">2</span><span class="s94">[3] </span><span class="s15">= </span><span class="p">0, while</span></p><p style="padding-left: 113pt;text-indent: 0pt;line-height: 10pt;text-align: justify;"><i>V</i><span class="s98">3</span>[3] <span class="s15">= </span>1.</p><p style="padding-left: 113pt;text-indent: 14pt;text-align: justify;">Manual intervention may be required to merge the updates. After merging the updates (perhaps manually), the version vectors are merged, by setting <i>V </i>[<i>k</i>] to the maximum of <i>V</i><span class="s145">i</span>[<i>k</i>] and <i>V</i><span class="s145">j </span>[<i>k</i>] for each <i>k</i>. The node <i>l </i>that performs the write then increments <i>V </i>[<i>l</i>] by 1 and then writes the data item and its version vector <i>V </i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 17pt;text-align: justify;">The version-vector scheme was initially designed to deal with failures in distrib- uted ﬁle systems. The scheme gained importance because mobile devices often store copies of data that are also present on server systems. The scheme is also widely used in distributed storage systems that allow updates to happen even if a node is not in a majority partition.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The version-vector scheme cannot solve the problem of how to reconcile incon- sistent copies of data detected by the scheme. We discuss reconciliation in Section 23.6.5.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The version-vector scheme works well for detecting inconsistent updates to a sin- gle data item. However, if a storage system has a very large number of replicated items, ﬁnding which items have been inconsistently updated can be quite expensive if done naively. In Section 23.6.6 we study a data structure called a Merkle tree that can eﬃ- ciently detect diﬀerences between sets of data items.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">23.6.5 Resolving Conflicting Updates</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Detection of conﬂicting updates may happen when a read operation fetches copies of a data item from multiple replicas or when the system executes a background process that compares data item versions.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">At that point, conﬂicting updates on the same data item need to be <span class="s63">resolved</span>, to create a single common version. Resolution of conﬂicting updates is also referred to as <span class="s63">reconciliation</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 17pt;text-align: justify;">There is no technique for resolution so that can be used across all applications. We discuss some techniques that have been used in several commonly used applications.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Many applications can perform reconciliation automatically by executing on each node all update operations that had been performed on other nodes during a period of disconnection. This solution requires that the system keep track of operations, for example, adding an item to a shopping cart, or deleting an item from a shopping cart. This solution works if operations <span class="s63">commute </span>— that is, they generate the same result, re- gardless of the order in which they are executed. The addition of items to a shopping cart clearly commutes. Deletions do not commute with additions in general, which should be clear if you consider what happens if an addition of an item is exchanged with a delete of the same item. However, as long as deletion always operates only on items already present in the cart, this problem does not arise.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">As another example, many banks allow customers to withdraw money from an ATM even if it is temporarily disconnected from the bank network. When the ATM gets reconnected, the withdrawal operation is applied to the account. Again, if there are multiple withdrawals, they may get merged in an order diﬀerent from the order in which they happened in the real world, but the end result (balance) is the same. Note that since the operation already took place in the physical world, it cannot be rejected because of a negative balance; the fact that an account has a negative balance has to be dealt with separately.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">There are other application-speciﬁc solutions for resolving conﬂicting updates. In the worst case, however, a system may need to alert humans to the conﬂicting updates, and let the humans decide how to resolve the conﬂict.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Dealing with such inconsistency automatically, and assisting users in resolving in- consistencies that cannot be handled automatically, remains an area of research.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">23.6.6 Detecting Differences Between Collections Using Merkle Tree</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">The <span class="s63">Merkle tree </span>(also known as <span class="s63">hash tree</span>) is a data structure that allows eﬃcient detec- tion of diﬀerences between sets of data items that may be stored at diﬀerent replicas. (To avoid confusion between tree nodes and system nodes, we shall refer to the latter as replicas in this section.)</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Detecting items that have inconsistent values across replicas due to weak consis- tency is merely one of motivations for Merkle trees. Another motivation is performing sanity checks of replicas that are synchronously updated, and should be consistent, but may be inconsistent due to bugs or other failures. We consider below a binary version of the Merkle tree.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">We assume that each data item has a key and a value; in case we are considering collections that do not have an explicit key, the data item value itself can be used as a key.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Each data item key <i>k</i><span class="s145">i </span>is hashed by a function <i>h</i><span class="s98">1</span>() to get a hash value with <i>n </i>bits, where <i>n </i>is chosen such that 2<i>n</i><i> </i>is within a small factor of the number of data items. Each data item value <i>v</i><span class="s97">i </span>is hashed by another function <i>h</i><span class="s98">2</span>() to get a hash value (which</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">is typically much longer than <i>n </i>bits). Finally, we assume a hash function <i>h</i><span class="s98">3</span>() which takes as input a collection of hash values and returns a hash value computed from the collection (this hash function must be computed in a way that does not depend on the input order of the hash values, which can be done, for example, by sorting the collection before computing the hash function).</p><p class="s13" style="padding-left: 88pt;text-indent: 17pt;line-height: 94%;text-align: justify;"><span class="p">Each node of a Merkle tree has associated with it an identiﬁer and stores a hash value. Each leaf of the tree can be identiﬁed by an </span>n<span class="p">-bit binary number. For a given leaf identiﬁed by number </span>k<span class="p">, consider the set of all data items </span>i <span class="p">whose key </span>k<span class="s145">i </span><span class="p">is such that </span>h<span class="s130">1</span><span class="s94">(</span>k<span class="s97">i</span><span class="p">) </span><span class="s15">= </span>k<span class="p">. Then, the hash value </span>v<span class="s97">k </span><span class="p">stored at leaf </span>k <span class="p">is computed by applying </span>h<span class="s130">2</span><span class="s94">() on each of the data item values </span>v<span class="s97">i</span><span class="p">, and then applying </span>h<span class="s98">3</span><span class="p">() on the resultant collection of hash values. The system also maintains an index that can retrieve all the data items with a given hash value computed by function </span>h<span class="s98">2</span><span class="p">().</span></p><p class="s13" style="padding-left: 88pt;text-indent: 17pt;text-align: justify;"><span class="p">Figure 23.8 shows an example of a Merkle tree on 8 data items. The hash value of these data items on </span>h<span class="s93">1 </span><span class="s94">are shown on the left. Note that if for an item </span>i<span class="s145">j </span><span class="p">, </span>h<span class="s93">1</span><span class="s94">(</span>i<span class="s145">j </span><span class="p">) </span><span class="s15">= </span>k<span class="p">, then the data item </span>i<span class="s145">j </span><span class="p">is associated with the leaf with identiﬁer </span>k<span class="p">.</span></p><p style="padding-left: 106pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">Each internal node of the Merkle tree is identiﬁed by a hash value that is <i>j </i>bits long</p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">if the node is at depth <i>j</i>; leaves are at depth <i>n</i>, and the root at depth 0. The internal node identiﬁed by a number <i>k </i>has as children nodes identiﬁed by 2<i>k </i>and 2<i>k </i><span class="s15">+ </span>1. The hash value stored <i>v</i><span class="s97">k </span>at node <i>k </i>is computed by applying <i>h</i><span class="s98">3</span>() to the hash value stored at nodes 2<i>k </i>and 2<i>k </i><span class="s15">+ </span>1.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Now, suppose this Merkle tree is constructed on the data at two replicas (the repli- cas may be whole database replicas, or replicas of a partition of the database). If all items at the two replicas are identical, the stored hash values at the root nodes will also be identical.</p><p class="s13" style="padding-top: 1pt;padding-left: 88pt;text-indent: 17pt;line-height: 88%;text-align: justify;"><span class="p">As long as </span>h<span class="s98">2</span><span class="p">() computes a long enough hash value, and is suitably chosen, it is very unlikely that </span>h<span class="s130">2</span><span class="s94">(</span>v<span class="s130">1</span><span class="s94">) </span><span class="s15">= </span>h<span class="s130">2</span><span class="s94">(</span>v<span class="s130">2</span><span class="s94">) if </span>v<span class="s130">1 </span><span class="s86">≠ </span>v<span class="s130">2</span><span class="s94">, and similarly for </span>h<span class="s130">3</span><span class="s94">(). The SHA1 hash function with a 160-bit hash value is an example of a hash function that satisﬁes this</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="76" height="30" alt="image" src="Image_3286.png"/></span></p><p class="s250" style="padding-left: 14pt;text-indent: 0pt;text-align: left;">h<span class="s515">1</span><span class="s514"> (</span>i <span class="s515">1</span><span class="s514"> )=00</span></p><p class="s250" style="padding-left: 14pt;text-indent: 0pt;line-height: 10pt;text-align: left;">h<span class="s516">1 </span><span class="s517">(</span>i<span class="s516">2 </span><span class="s517">)=01</span></p><p class="s250" style="padding-left: 14pt;text-indent: 0pt;line-height: 8pt;text-align: left;">h<span class="s515">1</span><span class="s514"> (</span>i<span class="s515">3</span><span class="s514"> )=11</span></p><p class="s250" style="padding-left: 14pt;text-indent: 0pt;text-align: left;">h<span class="s515">1</span><span class="s514"> (</span>i<span class="s515">4</span><span class="s514"> )=00</span></p><p class="s250" style="padding-left: 14pt;text-indent: 0pt;text-align: left;">h<span class="s515">1</span><span class="s514"> (</span>i<span class="s515">5</span><span class="s514"> )=10</span></p><p class="s250" style="padding-left: 14pt;text-indent: 0pt;text-align: left;">h<span class="s515">1</span><span class="s514"> (</span>i<span class="s515">6</span><span class="s514"> )=11</span></p><p class="s250" style="padding-left: 14pt;text-indent: 0pt;line-height: 10pt;text-align: left;">h<span class="s516">1 </span><span class="s517">(</span>i<span class="s516">7 </span><span class="s517">)=10</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s514" style="padding-left: 5pt;text-indent: 0pt;line-height: 110%;text-align: left;">Hash values of data items</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="212" height="29" alt="image" src="Image_3287.png"/></span></p><p class="s250" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">h<span class="s515">2</span><span class="s514"> (</span>v <span class="s515">0</span><span class="s514"> </span>, v<span class="s515">1</span><span class="s514">)</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s322" style="padding-top: 4pt;padding-left: 175pt;text-indent: 0pt;text-align: left;">Merkle Tree</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s514" style="padding-top: 5pt;padding-left: 200pt;text-indent: 0pt;text-align: left;">0                              1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="98" height="32" alt="image" src="Image_3288.png"/></span></p><p class="s250" style="padding-top: 5pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">h <span class="s515">2</span><span class="s514"> (</span>h<span class="s515">3</span><span class="s514"> (</span>i<span class="s515">2</span><span class="s514"> ))</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s250" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">h<span class="s515">2</span><span class="s514"> (</span>h<span class="s515">3</span><span class="s514"> (</span>i<span class="s515">1</span><span class="s514"> ), </span>h<span class="s515">3</span><span class="s514"> (</span>i<span class="s515">4</span><span class="s514"> ))</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s250" style="padding-top: 5pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">h<span class="s515">2</span><span class="s514"> (</span>v<span class="s515">00</span><span class="s514"> </span>, v<span class="s515">01</span><span class="s514"> )</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="98" height="32" alt="image" src="Image_3289.png"/></span></p><p class="s250" style="padding-top: 5pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">h<span class="s515">2</span><span class="s514"> (</span>h <span class="s515">3</span><span class="s514">(</span>i<span class="s515">3</span><span class="s514"> ), </span>h<span class="s515">3</span><span class="s514"> (</span>i<span class="s515">6</span><span class="s514"> ))</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s250" style="padding-top: 5pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">h<span class="s515">2</span><span class="s514"> (</span>h<span class="s515">3</span><span class="s514"> (</span>i<span class="s515">5</span><span class="s514"> ), </span>h<span class="s515">3</span><span class="s514"> (</span>i<span class="s515">7</span><span class="s514"> ))</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s250" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">h<span class="s515">2</span><span class="s514"> (</span>v<span class="s515">10</span><span class="s514"> </span>, v<span class="s515">11</span><span class="s514">)</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s518" style="padding-left: 164pt;text-indent: 0pt;text-align: left;">00                <span class="s514">01     10                </span>11</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s514" style="padding-top: 4pt;padding-left: 200pt;text-indent: 0pt;text-align: left;">Node identifier shown above node, and has value shown inside node,</p><p class="s250" style="padding-left: 200pt;text-indent: 0pt;text-align: left;">v<span class="s363">i </span><span class="s514">denotes stored hash value in node </span>i</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 59pt;text-indent: 0pt;text-align: center;">Figure 23.8 <span class="s74">Example of Merkle tree.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">requirement. Thus, we can assume that if two nodes have the same stored hash values, all the data items under the two nodes are identical.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">If, in fact, there is a diﬀerence in the value of any items at the two replicas, or if an item is present at one replica but not at the other, the stored hash values at the root will be diﬀerent, with high probability.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Then, the stored hash values at each of the children are compared with the hash values at the corresponding child in the other tree. Search traverses down each child whose hash value diﬀers, until a leaf is reached. The traversal is done in parallel on both trees and requires communication to send tree node contents from one replica to the other.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">At the leaf, if the hash values diﬀer, the list of data item keys associated with the leaves, and the corresponding data item values are compared across the two trees, to ﬁnd data items whose values diﬀer as well as data items that are present in one of the trees but not in the other.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">One such traversal takes time at most logarithmic in the number of leaf nodes of the tree; since the number of leaf nodes is chosen to be close to the number of data items, the traversal time is also logarithmic in the number of data items. This cost is paid at most once for each data item that diﬀers between the two replicas. Furthermore, a path to a leaf is traversed only if there is, in fact, a diﬀerence at the leaf.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Thus, the overall cost for ﬁnding diﬀerences between two (potentially very large) sets is <i>O</i>(<i>m </i><span class="s15">log</span><span class="s98">2</span> <i>N </i>), where <i>m </i>is the number of data items that diﬀer and <i>N </i>is the total number of data items. Wider trees can be used to reduce the number of nodes encoun- tered in a traversal, which would be <span class="s15">log</span><i>K</i><i> N </i>if each node has <i>K </i>children, at the cost of more data being transferred for each node. Wider trees are preferred if network latency is high compared to the network bandwidth.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Merkle trees have many applications; they can be used to ﬁnd the diﬀerence in contents of two databases that are almost identical without transferring large amounts of data. Such inconsistencies can occur due to the use of protocols that only guarantee weak consistency. They could also occur because of message or network failures that result in diﬀerences in replicas, even if consensus or other protocols that guarantee consistent reads are used.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The original use of Merkle trees was for <span class="s63">verification of the contents </span>of a collection that may have potentially been corrupted by malicious users. Here, the Merkle tree leaf nodes must store the hash values of all data items that map to it, or a tree variant that only stores one data item at a leaf may be used. Further, the stored hash value at the root is digitally signed, meaning its contents cannot be modiﬁed by a malicious user who does not have the private key used for signing the value.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">To check an entire relation, the hash values can be recomputed from the leaves upwards, and the recomputed hash value at the root can be compared with the digitally signed hash value stored at the root.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">To check consistency of a single data item, its hash value is recomputed; and then so is the hash value for its leaf node <i>n</i><span class="s145">i</span>, using existing hash values for other data items</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 88pt;text-indent: 0pt;line-height: 88%;text-align: justify;"><a name="bookmark491">that hash to the same leaf. Next consider the parent node </a><i>n</i><span class="s145">j </span>of node <i>n</i><span class="s145">i </span>in the tree. The hash value of <i>n</i><span class="s97">j </span>is computed using the recomputed hash value of <i>n</i><span class="s97">i </span>and the already stored hash values of other children of <i>n</i><span class="s97">j </span>. This process is continued upward until the root of the tree. If the recomputed hash value at the root matches the signed hash value<a name="bookmark536">&zwnj;</a></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: right;">stored with the root, the contents of the data item can be determined to be uncorrupted. The above technique works for detecting corruption since with suitably chosen hash functions, it is very hard for a malicious user to create replacement values for data items in a way that the recomputed hash value is identical to the signed hash</p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">value stored at the root.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part422.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part424.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
