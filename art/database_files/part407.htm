<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>22.8  Parallel Processing of Streaming Data</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part406.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part408.htm">下一个 &gt;</a></p><p class="s65" style="padding-left: 40pt;text-indent: 0pt;text-align: left;">22.8  <span style=" color: #00AEEF;">Parallel Processing of Streaming Data</span></p><p style="padding-top: 12pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">We saw several applications of streaming data in Section 10.5. Many of the streaming data applications that we saw in that section, such as network monitoring or stock market applications, have very high rates of tuple arrival. Incoming tuples cannot be processed by a single computer, and parallel processing is essential for such systems. Streaming data systems apply a variety of operations on incoming data. We now see how some of these operations can be executed in parallel.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Parallelism is essential at all stages of query processing, starting with the entry of tuples from the sources. Thus, a parallel stream processing system needs to support a large number of entry points for data.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">For example, a system that is monitoring queries posed on a search engine such as Google or Bing search has to keep up with a very high rate of queries. Search en- gines have a large number of machines across which user queries are distributed and executed. Each of these machines becomes a source for the query stream. The stream</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">processing system must have multiple entry points for the data, which receive data from the original sources and route them within the stream processing system.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Processing of data must be done by routing tuples from producers to consumers. We discuss routing of tuples in Section 22.8.1. Parallel processing of stream operations is discussed in Section 22.8.2, while fault tolerance is discussed in Section 22.8.3.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">It is also worth noting that many applications that perform real-time analytics on streaming data also need to store the data and analyze it in other ways subsequently. Thus, many systems duplicate incoming data streams, sending one copy to a storage system for subsequent analysis and sending the other copy to a streaming data system; such an architecture is called the <span class="s63">lambda </span>architecture: the Greek symbol <span class="s15">λ </span>is used to pictorially denote that incoming data are forked into two copies, sent to two diﬀerent systems.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">While the lambda architecture allows streaming systems to be built quickly, it also results in duplication of eﬀort: programmers need to write code to store and query the data in the format/language supported by a database, as well as to query the data in the language supported by a streaming data system. More recently, there have been eﬀorts to perform stream processing as well as query processing on stored data within the same system to avoid this duplication.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">22.8.1 Routing of Tuples</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Since processing of data typically involves multiple operators, routing of data to oper- ators is an important task. We ﬁrst consider the logical structure of such routing, and address the physical structure, which takes parallel processing into account, later.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The <i>logical routing </i>of tuples is done by creating a directed acyclic graph (DAG) with operators as nodes. Edges between nodes deﬁne the ﬂow of tuples. Each tuple output by an operator is sent along all the out-edges of the operator, to the consuming operators. Each operator receives tuples from all its in-edges. Figure 22.8a depicts the logical</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s277" style="padding-left: 125pt;text-indent: 0pt;text-align: left;">	</p><p style="text-indent: 0pt;text-align: left;"><span><img width="191" height="170" alt="image" src="Image_3224.png"/></span></p><p class="s109" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Op</p><p style="text-indent: 0pt;text-align: left;"/><p class="s109" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Op</p><p style="text-indent: 0pt;text-align: left;"/><p class="s109" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Op</p><p style="text-indent: 0pt;text-align: left;"/><p class="s109" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Op</p><p style="text-indent: 0pt;text-align: left;"/><p class="s118" style="padding-top: 1pt;padding-left: 2pt;text-indent: 3pt;text-align: left;">Data Source</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s118" style="padding-left: 26pt;text-indent: -18pt;text-align: left;">Publish-Subscribe System</p><p style="text-indent: 0pt;text-align: left;"/><p class="s118" style="padding-top: 2pt;padding-left: 2pt;text-indent: 3pt;text-align: left;">Data Source</p><p style="text-indent: 0pt;text-align: left;"/><p class="s118" style="padding-top: 1pt;padding-left: 2pt;text-indent: 3pt;text-align: left;">Data Source</p><p style="text-indent: 0pt;text-align: left;"/><p class="s118" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Data Sink</p><p style="text-indent: 0pt;text-align: left;"/><p class="s118" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Data Sink</p><p style="text-indent: 0pt;text-align: left;"/><p class="s118" style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Data Sink</p><p style="text-indent: 0pt;text-align: left;"/><p class="s118" style="padding-top: 1pt;padding-left: 2pt;text-indent: 3pt;text-align: left;">Data Source</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="182" height="148" alt="image" src="Image_3225.png"/></span></p><p class="s109" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Op</p><p style="text-indent: 0pt;text-align: left;"/><p class="s109" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Op</p><p style="text-indent: 0pt;text-align: left;"/><p class="s109" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Op</p><p style="text-indent: 0pt;text-align: left;"/><p class="s109" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Op</p><p style="text-indent: 0pt;text-align: left;"/><p class="s118" style="padding-top: 1pt;padding-left: 2pt;text-indent: 3pt;text-align: left;">Data Source</p><p style="text-indent: 0pt;text-align: left;"/><p class="s118" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Data Sink</p><p style="text-indent: 0pt;text-align: left;"/><p class="s118" style="padding-top: 2pt;padding-left: 2pt;text-indent: 3pt;text-align: left;">Data Source</p><p style="text-indent: 0pt;text-align: left;"/><p class="s118" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Data Sink</p><p style="text-indent: 0pt;text-align: left;"/><p class="s118" style="padding-top: 1pt;padding-left: 2pt;text-indent: 3pt;text-align: left;">Data Source</p><p style="text-indent: 0pt;text-align: left;"/><p class="s118" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Data Sink</p><p style="text-indent: 0pt;text-align: left;"/><p class="s118" style="padding-top: 1pt;padding-left: 2pt;text-indent: 3pt;text-align: left;">Data Source</p><p style="text-indent: 0pt;text-align: left;"/><p class="s118" style="padding-top: 6pt;padding-left: 127pt;text-indent: 0pt;text-align: left;">(a) DAG representation of streaming data ﬂow     (b) Publish-subscribe representation of streaming data ﬂow</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-left: 126pt;text-indent: 0pt;text-align: left;">Figure 22.8 <span class="s74">Routing of streams using DAG and publish-subscribe representations.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">routing of stream tuples through a DAG structure. Operation nodes are denoted as “Op” nodes in the ﬁgure. The entry points to the stream processing system are the data source nodes of the DAG; these nodes consume tuples from the stream sources and inject them into the stream processing system. The exit points of the stream processing system are data sink nodes; tuples exiting the system through a data sink may be stored in a data store or ﬁle system or may be output in some other manner.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">One way of implementing a stream processing system is by specifying the graph as part of the system conﬁguration, which is read when the system starts processing tuples and is then used to route tuples. The Apache Storm stream processing system is an example of a system that uses a conﬁguration ﬁle to deﬁne the graph, which is called a <i>topology </i>in the Storm system. (Data source nodes are called <i>spouts </i>in the Storm system, while operator nodes are called <i>bolts</i>, and edges connect these nodes.)</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">An alternative way of creating such a routing graph is by using <span class="s63">publish-subscribe </span>systems. A publish-subscribe system allows publication of documents or other forms of data, with an associated topic. Subscribers correspondingly subscribe to speciﬁed topics. Whenever a document is published to a particular topic, a copy of the document is sent to all subscribers who have subscribed to that topic. Publish-subscribe systems are also referred to as <span class="s63">pub-sub </span>systems for short.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">When a publish-subscribe system is used for routing tuples in a stream processing system, tuples are considered documents, and each tuple is tagged with a topic. The entry points to the system conceptually “publish” tuples, each with an associated topic. Operators subscribe to one or more topics; the system routes all tuples with a speciﬁc topic to all subscribers of that topic. Operators can also publish their outputs back to the publish-subscribe system, with an associated topic.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">A major beneﬁt of the publish-subscribe approach is that operators can be added to the system, or removed from it, with relative ease. Figure 22.8b depicts the routing of tuples using a publish-subscribe representation. Each data source is assigned a unique topic name; the output of each operator is also assigned a unique topic name. Each operator subscribes to the topics of its inputs and publishes to the topics corresponding to its output. Data sources publish to their associated topic, while data sinks subscribe to the topics of the operators whose output goes to the sink.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The Apache Kafka system uses the publish-subscribe model to manage routing of tuples in streams. In the Kafka system, tuples published for a topic are retained for a speciﬁed period of time (called the retention period), even if there is currently no subscriber for the topic. Subscribers usually process tuples at the earliest possible time, but in case processing is delayed or temporarily stopped due to failures, the tuples are still available for processing until the retention time expires.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Many streaming data systems, such as Google’s Millwheel, and the Muppet stream processing system, use the term <span class="s63">stream </span>in place of the term <i>topic</i>. In such systems, streams are assigned names; operators can publish tuples to a stream, or subscribe to a stream, based on the name.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">We now consider the <i>physical routing </i>of tuples. Regardless of the model used above, each logical operator must have multiple physical instances running in parallel on dif-</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">ferent nodes. Incoming tuples for a logical operator must be routed to the appropriate physical instance(s) of the operator. A partitioning function is used to determine which tuple goes to which instance of the operator.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">In the context of a publish-subscribe system, each topic can be thought of as a separate logical operator that accepts tuples and passes them on to all subscribers of the topic. Since there may be a very large number of tuples for a given topic, they must be processed in parallel across multiple nodes in a parallel publish-subscribe system. In the Kafka system, for example, a topic is divided into multiple partitions, called a <span class="s63">topic-partition</span>; each tuple for that topic is sent to only one of the topic-partitions. Kafka allows a partition key to be attached to each tuple, and ensures that tuples with the same key are delivered to the same partition.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">To allow processing by consumers, Kafka allows consumer operators register with a speciﬁed “consumer group.” The consumer group corresponds to a logical operator, while the individual consumers correspond to physical instances of the logical operator that run in parallel. Each tuple of a topic is sent to only one consumer in the consumer group. More precisely, all tuples in a particular topic-partition are sent to a single con- sumer in a consumer group; however, tuples from multiple partitions may be sent to the same consumer, leading to a many-to-one relationship from partitions to consumers.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Kafka is used in many streaming data processing implementations for routing tuples. Kafka Streams provides a client library supporting algebraic operations on streams, which can be used to build streaming applications on top of the Kafka publish- subscribe system.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">22.8.2 Parallel Processing of Stream Operations</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">For standard relational operations, the techniques that we have seen for parallel evalu- ation of the operations can be used with streaming data.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Some of these, such as selection and projection operations, can be done in parallel on diﬀerent tuples. Others, such as grouping, have to bring all tuples of a group together to one machine.<span class="s76">6</span> When grouping is done with aggregation, optimizations such as pre- aggregation can be used to reduce the data transferred, but information about the tuples in a group must still be delivered to a single machine.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Windowing is an important operation in streaming data systems. Recall from Sec- tion 10.5.2.1 that incoming data are divided into windows, typically based on times- tamps (windows can also be deﬁned based on the number of tuples). Windowing is often combined with grouping/aggregation, with aggregates computed on groups of tu- ples within a window. The use of windows ensures that once the system can determine that new tuples will no longer belong to a particular window, aggregates for that win- dow can be output. For example, suppose a window is based on time, say with each 5 minutes deﬁning a window; once the system determines that future tuples will have a</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="96" height="1" alt="image" src="Image_3226.png"/></span></p><p class="s77" style="padding-top: 3pt;padding-left: 119pt;text-indent: 0pt;text-align: left;">6<span class="s78">When grouping is combined with windowing (Section 10.5.2.1), a group contains all tuples in a window that have the same values for their grouping attributes.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">timestamp larger than the end of a particular window, aggregates on that window can be output. Unlike grouping, windows can overlap each other.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">When windowing and grouping are used together to compute aggregates, if there are overlapping windows, it is best to partition on just the grouping attributes. Oth- erwise, tuples which belong to multiple windows would have to be sent to multiple windows, an overhead that is avoided by partitioning on only the grouping attributes.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Many streaming systems allow users to create their own operators. It is important to be able to parallelize user-deﬁned operators by allowing multiple instances of the operator to run concurrently. Such systems typically require each tuple to have an as- sociated key, and all tuples with a particular key are sent to a particular instance of the operator. Tuples with diﬀerent keys can be sent to diﬀerent operator instances, allowing parallel processing.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Stream operations often need to store state. For example, a windowing operator may need to retain all tuples that it has seen in a particular window, as long as the window is active. Or, it may need to store aggregates computed at some resolution (say per minute) to later compute coarser resolution aggregates (say per hour). There are many other reasons for operators to store state. User-deﬁned operators often deﬁne state internal to the operator (local variables), which needs to be stored.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Such state may be stored locally, at each node where a copy of the operator is executed. Alternatively, it may be stored centrally in a parallel data-storage system. The store-locally alternative has a lower cost but a higher risk of losing state information on failure, as compared to storing state in a parallel data-storage system. This aspect is discussed further in Section 22.8.3.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">22.8.3 Fault Tolerance with Streaming Data</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Fault tolerance when querying stored data can be achieved by reexecuting the query, or parts of the query, as we have seen in Section 22.5.4. However, such an approach to fault tolerance does not work well in a streaming setting for multiple reasons. First, many streaming data applications are latency sensitive, and delays in delivering results due to restarts are not desirable. Second, streaming systems provide a continuous stream of outputs. In the event of a failure, reexecuting the entire system or parts of it could potentially lead to duplicate copies of output tuples, which is not acceptable for many applications.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Thus, streaming data systems need to provide guarantees about delivery of output tuples, which can be one of: at-least once, at-most once, and exactly-once. The <span class="s63">at-least- once </span>semantics guarantees that each tuple is output at least once, but allows duplicate delivery during recovery from failures. The <span class="s63">at-most-once </span>semantics guarantees that each tuple is delivered at most once, without duplicates, but some tuples may be lost in the event of a failure. The <span class="s63">exactly-once </span>semantics guarantees that each tuple will be delivered exactly once, regardless of failures. This is the model that most applications require, although some applications may not care about duplicates and may accept at-least-once semantics.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 17pt;text-align: justify;">To ensure such semantics, streaming systems must track what tuples have been processed at each operator and what tuples have been output. Duplicates can be de- tected by comparing against tuples output earlier and removed. (This can be done only if the system guarantees the absence of duplicates during normal processing, since otherwise the semantics of the streaming query may be aﬀected by removal of genuine duplicates.)</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">One way to implement fault tolerance is to support it in the subsystem that routes tuples between operators. For example, in Kafka, tuples are published to topics, and each topic-partition can be stored in two or more nodes so that even if one of the nodes fails, the other one is available. Further, the tuples are stored on disk in each of the nodes so that they are not lost on power failure or system restart. Thus, the streaming data system can use this underlying fault tolerance and high availability mechanism to implement fault-tolerance and high availability at a higher level of the system.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">In such a system, if an operator was executing on a failed node, it can be restarted on another node. The system must also (at least periodically) record up to what point each input stream had been consumed by the operator. The operator must be restarted and each input stream replayed from a point such that the operator can correctly output all tuples that it had not already output before the failure. This is relatively easy for operators without any state; operators without any state need to do extra work to restore the state that existed before failure. For example, a window operator needs to start from a point in the stream corresponding to the start of a window and replay tuples from that point.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">If the window is very large, restarting from a very old point in the stream would be very ineﬃcient. Instead, the operator may checkpoint its state periodically, along with points in the input stream up to which processing has been done. In the event of a failure, the latest checkpoint may be restored, and only input stream tuples that were processed since the last checkpoint need to be replayed.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The same approach can be for other operators that have state information; the state can be checkpointed periodically, and replay starts from the last checkpoint. The checkpointed state may be stored locally; however, this means that until the node re- covers, stream processing cannot proceed. As an alternative, the state may be stored in a distributed ﬁle system or in a parallel data-storage system. Such systems replicate data to ensure high availability even in the event of failures. Thus, if a node has failed, its functions can be restarted on a diﬀerent node, starting with the last checkpoint, and replaying the stream contents.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">If the underlying system does not implement fault tolerance, operators can im- plement their own fault-tolerance mechanisms to avoid tuple loss. For example, each operator may store all tuples that it has output; a tuple can be discarded only after the operator knows that no consumer will need the tuple, even in the event of a failure.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Further, streaming systems must often guarantee low latency, even in the event of failures. To do so, some streaming systems have replicas of each operator, running concurrently. If one replica fails, the output can be fetched from the other replica. The system must make sure that duplicate tuples from the replicas are not output to</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;"><a name="bookmark480">consumers. In such systems, one copy of an operator is treated as a primary copy, and the other copy as a hot-spare replica (recall that we discussed hot-spares in Section 19.7).</a><a name="bookmark525">&zwnj;</a></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">What we have described above is a high-level view of how streaming data systems implement fault tolerance. References to more information on how to implement fault tolerance in streaming systems may be found in the Further Reading section at the end of the chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part406.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part408.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
