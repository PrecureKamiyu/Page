<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2023-05-31 Wed 12:51 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Memory From RISCV book</title>
<meta name="generator" content="Org mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2019 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Memory From RISCV book</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgaebea5d">1. Chapter six: Memory Heirarchy</a>
<ul>
<li><a href="#org028e5bb">1.1. Intro</a></li>
<li><a href="#org1f457df">1.2. Memory Technologies</a></li>
<li><a href="#org03f72a4">1.3. The Basic of Caches</a>
<ul>
<li><a href="#org7e3f789">1.3.1. A Small Intro</a></li>
<li><a href="#orga85c814">1.3.2. Accessing to the Cache</a></li>
<li><a href="#orgb8d28a8">1.3.3. Handling Cache Misses</a></li>
<li><a href="#org6b5eead">1.3.4. Handling Writes</a></li>
<li><a href="#org57819bf">1.3.5. Summary of the Section</a></li>
</ul>
</li>
<li><a href="#org0454f45">1.4. Measuring and Improving Cache Performance</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgaebea5d" class="outline-2">
<h2 id="orgaebea5d"><span class="section-number-2">1</span> Chapter six: Memory Heirarchy</h2>
<div class="outline-text-2" id="text-1">
<p>
The chapter is about <i>Memory Heirarchy</i>. You should already know a
little about it if you have read <i>CSAPP</i>.
</p>

<p>
Note that the note is based on the side notes. You need to check both side notes and books. 
</p>
</div>

<div id="outline-container-org028e5bb" class="outline-3">
<h3 id="org028e5bb"><span class="section-number-3">1.1</span> Intro</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li><i>principle of locality</i> 
<ul class="org-ul">
<li><i>temporal locality</i>: like frequently-accessed data</li>
<li><i>spatial locality</i>: like an array</li>
</ul></li>

<li><i>memory heirarchy</i>: something like a pyramid.</li>
</ul>

<hr />

<ul class="org-ul">
<li><i>Blocks</i>: Cache memory is organized into a series of blocks, also known as lines 1. Each block contains a certain amount of data and has its own block address 1. The size of the block and the overall capacity of the cache are two important factors that affect the performance of the cache memory 1. The spatial aspect suggests that instead of fetching just one item from the main memory to the cache, it is useful to fetch several items that reside at adjacent addresses as well 2.</li>
<li><i>hit</i>, <i>hit rate</i>: if the block is the block that we want, then we hit. We could also miss, that is, not to hit. If we miss, we have to access lower level to find the block that we want, and send it to the cache. Hit rate is <b>the fraction of memory accesses found in the upper level</b>.  Hit rate is the measure of the performance of the cache.</li>
<li><i>hit time</i>, <i>miss penalty</i>: Hit time is the time we need to access data when hit. When we miss, we have to retrieve data from lower level. The extra time we spend is the miss penalty.</li>
</ul>

<div class="NOTE">
<p>
miss penalty: The time required to fetch a block into a level of
the memory hierarchy from the lower level, including the time to
<b>access the block</b>, <b>transmit it from one level to the other</b>, <b>insert it
in the level that experienced the miss</b>, and then <b>pass the block to the
requestor</b>.
</p>

</div>

<hr />

<p>
Here is a self check: 
</p>

<p>
Which of the following statements are generally true?
</p>
<ol class="org-ol">
<li>Memory hierarchies take advantage of temporal locality.</li>
<li>On a read, the value returned depends on which blocks are in the cache.</li>
<li>Most of the cost of the memory hierarchy is at the highest level.</li>
<li>Most of the capacity of the memory hierarchy is at the lowest level.</li>
</ol>

<p>
And here is the answer:
</p>
<ol class="org-ol">
<li>True. Memory hierarchies take advantage of temporal locality by keeping recently accessed data in the faster, higher levels of the hierarchy so that future accesses to that data can be faster.</li>
<li>True. On a read, the value returned depends on whether the requested data is present in the cache or not. If it is present, the value is returned quickly from the cache. If it is not present, the value must be fetched from a lower level of the memory hierarchy, which takes longer.</li>
<li>False. Most of the cost of the memory hierarchy is at the lowest level (usually secondary storage), which is typically much larger and more expensive than the higher levels (such as cache and main memory).</li>
<li>True. Most of the capacity of the memory hierarchy is at the lowest level (usually secondary storage), which is typically much larger than the higher levels (such as cache and main memory).</li>
</ol>
</div>
</div>

<div id="outline-container-org1f457df" class="outline-3">
<h3 id="org1f457df"><span class="section-number-3">1.2</span> Memory Technologies</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li><i>DRAM</i> and <i>SRAM</i>: <i>D</i> stands for <b>dynamic</b>, which is to say that the value it stored have to be refreshed every once a while or it will fade away. <i>S</i> stands for <b>static</b>, saying that the value will not fade away.</li>
<li><i>Flash Memory</i>: widely used in cell phone. The store unit can be worn out, that is to say, if you use the unit too often, it will not working. To avoid the exact problem, Flash Memory can spread the writes to the flash memory. The controller may slow down the performance a little bit, but worth it. The controller is always needed, unless you have an external controller. Also, the controller can used to avoid the badly-manufactured part of the flash memory.</li>
<li><i>Magnetic Disk</i>: with a big capacity, whose structure is introduced in later section.</li>
</ul>

<hr />

<p>
The difference of <i>DRAM</i> and <i>SRAM</i>: Answered by <b>Google Bard</b>
</p>

<p>
Static RAM (SRAM) is a type of random access memory (RAM) that stores data in flip-flops. Flip-flops are electronic circuits that can store a single bit of data, either a 0 or a 1. As long as power is applied to the SRAM, the data stored in the flip-flops will be maintained indefinitely.
</p>

<p>
Dynamic RAM (DRAM) is a type of random access memory (RAM) that stores data in capacitors. Capacitors are electronic components that can store an electric charge. The amount of charge stored in a capacitor can be used to represent a 0 or a 1. However, the charge in a capacitor will slowly leak away over time. Therefore, DRAM must be refreshed periodically to prevent the data from being lost.
The main difference between SRAM and DRAM is that SRAM stores data in flip-flops, while DRAM stores data in capacitors. This difference in storage mechanism has several implications:
</p>

<ul class="org-ul">
<li>Basic Unit: SRAM use <i>flip-flops</i> and DRAM use <i>capacitors</i>. A filp-flop consists of six transistors maybe, and a capacitor is made of one or two transistors.</li>
<li>Speed: SRAM is faster than DRAM because it does not need to be refreshed periodically.</li>
<li>Cost: SRAM is more expensive than DRAM because it requires more transistors per bit of storage.</li>
<li>Density: DRAM is denser than SRAM because it can store more data in the same amount of space.</li>
<li>Refresh rate: SRAM does not need refreshing, while that DRAM need refreshing.</li>
</ul>

<hr />

<ul class="org-ul">
<li><i>two-level coding structure</i>: we refresh a row every time, leaving time for we to access.</li>
</ul>

<p>
In the context of DRAM (Dynamic Random Access Memory), a row refers to a set of memory cells that share a common word line. A word line is an electrical line that selects a row of cells in the memory array. When a row is selected, all the cells in that row can be accessed simultaneously.
</p>

<p>
If a word is 32 bits long and there are 1024 words, then there would be 1024 rows and each row would have 32 bits. Each bit in a row would correspond to one memory cell. 
</p>

<ul class="org-ul">
<li><i>buffer, bank, and the structure</i></li>
</ul>

<p>
A DRAM has a <b>buffer</b> (if it has one bank), the buffer is used to hold a row of data. It works like SRAM. A <b>row</b> is loaded to the buffer for the sake of temporal locality. There is a buffer for one <b>bank</b>. A DRAM can have many banks. So DRAM has many buffers available. For more information please check the book.
</p>

<p>
There is the quote to the book: 
</p>

<blockquote>
<p>
The row organization that helps with refresh also helps with
performance. To improve performance, DRAMs buffer rows for repeated
access. The buffer acts like an SRAM; by changing the address, random
bits can be accessed in the buffer until the next row access. This
capability improves the access time significantly, since the access
time to bits in the row is much lower. Making the chip wider also
improves the memory bandwidth of the chip. When the row is in the
buffer, it can be transferred by successive addresses at whatever the
width of the DRAM is (typically 4, 8, or 16 bits), or by specifying a
block transfer and the starting address within the buffer.
</p>

<p>
Sustaining that much bandwidth requires clever organization inside the
DRAM.  Instead of just a faster row buffer, the DRAM can be internally
organized to read or write from multiple banks, with each having its
own row buffer. Sending an address to several banks permits them all
to read or write simultaneously. For example, with four banks, there
is just one access time and then accesses rotate between the four
banks to supply four times the bandwidth. This rotating access scheme
is called <b>address interleaving</b>.
</p>
</blockquote>
<p>
End of the Quote.
</p>

<hr />
<ul class="org-ul">
<li><i>SDRAM, in a burst</i></li>
</ul>

<p>
<i>S</i> stands for synchronize. It is to say, a clock is added to DRAM to deal with synchronization problem. Let us see what <b>New Bing</b> said: 
</p>

<blockquote>
<p>
SDRAM (Synchronous Dynamic Random Access Memory) is able to transfer data in a burst without having to specify additional address bits by using a clock to synchronize the data transfers. Once the initial address is specified, the clock is used to transfer the successive data in the burst. The clock generates a series of pulses at regular intervals, and each pulse triggers the transfer of the next piece of data in the burst. This means that the data can be transferred more quickly and efficiently without having to specify additional address bits for each piece of data in the burst.
</p>
</blockquote>

<p>
End of the Quote.
</p>

<p>
And there is actually a weird question: what is "in a burst"? It seems that "a burst transfer" is different to "a block transfer". A burst is like ejaculation&#x2014;it starts and just goes on till the end. A block is like I don't know, 搬砖? Anyway, this is what <b>NB</b> said: 
</p>

<blockquote>
<p>
A burst transfer refers to the transfer of a group of data sequentially in rapid succession. In the context of synchronous DRAMs (Dynamic Random Access Memory), this means that once the initial address is specified, the successive data in the burst can be transferred without having to specify additional address bits. Instead, the clock is used to transfer the successive data in the burst.
</p>

<p>
A block transfer, on the other hand, refers to the transfer of a block of data from one location to another. The difference between a burst and a block transfer is that a burst transfer transfers data sequentially in rapid succession, while a block transfer transfers a block of data as a whole.
</p>
</blockquote>
<p>
End of the Quote.
</p>
<hr />
<ul class="org-ul">
<li><i>flash memory, wear leveling</i></li>
</ul>

<p>
I shall just copy the text, since it is not long.
</p>

<blockquote>
<p>
<b>Flash memory</b> is a type of electrically erasable programmable read-only memory
(EEPROM).
</p>

<p>
Unlike disks and DRAM, but like other EEPROM technologies, <i>writes can wear out
flash memory bits</i>. To cope with such limits, most flash products include a controller
to <i>spread the writes</i> by remapping blocks that have been written many times to less
trodden blocks. This technique is called <b>wear leveling</b>. With wear leveling, personal
mobile devices are very unlikely to exceed the write limits in the flash. Such wear
leveling lowers the potential performance of flash, but it is needed unless higher-level 
software monitors block wear. Flash controllers that perform wear leveling can
also improve yield by mapping out memory cells that were manufactured incorrectly.
</p>
</blockquote>
<p>
End of the Quote. "trodden" is "tread".
</p>

<hr />
<ul class="org-ul">
<li><i>DIMM</i></li>
</ul>

<p>
You may skip the part. This part seems not so important. And here the text that is processed by <b>NB</b>:
</p>
<blockquote>
<p>
A DIMM (Dual In-line Memory Module) is a small circuit board that contains one or several dynamic random-access memory (DRAM) chips. DIMMs are commonly used to add memory to personal computers, workstations, printers, and servers. They are designed to be 64 bits wide, enabling fast data transfer.
</p>

<p>
The text you provided explains that memory for servers is commonly sold on DIMMs. These modules typically contain 4-16 DRAMs and are normally organized to be 8 bytes wide for server systems. A DIMM using DDR4-3200 SDRAMs could transfer at a rate of 8 × 3200 = 25,600 megabytes per second. Such DIMMs are named after their bandwidth: PC25600.
</p>

<p>
Since a DIMM can have many DRAM chips and only a portion of them are used for a particular transfer, the term “memory rank” is used to refer to the subset of chips in a DIMM that share common address lines.
</p>
</blockquote>
<p>
End of the Quote. I have no idea what is "memory rank".
</p>

<hr />
<ul class="org-ul">
<li><i>Magnetic Disk</i></li>
</ul>

<p>
Let us just skip this part
</p>
<hr />
</div>
</div>
<div id="outline-container-org03f72a4" class="outline-3">
<h3 id="org03f72a4"><span class="section-number-3">1.3</span> The Basic of Caches</h3>
<div class="outline-text-3" id="text-1-3">
</div>
<div id="outline-container-org7e3f789" class="outline-4">
<h4 id="org7e3f789"><span class="section-number-4">1.3.1</span> A Small Intro</h4>
<div class="outline-text-4" id="text-1-3-1">
<ul class="org-ul">
<li><i>Cache</i>: cache is word referring to the safe space. Just like in the example, you table where there are books that you need is the cache. Here we called the space that is between the memory and the process as <b>cache</b>.</li>

<li><i>The structure of Cache</i>: Cache is a smaller space between the memory and processor. When the processor want to access some data that is in the memory it does not directly access to the memory. Instead, it access to the cache. If the cache does hold the data (hit), the speed is considerably enhanced.</li>

<li><i>block</i>: A block has many bytes (or just one). Block size is very critical to the performance of the cache.</li>

<li><i>direct mapped</i>:we use the address of the <b>block</b> to determine the position in the cache. And usually we know that the address in the cache is:</li>
</ul>
<p>
\[
\rm (Block\ Address) \bmod{(Number\ of\ blocks\ in\ the\ cache)}
\]
</p>
</div>
</div>

<div id="outline-container-orga85c814" class="outline-4">
<h4 id="orga85c814"><span class="section-number-4">1.3.2</span> Accessing to the Cache</h4>
<div class="outline-text-4" id="text-1-3-2">
<ul class="org-ul">
<li><i>cache index field</i>: the index of the block in cache.</li>
<li><i>tag field</i>: the tag is used to identify the blocks. For example, we have block address</li>
<li><i>valid bit</i>: There is a valid bit every one block to indicate if the block are valid.</li>
</ul>

<hr />

<ul class="org-ul">
<li><i>calculation of the field</i></li>
</ul>
<p>
Three steps:
</p>

<ol class="org-ol">
<li>We find the block address: \( {\rm Block\ Address =  (Address ) } /2 ^{m+2}\), that is we get rid of the low \(m+2\) bits.</li>
<li>We find the lower \(n\) bits of the block address. \( {\rm Index = (Block \ Address) }\bmod{2^n} \), that is we get the lower \(n\) bits. that is the index.</li>
<li>We find the higher \( 64 - m -  2 - n \) bits of block address, which is the <i>tag field</i>.</li>
</ol>

<hr />
<ul class="org-ul">
<li><i>calculate the bits in the cache</i></li>
</ul>

<p>
You shall see the example on the book.
</p>

<hr />
<ul class="org-ul">
<li><i>Block size</i></li>
</ul>

<p>
It is wrong that the bigger the block size, the better. While big block size may enhance hit rate, it also enhance the miss penalty.
You shall check for the book. Here is the quote:
</p>
<blockquote>
<p>
Larger blocks exploit spatial locality to lower miss rates. As Figure 5.11 shows,
increasing the block size usually decreases the miss rate. The miss rate may go up
eventually if the block size becomes a significant fraction of the cache size, because
the number of blocks that can be held in the cache will become small, and there will
be a great deal of competition for those blocks. As a result, a block will be bumped
out of the cache before many of its words are accessed. Stated alternatively, spatial
locality among the words in a block decreases with a very large block; consequently,
the benefits to the miss rate become smaller.
</p>

<p>
A more serious problem associated with just increasing the block size is that the
cost of a miss rises. The miss penalty is determined by the time required to fetch
the block from the next lower level of the hierarchy and load it into the cache. The
time to fetch the block has two parts: the latency to the first word and the transfer
time for the rest of the block. Clearly, unless we change the memory system, the
transfer time—and hence the miss penalty—will likely increase as the block size
expands. Furthermore, the improvement in the miss rate starts to decrease as the
blocks become larger. The result is that the increase in the miss penalty overwhelms
the decrease in the miss rate for blocks that are too large, and cache performance
thus decreases. Of course, if we design the memory to transfer larger blocks more
efficiently, we can increase the block size and obtain further improvements in cache
performance. We discuss this topic in the next section.
</p>
</blockquote>
</div>
</div>

<div id="outline-container-orgb8d28a8" class="outline-4">
<h4 id="orgb8d28a8"><span class="section-number-4">1.3.3</span> Handling Cache Misses</h4>
<div class="outline-text-4" id="text-1-3-3">
<ul class="org-ul">
<li><i>Handling miss</i></li>
</ul>

<p>
Four steps when miss happens.
</p>
</div>
</div>
<div id="outline-container-org6b5eead" class="outline-4">
<h4 id="org6b5eead"><span class="section-number-4">1.3.4</span> Handling Writes</h4>
<div class="outline-text-4" id="text-1-3-4">
<ul class="org-ul">
<li><i>Consistent and Inconsistent</i></li>
<li><i>Write Through</i>
<ul class="org-ul">
<li><i>Write Queue</i></li>
</ul></li>
<li><i>Write Back</i></li>
</ul>
<hr />
<p>
Elaboration: WHAT THE FUCK?
</p>
<hr />
<ul class="org-ul">
<li><i>An Example of the Intrinsity FastMATH Processor</i></li>
</ul>
</div>
</div>

<div id="outline-container-org57819bf" class="outline-4">
<h4 id="org57819bf"><span class="section-number-4">1.3.5</span> Summary of the Section</h4>
<div class="outline-text-4" id="text-1-3-5">
<p>
I will just copy the summary parts.
</p>

<blockquote>
<p>
We began the previous section by examining the simplest of caches: a direct-mapped
cache with a one-word block. In such a cache, both hits and misses are simple, since
a word can go in exactly one location and there is a separate tag for every word. To
keep the cache and memory consistent, a write-through scheme can be used, so
that every write into the cache also causes memory to be updated. The alternative
to write-through is a write-back scheme that copies a block back to memory when
it is replaced; we’ll discuss this scheme further in upcoming sections.
</p>

<p>
To take advantage of spatial locality, a cache must have a block size larger than
one word. The use of a bigger block decreases the miss rate and improves the
efficiency of the cache by reducing the amount of tag storage relative to the amount
of data storage in the cache. Although a larger block size decreases the miss rate, it
can also increase the miss penalty. If the miss penalty increased linearly with the
block size, larger blocks could easily lead to lower performance.
</p>

<p>
To avoid performance loss, the bandwidth of main memory is increased to
transfer cache blocks more efficiently. Common methods for increasing bandwidth
external to the DRAM are making the memory wider and interleaving. DRAM
designers have steadily improved the interface between the processor and memory
to increase the bandwidth of burst mode transfers to reduce the cost of larger cache
block sizes.
</p>
</blockquote>

<p>
End of the Quote. Check yourself:
</p>

<p>
The speed of the memory system affects the designer’s decision on the size of
the cache block. Which of the following cache designer guidelines is generally
valid?
</p>
<ol class="org-ol">
<li>The shorter the memory latency, the smaller the cache block</li>
<li>The shorter the memory latency, the larger the cache block</li>
<li>The higher the memory bandwidth, the smaller the cache block</li>
<li>The higher the memory bandwidth, the larger the cache block</li>
</ol>

<p>
Here is the answer from <b>NB</b>:
</p>
<blockquote>
<p>
Memory latency refers to the time it takes to access data from memory. When memory latency is short, it takes less time to transfer data from memory to the cache. This means that larger cache blocks can be used without significantly impacting performance. Larger cache blocks can take advantage of spatial locality by fetching more data that is likely to be accessed in the near future.
</p>

<p>
Memory bandwidth refers to the amount of data that can be transferred from memory to the cache in a given amount of time. When memory bandwidth is high, more data can be transferred from memory to the cache in a given amount of time. This means that larger cache blocks can be used without significantly impacting performance. Larger cache blocks can take advantage of spatial locality by fetching more data that is likely to be accessed in the near future.
</p>
</blockquote>
</div>
</div>
</div>

<div id="outline-container-org0454f45" class="outline-3">
<h3 id="org0454f45"><span class="section-number-3">1.4</span> Measuring and Improving Cache Performance</h3>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Created: 2023-05-31 Wed 12:51</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>