<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>20.4  Parallel Systems</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part368.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part370.htm">下一个 &gt;</a></p><p class="s65" style="padding-top: 7pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">20.4  <span style=" color: #00AEEF;">Parallel Systems</span></p><p style="padding-top: 11pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Parallel systems improve processing and <span class="s44">I/O </span>speeds by using a large number of com- puters in parallel. Parallel machines are becoming increasingly common, making the study of parallel database systems correspondingly more important.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">In parallel processing, many operations are performed simultaneously, as opposed to serial processing, in which the computational steps are performed sequentially. A <span class="s63">coarse-grain </span>parallel machine consists of a small number of powerful processors; a <span class="s63">mas- sively parallel </span>or <span class="s63">fine-grain parallel </span>machine uses thousands of smaller processors. Vir- tually all high-end server machines today oﬀer some degree of coarse-grain parallelism, with up to two or four processors each of which may have 20 to 40 cores.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Massively parallel computers can be distinguished from the coarse-grain parallel machines by the much larger degree of parallelism that they support. It is not practical to share memory between a large number of processors. As a result, massively parallel computers are typically built using a large number of computers, each of which has its own memory, and often, its own set of disks. Each such computer is referred to as a <span class="s63">node </span>in the system.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Parallel systems at the scale of hundreds to thousands of nodes or more are housed in a <span class="s63">data center</span>, which is a facility that houses a large number of servers. Data centers provide high-speed network connectivity within the data center, as well as to the outside world. The numbers and sizes of data centers have grown tremendously in the last decade, and modern data centers may have several hundred thousand servers.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">20.4.1 Motivation for Parallel Databases</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">The transaction requirements of organizations have grown with the increasing use of computers. Moreover, the growth of the World Wide Web has created many sites with millions of viewers, and the increasing amounts of data collected from these viewers has produced extremely large databases at many companies.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 17pt;text-align: justify;">The driving force behind parallel database systems is the demands of applications that have to query extremely large databases (of the order of petabytes — that is, 1000 terabytes, or equivalently, 10<span class="s76">15</span> bytes) or that have to process an extremely large number of transactions per second (of the order of thousands of transactions per second). Centralized and client – server database systems are not powerful enough to handle such applications.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Web-scale applications today often require hundreds to thousands of nodes (and in some cases, tens of thousands of nodes) to handle the vast number of users on the web.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Organizations are using these increasingly large volumes of data — such as data about what items people buy, what web links users click on, and when people make telephone calls— to plan their activities and pricing. Queries used for such purposes are called <span class="s63">decision-support queries</span>, and the data requirements for such queries may run into terabytes. Single-node systems are not capable of handling such large volumes of data at the required rates.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The set-oriented nature of database queries naturally lends itself to parallelization. A number of commercial and research systems have demonstrated the power and scal- ability of parallel query processing.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">As the cost of computing systems has reduced signiﬁcantly over the years, parallel machines have become common and relatively inexpensive. Individual computers have themselves become parallel machines using multicore architectures. Parallel databases are thus quite aﬀordable even for small organizations.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Parallel database systems which can support hundreds of nodes have been avail- able commercially for several decades, but the number of such products has seen a signiﬁcant increase since the mid 2000s. Open-source platforms for parallel data stor- age such as the Hadoop File System (HDFS), and HBase, and for query processing, such as Hadoop Map-Reduce and Hive (among many others), have also seen extensive adoption.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">It is worth noting that application programs are typically built such that they can be executed in parallel on a number of application servers, which communicate over a network with a database server, which may itself be a parallel system. The parallel architectures described in this section can be used not only for data storage and query processing in the database but also for parallel processing of application programs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">20.4.2 Measures of Performance for Parallel Systems</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">There are two main measures of performance of a database system: (1) <span class="s63">throughput</span>, the number of tasks that can be completed in a given time interval, and (2) <span class="s63">response time</span>, the amount of time it takes to complete a single task from the time it is submitted. A system that processes a large number of small transactions can improve throughput by processing many transactions in parallel. A system that processes large transactions can improve response time as well as throughput by performing subtasks of each trans- action in parallel.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="251" height="168" alt="image" src="Image_3096.png"/></span></p><p class="s42" style="text-indent: 0pt;line-height: 11pt;text-align: left;">linear speedup</p><p style="text-indent: 0pt;text-align: left;"/><p class="s42" style="text-indent: 0pt;line-height: 11pt;text-align: left;">sublinear speedup</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="35" height="5" alt="image" src="Image_3097.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="35" alt="image" src="Image_3098.png"/></span></p><p class="s42" style="padding-left: 1pt;text-indent: 0pt;text-align: left;">speed</p><p style="text-indent: 0pt;text-align: left;"/><p class="s42" style="padding-left: 45pt;text-indent: 0pt;text-align: center;">resources</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 59pt;text-indent: 0pt;text-align: center;">Figure 20.2 <span class="s74">Speedup with increasing resources.</span></p><p style="padding-top: 8pt;padding-left: 88pt;text-indent: 17pt;text-align: justify;">Parallel processing within a computer system allows database-system activities to be speeded up, allowing faster response to transactions, as well as more transactions to be executed per second. Queries can be processed in a way that exploits the parallelism oﬀered by the underlying computer system.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Two important issues in studying parallelism are speedup and scaleup. Running a given task in less time by increasing the degree of parallelism is called <span class="s63">speedup</span>. Han- dling larger tasks by increasing the degree of parallelism is called <span class="s63">scaleup</span>.</p><p class="s13" style="padding-left: 88pt;text-indent: 17pt;line-height: 13pt;text-align: justify;"><span class="p">Consider a database application running on a parallel system with a certain num- ber of processors and disks. Now suppose that we increase the size of the system by increasing the number of processors, disks, and other components of the system. The goal is to process the task in time inversely proportional to the number of processors and disks allocated. Suppose that the execution time of a task on the larger machine is </span>T<span class="s145">L</span><span class="p">, and that the execution time of the same task on the smaller machine is </span>T<span class="s145">S</span><span class="p">. The speedup due to parallelism is deﬁned as </span>T<span class="s145">S</span><span class="s15">∕</span>T<span class="s145">L</span><span class="p">. The parallel system is said to demon-</span></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">strate <span class="s63">linear speedup </span>if the speedup is <i>N </i>when the larger system has <i>N </i>times the re- sources (processors, disk, and so on) of the smaller system. If the speedup is less than <i>N </i>, the system is said to demonstrate <span class="s63">sublinear speedup</span>. Figure 20.2 illustrates linear and sublinear speedup.<span class="s76">2</span></p><p class="s13" style="padding-left: 88pt;text-indent: 17pt;line-height: 92%;text-align: justify;"><span class="p">Scaleup relates to the ability to process larger tasks in the same amount of time by providing more resources. Let </span>Q <span class="p">be a task, and let </span>Q<span class="s145">N </span><span class="p">be a task that is </span>N <span class="p">times bigger than </span>Q<span class="p">. Suppose that the execution time of task </span>Q <span class="p">on a given machine </span>M<span class="s145">S </span><span class="p">is </span>T<span class="s97">S</span><span class="p">, and the execution time of task </span>Q<span class="s97">N </span><span class="p">on a parallel machine </span>M<span class="s97">L </span><span class="p">that is </span>N <span class="p">times larger than </span>M<span class="s97">S </span><span class="p">is </span>T<span class="s97">L</span><span class="p">. The scaleup is then deﬁned as </span>T<span class="s97">S</span><span class="s15">∕</span>T<span class="s97">L</span><span class="p">. The parallel system </span>M<span class="s97">L </span><span class="p">is said</span></p><p class="s13" style="padding-left: 88pt;text-indent: 0pt;line-height: 16pt;text-align: justify;"><span class="p">to demonstrate </span><span class="s63">linear scaleup </span><span class="p">on task </span>Q <span class="p">if </span>T<span class="s97">L </span><span class="s15">= </span>T<span class="s97">S</span><span class="p">. If </span>T<span class="s97">L </span><span class="s83">&gt; </span>T<span class="s97">S</span><span class="p">, the system is said</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="95" height="1" alt="image" src="Image_3099.png"/></span></p><p class="s80" style="padding-top: 3pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;"><span class="s77">2</span><span class="s78">In some cases, a parallel system may provide superlinear speedup, that is, an </span><i>N </i>times larger system may provide speedup greater than <i>N </i>. This could happen, for example, because data that did not ﬁt in the main memory of a smaller system do ﬁt in the main memory of a larger system, avoiding disk <span class="s161">I/O</span>. Similarly, data may ﬁt in the cache of a larger system, reducing memory accesses compared to a smaller system, which could lead to superlinear speedup.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="251" height="168" alt="image" src="Image_3100.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="35" alt="image" src="Image_3101.png"/></span></p><p class="s42" style="padding-top: 4pt;padding-left: 356pt;text-indent: 0pt;text-align: left;">linear scaleup</p><p class="s394" style="padding-top: 8pt;padding-left: 191pt;text-indent: 0pt;line-height: 88%;text-align: center;"><span class="s99">T</span><u>S</u> <span class="s69">T</span>L</p><p class="s42" style="padding-top: 3pt;padding-left: 350pt;text-indent: 0pt;text-align: left;">sublinear scaleup</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="35" height="5" alt="image" src="Image_3102.png"/></span></p><p class="s42" style="padding-left: 84pt;text-indent: 0pt;text-align: center;">problem size</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 162pt;text-indent: 0pt;text-align: left;">Figure 20.3 <span class="s74">Scaleup with increasing problem size and resources.</span></p><p style="padding-top: 8pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">to demonstrate <span class="s63">sublinear scaleup</span>. Figure 20.3 illustrates linear and sublinear scaleups (where the resources increase in proportion to problem size). There are two kinds of scaleup that are relevant in parallel database systems, depending on how the size of the task is measured:</p><p style="padding-top: 9pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><span class="s40">In </span><span class="s63">batch scaleup</span>, the size of the database increases, and the tasks are large jobs whose runtime depends on the size of the database. An example of such a task is a scan of a relation whose size is proportional to the size of the database. Thus, the size of the database is the measure of the size of the problem. Batch scaleup also applies in scientiﬁc applications, such as executing a weather simulation at an <i>N </i>-times ﬁner resolution,<span class="s76">3</span> or performing the simulation for an <i>N </i>-times longer period of time.</p><p class="s39" style="padding-top: 3pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s40">In </span><span class="s63">transaction scaleup</span><span class="p">, the rate at which transactions are submitted to the database increases, and the size of the database increases proportionally to the transaction rate. This kind of scaleup is what is relevant in transaction-processing systems where the transactions are small updates— for example, a deposit or withdrawal from an account — and transaction rates grow as more accounts are created. Such transaction processing is especially well adapted for parallel execution, since trans- actions can run concurrently and independently on separate nodes, and each trans- action takes roughly the same amount of time, even if the database grows.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Scaleup is usually the more important metric for measuring the eﬃciency of par- allel database systems. The goal of parallelism in database systems is usually to make sure that the database system can continue to perform at an acceptable speed, even as the size of the database and the number of transactions increases. Increasing the ca-</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="95" height="1" alt="image" src="Image_3103.png"/></span></p><p class="s77" style="padding-top: 3pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">3<span class="s78">For example, a weather simulation that divides the atmosphere in a particular region into cubes of side 200 meters may need to be modiﬁed to use a ﬁner resolution, with cubes of side 100 meters; the number of cubes would thus be scaled up by a factor of 8.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">pacity of the system by increasing the parallelism provides a smoother path for growth for an enterprise than does replacing a centralized system with a faster machine (even assuming that such a machine exists). However, we must also look at absolute perfor- mance numbers when using scaleup measures; a machine that scales up linearly may perform worse than a machine that scales less than linearly, simply because the latter machine is much faster to start oﬀ with.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">A number of factors work against eﬃcient parallel operation and can diminish both speedup and scaleup.</p><p class="s119" style="text-indent: 0pt;text-align: left;">(1<span class="s118">−</span><i>p</i>)<span class="s118">+</span>(<i>p</i><span class="s118">∕</span><i>n</i>)</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><span class="s63">Sequential computation</span>. Many tasks have some components that can beneﬁt from parallel processing, and some components that have to be executed sequentially. Consider a task that takes time <i>T </i>to run sequentially. Suppose the fraction of the total execution time that can beneﬁt from parallelization is <i>p</i>, and that part is exe- cuted by <i>n </i>nodes in parallel. Then the total time taken would be (1<span class="s15">−</span><i>p</i>)<i>T </i><span class="s15">+</span>(<i>p</i><span class="s15">∕</span><i>n</i>)<i>T </i>, and the speedup would be <u>&nbsp; </u><u>1</u><u>  </u> . (This formula is referred to as <span class="s63">Amdahl’s law</span>.)</p><p style="padding-top: 5pt;padding-left: 107pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">If the fraction <i>p </i>is, say <u>&nbsp;9</u> , then the maximum speedup possible, even with very large</p><p class="s119" style="padding-left: 14pt;text-indent: 0pt;line-height: 3pt;text-align: center;">10</p><p class="s13" style="padding-left: 107pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">n<span class="p">, would be 10.</span></p><p style="padding-left: 107pt;text-indent: 16pt;text-align: justify;">Now consider scaleup, where the problem size increases. If the time taken by the sequential part of a task increases along with the problem size, scaleup will be similarly limited. Suppose fraction <i>p </i>of the execution time of a problem beneﬁts from increasing resources, while fraction (1<span class="s15">−</span><i>p</i>) is sequential and does not beneﬁt from increasing resources. Then the scaleup with <i>n </i>times more resources</p><p class="s118" style="text-indent: 0pt;text-align: left;">− +</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 107pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">on a problem that is <i>n </i>times larger will be <u>&nbsp; </u><u>1</u><u>  </u> . (This formula is referred to</p><p class="s109" style="padding-left: 142pt;text-indent: 0pt;line-height: 6pt;text-align: center;">n<span class="s119">(1 </span>p<span class="s119">) </span>p</p><p style="padding-left: 107pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">as <span class="s63">Gustafson’s law</span>.) However, if the time taken by the sequential part does not</p><p style="padding-left: 107pt;text-indent: 0pt;text-align: justify;">increase with problem size, its impact on scaleup will be less as the problem sizes.</p><p style="padding-left: 107pt;text-indent: 13pt;text-align: justify;"><span class="s63">Start-up costs</span>. There is a start-up cost associated with initiating a single process. In a parallel operation consisting of thousands of processes, the <i>start-up time </i>may overshadow the actual processing time, aﬀecting speedup adversely.</p><p style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><span class="s63">Interference</span>. Since processes executing in a parallel system often access shared resources, a slowdown may result from the <i>interference </i>of each new process as it competes with existing processes for commonly held resources, such as a system bus, or shared disks, or even locks. Both speedup and scaleup are aﬀected by this phenomenon.</p><p style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><span class="s63">Skew</span>. By breaking down a single task into a number of parallel steps, we reduce the size of the average step. Nonetheless, the service time for the single slowest step will determine the service time for the task as a whole. It is often diﬃcult to divide a task into exactly equal-sized parts, and the way that the sizes are distributed is therefore <i>skewed</i>. For example, if a task of size 100 is divided into 10 parts, and the division is skewed, there may be some tasks of size less than 10 and some tasks of size more than 10; if even one task happens to be of size 20, the speedup obtained by running the tasks in parallel is only 5, instead of 10 as we would have hoped.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s258" style="padding-left: 145pt;text-indent: 0pt;text-align: left;"><span><img width="178" height="50" alt="image" src="Image_3104.png"/></span>	<span><img width="151" height="28" alt="image" src="Image_3105.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="20" height="10" alt="image" src="Image_3106.png"/></span></p><p class="toc">&nbsp;</p><div class="toc"><a class="toc0" href="part370.htm">(a) bus                  (b) ring</a><a class="toc0" href="part371.htm">(c) mesh               (d) hypercube</a><a class="toc0" href="part372.htm">(e) tree-like topology</a></div><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part368.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part370.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
