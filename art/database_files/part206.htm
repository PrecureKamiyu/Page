<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>10.4  Beyond MapReduce: Algebraic Operations</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part205.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part207.htm">下一个 &gt;</a></p><p class="s65" style="padding-left: 40pt;text-indent: 0pt;text-align: left;">10.4  <span style=" color: #00AEEF;">Beyond MapReduce: Algebraic Operations</span></p><p style="padding-top: 12pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Relational algebra forms the foundation of relational query processing, allowing queries to be modeled as trees of operations. This idea is extended to settings with more com- plex data types by supporting algebraic operators that can work on datasets containing records with complex data types, and returning datasets with records containing similar complex data types.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: left;">10.4.1 Motivation for Algebraic Operations</p><p style="padding-top: 6pt;padding-left: 87pt;text-indent: 0pt;text-align: right;">As we saw in Section 10.3.6, relational operations can be expressed by a sequence of</p><p class="s49" style="padding-left: 87pt;text-indent: 0pt;text-align: right;">map <span class="p">and </span>reduce <span class="p">steps. Expressing tasks in such as fashion can be quite cumbersome.</span></p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">For example, if programmers need to compute the join of two inputs, they should be able to express it as a single algebraic operation, instead of having to express it indirectly via <span class="s49">map </span>and <span class="s49">reduce </span>functions. Having access to functions such as joins can greatly simplify the job of a programmer.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The join operation can be executed in parallel, using a variety of techniques that we will see later in Section 22.3. In fact, doing so can be much more eﬃcient than implementing the join using <span class="s49">map </span>and <span class="s49">reduce </span>functions. Thus, even systems like Hive, where programmers do not directly write MapReduce code, can beneﬁt from direct support for operations such as join.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Later-generation parallel data-processing systems therefore added support for other relational operations such as joins (including variants such as outerjoins and semi- joins), as well as a variety of other operations to support data analytics. For example, many machine-learning models can be modeled as operators that take a set of records as input then output a set of records that have an extra attribute containing the value predicted by the model based on the other attributes of the record. Machine-learning algorithms can themselves be modeled as operators that take a set of training records as input and output a learned model. Processing of data often involves multiple steps, which can be modeled as a sequence (pipeline) or tree of operators.</p><p style="padding-left: 137pt;text-indent: 0pt;text-align: justify;">A unifying framework for these operations is to treat them as <i>algebraic operations</i></p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">that take one or more datasets as inputs and output one or more datasets.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Recall that in the relational algebra (Section 2.6) each operation takes one or more relations as input, and outputs a relation. These later-generation parallel query- processing systems are based on the same idea, but there are several diﬀerences. A key diﬀerence is that the input data could be of arbitrary types, instead of just consisting of columns with atomic data types as in the relational model. Recall that the extended relational algebra required to support <span class="s44">SQL </span>could restrict itself to simple arithmetic, string, and boolean expressions. In contrast, the new-generation algebraic operators need to support more complex expressions, requiring the full power of a programming language.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">There are a number of frameworks that support algebraic operations on complex data; the most widely used ones today are Apache Tez and Apache Spark.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Apache Tez provides a low-level <span class="s44">API </span>which is suitable for system implementors. For example, Hive on Tez compiles <span class="s44">SQL </span>queries into algebraic operations that run on Tez. Tez programmers can create trees (or in general Directed Acyclic Graphs, or <span class="s44">DAG</span>s) of nodes, and they provide code that is to be executed on each of the nodes. Input nodes would read in data from data sources and pass them to other nodes, which operate on the data. Data can be partitioned across multiple machines, and the code for each node can be executed on each of the machines. Since Tez is not really designed for application programmers to use directly, we do not describe it in further detail.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 17pt;text-align: justify;">However, Apache Spark provides higher-level <span class="s44">API</span>s which are suitable for applica- tion programmers. We describe Spark in more detail next.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 7pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">10.4.2 Algebraic Operations in Spark</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: right;">Apache Spark is a widely used parallel data processing system that supports a variety of algebraic operations. Data can be input from or output to a variety of storage systems. Just as relational databases use a relation as the primary abstraction for data rep- resentation, Spark uses a representation called a <span class="s63">Resilient Distributed Dataset </span>(<span class="s64">RDD</span>), which is a collection of records that can be stored across multiple machines. The term <i>distributed </i>refers to the records being stored on diﬀerent machines, and <i>resilient </i>refers to the resilience to failure, in that even if one of the machines fails, records can be</p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">retrieved from other machines where they are stored.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Operators in Spark take one or more <span class="s44">RDD</span>s as input, and their output is an <span class="s44">RDD</span>. The types of records stored in <span class="s44">RDD</span>s is not predeﬁned and can be anything that the ap- plication desires. Spark also supports a relational data representation called a DataSet, which we describe later.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Spark provides <span class="s44">API</span>s for Java, Scala, and Python. Our coverage of Spark is based on the Java <span class="s44">API</span>.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Figure 10.10 shows our word count application, written in Java using Apache Spark; this program uses the <span class="s44">RDD </span>data representation, whose Java type is called <span class="s49">JavaRDD</span>. Note that <span class="s49">JavaRDD</span>s require a type for the record, speciﬁed in angular brack-</p><p style="padding-top: 1pt;padding-left: 88pt;text-indent: 0pt;line-height: 70%;text-align: justify;">ets (“<span class="s83">&lt;&gt;</span>”). In the program we have <span class="s44">RDD</span>s of Java Strings. The program also has <span class="s49">Java- PairRDD </span>types, which store records with two attributes of speciﬁed types. Records with</p><p style="padding-top: 1pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">multiple attributes can be represented by using structured data types instead of primi- tive data types. While any user-deﬁned data type can be used, the predeﬁned data types <span class="s49">Tuple2 </span>which stores two attributes, <span class="s49">Tuple3</span>, which stores three attributes, and <span class="s49">Tuple4</span>, which stores four attributes, are widely used.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The ﬁrst step in processing data using Spark is to convert data from input rep- resentation to the <span class="s44">RDD </span>representation, which is done by the <span class="s49">spark.read().textfile() </span>function, which creates a record for each line in the input. Note that the input can be a ﬁle or a directory with multiple ﬁles; a Spark system running on multiple nodes will actually partition the <span class="s44">RDD </span>across multiple machines, although the program can treat it (for most purposes) as if it is a data structure on a single machine. In our sample code in Figure 10.10, the result is the <span class="s44">RDD </span>called <span class="s49">lines</span>.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The next step in our Spark program is to split each line into an array of words, by calling <span class="s49">s.split(&quot; &quot;)) </span>on the line; this function breaks up the line based on spaces and returns an array of words; a more complete function would split the input on other punctuation characters such as periods, semicolons, and so on. The split function can be invoked on each line in the input <span class="s44">RDD </span>by calling the <span class="s49">map() </span>function, which in Spark returns a single record for each input record. In our example, we instead use a variant called <span class="s49">flatMap()</span>, which works as follows: like <span class="s49">map()</span>, <span class="s49">flatMap() </span>invokes a user-deﬁned</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="494" height="1" alt="image" src="Image_2353.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s151" style="padding-top: 4pt;padding-left: 123pt;text-indent: 0pt;text-align: left;">import java.util.Arrays; import java.util.List; import scala.Tuple2;</p><p class="s151" style="padding-left: 123pt;text-indent: 0pt;text-align: left;">import org.apache.spark.api.java.JavaPairRDD; import org.apache.spark.api.java.JavaRDD; import org.apache.spark.sql.SparkSession; public class WordCount {</p><p class="s151" style="padding-top: 2pt;padding-left: 161pt;text-indent: -19pt;line-height: 22pt;text-align: left;">public static void main(String[] args) throws Exception { if (args.length <span class="s276">&lt; </span>1) {</p><p class="s151" style="padding-left: 180pt;text-indent: 0pt;line-height: 9pt;text-align: left;">System.err.println(&quot;Usage: WordCount &lt;file-or-directory-name&gt;&quot;);</p><p class="s151" style="padding-left: 180pt;text-indent: 0pt;text-align: left;">System.exit(1);</p><p class="s151" style="padding-left: 161pt;text-indent: 0pt;text-align: left;">}</p><p class="s151" style="padding-left: 180pt;text-indent: -19pt;text-align: left;">SparkSession spark = SparkSession.builder().appName(&quot;WordCount&quot;).getOrCreate();</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s151" style="padding-left: 161pt;text-indent: 0pt;text-align: left;">JavaRDD&lt;String&gt; lines = spark.read().textFile(args[0]).javaRDD(); JavaRDD&lt;String&gt; words = lines.flatMap(s -&gt; Arrays.asList(s.split(&quot; &quot;)).iterator()); JavaPairRDD&lt;String, Integer&gt; ones = words.mapToPair(s -&gt; new Tuple2&lt;&gt;(s, 1)); JavaPairRDD&lt;String, Integer&gt; counts = ones.reduceByKey((i1, i2) -&gt; i1 + i2);</p><p class="s151" style="padding-top: 2pt;padding-left: 161pt;text-indent: 0pt;line-height: 22pt;text-align: left;">counts.saveAsTextFile(&quot;outputDir&quot;); // Save output files in this directory List&lt;Tuple2&lt;String, Integer» output = counts.collect();</p><p class="s151" style="padding-left: 161pt;text-indent: 0pt;line-height: 9pt;text-align: left;">for (Tuple2&lt;String,Integer&gt; tuple : output) {</p><p class="s151" style="padding-left: 64pt;text-indent: 0pt;text-align: center;">System.out.println(tuple);</p><p class="s151" style="padding-left: 161pt;text-indent: 0pt;text-align: left;">}</p><p class="s151" style="padding-left: 161pt;text-indent: 0pt;text-align: left;">spark.stop();</p><p class="s151" style="padding-left: 142pt;text-indent: 0pt;text-align: left;">}</p><p class="s151" style="padding-left: 123pt;text-indent: 0pt;text-align: left;">}</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="494" height="1" alt="image" src="Image_2354.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 208pt;text-indent: 0pt;text-align: left;">Figure 10.10 <span class="s74">Word count program in Spark.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">function on each input record; the function is expected to return an iterator. A Java iterator supports a <span class="s49">next() </span>function that can be used to fetch multiple results by calling the function multiple times. The <span class="s49">flatMap() </span>function invokes the user-deﬁned function to get an iterator, invokes the <span class="s49">next() </span>function repeatedly on the iterator to get multiple values, and then returns an <span class="s44">RDD </span>containing the union of all the values across all input records.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The code shown in Figure 10.10 uses the “lambda expression” syntax introduced in Java 8, which allows functions to be deﬁned compactly, without even giving them a name; in the Java code, the syntax</p><p class="s49" style="padding-top: 4pt;padding-left: 29pt;text-indent: 0pt;text-align: center;">s <span class="s185">− </span><span class="s186">&gt; </span>Arrays.asList(s.split(&quot; &quot;)).iterator()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">deﬁnes a function that takes a parameter <span class="s49">s </span>and returns an expression that does the following: it applies the split function described earlier to create an array of words, then uses <span class="s49">Arrays.asList </span>to convert the array to a list, and ﬁnally applies the <span class="s49">iterator() </span>method on the list to create an iterator. The <span class="s49">flatMap() </span>function works on this iterator as described earlier.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The result of the above steps is an <span class="s44">RDD </span>called <span class="s49">words</span>, where each record contains a single word.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The next step is to create a <span class="s49">JavaPairRDD </span>called <span class="s49">ones</span>, which contains pairs of the form “(word, 1)” for each word in <span class="s49">words</span>; if a word appears multiple times in the input ﬁle, there would correspondingly be as many records in <span class="s49">words </span>and in <span class="s49">ones</span>.</p><p class="s49" style="padding-left: 88pt;text-indent: 17pt;text-align: justify;"><span class="p">Finally the algebraic operation </span>reduceByKey() <span class="p">implements a group by and aggre- gation step. In the sample code, we specify that addition is to be used for aggregation, by passing the lambda function </span>(i1, i2) <span class="s185">− </span><span class="s186">&gt; </span>i1+i2 <span class="p">to the </span>reduceByKey() <span class="p">function. The</span></p><p class="s49" style="padding-left: 88pt;text-indent: 0pt;line-height: 8pt;text-align: justify;">reduceByKey() <span class="p">function works on a </span>JavaPairRDD<span class="p">, grouping by the ﬁrst attribute, and</span></p><p class="s49" style="padding-left: 88pt;text-indent: 0pt;text-align: justify;"><span class="p">aggregating the values of the second attribute using the provided lambda function. When applied on the </span>ones <span class="s42">RDD</span><span class="s43">, grouping would be on the word, which is the ﬁrst attribute, and the values of the second attribute (all ones, in the </span>ones <span class="s42">RDD</span><span class="s43">) would be added up. The result is stored in the </span>JavaPairRDD counts<span class="p">.</span></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">In general, any binary function can be used to perform the aggregation, as long as it gives the same result regardless of the order in which it is applied on a collection of values.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Finally, the <span class="s49">counts </span><span class="s42">RDD </span><span class="s43">is stored to the ﬁle system by </span><span class="s49">saveAsTextFile()</span>. Instead of creating just one ﬁle, the function creates multiple ﬁles if the <span class="s44">RDD </span>itself is partitioned across machines.</p><p style="padding-left: 106pt;text-indent: 0pt;text-align: justify;">Key to understanding how parallel processing is achieved is to understand that</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s39" style="padding-left: 91pt;text-indent: 0pt;text-align: justify;">• <span class="s41">RDD</span><span class="s40">s may be partitioned and stored on multiple machines, and</span></p><p class="s40" style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span>each operation may be executed in parallel on multiple machines, on the <span class="s41">RDD </span>partition available at the machine. Operations may ﬁrst repartition their input, to bring related records to the same machine before executing operations in parallel. For example, <span class="s49">reduceByKey() </span><span class="p">would repartition the input </span><span class="s44">RDD </span><span class="p">to bring all records belonging to a group together on a single machine; records of diﬀerent groups may be on diﬀerent machines.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Another important aspect of Spark is that the algebraic operations are not neces- sarily evaluated immediately on the function call, although the code seems to imply that this is what happens. Instead, the code shown in the ﬁgure actually creates a tree of operations; in our code, the leaf operation <span class="s49">textFile() </span>reads data from a ﬁle; the next operation <span class="s49">flatMap() </span>has the <span class="s49">textFile() </span>operation as its child; the <span class="s49">mapToPairs() </span>in turn has <span class="s49">flatMap() </span>as child, and so on. The operators can be thought of in relational terms as deﬁning views, which are not executed as soon as they are deﬁned but get executed later.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 17pt;text-align: justify;">The entire tree of operations actually get evaluated only when certain operations demand that the tree be evaluated. For example, <span class="s49">saveAsTextFile() </span>forces the tree to be evaluated; other such functions include <span class="s49">collect()</span>, which evaluates the tree and brings all records to a single machine, where they can subsequently be processed, for example by printing them out.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">An important beneﬁt of such <i>lazy evaluation </i>of the tree (i.e., the tree is evaluated when required, rather than when it is deﬁned) is that before actual evaluation, it is possible for a query optimizer to rewrite the tree to another tree that computes the same result but may execute faster. Query optimization techniques, which we study in Chapter 16 can be applied to optimize such trees.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">While the preceding example created a tree of operations, in general the operations may form a <i>Directed Acyclic Graph </i>(<span class="s69">DAG</span>) structure, if the result of an operation is con- sumed by more than one other operation. That would result in operations having more than one parent, leading to a <span class="s44">DAG </span>structure, whereas operations in a tree can have at most one parent.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">While <span class="s44">RDD</span>s are well suited for representing certain data types such as textual data, a very large fraction of Big Data applications need to deal with structured data, where each record may have multiple attributes. Spark therefore introduced the <span class="s49">DataSet </span>type, which supports records with attributes. The <span class="s49">DataSet </span>type works well with widely used Parquet, <span class="s44">ORC</span>, and Avro ﬁle formats (discussed in more detail later in Section 13.6), which are designed to store records with multiple attributes in a compressed fashion. Spark also supports <span class="s44">JDBC </span>connectors that can read relations from a database.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The following code illustrates how data in Parquet format can be read and pro- cessed in Spark, where <span class="s49">spark </span>is a Spark session that has been opened earlier.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s49" style="padding-left: 176pt;text-indent: 0pt;line-height: 66%;text-align: left;">Dataset<span class="s186">&lt;</span>Row<span class="s186">&gt; </span>instructor = spark.read().parquet(&quot;...&quot;); Dataset<span class="s186">&lt;</span>Row<span class="s186">&gt; </span>department = spark.read().parquet(&quot;...&quot;); instructor.filter(instructor.col(&quot;salary&quot;).gt(100000))</p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="1" alt="image" src="Image_2355.png"/></span></p><p class="s49" style="padding-left: 197pt;text-indent: 0pt;text-align: left;">.join(department, instructor.col(&quot;dept name&quot;)</p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="1" alt="image" src="Image_2356.png"/></span></p><p class="s49" style="padding-left: 197pt;text-indent: 0pt;text-align: left;">.equalTo(department.col(&quot;dept name&quot;)))</p><p class="s49" style="padding-left: 197pt;text-indent: 0pt;text-align: left;">.groupBy(department.col(&quot;building&quot;))</p><p class="s49" style="padding-left: 197pt;text-indent: 0pt;text-align: left;">.agg(count(instructor.col(&quot;ID&quot;)));</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 17pt;line-height: 73%;text-align: justify;">The <span class="s49">DataSet</span><span class="s186">&lt;</span><span class="s49">Row</span><span class="s186">&gt; </span>type above uses the type Row, which allows access to column values by name. The code reads <i>instructor </i>and <i>department </i>relations from Parquet ﬁles</p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="1" alt="image" src="Image_2357.png"/></span></p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">(whose names are omitted in the code above); Parquet ﬁles store metadata such as column names in addition to the values, which allows Spark to create a schema for the relations. The Spark code then applies a ﬁlter (selection) operation on the <i>instructor </i>relation, which retains only instructors with salary greater than 100000, then joins the result with the <i>department </i>relation on the <i>dept name </i>attribute, performs a group by on the <i>building </i>attribute (an attribute of the department relation), and for each group (here, each building), a count of the number of <i>ID </i>values is computed.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 17pt;text-align: justify;"><a name="bookmark223">The ability to deﬁne new algebraic operations and to use them in queries has been found to be very useful for many applications and has led to wide adoption of Spark. The Spark system also supports compilation of Hive </a><span class="s44">SQL </span>queries into Spark operation trees, which are then executed.<a name="bookmark241">&zwnj;</a></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Spark also allows classes other than <span class="s49">Row </span>to be used with DataSets. Spark requires that for each attribute <i>Attrk </i>of the class, methods <i>getAttrk</i>() and <i>setAttrk</i>() must be deﬁned to allow retrieval and storage of attribute values. Suppose we have created a class <i>Instructor</i>, and we have a Parquet ﬁle whose attributes match those of the class. Then we can read data from Parquet ﬁles as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s49" style="padding-left: 159pt;text-indent: -21pt;line-height: 70%;text-align: left;">Dataset<span class="s186">&lt;</span>Instructor<span class="s186">&gt; </span>instructor = spark.read().parquet(&quot;...&quot;). as(Encoders.bean(Instructor.class));</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">In this case Parquet provides the names of attributes in the input ﬁle, which are used to map their values to attributes of the <span class="s49">Instructor </span>class. Unlike with <span class="s49">Row</span>, where the types are not known at compile time the types of attributes of <span class="s49">Instructor </span>are known at compile time, and can be represented more compactly than if we used the <span class="s49">Row </span>type. Further, the methods of the class <span class="s49">Instructor </span>can be used to access attributes; for example, we could use <span class="s49">getSalary() </span>instead of using <span class="s49">col(&quot;salary&quot;)</span>, which avoids the runtime cost of mapping attribute names to locations in the underlying records. More information on how to use these constructs can be found on the Spark documentation available online at <span class="s49">spark.apache.org</span>.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Our coverage of Spark has focused on database operations, but as mentioned ear- lier, Spark supports a number of other algebraic operations such as those related to machine learning, which can be invoked on DataSet types.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part205.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part207.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
