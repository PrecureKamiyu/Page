<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>12.5  RAID</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part235.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part237.htm">下一个 &gt;</a></p><p class="s65" style="padding-top: 8pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">12.5  <span style=" color: #00AEEF;">RAID</span></p><p style="padding-top: 12pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">The data-storage requirements of some applications (in particular web, database, and multimedia applications) have been growing so fast that a large number of disks are needed to store their data, even though disk-drive capacities have been growing very fast.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Having a large number of disks in a system presents opportunities for improving the rate at which data can be read or written, if the disks are operated in parallel. Several independent reads or writes can also be performed in parallel. Furthermore, this setup oﬀers the potential for improving the reliability of data storage, because redundant information can be stored on multiple disks. Thus, failure of one disk does not lead to loss of data.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">A variety of disk-organization techniques, collectively called <span class="s63">redundant arrays of independent disks </span>(<span class="s64">RAID</span>), have been proposed to achieve improved performance and reliability.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">In the past, system designers viewed storage systems composed of several small, cheap disks as a cost-eﬀective alternative to using large, expensive disks; the cost per megabyte of the smaller disks was less than that of larger disks. In fact, the <span class="s44">I </span>in <span class="s44">RAID</span>, which now stands for <i>independent</i>, originally stood for <i>inexpensive</i>. Today, however, all disks are physically small, and larger-capacity disks actually have a lower cost per megabyte. <span class="s44">RAID </span>systems are used for their higher reliability and higher performance rate, rather than for economic reasons. Another key justiﬁcation for <span class="s44">RAID </span>use is easier management and operations.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">12.5.1 Improvement of Reliability via Redundancy</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Let us ﬁrst consider reliability. The chance that at least one disk out of a set of <i>N </i>disks will fail is much higher than the chance that a speciﬁc single disk will fail. Suppose that the mean time to failure of a disk is 100,000 hours, or slightly over 11 years. Then, the mean time to failure of some disk in an array of 100 disks will be 100,000<span class="s15">∕</span>100 = 1000 hours, or around 42 days, which is not long at all! If we store only one copy of the data, then each disk failure will result in loss of a signiﬁcant amount of data (as discussed in Section 12.3.1). Such a high frequency of data loss is unacceptable.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The solution to the problem of reliability is to introduce <span class="s63">redundancy</span>; that is, we store extra information that is not needed normally but that can be used in the event of failure of a disk to rebuild the lost information. Thus, even if a disk fails, data are not lost, so the eﬀective mean time to failure is increased, provided that we count only failures that lead to loss of data or to non-availability of data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-top: 3pt;padding-left: 119pt;text-indent: 17pt;text-align: justify;">The simplest (but most expensive) approach to introducing redundancy is to du- plicate every disk. This technique is called <span class="s63">mirroring </span>(or, sometimes, <span class="s13">shadowing</span>). A logical disk then consists of two physical disks, and every write is carried out on both disks. If one of the disks fails, the data can be read from the other. Data will be lost only if the second disk fails before the first failed disk is repaired.</p><p class="s15" style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The mean time to failure (where failure is the loss of data) of a mirrored disk depends on the mean time to failure of the individual disks, as well as on the <span class="s63">mean time to repair</span>, which is the time it takes (on an average) to replace a failed disk and to restore the data on it. Suppose that the failures of the two disks are <span class="s13">independent</span>; that is, there is no connection between the failure of one disk and the failure of the other. Then, if the mean time to failure of a single disk is 100,000 hours, and the mean time to repair is 10 hours, the <span class="s63">mean time to data loss </span>of a mirrored disk system is 100, 000<span class="s181">2</span>∕(2 ∗ 10) = 500 ∗ 10<span class="s181">6</span> hours, or 57,000 years! (We do not go into the derivations here; references in the bibliographical notes provide the details.)</p><p class="s15" style="padding-left: 119pt;text-indent: 17pt;text-align: right;">You should be aware that the assumption of independence of disk failures is not valid. Power failures and natural disasters such as earthquakes, fires, and floods may result in damage to both disks at the same time. As disks age, the probability of failure increases, increasing the chance that a second disk will fail while the first is being repaired. In spite of all these considerations, however, mirrored-disk systems offer much higher reliability than do single-disk systems. Mirrored-disk systems with mean time to data loss of about 500,000 to 1,000,000 hours, or 55 to 110 years, are available today. Power failures are a particular source of concern, since they occur far more fre- quently than do natural disasters. Power failures are not a concern if there is no data transfer to disk in progress when they occur. However, even with mirroring of disks, if writes are in progress to the same block in both disks, and power fails before both blocks are fully written, the two blocks can be in an inconsistent state. The solution to this problem is to write one copy first, then the next, so that one of the two copies is al- ways consistent. Some extra actions are required when we restart after a power failure, to recover from incomplete writes. This matter is examined in Practice Exercise 12.6.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s308" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">12.5.2 Improvement in Performance via Parallelism</p><p class="s15" style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Now let us consider the benefit of parallel access to multiple disks. With disk mirroring, the rate at which read requests can be handled is doubled, since read requests can be sent to either disk (as long as both disks in a pair are functional, as is almost always the case). The transfer rate of each read is the same as in a single-disk system, but the number of reads per unit time has doubled.</p><p class="s15" style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">With multiple disks, we can improve the transfer rate as well (or instead) by <span class="s63">striping data </span>across multiple disks. In its simplest form, data striping consists of splitting the bits of each byte across multiple disks; such striping is called <span class="s63">bit-level striping</span>. For example, if we have an array of eight disks, we write bit <span class="s13">i </span>of each byte to disk <span class="s13">i</span>. In such an organization, every disk participates in every access (read or write), so the number of accesses that can be processed per second is about the same as on a single disk, but each access can read eight times as much data in the same time as on a single disk.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="text-indent: 0pt;text-align: left;">⌊ ⌋</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="padding-top: 4pt;padding-left: 88pt;text-indent: 17pt;line-height: 93%;text-align: justify;"><span class="s63">Block-level striping </span>stripes blocks across multiple disks. It treats the array of disks as a single large disk, and it gives blocks logical numbers; we assume the block numbers start from 0. With an array of <span class="s309">n </span>disks, block-level striping assigns logical block <span class="s309">i </span>of the</p><p class="s15" style="padding-left: 59pt;text-indent: 0pt;line-height: 12pt;text-align: center;">disk array to disk (<span class="s309">i </span>mod <span class="s309">n</span>) + 1; it uses the <span class="s309">i</span>∕<span class="s309">n </span>th physical block of the disk to store</p><p class="s15" style="padding-left: 88pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">logical block <span class="s309">i</span>. For example, with eight disks, logical block 0 is stored in physical block 0 of disk 1, while logical block 11 is stored in physical block 1 of disk 4. When reading a large file, block-level striping fetches <span class="s309">n </span>blocks at a time in parallel from the <span class="s309">n </span>disks, giving a high data-transfer rate for large reads. When a single block is read, the data- transfer rate is the same as on one disk, but the remaining <span class="s309">n</span>−1 disks are free to perform other actions.</p><p class="s15" style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Block-level striping offers several advantages over bit-level striping, including the ability to support a larger number of block reads per second, and lower latency for single block reads. As a result, bit-level striping is not used in any practical system.</p><p class="s15" style="padding-left: 106pt;text-indent: 0pt;text-align: justify;">In summary, there are two main goals of parallelism in a disk system:</p><p class="s63" style="padding-top: 7pt;padding-left: 113pt;text-indent: -16pt;text-align: left;">1. <span class="s15">Load-balance multiple small accesses (block accesses), so that the throughput of such accesses increases.</span></p><p class="s63" style="padding-top: 6pt;padding-left: 96pt;text-indent: 0pt;text-align: left;">2. <span class="s15">Parallelize large accesses so that the response time of large accesses is reduced.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">12.5.3 RAID Levels</p><p class="s15" style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Mirroring provides high reliability, but it is expensive. Striping provides high data- transfer rates, but does not improve reliability. Various alternative schemes aim to pro- vide redundancy at lower cost by combining disk striping with “parity blocks”.</p><p class="s15" style="padding-left: 88pt;text-indent: 17pt;line-height: 13pt;text-align: justify;">Blocks in a <span class="s16">RAID </span>system are partitioned into sets, as we shall see. For a given set of blocks, a <span class="s63">parity block </span>can be computed and stored on disk; the <span class="s309">i</span>th bit of the parity block is computed as the “exclusive or” (XOR) of the <span class="s309">i</span>th bits of the all blocks in the set. If the contents of any one of the blocks in a set is lost due to a failure, the block contents can be recovered by computing the bitwise-XOR of the remaining blocks in the set, along with the parity block.</p><p class="s15" style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Whenever a block is written, the parity block for its set must be recomputed and written to disk. The new value of the parity block can be computed by either (i) reading all the other blocks in the set from disk and computing the new parity block, or (ii) by computing the XOR of the old value of the parity block with the old and new value of the updated block.</p><p class="s15" style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">These schemes have different cost-performance trade-offs. The schemes are classi- fied into <span class="s64">RAID </span><span class="s84">levels.</span><span class="s181">4</span>. Figure 12.4 illustrates the four levels that are used in practice. In the figure, P indicates error-correcting bits, and C indicates a second copy of the data. For all levels, the figure depicts four disks’ worth of data, and the extra disks depicted are used to store redundant information for failure recovery.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="96" height="1" alt="image" src="Image_2536.png"/></span></p><p class="s230" style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: left;"><span class="s310">4</span>There are 7 different <span class="s311">RAID </span>levels, numbered 0 to 6; Levels 2, 3, and 4 are not used in practice anymore and thus are not covered in the text</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 252pt;text-indent: 0pt;text-align: left;"><span><img width="122" height="31" alt="image" src="Image_2537.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="216" height="31" alt="image" src="Image_2538.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s312" style="padding-left: 102pt;text-indent: 0pt;text-align: left;">C   C   C</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="28" height="31" alt="image" src="Image_2539.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s312" style="text-indent: 0pt;text-align: center;">C</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="padding-top: 3pt;padding-left: 239pt;text-indent: 0pt;text-align: left;">(a) RAID 0: nonredundant striping</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s465" style="padding-left: 205pt;text-indent: 0pt;text-align: left;">	</p><p style="text-indent: 0pt;text-align: left;"><span><img width="154" height="29" alt="image" src="Image_2540.png"/></span></p><p class="s312" style="text-indent: 0pt;line-height: 9pt;text-align: left;">P  <span class="s313">P</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s312" style="text-indent: 0pt;line-height: 7pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s312" style="text-indent: 0pt;line-height: 7pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s312" style="text-indent: 0pt;line-height: 7pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="padding-top: 5pt;padding-left: 251pt;text-indent: 0pt;text-align: left;">(b) RAID 1: mirrored disks</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="187" height="31" alt="image" src="Image_2541.png"/></span></p><p class="s312" style="padding-left: 3pt;text-indent: -3pt;text-align: left;">P Q</p><p style="text-indent: 0pt;text-align: left;"/><p class="s313" style="text-indent: 0pt;line-height: 9pt;text-align: left;">P <span class="s312">Q</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s313" style="text-indent: 0pt;line-height: 9pt;text-align: left;">P<span class="s312">Q</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s313" style="text-indent: 0pt;line-height: 9pt;text-align: left;">P<span class="s312">Q</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s313" style="text-indent: 0pt;line-height: 9pt;text-align: left;">P <span class="s312">Q</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s313" style="text-indent: 0pt;line-height: 9pt;text-align: left;">P <span class="s312">Q</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="padding-top: 4pt;padding-left: 218pt;text-indent: 0pt;text-align: left;">(c) RAID 5: block-interleaved distributed parity</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s85" style="padding-top: 4pt;padding-left: 246pt;text-indent: 0pt;text-align: left;">(d) RAID 6: P + Q redundancy</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 252pt;text-indent: 0pt;text-align: left;">Figure 12.4 <span class="s151">RAID </span><span class="s152">levels.</span></p><p class="s39" style="padding-top: 5pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s64">RAID </span><span class="s84">level 0 </span><span class="p">refers to disk arrays with striping at the level of blocks, but without any redundancy (such as mirroring or parity bits). Figure 12.4a shows an array of size 4.</span></p><p class="s39" style="padding-top: 4pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s64">RAID </span><span class="s84">level 1 </span><span class="p">refers to disk mirroring with block striping. Figure 12.4b shows a mirrored organization that holds four disks’ worth of data.</span></p><p style="padding-left: 139pt;text-indent: 17pt;text-align: justify;">Note that some vendors use the term <span class="s64">RAID </span><span class="s84">level 1+0 </span>or <span class="s64">RAID </span><span class="s84">level 10 </span>to refer to mirroring with striping, and they use the term <span class="s44">RAID </span>level 1 to refer to mirroring without striping. Mirroring without striping can also be used with arrays of disks, to give the appearance of a single large, reliable disk: if each disk has <i>M </i>blocks, logical blocks 0 to <i>M </i><span class="s15">− </span>1 are stored on disk 0, <i>M </i>to 2<i>M </i><span class="s15">− </span>1 on disk 1(the second disk), and so on, and each disk is mirrored.<span class="s76">5</span></p><p style="padding-top: 3pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><span class="s64">RAID </span><span class="s84">level 5 </span>refers to block-interleaved distributed parity. The data and parity are partitioned among all <i>N </i><span class="s15">+ </span>1 disks. For each set of <i>N </i>logical blocks, one of the disks stores the parity, and the other <i>N </i>disks store the blocks. The parity blocks are stored on diﬀerent disks for diﬀerent sets of <i>N </i>blocks. Thus, all disks can participate in satisfying read requests.<span class="s76">6</span></p><p style="padding-left: 139pt;text-indent: 17pt;text-align: justify;">Figure 12.4c shows the setup. The P’s are distributed across all the disks. For example, with an array of ﬁve disks, the parity block, labeled Pk, for logical blocks</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="96" height="1" alt="image" src="Image_2542.png"/></span></p><p class="s78" style="padding-top: 3pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;"><span class="s77">5</span>Note that some vendors use the term <span class="s153">RAID </span>0+1 to refer to a version of <span class="s153">RAID </span>that uses striping to create a <span class="s153">RAID </span>0 array, and mirrors the array onto another array, with the diﬀerence from <span class="s153">RAID </span>1 being that if a disk fails, the <span class="s153">RAID </span>0 array containing the disk becomes unusable. The mirrored array can still be used, so there is no loss of data. This arrangement is inferior to <span class="s153">RAID </span>1 when a disk has failed, since the other disks in the <span class="s153">RAID </span>0 array can continue to be used in <span class="s153">RAID </span>1, but remain idle in <span class="s153">RAID </span>0+1.</p><p class="s78" style="padding-left: 119pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><span class="s77">6</span>In <span class="s153">RAID </span>level 4 (which is not used in practice) all parity blocks are stored on one disk. That disk would not be useful</p><p class="s80" style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">for reads, and it would also have a higher load than other disks if there were many random writes.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 107pt;text-indent: 0pt;text-align: justify;">4<i>k</i>, 4<i>k </i><span class="s15">+ </span>1, 4<i>k </i><span class="s15">+ </span>2, 4<i>k </i><span class="s15">+ </span>3 is stored in disk <i>k </i>mod 5; the corresponding blocks of the other four disks store the four data blocks 4<i>k </i>to 4<i>k </i><span class="s15">+ </span>3. The following table indicates how the ﬁrst 20 blocks, numbered 0 to 19, and their parity blocks are laid out. The pattern shown gets repeated on further blocks.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:222.67pt" cellspacing="0"><tr style="height:13pt"><td style="width:21pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 6pt;text-indent: 0pt;line-height: 11pt;text-align: right;">P0</p></td><td style="width:27pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 7pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0</p></td><td style="width:25pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 7pt;text-indent: 0pt;line-height: 11pt;text-align: right;">1</p></td><td style="width:26pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: center;">2</p></td><td style="width:22pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 5pt;text-indent: 0pt;line-height: 11pt;text-align: right;">3</p></td></tr><tr style="height:12pt"><td style="width:21pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 6pt;text-indent: 0pt;line-height: 11pt;text-align: right;">4</p></td><td style="width:27pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 7pt;text-indent: 0pt;line-height: 11pt;text-align: right;">P1</p></td><td style="width:25pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 7pt;text-indent: 0pt;line-height: 11pt;text-align: right;">5</p></td><td style="width:26pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: center;">6</p></td><td style="width:22pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 5pt;text-indent: 0pt;line-height: 11pt;text-align: right;">7</p></td></tr><tr style="height:12pt"><td style="width:21pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 6pt;text-indent: 0pt;line-height: 11pt;text-align: right;">8</p></td><td style="width:27pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 7pt;text-indent: 0pt;line-height: 11pt;text-align: right;">9</p></td><td style="width:25pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 7pt;text-indent: 0pt;line-height: 11pt;text-align: right;">P2</p></td><td style="width:26pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-left: 5pt;padding-right: 6pt;text-indent: 0pt;line-height: 11pt;text-align: center;">10</p></td><td style="width:22pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 5pt;text-indent: 0pt;line-height: 11pt;text-align: right;">11</p></td></tr><tr style="height:12pt"><td style="width:21pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 6pt;text-indent: 0pt;line-height: 11pt;text-align: right;">12</p></td><td style="width:27pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 7pt;text-indent: 0pt;line-height: 11pt;text-align: right;">13</p></td><td style="width:25pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 7pt;text-indent: 0pt;line-height: 11pt;text-align: right;">14</p></td><td style="width:26pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-left: 4pt;padding-right: 7pt;text-indent: 0pt;line-height: 11pt;text-align: center;">P3</p></td><td style="width:22pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 5pt;text-indent: 0pt;line-height: 11pt;text-align: right;">15</p></td></tr><tr style="height:13pt"><td style="width:21pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 6pt;text-indent: 0pt;line-height: 11pt;text-align: right;">16</p></td><td style="width:27pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 7pt;text-indent: 0pt;line-height: 11pt;text-align: right;">17</p></td><td style="width:25pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 7pt;text-indent: 0pt;line-height: 11pt;text-align: right;">18</p></td><td style="width:26pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-left: 5pt;padding-right: 6pt;text-indent: 0pt;line-height: 11pt;text-align: center;">19</p></td><td style="width:22pt;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#C7EAFB"><p class="s154" style="padding-right: 5pt;text-indent: 0pt;line-height: 11pt;text-align: right;">P4</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 107pt;text-indent: 17pt;text-align: justify;">Note that a parity block cannot store parity for blocks in the same disk, since then a disk failure would result in loss of data as well as of parity, and hence would not be recoverable.</p><p style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><span class="s64">RAID </span><span class="s84">level 6</span>, the P + Q redundancy scheme, is much like <span class="s44">RAID </span>level 5, but it stores extra redundant information to guard against multiple disk failures. Instead of us- ing parity, level 6 uses error-correcting codes such as the Reed-Solomon codes (see the bibliographical notes). In the scheme in Figure 12.4g, two bits of redundant data are stored for every four bits of data — unlike one parity bit in level 5 — and the system can tolerate two disk failures.</p><p style="padding-left: 107pt;text-indent: 17pt;text-align: justify;">The letters P and Q in the ﬁgure denote blocks containing the two correspond- ing error-correcting blocks for a given set of data blocks. The layout of blocks is an extension of that for <span class="s44">RAID </span>5. For example, with six disks, the two parity blocks, labeled Pk and Qk, for logical blocks 4<i>k</i>, 4<i>k </i><span class="s15">+ </span>1, 4<i>k </i><span class="s15">+ </span>2, <i>and</i>4<i>k </i><span class="s15">+ </span>3 are stored in disk <i>k </i>mod 6 and (<i>k </i><span class="s15">+ </span>1) mod 6, and the corresponding blocks of the other four disks store the four data blocks 4<i>k </i>to 4<i>k </i><span class="s15">+ </span>3.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Finally, we note that several variations have been proposed to the basic <span class="s44">RAID </span>schemes described here, and diﬀerent vendors use diﬀerent terminologies for the vari- ants. Some vendors support nested schemes that create multiple separate <span class="s44">RAID </span>arrays, and then stripe data across the <span class="s44">RAID </span>arrays; one of <span class="s44">RAID </span>levels 1, 5 or 6 is chosen for the individual arrays. References to further information on this idea are provided in the Further Reading section at the end of the chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">12.5.4 Hardware Issues</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;"><span class="s42">RAID </span><span class="s43">can be implemented with no change at the hardware level, using only software modiﬁcation. Such </span><span class="s42">RAID </span><span class="s43">implementations are called </span><span class="s63">software </span><span class="s82">RAID</span>. However, there are signiﬁcant beneﬁts to be had by building special-purpose hardware to support <span class="s44">RAID</span>, which we outline below; systems with special hardware support are called <span class="s63">hardware </span><span class="s82">RAID </span>systems.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 17pt;text-align: justify;">Hardware <span class="s44">RAID </span>implementations can use non-volatile <span class="s44">RAM </span>to record writes be- fore they are performed. In case of power failure, when the system comes back up, it retrieves information about any incomplete writes from non-volatile <span class="s44">RAM </span>and then completes the writes. Normal operations can then commence.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">In contrast, with software <span class="s44">RAID </span>extra work needs to be done to detect blocks that may have been partially written before power failure. For <span class="s44">RAID </span>1, all blocks of the disks are scanned to see if any pair of blocks on the two disks have diﬀerent contents. For <span class="s44">RAID </span>5, the disks need to be scanned and parity recomputed for each set of blocks and compared to the stored parity. Such scans take a long time, and they are done in the background using a small fraction of the disks’ available bandwidth. See Practice Ex- ercise 12.6 for details of how to recover data to the latest value, when an inconsistency is detected; we revisit this issue in the context of database system recovery in Section</p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">19.2.1. The <span class="s44">RAID </span>system is said to be <i>resynchronizing </i>(or <i>resynching</i>) during this phase; normal reads and writes are allowed while resynchronization is in progress, but a failure of a disk during this phase could result in data loss for blocks with incomplete writes. Hardware <span class="s44">RAID </span>does not have this limitation.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Even if all writes are completed properly, there is a small chance of a sector in a disk becoming unreadable at some point, even though it was successfully written earlier. Reasons for loss of data on individual sectors could range from manufacturing defects to data corruption on a track when an adjacent track is written repeatedly. Such loss of data that were successfully written earlier is sometimes referred to as a <i>latent failure</i>, or as <i>bit rot</i>. When such a failure happens, if it is detected early the data can be recovered from the remaining disks in the <span class="s44">RAID </span>organization. However, if such a failure remains undetected, a single disk failure could lead to data loss if a sector in one of the other disks has a latent failure.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">To minimize the chance of such data loss, good <span class="s44">RAID </span>controllers perform <span class="s63">scrub- bing</span>; that is, during periods when disks are idle, every sector of every disk is read, and if any sector is found to be unreadable, the data are recovered from the remaining disks in the <span class="s44">RAID </span>organization, and the sector is written back. (If the physical sector is dam- aged, the disk controller would remap the logical sector address to a diﬀerent physical sector on disk.)</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Server hardware is often designed to permit <span class="s63">hot swapping</span>; that is, faulty disks can be removed and replaced by new ones without turning power oﬀ. The <span class="s44">RAID </span>controller can detect that a disk was replaced by a new one and can immediately proceed to reconstruct the data that was on the old disk, and write it to the new disk. Hot swapping reduces the mean time to repair, since replacement of a disk does not have to wait until a time when the system can be shut down. In fact, many critical systems today run on a 24 <span class="s15">× </span>7 schedule; that is, they run 24 hours a day, 7 days a week, providing no time for shutting down and replacing a failed disk. Further, many <span class="s44">RAID </span>implementations assign a spare disk for each array (or for a set of disk arrays). If a disk fails, the spare disk is immediately used as a replacement. As a result, the mean time to repair is reduced greatly, minimizing the chance of any data loss. The failed disk can be replaced at leisure.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 17pt;text-align: justify;">The power supply, or the disk controller, or even the system interconnection in a <span class="s44">RAID </span>system could become a single point of failure that could stop the functioning of the <span class="s44">RAID </span>system. To avoid this possibility, good <span class="s44">RAID </span>implementations have multi- ple redundant power supplies (with battery backups so they continue to function even if power fails). Such <span class="s44">RAID </span>systems have multiple disk interfaces and multiple inter- connections to connect the <span class="s44">RAID </span>system to the computer system (or to a network of computer systems). Thus, failure of any single component will not stop the functioning of the <span class="s44">RAID </span>system.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">12.5.5 Choice of RAID Level</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">The factors to be taken into account in choosing a <span class="s44">RAID </span>level are:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s39" style="padding-left: 91pt;text-indent: 0pt;text-align: left;">• <span class="s40">Monetary cost of extra disk-storage requirements.</span></p><p class="s40" style="padding-top: 3pt;padding-left: 91pt;text-indent: 0pt;text-align: left;"><span class="s39">• </span>Performance requirements in terms of number of <span class="s41">I/O </span>operations per second.</p><p class="s39" style="padding-top: 3pt;padding-left: 91pt;text-indent: 0pt;text-align: left;">• <span class="s40">Performance when a disk has failed.</span></p><p class="s39" style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: left;">• <span class="s40">Performance during rebuild (i.e., while the data in a failed disk are being rebuilt on a new disk).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: right;">The time to rebuild the data of a failed disk can be signiﬁcant, and it varies with the <span class="s44">RAID </span>level that is used. Rebuilding is easiest for <span class="s44">RAID </span>level 1, since data can be copied from another disk; for the other levels, we need to access all the other disks in the array to rebuild data of a failed disk. The <span class="s63">rebuild performance </span>of a <span class="s44">RAID </span>system may be an important factor if continuous availability of data is required, as it is in high- performance database systems. Furthermore, since rebuild time can form a signiﬁcant part of the repair time, rebuild performance also inﬂuences the mean time to data loss.</p><p class="s42" style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">RAID <span class="s43">level 0 is used in a few high-performance applications where data safety is not critical, but not anywhere else.</span></p><p class="s42" style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">RAID <span class="s43">level 1 is popular for applications such as storage of log ﬁles in a database system, since it oﬀers the best write performance. </span>RAID <span class="s43">level 5 has a lower storage overhead than level 1, but it has a higher time overhead for writes. For applications where data are read frequently, and written rarely, level 5 is the preferred choice.</span></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Disk-storage capacities have been increasing rapidly for many years. Capacities were eﬀectively doubling every 13 months at one point; although the current rate of growth is much less now, capacities have continued to increase rapidly. The cost per byte of disk storage has been falling at about the same rate as the capacity increase. As a result, for many existing database applications with moderate storage requirements, the monetary cost of the extra disk storage needed for mirroring has become rela- tively small (the extra monetary cost, however, remains a signiﬁcant issue for storage- intensive applications such as video data storage). Disk access speeds have not im- proved signiﬁcantly in recent years, while the number of <span class="s44">I/O </span>operations required per second has increased tremendously, particularly for web application servers.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s185" style="padding-top: 3pt;padding-left: 119pt;text-indent: 17pt;text-align: right;"><a name="bookmark260">RAID </a><span class="s314">level 5 has a significant overhead for random writes, since a single random block write requires 2 block reads (to get the old values of the block and parity block) and 2 block writes to write these blocks back. In contrast, the overhead is low for large sequential writes, since the parity block can be computed from the new blocks in most cases, without any reads. </span>RAID <span class="s314">level 1 is therefore the </span>RAID <span class="s314">level of choice for many applications with moderate storage requirements and high random </span>I/O <span class="s314">requirements. </span>RAID <span class="s314">level 6 offers better reliability than level 1 or 5, since it can tolerate two disk failures without losing data. In terms of performance during normal operation, it is similar to </span>RAID <span class="s314">level 5, but it has a higher storage cost than </span>RAID <span class="s314">level 5. </span>RAID <span class="s314">level 6 is used in applications where data safety is very important. It is being viewed as increasingly important since latent sector failures are not uncommon, and it may take a long time to be detected and repaired. A failure of a different disk before a latent failure is detected and repaired would then be similar to a two-disk failure for that sector and result in loss of data of that sector. </span>RAID <span class="s314">levels 1 and 5 would suffer from data loss in</span><a name="bookmark295">&zwnj;</a></p><p class="s15" style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">such a scenario, unlike level 6.</p><p class="s15" style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Mirroring can also be extended to store copies on three disks instead of two to survive two-disk failures. Such triple-redundancy schemes are not commonly used in <span class="s16">RAID </span>systems, although they are used in distributed file systems, where data are stored in multiple machines, since the probability of machine failure is significantly higher than that of disk failure.</p><p class="s185" style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">RAID <span class="s314">system designers have to make several other decisions as well. For example, how many disks should there be in an array? How many bits should be protected by each parity bit? If there are more disks in an array, data-transfer rates are higher, but the system will be more expensive. If there are more bits protected by a parity bit, the space overhead due to parity bits is lower, but there is an increased chance that a second disk will fail before the first failed disk is repaired, and that will result in data loss.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s315" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">12.5.6 Other RAID Applications</p><p class="s15" style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">The concepts of <span class="s16">RAID </span>have been generalized to other storage devices, including in the flash memory devices within <span class="s16">SSDs</span>, arrays of tapes, and even to the broadcast of data over wireless systems. Individual flash pages have a higher rate of data loss than sectors of magnetic disks. Flash devices such as <span class="s16">SSDs </span>implement <span class="s16">RAID </span>internally, to ensure that the device does not lose data due to the loss of a flash page. When applied to arrays of tapes, the <span class="s16">RAID </span>structures are able to recover data even if one of the tapes in an array of tapes is damaged. When applied to broadcast of data, a block of data are split into short units and is broadcast along with a parity unit; if one of the units is not received for any reason, it can be reconstructed from the other units.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part235.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part237.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
