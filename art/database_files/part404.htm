<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>22.5  Parallel Evaluation of Query Plans</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part403.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part405.htm">下一个 &gt;</a></p><p class="s65" style="padding-top: 10pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">22.5  <span style=" color: #00AEEF;">Parallel Evaluation of Query Plans</span></p><p style="padding-top: 11pt;padding-left: 88pt;text-indent: 0pt;text-align: right;">As discussed in Section 22.1, there are two types of parallelism: intraoperation and in- teroperation. Until now in this chapter, we have focused on intraoperation parallelism. In this section, we consider execution plans for queries containing multiple operations. We ﬁrst consider how to exploit interoperator parallelism. We then consider a model of parallel query execution which breaks parallel query processing into two types of steps: partitioning of data using the <i>exchange operator</i>, and execution of operations on purely local data, without any data exchange. This model is surprisingly powerful</p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">and is widely used in parallel database implementations.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 8pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">22.5.1 Interoperation Parallelism</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">There are two forms of interoperation parallelism: pipelined parallelism and indepen- dent parallelism. We ﬁrst describe these forms of parallelism, assuming each operator runs on a single node without intraoperation parallelism.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 17pt;text-align: justify;">We then describe a model for parallel execution based on the exchange operator, in Section 22.5.2. Finally, in Section 22.5.3, we describe how a complete plan can be executed, combining all the forms of parallelism.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-top: 7pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">22.5.1.1 Pipelined Parallelism</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Recall from Section 15.7.2 that in pipelining, the output tuples of one operation, <i>A</i>, are consumed by a second operation, <i>B</i>, even before the ﬁrst operation has produced the entire set of tuples in its output. The major advantage of pipelined execution in a sequential evaluation is that we can carry out a sequence of such operations without writing any of the intermediate results to disk.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Parallel systems use pipelining primarily for the same reason that sequential sys- tems do. However, pipelines are a source of parallelism as well, since it is possible to run operations <i>A </i>and <i>B </i>simultaneously on diﬀerent nodes (or diﬀerent cores of a sin- gle node), so that <i>B </i>consumes tuples in parallel with <i>A </i>producing them. This form of parallelism is called <span class="s63">pipelined parallelism</span>.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Pipelined parallelism is useful with a small number of nodes, but it does not scale up well. First, pipeline chains generally do not attain suﬃcient length to provide a high degree of parallelism. Second, it is not possible to pipeline relational operators that do not produce output until all inputs have been accessed, such as the set-diﬀerence operation. Third, only marginal speedup is obtained for the frequent cases in which one operator’s execution cost is much higher than are those of the others.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">All things considered, when the degree of parallelism is high, pipelining is a less important source of parallelism than partitioning. The real reason for using pipelining with parallel query processing is the same reason that pipelining is used with sequen- tial query processing: namely, that pipelined executions can avoid writing intermediate results to disk.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Pipelining in centralized databases was discussed in Section 15.7.2; as mentioned there, pipelining can be done using a demand-driven, or pull, model of computation, or using a producer-driven, or push, model of computation. The pull model is widely used in centralized database systems.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">However, the push model is greatly preferred in parallel database systems, since, unlike the pull model, the push model allows both the producer and consumer to exe- cute in parallel.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Unlike the pull model, the push model requires a buﬀer that can hold multiple tuples, between the producer and consumer; without such a buﬀer, the producer would stall as soon as it generates one tuple. Figure 22.5 shows a producer and consumer with a buﬀer in-between. If the producer and consumer are on the same node, as shown in Figure 22.5a, the buﬀer can be in shared memory. However, if the producer and consumer are in diﬀerent nodes, as shown in Figure 22.5b, there will be two buﬀers: one at the producer node to collect tuples as they are produced, and another at the consumer node to collect them as they are sent across the network.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="322" height="27" alt="image" src="Image_3196.png"/></span></p><p class="s491" style="text-indent: 0pt;line-height: 9pt;text-align: left;">producer</p><p style="text-indent: 0pt;text-align: left;"/><p class="s491" style="text-indent: 0pt;line-height: 9pt;text-align: left;">consumer</p><p style="text-indent: 0pt;text-align: left;"/><p class="s491" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: center;">buﬀer</p><p class="s491" style="padding-top: 6pt;padding-left: 192pt;text-indent: 0pt;text-align: left;">(a) Producer-consumer in shared memory</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="322" height="129" alt="image" src="Image_3197.png"/></span></p><p class="s491" style="text-indent: 0pt;line-height: 9pt;text-align: left;">consumer</p><p style="text-indent: 0pt;text-align: left;"/><p class="s491" style="padding-left: 3pt;text-indent: 0pt;line-height: 9pt;text-align: left;">buﬀer</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s491" style="text-indent: 0pt;line-height: 10pt;text-align: left;">network</p><p style="text-indent: 0pt;text-align: left;"/><p class="s491" style="text-indent: 0pt;line-height: 9pt;text-align: left;">producer</p><p style="text-indent: 0pt;text-align: left;"/><p class="s491" style="padding-top: 2pt;padding-left: 57pt;text-indent: 0pt;text-align: center;">buﬀer</p><p class="s491" style="padding-top: 5pt;padding-left: 192pt;text-indent: 0pt;text-align: left;">(b) Producer-consumer across a network</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 59pt;text-indent: 0pt;text-align: center;">Figure 22.5 <span class="s74">Producer and consumer with buffer.</span></p><p style="padding-top: 8pt;padding-left: 88pt;text-indent: 17pt;text-align: right;">When sending tuples across a network, it makes sense to collect multiple tuples and send them as a single <i>batch</i>, rather than send tuples one at a time, since there is usually a very signiﬁcant overhead per message. Batching greatly reduces this overhead. If the producer and consumer are on the same node and can communicate via a shared memory buﬀer, mutual exclusion needs to be ensured when inserting tuples into, or fetching tuples from, the buﬀer. Mutual exclusion protocols have some overhead, which can be reduced by inserting/retrieving a batch of tuples at a time, instead of one</p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">tuple at a time.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Note that with the pull model, either the producer or the consumer, but not both, can be executing at a given time; while this avoids the contention on the shared buﬀer that arises with the use of the push model, it also prevents the producer and consumer from running concurrently.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">22.5.1.2 Independent Parallelism</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">Operations in a query expression that do not depend on one another can be executed in parallel. This form of parallelism is called <span class="s63">independent parallelism</span>.</p><p class="s13" style="padding-left: 88pt;text-indent: 17pt;line-height: 81%;text-align: justify;"><span class="p">Consider the join </span>r<span class="s93">1 </span><span class="s86">⋈ </span>r<span class="s93">2 </span><span class="s86">⋈ </span>r<span class="s93">3 </span><span class="s86">⋈ </span>r<span class="s93">4</span><span class="s94">. One possible plan is to compute intermedi- ate result </span>t<span class="s93">1 </span><span class="s86">← </span>r<span class="s93">1 </span><span class="s86">⋈ </span>r<span class="s93">2 </span><span class="s94">in parallel with intermediate result </span>t<span class="s93">2 </span><span class="s86">← </span>r<span class="s93">3 </span><span class="s86">⋈ </span>r<span class="s93">4</span><span class="s94">. Neither of these computations depends on each other, and hence they can be parallelized by inde-</span></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">pendent parallelism. In other words, the execution of these two joins can be scheduled in parallel.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 137pt;text-indent: 0pt;text-align: justify;">When these two computations complete, we can compute:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 84pt;text-indent: 0pt;text-align: center;">t<span class="s93">1 </span><span class="s86">⋈ </span>t<span class="s93">2</span></p><p style="padding-top: 10pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Note that computation of the above join depends on the results of the ﬁrst two joins, hence it cannot be done using independent parallelism.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Like pipelined parallelism, independent parallelism does not provide a high degree of parallelism and is less useful in a highly parallel system, although it is useful with a lower degree of parallelism.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">22.5.2 The Exchange Operator Model</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">The Volcano parallel database popularized a model of parallelization called the <i>exchange-operator </i>model. The exchange operation repartitions data in a speciﬁed way; data interchange between nodes is done only by the exchange operator. All other oper- ations work on local data, just as they would in a centralized database system; the data may be available locally either because it is already present, or because of the execution of a preceding exchange operator.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The <span class="s63">exchange operator </span>has two components: a scheme for <i>partitioning </i>outgoing data, applied at each source node, and a scheme for <i>merging </i>incoming data, applied at each destination node. The operator is shown pictorially in Figure 22.6, with the parti- tioning scheme denoted as “Partition,” and the merging scheme denoted as “Merge.”</p><p style="padding-left: 137pt;text-indent: 0pt;text-align: justify;">The exchange operator can <i>partition </i>data in one of several ways:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 128pt;text-indent: 0pt;text-align: left;">1. <span class="p">By hash partitioning on a speciﬁed set of attributes.</span></p><p class="s63" style="padding-top: 6pt;padding-left: 128pt;text-indent: 0pt;text-align: left;">2. <span class="p">By range partitioning on a speciﬁed set of attributes.</span></p><p class="s63" style="padding-top: 6pt;padding-left: 128pt;text-indent: 0pt;text-align: left;">3. <span class="p">By replicating the input data at all nodes, referred to as </span>broadcasting<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="27" height="56" alt="image" src="Image_3198.png"/></span></p><p class="s200" style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;text-align: center;">r<span class="s492">1</span></p><p class="s200" style="padding-top: 9pt;padding-left: 6pt;text-indent: 0pt;text-align: center;">r<span class="s493">2</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="27" height="56" alt="image" src="Image_3199.png"/></span></p><p class="s494" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">r<span class="s495">1</span><span class="s90">‘</span></p><p class="s200" style="padding-top: 9pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">r<span class="s492">2</span><span class="s90">‘</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-top: 6pt;padding-left: 128pt;text-indent: 0pt;text-align: left;">4. <span class="p">By sending all data to a single node.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s496" style="padding-left: 197pt;text-indent: 0pt;text-align: left;">	<span class="s465">	</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s90" style="text-indent: 0pt;text-align: left;">‘</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="27" height="26" alt="image" src="Image_3200.png"/></span></p><p class="s200" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: center;">r<span class="s492">3</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="27" height="58" alt="image" src="Image_3201.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s200" style="padding-left: 6pt;text-indent: 0pt;text-align: left;">r<span class="s497">n</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="3" height="28" alt="image" src="Image_3202.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="3" height="28" alt="image" src="Image_3203.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="27" height="26" alt="image" src="Image_3204.png"/></span></p><p class="s494" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">r<span class="s498">3</span><span class="s90">‘</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="27" height="58" alt="image" src="Image_3205.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s200" style="padding-left: 6pt;text-indent: 0pt;text-align: left;">r<span class="s499">m</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="123" height="130" alt="image" src="Image_3206.png"/></span></p><p class="s469" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Merge</p><p style="text-indent: 0pt;text-align: left;"/><p class="s469" style="padding-top: 2pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">Partition</p><p style="text-indent: 0pt;text-align: left;"/><p class="s469" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Merge</p><p style="text-indent: 0pt;text-align: left;"/><p class="s469" style="padding-top: 2pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">Partition</p><p style="text-indent: 0pt;text-align: left;"/><p class="s469" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Merge</p><p style="text-indent: 0pt;text-align: left;"/><p class="s469" style="padding-top: 2pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">Partition</p><p style="text-indent: 0pt;text-align: left;"/><p class="s469" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Merge</p><p style="text-indent: 0pt;text-align: left;"/><p class="s469" style="padding-top: 3pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">Partition</p><p style="text-indent: 0pt;text-align: left;"/><p class="s73" style="padding-top: 4pt;padding-left: 173pt;text-indent: 0pt;text-align: left;">Figure 22.6 <span class="s74">The exchange operator used for repartitioning.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: right;">Broadcasting data to all nodes is required for operations such as the asymmetric fragment-and-replicate join. Sending all data to a single node is usually done as a ﬁ- nal step of parallel query processing, to get partitioned results together at a single site.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Note also that the input to the exchange operator can be at a single site (referred to as <span class="s63">unpartitioned</span>), or already partitioned across multiple sites. Repartitioning of already partitioned data results in each destination node receiving data from multiple source nodes, as shown in Figure 22.6.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Each destination node <i>merges </i>the data items received from the source nodes. This merge step can store data in the order received (which may be nondeterministic, since it depends on the speeds of the machines and unpredictable network delays); such merging is called <span class="s63">random merge</span>.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">On the other hand, if the input data from each source is sorted, the merge step can exploit the sort order by performing an <span class="s63">ordered merge</span>. Suppose, for example, nodes <i>N</i><span class="s98">1</span>, <span class="s15">… </span>, <i>N</i><span class="s145">m </span>ﬁrst sort a relation locally, and then repartition the sorted relation using range partitioning. Each node performs an ordered merge operation on the tuples that it receives, to generate a sorted output locally.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Thus, the exchange operator performs the partitioning of data at the source nodes, as well the merging of data at the destination nodes.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">All the parallel operator implementations we have seen so far can be modeled as a sequence of exchange operations, and local operators, at each node, that are completely unaware of parallelism.</p><p class="s39" style="padding-top: 7pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Range partitioning sort: can be implemented by an exchange operation that per- forms range partitioning, with random merge at the destination nodes, followed by a local sort operation at each destination node.</span></p><p class="s39" style="padding-top: 4pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Parallel external sort-merge: can be implemented by local sorting at the source nodes, followed by an exchange operation that performs range partitioning, along with ordered merging.</span></p><p class="s39" style="padding-top: 4pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Partitioned join: can be implemented by an exchange operation that performs the desired partitioning, followed by local join at each node.</span></p><p class="s39" style="padding-top: 4pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Asymmetric fragment-and-replicate join: can be implemented by an exchange op- eration that performs broadcast “partitioning” of the smaller relation, followed by a local join at each node.</span></p><p class="s39" style="padding-top: 4pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Symmetric fragment-and-replicate join: can be implemented by an exchange oper- ation that partitions, and partially broadcasts each partition, followed by a local join at each node.</span></p><p class="s39" style="padding-top: 4pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Aggregation: can be implemented by an exchange operation that performs hash- partitioning on the grouping attributes, followed by a local aggregation operation at each node. The partial-aggregation optimization simply requires an extra local aggregation operation at each node, before the exchange operation.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Other relational operations can be implemented similarly, by a sequence of local oper- ations running in parallel at each node, interspersed with exchange operations.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">As noted earlier, parallel execution where data are partitioned, and operations are executed locally at each node, is referred to as <i>data parallelism</i>. The use of the exchange operator model to implement data parallel execution has the major beneﬁt of allowing existing database query engines to be used at each of the local nodes, without any signiﬁcant code changes. As a result, the exchange-operator model of parallel execution is widely used in parallel database systems.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">There are, however, some operator implementations that can beneﬁt from being aware of the parallel nature of the system they are running on. For example, an indexed nested-loops join where the inner relation is indexed on a parallel data-store would require remote access for each index lookup; the index lookup operation is thus aware of the parallel nature of the underlying system. Similarly, in a shared-memory system it may make sense to have a hash table or index in shared-memory, which is accessed by multiple processors (this approach is discussed brieﬂy in Section 22.6); the operations running on each processor are then aware of the parallel nature of the system.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">As we discussed in Section 22.5.1.1, while the demand-driven (or pull) iterator model for pipelined execution of operators is widely used in centralized database en- gines, the push model is preferred for parallel execution of operators in a pipeline.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The exchange operator can be used to implement the push model between nodes in a parallel system, while allowing existing implementations of local operators to run using the pull model. To do so, at each source node of an exchange operator, the op- erator can pull multiple tuples from its input and create a batch of tuples destined for each destination node. The input may be computed by a local operation, whose imple- mentation can use the demand-driven iterator model.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The exchange operator then sends batches of tuples to the destination nodes, where they are merged and kept in a buﬀer. The local operations can then consume the tuples in a demand-driven manner.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 7pt;padding-left: 119pt;text-indent: 0pt;text-align: left;">22.5.3 Putting It All Together</p><p class="s109" style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;"><span class="p">Figure 22.7 shows a query, along with a sequential and two alternative parallel query execution plans. The query, shown in Figure 22.7a, computes a join of two relations, </span><span class="s13">r </span><span class="p">and </span><span class="s13">s</span><span class="p">, and then computes an aggregate on the join result. Assume for concreteness </span><span class="s122">that the query is </span>r<span class="s126">.</span>C<span class="s119">,</span>s<span class="s126">.</span>D<span class="s117">γ</span><span class="s119">sum(</span>s<span class="s126">.</span>E<span class="s119">)</span><span class="s120">(</span><span class="s121">r </span><span class="s138">⋈</span>r<span class="s126">.</span>A<span class="s118">=</span>s<span class="s126">.</span>B <span class="s139">s</span><span class="s122">).</span></p><p style="padding-left: 137pt;text-indent: 0pt;line-height: 7pt;text-align: justify;">The sequential plan, shown in Figure 22.7b, uses a hash join (denoted as “HJ” in</p><p class="s13" style="padding-top: 2pt;padding-left: 119pt;text-indent: 0pt;line-height: 76%;text-align: justify;"><span class="p">the ﬁgure), which executes in three separate stages. The ﬁrst stage partitions the ﬁrst input (</span>r<span class="p">) locally on </span>r<span class="s83">.</span>A<span class="p">; the second stage partitions the second input (</span>s<span class="p">) locally on </span>s<span class="s83">.</span>B<span class="p">; and the third stage computes the join of each of the corresponding partitions of </span>r <span class="p">and </span>s<span class="p">.</span></p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">The aggregate is computed using in-memory hash-aggregation, denoted by the operator HA; we assume that the number of groups is small enough that the hash table ﬁts in memory.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s174" style="padding-left: 102pt;text-indent: 0pt;text-align: left;">	</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="170" height="78" alt="image" src="Image_3207.png"/></span></p><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">r</p><p style="text-indent: 0pt;text-align: left;"/><p class="s384" style="text-indent: 0pt;text-align: left;"></p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">s</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="185" height="87" alt="image" src="Image_3208.png"/></span></p><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">HJ</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">HA</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 107%;text-align: left;">Loc. Part.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">s</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 107%;text-align: left;">Loc. Part.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">r</p><p style="text-indent: 0pt;text-align: left;"/><p class="s230" style="padding-top: 3pt;padding-left: 146pt;text-indent: 0pt;text-align: left;">(a) Logical Query               (b) Sequential Plan</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="389" height="91" alt="image" src="Image_3209.png"/></span></p><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">r</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;text-align: left;">E<span class="s484">1</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 107%;text-align: left;">Loc. Part.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">HJ</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;text-align: left;">E<span class="s484">3</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">HA</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;text-align: left;">E<span class="s484">4</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">Result</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">s</p><p style="text-indent: 0pt;text-align: left;"/><p class="s500" style="padding-left: 26pt;text-indent: -27pt;line-height: 59%;text-align: left;">E<span class="s501">2 </span><span class="s111">Loc. Part.</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">E : <span class="s230">partition on </span>r.A</p><p style="text-indent: 0pt;text-align: left;"/><p class="s173" style="text-indent: 0pt;line-height: 8pt;text-align: left;">1</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">E : <span class="s230">partition on </span>s.B</p><p style="text-indent: 0pt;text-align: left;"/><p class="s173" style="text-indent: 0pt;line-height: 8pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="padding-top: 1pt;padding-left: 205pt;text-indent: 0pt;text-align: left;">E<span class="s502">3 </span>: <span class="s230">partition on (</span>r.C,s.D<span class="s230">)  </span>E<span class="s502">4 </span>: <span class="s230">collect results</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s230" style="padding-top: 4pt;padding-left: 230pt;text-indent: 0pt;text-align: left;">(c) Parallel Plan</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="408" height="94" alt="image" src="Image_3210.png"/></span></p><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">r</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;text-align: left;">E<span class="s484">1</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 107%;text-align: left;">Loc. Part.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;text-align: left;">HJ  <span class="s396">HA</span><span class="s484">1</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;text-align: left;">E<span class="s484">3</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 94%;text-align: left;">HA<span class="s503">2</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;text-align: left;">E<span class="s484">4</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">Result</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">s</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;text-align: left;">E<span class="s484">2</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 107%;text-align: left;">Loc. Part.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">E <span class="s230">: partition on </span>r.A</p><p style="text-indent: 0pt;text-align: left;"/><p class="s483" style="text-indent: 0pt;line-height: 7pt;text-align: left;">1</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 9pt;text-align: left;">E <span class="s230">: partition on </span>s.B</p><p style="text-indent: 0pt;text-align: left;"/><p class="s483" style="text-indent: 0pt;line-height: 7pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="padding-top: 1pt;padding-left: 212pt;text-indent: 0pt;text-align: left;">E<span class="s484">3 </span><span class="s504">: partition on (</span>r.C,s.D<span class="s230">)  </span>E<span class="s484">4 </span><span class="s504">: collect results</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s230" style="padding-left: 189pt;text-indent: 0pt;text-align: left;">(d) Parallel Plan with Partial Aggregation</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 59pt;text-indent: 0pt;text-align: center;">Figure 22.7 <span class="s74">Parallel query execution plans.</span></p><p style="padding-top: 8pt;padding-left: 88pt;text-indent: 17pt;text-align: justify;">The dashed boxes in the ﬁgure show which steps run in a pipelined fashion. In the sequential plan, the read of the relation <i>r </i>is pipelined to the ﬁrst partitioning stage of the sequential hash join; similarly, the read of relations <i>s </i>is pipelined to the second partitioning stage of the hash join. The third stage of the hash join pipelines its output tuples to the hash aggregation operator.</p><p style="padding-top: 1pt;padding-left: 88pt;text-indent: 17pt;line-height: 92%;text-align: justify;">The parallel query evaluation plan, shown in Figure 22.7c, starts with <i>r </i>and <i>s </i>al- ready partitioned, but not on the required join attributes.<span class="s76">3</span> The plan, therefore, uses the exchange operation <i>E</i><span class="s98">1</span> to repartition <i>r </i>using attribute <i>r</i><span class="s83">.</span><i>A</i>; similarly, exchange operator</p><p class="s13" style="padding-left: 88pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">E<span class="s98">2</span><span class="p"> repartitions </span>s <span class="p">using </span>s<span class="s83">.</span>B<span class="p">. Each node then uses hash join locally to compute the join</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="95" height="1" alt="image" src="Image_3211.png"/></span></p><p class="s77" style="padding-top: 3pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">3<span class="s78">Note the multiple boxes indicating a relation is stored in multiple nodes; similarly, multiple circles indicate that an operation is executed in parallel on multiple nodes.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">of its partition of <i>r </i>and <i>s</i>. Note that these partitions are not assumed to ﬁt in memory, so they must be further partitioned by the ﬁrst two stages of hash join; this local par- titioning step is denoted as “Loc. Part.” in the ﬁgure. The dashed boxes indicate that the output of the exchange operator can be pipelined to the local partitioning step. As in the sequential plan, there are two pipelined stages, one each for <i>r </i>and <i>s</i>. Note that exchange of tuples across nodes is done only by the exchange operator, and all other edges denote tuple ﬂows within each node.</p><p class="s13" style="padding-top: 1pt;padding-left: 119pt;text-indent: 17pt;line-height: 92%;text-align: justify;"><span class="p">Subsequently, the hash join algorithm is executed in parallel at all participating nodes, and its output pipelined to the exchange operator </span>E<span class="s98">3</span><span class="p">. This exchange operator repartitions its input on the pair of attributes (</span>r<span class="s83">.</span>C<span class="p">, </span>s<span class="s83">.</span>D<span class="p">), which are the grouping at-</span></p><p style="padding-left: 119pt;text-indent: 0pt;line-height: 8pt;text-align: justify;">tributes of the subsequent aggregation. At the receiving end of the exchange operator,</p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">tuples are pipelined to the hash aggregation operator. Note that the above steps all run together as a single pipelined stage, even though there is an exchange operator as part of the stage. Note that the local operators computing hash join and hash aggregate need not be aware of the parallel execution.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The results of the aggregates are then collected together at a central location by the ﬁnal exchange operator <i>E</i><span class="s98">4</span>, to create the ﬁnal result relation.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Figure 22.7d shows an alternative plan that performs partial aggregation on the results of the hash join, before partitioning the results. The partial aggregation is com- puted locally at each node by the operator <i>HA</i><span class="s98">1</span>. Since no tuple is output by the partial aggregation operator until all its input is consumed, the pipelined stage contains only the local hash join and hash aggregation operators. The subsequent exchange operator</p><p class="s13" style="padding-top: 1pt;padding-left: 119pt;text-indent: 0pt;line-height: 70%;text-align: justify;">E<span class="s98">3</span><span class="p"> which partitions its input on (</span>r<span class="s83">.</span>C<span class="p">, </span>s<span class="s83">.</span>D<span class="p">) is part of a subsequent pipelined stage along with the hash aggregation operation </span>HA<span class="s98">2</span><span class="p"> which computes the ﬁnal aggregate values.</span></p><p style="padding-top: 1pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">As before, the exchange operator <i>E</i><span class="s98">4</span> collects the results at a centralized location.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The above example shows how pipelined execution can be performed across nodes, as well as within nodes, and further how it can be done along with intra-operator parallel execution. The example also shows that some pipelined stages depend on the output of earlier pipelined stages; therefore their execution can start only after the previous step ﬁnishes. On the other hand, the initial exchange and partitioning of <i>r </i>and <i>s </i>occur in pipelined stages that are independent of each other; such independent stages can be scheduled concurrently, that is, at the same time, if desired.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">To execute a parallel plan such as the one in our example, the diﬀerent pipelined stages have to be scheduled for execution, in an order that ensures inter-stage depen- dencies are met. When executing a particular stage, the system must decide how many nodes an operation should be executed on. These decisions are usually made as part of the scheduling phase, before query execution starts.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">22.5.4 Fault Tolerance in Query Plans</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">Parallel processing of queries across a moderate number of nodes, for example, hun- dreds of nodes, can be done without worrying about fault tolerance. If a fault occurs, the query is rerun, after removing any failed nodes from the system (replication of data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">at the storage layer ensures that data continues to be available even in the event of a failure). However, this simple solution does not work well when operating at the scale of thousands or tens of thousands of nodes: if a query runs for several hours, there is a signiﬁcant chance that there will be a failure while the query is being executed. If the query is restarted, there is a signiﬁcant chance of another failure while it is executing, which is obviously an undesirable situation.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">To deal with this problem, the query processing system should ideally just be able to redo the actions of a failed node, without redoing the rest of the computation.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Implementations of MapReduce that are designed to work at a massively parallel scale can be made fault tolerant as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 97pt;text-indent: 0pt;text-align: justify;"><span class="s63">1. </span>Each <span class="s49">map </span>operation executed at each node writes its output to local ﬁles.</p><p style="padding-top: 6pt;padding-left: 113pt;text-indent: -17pt;text-align: justify;"><span class="s63">2. </span>The next operation, which is a <span class="s49">reduce </span>operation, executes at each node; the op- eration execution at a node reads data from the ﬁles stored at multiple nodes, collects the data, and starts processing the data only after it has got all its re- quired data.</p><p style="padding-top: 6pt;padding-left: 113pt;text-indent: -17pt;text-align: justify;"><span class="s63">3. </span>The <span class="s49">reduce </span>operation writes its output to a distributed ﬁle system (or distributed storage system) that replicates data, so that the data would be available even in the event of a failure.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: right;">Let us now examine the reason why things are done as above. First, if a particular <span class="s49">map </span>node fails, the work done at that node can be redone at a backup node; the work done at other <span class="s49">map </span>nodes is not aﬀected. Work is not carried out by <span class="s49">reduce </span>nodes until all the required data has been fetched; the failure of a <span class="s49">map </span>node just means the <span class="s49">reduce </span>nodes fetch data from the backup <span class="s49">map </span>nodes. There is certainly a delay while the backup node does its work, but there is no need to repeat the entire computation. Further, once a <span class="s49">reduce </span>node has ﬁnished its work, its output goes to replicated storage to ensure it is not lost even if a data storage node fails. This means that if a <span class="s49">reduce </span>node fails before it completes its work, it will have to be reexecuted at a backup node; other <span class="s49">reduce </span>nodes are not aﬀected. Once a <span class="s49">reduce </span>node has ﬁnished its work,</p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">there is no need to reexecute it.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Note that it is possible to store the output of a <span class="s49">map </span>node in a replicated storage system. However, this increases the execution cost signiﬁcantly, and hence <span class="s49">map </span>output is stored in local storage, even at the risk of having to reexecute the work done by a <span class="s49">map </span>node in case it fails before all the <span class="s49">reduce </span>nodes have fetched the data that they require from that <span class="s49">map </span>node.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">It is also worth noting that sometimes nodes do not completely fail, but run very slowly; such nodes are called <i>straggler </i>nodes. Even a single straggler node can delay all the nodes in the next step (if there is a following step), or delay task completion (if it is in the last step). Straggler nodes can be dealt with by treating them similar to failed nodes, and reexecuting their tasks on other nodes (the original task on the straggler node can also be allowed to continue, in case it ﬁnishes ﬁrst). Such reexecution to deal with</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;"><a name="bookmark477">stragglers has been found to signiﬁcantly improve time to completion of MapReduce tasks.</a><a name="bookmark522">&zwnj;</a></p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">While the above scheme for fault tolerance is quite eﬀective, there is an overhead that must be noted: a <span class="s49">reduce </span>stage cannot perform any work until the previous <span class="s49">map </span>stage has ﬁnished;<span class="s76">4</span> and if multiple <span class="s49">map </span>and <span class="s49">reduce </span>steps are executed, the next <span class="s49">map </span>stage cannot perform any work until the preceding <span class="s49">reduce </span>stage has ﬁnished. In par- ticular, this means that pipelining of data between stages cannot be supported; data are always materialized before it is sent to the next stage. Materialization carries a signiﬁcant overhead, which can slow down computation.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Apache Spark uses an abstraction called <i>Resilient Distributed Datasets </i>(<span class="s44">RDD</span>s) to implement fault tolerance. As we have seen in Section 10.4.2, <span class="s44">RDD</span>s can be viewed as collections, and Spark supports algebraic operations that take <span class="s44">RDD</span>s as input, and generate <span class="s44">RDD</span>s as output. Spark keeps track of the operations used to create an <span class="s44">RDD</span>. In case of failures that result in loss of an <span class="s44">RDD</span>, the operations used to create the <span class="s44">RDD </span>can be reexecuted to regenerate the <span class="s44">RDD</span>. However, this may be time-consuming, so Spark also supports replication to reduce the chance of data loss, as well as storing of local copies of data when a shuﬄe (exchange) step is executed, to allow reexecution to be restricted to computation that was performed on failed nodes.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">There has been a good deal of research on how to allow pipelining of data, while not requiring query execution to restart from the beginning in case of a single failure. Such schemes typically require nodes to track what data they have received from each source node. In the event of a source node failure, the work of the source node is redone on a backup node, which can result in some tuples that were received earlier being received again. Tracking the data received earlier is important to ensure duplicate tuples are detected and eliminated by the receiving node. The above ideas can also be used to implement fault tolerance for other algebraic operations, such as joins. In particular, if we use the exchange operator with data parallelism, fault tolerance can be implemented as an extension of the exchange operator.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">References to more information on fault tolerant pipelining based on extensions of the exchange operator, as well as on fault tolerance schemes used in MapReduce and in Apache Spark, may be found in the Further Reading section at the end of the chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part403.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part405.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
