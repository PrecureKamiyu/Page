<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML" lang="EN" xml:lang="en"><head><meta content="text/html; charset=utf-8" http-equiv="default-style"/><title>The Art of Multiprocessor Programming</title><link href="Elsevier_eBook.css" rel="stylesheet" type="text/css"/><link href="math.css" rel="stylesheet" type="text/css"/><link href="media.css" media="only screen" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4f1c4a5b-a3e2-48ff-98f3-ff17812cd57a" name="Adept.expected.resource"/></head><body><section epub:type="chapter" role="doc-chapter"><div aria-label="Page 431" epub:type="pagebreak" id="page_431" role="doc-pagebreak"/><div id="CN"><a id="c0010tit1"/></div><header><hgroup><h1 class="chaptitle" id="c0010tit">Chapter 18: Barriers</h1></hgroup><section epub:type="preamble"><div class="abstract"><h2 class="h1hd" id="ab0010"><a id="st0010"/>Abstract</h2><p class="abspara">Many parallel algorithms execute in phases such that all threads must complete each phase before any thread moves on to the next phase. Other algorithms, such as work stealing, must be able to detect when all of the threads have run out of work. This kind of synchronization is called a barrier. This chapter introduces barriers, discusses the basic philosophy behind them, and then explores several implementations that vary in complexity, reusability and efficiency on different kinds of architectures.</p></div></section><section id="ks0010"><h3 class="h2hd" id="st0015">Keywords</h3><p class="keywords">barriers; synchronization; program-wide coordination; termination detection; phases</p></section></header><section><h2 class="h1hd" id="s0010"><a id="st0020"/>18.1 Introduction</h2><p class="textfl" id="p0010">Imagine you are writing the graphical display for a computer game. Your program prepares a sequence of <i>frames</i> to be displayed by a graphics package (perhaps a hardware coprocessor). This kind of program is sometimes called a <i>soft real-time</i> application: real-time because it must display at least 35 frames per second to be effective, and soft because occasional failure is not catastrophic. On a single-thread machine, you might write a loop like this:</p><div class="pageavoid"><figure class="fig" id="f0010"><img alt="Image" class="img" height="60" src="images/B9780124159501000288/fx001.jpg" width="106"/></figure></div><p class="textfl"> If, instead, you have <i>n</i> parallel threads available, then it makes sense to split the frame into <i>n</i> disjoint parts, and have each thread prepare its part in parallel with the others.</p><div class="pageavoid"><figure class="fig" id="f0015"><img alt="Image" class="img" height="77" src="images/B9780124159501000288/fx002.jpg" width="143"/></figure></div><p class="textfl"> The problem with this approach is that different threads require different amounts of time to prepare and display their portions of the frame. Some threads might start displaying the <i>i</i>th frame before others have finished the <span class="hiddenClass"><mml:math><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000288/si1.png" style="vertical-align:middle" width="45"/></span>st.</p><p class="text" id="p0015">To avoid such synchronization problems, we can organize computations such as this as a sequence of <i>phases</i>, where no thread should start the <i>i</i>th phase until the others have finished the <span class="hiddenClass"><mml:math><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000288/si1.png" style="vertical-align:middle" width="45"/></span>st. We have seen this phased computation pattern before: In Chapter <a href="B9780124159501000227.xhtml">12</a>, the sorting network algorithms required each comparison phase to be separate from the others. Similarly, in the sample sorting algorithm, each phase had to make sure that prior phases had completed before proceeding.</p><p class="text" id="p0020">The mechanism for enforcing this kind of synchronization is called a <i>barrier</i>; its interface is shown in <a href="#f0020" id="cf0010">Fig. 18.1</a>. A barrier is a way of forcing asynchronous threads to act almost as if they were synchronous. When a thread finishing phase <i>i</i> calls the barrier's <img alt="Image" height="9" src="images/B9780124159501000288/fx004.jpg" width="32"/>() method, it is blocked until all <i>n</i> threads have also finished that phase. <a href="#f0025" id="cf0015">Fig. 18.2</a> shows how one could use a barrier to make the parallel rendering program work correctly. After preparing <span aria-label="Page 432" epub:type="pagebreak" id="page_432" role="doc-pagebreak"/>frame <i>i</i>, all threads synchronize at a barrier before starting to display that frame. This structure ensures that all threads concurrently displaying a frame display the same frame.</p><div class="pageavoid"><figure class="fig" id="f0020"><img alt="Image" height="43" src="images/B9780124159501000288/gr001.jpg" width="174"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.1</span> The <img alt="Image" height="9" src="images/B9780124159501000288/fx003.jpg" width="46"/> interface.</div></figcaption></figure></div><div class="pageavoid"><figure class="fig" id="f0025"><img alt="Image" height="109" src="images/B9780124159501000288/gr002.jpg" width="159"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.2</span> Using a barrier to synchronize concurrent displays.</div></figcaption></figure></div><p class="text" id="p0025">Barrier implementations raise many of the same performance issues as spin locks in Chapter <a href="B9780124159501000173.xhtml">7</a>, as well as some new issues. Clearly, barriers should be fast, in the sense that we want to minimize the duration between when the last thread reaches the barrier and when the last thread leaves the barrier. It is also important that threads leave the barrier at roughly the same time. A thread's <i>notification time</i> is the interval between when some thread has detected that all threads have reached the barrier, and when that specific thread leaves the barrier. Having uniform notification times is important for many soft real-time applications. For example, picture quality is enhanced if all portions of the frame are updated at more-or-less the same time.</p></section><section><h2 class="h1hd" id="s0015"><a id="st0025"/>18.2 Barrier implementations</h2><p class="textfl" id="p0030"><a href="#f0030" id="cf0020">Fig. 18.3</a> shows the <img alt="Image" height="11" src="images/B9780124159501000288/fx005.jpg" width="86"/> class, which creates an <img alt="Image" height="11" src="images/B9780124159501000288/fx006.jpg" width="87"/> counter initialized to <i>n</i>, the barrier size. Each thread applies <img alt="Image" height="11" src="images/B9780124159501000288/fx007.jpg" width="99"/>() to lower the counter. If the call returns 1 (line 10), then that thread is the last to reach the barrier, so it resets the counter for the next use (line 11). Otherwise, the thread spins on the counter, waiting for the value to fall to zero (line 13). This barrier class may look like it works, but it does not.</p><div class="pageavoid"><figure class="fig" id="f0030"><img alt="Image" height="257" src="images/B9780124159501000288/gr003.jpg" width="384"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.3</span> An incorrect implementation of the <img alt="Image" height="11" src="images/B9780124159501000288/fx005.jpg" width="86"/> class.</div></figcaption></figure></div><p class="text" id="p0035">Unfortunately, the attempt to make the barrier reusable breaks it. Suppose there are only two threads. Thread <i>A</i> applies <img alt="Image" height="11" src="images/B9780124159501000288/fx007.jpg" width="99"/>() to the counter, discovers it is not the last thread to reach the barrier, and spins waiting for the counter value to reach 0. When <i>B</i> arrives, it discovers it is the last thread to arrive, so it resets the counter to <i>n</i>, in this case 2. It finishes <span aria-label="Page 433" epub:type="pagebreak" id="page_433" role="doc-pagebreak"/>the next phase and calls <img alt="Image" height="9" src="images/B9780124159501000288/fx004.jpg" width="32"/>(). Meanwhile, <i>A</i> continues to spin; it never saw the counter reach 0. Eventually, <i>A</i> is waiting for phase 0 to finish, while <i>B</i> is waiting for phase 1 to finish, and the two threads starve.</p><p class="text" id="p0040">Perhaps the simplest way to fix this problem is to alternate between two barriers, using one for even-numbered phases and another for odd-numbered ones. However, such an approach wastes space, and requires too much bookkeeping from applications.</p></section><section><h2 class="h1hd" id="s0020"><a id="st0030"/>18.3 Sense reversing barrier</h2><p class="textfl" id="p0045">A <i>sense reversing</i> barrier is an elegant and practical solution to the problem of reusing barriers. As shown in <a href="#f0035" id="cf0025">Fig. 18.4</a>, a phase's <i>sense</i> is a Boolean value: <i>true</i> for even-numbered phases and <i>false</i> otherwise. Each <img alt="Image" height="9" src="images/B9780124159501000288/fx008.jpg" width="79"/> object has a Boolean <img alt="Image" height="6" src="images/B9780124159501000288/fx009.jpg" width="32"/> field indicating the sense of the currently executing phase. Each thread keeps its current sense as a thread-local object (Pragma <a href="#enun0010" id="cf0030">18.3.1</a>). Initially the barrier's <img alt="Image" height="6" src="images/B9780124159501000288/fx009.jpg" width="32"/> is the complement of the local sense of all the threads. When a thread calls <img alt="Image" height="9" src="images/B9780124159501000288/fx004.jpg" width="32"/>(), it checks whether it is the last thread to decrement the counter. If so, it reverses the barrier's sense and continues. Otherwise, it spins waiting for the barrier's <img alt="Image" height="6" src="images/B9780124159501000288/fx009.jpg" width="32"/> field to change to match its own local sense.</p><div class="pageavoid"><figure class="fig" id="f0035"><img alt="Image" height="307" src="images/B9780124159501000288/gr004.jpg" width="356"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.4</span> The <img alt="Image" height="9" src="images/B9780124159501000288/fx008.jpg" width="79"/> class: a sense reversing barrier.</div></figcaption></figure></div><p class="text" id="p0050">Decrementing the shared counter may cause memory contention, since all the threads are trying to access the counter at about the same time. Once the counter has been decremented, each thread spins on the <img alt="Image" height="6" src="images/B9780124159501000288/fx009.jpg" width="32"/> field. This implementation is well suited for cache-coherent architectures, since threads spin on locally cached copies of the field, and the field is modified <span aria-label="Page 434" epub:type="pagebreak" id="page_434" role="doc-pagebreak"/>only when threads are ready to leave the barrier. The <img alt="Image" height="6" src="images/B9780124159501000288/fx009.jpg" width="32"/> field is an excellent way of maintaining a uniform notification time on symmetric cache-coherent multiprocessors.</p><p class="text" id="p0055"/><div class="boxg1" id="enun0010"><p class="b1num">Pragma 18.3.1 </p><div><p class="b1textfl" id="p0060">The constructor code for the sense reversing barrier, shown in <a href="#f0035" id="cf0035">Fig. 18.4</a>, is mostly straightforward. The one exception occurs on lines 5 and 6, where we initialize the thread-local <img alt="Image" height="9" src="images/B9780124159501000288/fx010.jpg" width="72"/> field. This somewhat complicated syntax defines a thread-local Boolean value whose initial value is the complement of the <img alt="Image" height="6" src="images/B9780124159501000288/fx009.jpg" width="32"/> field's initial value. See Appendix <a href="B9780124159501000318.xhtml">A.2.4</a> for a more complete explanation of thread-local objects in Java.</p></div></div><p class="textfl"/></section><section><h2 class="h1hd" id="s0025"><a id="st0035"/>18.4 Combining tree barrier</h2><p class="textfl" id="p0065">One way to reduce memory contention (at the cost of increased latency) is to use the combining paradigm of Chapter <a href="B9780124159501000227.xhtml">12</a>. Split a large barrier into a tree of smaller barriers, and have threads combine requests going up the tree and distribute notifications going down the tree. A <i>tree barrier</i> (<a href="#f0040" id="cf0040">Fig. 18.5</a>) is characterized by a <i>size n</i>, the total number of threads, and a <i>radix r</i>, the number of children of each node For convenience, we assume there are exactly <span class="hiddenClass"><mml:math><mml:mi>n</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000288/si2.png" style="vertical-align:middle" width="60"/></span> threads, where <i>d</i> is the depth of the tree.</p><div class="pageavoid"><figure class="fig" id="f0040"><img alt="Image" height="191" src="images/B9780124159501000288/gr005.jpg" width="292"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.5</span> The <img alt="Image" height="9" src="images/B9780124159501000288/fx011.jpg" width="73"/> class: Each thread indexes into an array of leaf nodes and calls that leaf's <img alt="Image" height="9" src="images/B9780124159501000288/fx004.jpg" width="32"/>() method.</div></figcaption></figure></div><p class="text" id="p0070"><span aria-label="Page 435" epub:type="pagebreak" id="page_435" role="doc-pagebreak"/>Specifically, the combining tree barrier is a tree of <i>nodes</i>, where each node has a counter and a sense, just as in the sense reversing barrier. A node's implementation is shown in <a href="#f0045" id="cf0045">Fig. 18.6</a>. Thread <i>i</i> starts at leaf node <span class="hiddenClass"><mml:math><mml:mo stretchy="true">⌊</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="true">⌋</mml:mo></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000288/si3.png" style="vertical-align:middle" width="35"/></span>. The node's <img alt="Image" height="9" src="images/B9780124159501000288/fx004.jpg" width="32"/>() method is similar to the sense reversing barrier's <img alt="Image" height="9" src="images/B9780124159501000288/fx004.jpg" width="32"/>(), the principal difference being that the last thread to arrive, the one that completes the barrier, visits the parent barrier before waking up the other threads. When <i>r</i> threads have arrived at the root, the barrier is complete, and the sense is reversed. As before, thread-local Boolean sense values allow the barrier to be reused without reinitialization.</p><div class="pageavoid"><figure class="fig" id="f0045"><img alt="Image" height="487" src="images/B9780124159501000288/gr006.jpg" width="289"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.6</span> The <img alt="Image" height="9" src="images/B9780124159501000288/fx011.jpg" width="73"/> class: internal tree node.</div></figcaption></figure></div><p class="text" id="p0075"/><div class="boxg1" id="enun0015"><p class="b1num">Pragma 18.4.1 </p><div><p class="b1textfl" id="p0080">Tree nodes are declared as an <i>inner class</i> of the tree barrier class, so nodes are not accessible outside the class. As shown in <a href="#f0050" id="cf0050">Fig. 18.7</a>, the tree is initialized by a recursive <img alt="Image" height="9" src="images/B9780124159501000288/fx012.jpg" width="32"/>() method. The method takes a parent node and a depth. If the depth is nonzero, it creates <i>radix</i> children, and recursively creates the children's children. If the depth is 0, it places each node in a <img alt="Image" height="9" src="images/B9780124159501000288/fx013.jpg" width="25"/>[] array. When a thread enters the barrier, it uses this array to choose a leaf to start from. See Appendix <a href="B9780124159501000318.xhtml">A.2.1</a> for a more complete discussion of inner classes in Java.</p><div class="pageavoid"><figure class="fig" id="f0050"><img alt="Image" height="569" src="images/B9780124159501000288/gr007.jpg" width="356"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.7</span> The <img alt="Image" height="9" src="images/B9780124159501000288/fx011.jpg" width="73"/> class: initializing a combining tree barrier. The <img alt="Image" height="9" src="images/B9780124159501000288/fx012.jpg" width="32"/>() method creates <i>r</i> children for each node, and then recursively creates the children's children. At the bottom, it places leaves in an array.</div></figcaption></figure></div></div></div><p class="textfl"/><p class="text" id="p0085">The tree-structured barrier reduces memory contention by spreading memory accesses across multiple barriers. It may or may not reduce latency, depending on whether it is faster to decrement a single location or to visit a logarithmic number of barriers.</p><p class="text" id="p0090">The root node, once its barrier is complete, lets notifications percolate down the tree. This approach may be good for a NUMA architecture, but it may cause nonuniform notification times. Because threads visit an unpredictable sequence of locations as they move up the tree, this approach may not work well on cacheless NUMA architectures.<span aria-label="Page 436" epub:type="pagebreak" id="page_436" role="doc-pagebreak"/></p></section><section><h2 class="h1hd" id="s0030"><a id="st0040"/>18.5 Static tree barrier</h2><p class="textfl" id="p0095">The barriers seen so far either suffer from contention (the simple and sense reversing barriers) or have excessive communication (the combining tree barrier). In the latter barrier, which threads traverse up the tree is varying and unpredictable, which makes it difficult to lay out the barriers on cacheless NUMA architectures. Surprisingly, there is a simple barrier that allows a static layout and yet has low contention.</p><p class="text" id="p0100">The <i>static tree barrier</i> of <a href="#f0055" id="cf0055">Fig. 18.8</a> works as follows: Each thread is assigned to a node in a tree (<a href="#f0060" id="cf0060">Fig. 18.9</a>). The thread at a node waits until all nodes below it in the tree have finished, and then informs its parent. It then spins waiting for the global sense bit to change. Once the root learns that its children are done, it toggles the global sense bit to notify the waiting threads that all threads are <span aria-label="Page 437" epub:type="pagebreak" id="page_437" role="doc-pagebreak"/>done. On a cache-coherent multiprocessor, completing the barrier requires <span class="hiddenClass"><mml:math><mml:mi mathvariant="normal">log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000288/si4.png" style="vertical-align:middle" width="43"/></span> steps moving up the tree, while notification simply requires changing the global sense, which is propagated by the cache-coherence mechanism. On machines without coherent caches, threads propagate notification down the tree as in the combining barrier we saw earlier.<span aria-label="Page 438" epub:type="pagebreak" id="page_438" role="doc-pagebreak"/></p><div class="pageavoid"><figure class="fig" id="f0055"><img alt="Image" height="619" src="images/B9780124159501000288/gr008.jpg" width="368"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.8</span> The <img alt="Image" height="9" src="images/B9780124159501000288/fx014.jpg" width="113"/> class: Each thread indexes into a statically assigned tree node and calls that node's <img alt="Image" height="9" src="images/B9780124159501000288/fx004.jpg" width="32"/>() method.</div></figcaption></figure></div><div class="pageavoid"><figure class="fig" id="f0060"><img alt="Image" height="323" src="images/B9780124159501000288/gr009.jpg" width="283"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.9</span> The <img alt="Image" height="9" src="images/B9780124159501000288/fx014.jpg" width="113"/> class: internal <img alt="Image" height="9" src="images/B9780124159501000288/fx015.jpg" width="25"/> class.</div></figcaption></figure></div></section><section><h2 class="h1hd" id="s0035"><a id="st0045"/>18.6 Termination detection barriers</h2><p class="textfl" id="p0105">All the barriers considered so far were directed at computations organized in phases, where each thread finishes the work for a phase, reaches the barrier, and then starts a new phase.</p><p class="text" id="p0110"><span aria-label="Page 439" epub:type="pagebreak" id="page_439" role="doc-pagebreak"/>There is another interesting class of programs, in which each thread finishes its own part of the computation, only to be put to work again when another thread generates new work. An example of such a program is the simplified work stealing executor pool from Chapter <a href="B9780124159501000264.xhtml">16</a> (<a href="#f0065" id="cf0065">Fig. 18.10</a>). Once a thread exhausts the tasks in its local queue, it tries to steal work from other threads' queues. The <img alt="Image" height="8" src="images/B9780124159501000288/fx016.jpg" width="46"/>() method itself may push new tasks onto the calling thread's local queue. Once all threads have exhausted all tasks in their queues, the threads will run forever while repeatedly attempting to steal items. Instead, we would like to devise a <i>termination detection</i> barrier so that these threads can all terminate once they have finished all their tasks.</p><div class="pageavoid"><figure class="fig" id="f0065"><img alt="Image" height="356" src="images/B9780124159501000288/gr010.jpg" width="446"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.10</span> Work stealing executor pool revisited.</div></figcaption></figure></div><p class="text" id="p0115">Each thread is either <i>active</i> (it has a task to execute) or <i>inactive</i> (it has none). Note that any inactive thread may become active as long as some other thread is active, since an inactive thread may steal a task from an active one. Once all threads have become inactive, then no thread will ever become active again. Detecting that the computation as a whole has terminated is the problem of determining that at some instant in time, there are no longer any active threads.</p><p class="text" id="p0120">None of the barrier algorithms studied so far can solve this problem. Termination cannot be detected by having each thread announce that it has become inactive, and simply count how many have done so, because threads may repeatedly change from inactive to active and back. For example, we have suppose work stealing threads <i>A</i>, <i>B</i>, and <i>C</i>. We would like the threads to be able to exit from the loop on line 9. An incorrect strategy would assign each thread a Boolean value <span aria-label="Page 440" epub:type="pagebreak" id="page_440" role="doc-pagebreak"/>indicating whether it is active or inactive. When <i>A</i> becomes inactive, it may then observe that <i>B</i> is also inactive, and then observe that <i>C</i> is inactive. Nevertheless, <i>A</i> cannot conclude that the overall computation has completed, as <i>B</i> might have stolen work from <i>C</i> after <i>A</i> checked <i>B</i>, but before it checked <i>C</i>.</p><p class="text" id="p0125">A <i>termination detection</i> barrier (<a href="#f0070" id="cf0070">Fig. 18.11</a>) provides methods <img alt="Image" height="9" src="images/B9780124159501000288/fx017.jpg" width="59"/>(<i>v</i>) and <img alt="Image" height="9" src="images/B9780124159501000288/fx018.jpg" width="78"/>(). Each thread calls <img alt="Image" height="9" src="images/B9780124159501000288/fx017.jpg" width="59"/><span class="hiddenClass"><mml:math><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="italic">true</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000288/si5.png" style="vertical-align:middle" width="38"/></span> to notify the barrier when it becomes active, and <img alt="Image" height="9" src="images/B9780124159501000288/fx017.jpg" width="59"/><span class="hiddenClass"><mml:math><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="italic">false</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000288/si6.png" style="vertical-align:middle" width="43"/></span> to notify the barrier when it becomes inactive. The <img alt="Image" height="9" src="images/B9780124159501000288/fx018.jpg" width="78"/>() method returns <i>true</i> if and only if all threads had become inactive at some earlier instant. <a href="#f0075" id="cf0075">Fig. 18.12</a> shows a simple implementation of a termination detection barrier.</p><div class="pageavoid"><figure class="fig" id="f0070"><img alt="Image" height="60" src="images/B9780124159501000288/gr011.jpg" width="209"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.11</span> Termination detection barrier interface.</div></figcaption></figure></div><div class="pageavoid"><figure class="fig" id="f0075"><img alt="Image" height="257" src="images/B9780124159501000288/gr012.jpg" width="328"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.12</span> A simple termination detection barrier.</div></figcaption></figure></div><p class="text" id="p0130"><span aria-label="Page 441" epub:type="pagebreak" id="page_441" role="doc-pagebreak"/>The barrier encompasses an <img alt="Image" height="11" src="images/B9780124159501000288/fx006.jpg" width="87"/> initialized to 0. Each thread that becomes active increments the counter (line 8) and each thread that becomes inactive decrements it (line 10). The computation is deemed to have terminated when the counter reaches 0 (line 14).</p><p class="text" id="p0135">The termination detection barrier works only if used correctly. <a href="#f0080" id="cf0080">Fig. 18.13</a> shows how to modify the work stealing thread's <img alt="Image" height="6" src="images/B9780124159501000288/fx019.jpg" width="18"/>() method to return when the computation has terminated. Initially, every thread registers as active (line 3). Once a thread has exhausted its local queue, it registers as inactive (line 10). Before it tries to steal a new task, however, it must register as active (line 14). If the theft fails, it registers as inactive again (line 17).</p><div class="pageavoid"><figure class="fig" id="f0080"><img alt="Image" height="422" src="images/B9780124159501000288/gr013.jpg" width="446"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.13</span> Work stealing executor pool: the <img alt="Image" height="6" src="images/B9780124159501000288/fx019.jpg" width="18"/>() method with termination.</div></figcaption></figure></div><p class="text" id="p0140">Note that a thread sets its state to active before stealing a task. Otherwise, if a thread were to steal a task while inactive, then the thread whose task was stolen might also declare itself inactive, resulting in a computation where all threads declare themselves inactive while the computation continues.</p><p class="text" id="p0145">Here is a subtle point: A thread tests whether the queue is empty (line 13) before it attempts to steal a task. This way, it avoids declaring itself active if there is no chance the theft will succeed. Without this precaution, it is possible that the threads will not detect termination because each one repeatedly switches to an active state before a steal attempt that is doomed to fail.</p><p class="text" id="p0150">Correct use of the termination detection barrier must satisfy both a safety and a liveness property. The safety property is that if <img alt="Image" height="9" src="images/B9780124159501000288/fx018.jpg" width="78"/>() returns <i>true</i>, then the computation really has terminated. Safety requires that no active thread ever declare itself inactive, because it could trigger an incorrect termination detection. For example, the work stealing thread of <a href="#f0080" id="cf0085">Fig. 18.13</a> would be incorrect if the thread declared itself to be active only after <span aria-label="Page 442" epub:type="pagebreak" id="page_442" role="doc-pagebreak"/>successfully stealing a task. By contrast, it is safe for an inactive thread to declare itself active, which may occur if the thread is unsuccessful in stealing work at line 15.</p><p class="text" id="p0155">The liveness property is that if the computation terminates, then <img alt="Image" height="9" src="images/B9780124159501000288/fx018.jpg" width="78"/>() eventually returns <i>true</i>. (It is not necessary that termination be detected instantly.) While safety is not jeopardized if an inactive thread declares itself active, liveness will be violated if a thread that does not succeed in stealing work fails to declare itself inactive again (line 15), because termination will not be detected when it occurs.<span aria-label="Page 443" epub:type="pagebreak" id="page_443" role="doc-pagebreak"/></p></section><section><h2 class="h1hd" id="s0040"><a id="st0050"/>18.7 Chapter notes</h2><p class="textfl" id="p0160">John Mellor-Crummey and Michael Scott <a epub:type="noteref" href="#br0620" id="cf0090" role="doc-noteref">[124]</a> provide a survey of several barrier algorithms, though the performance numbers they provide should be viewed from a historical perspective. The combining tree barrier is based on code due to John Mellor-Crummey and Michael Scott <a epub:type="noteref" href="#br0620" id="cf0095" role="doc-noteref">[124]</a>, which is in turn based on the combining tree algorithm of Pen-Chung Yew, Nian-Feng Tzeng, and Duncan Lawrie <a epub:type="noteref" href="#br0840" id="cf0100" role="doc-noteref">[168]</a>. The dissemination barrier is credited to Debra Hensgen, Raphael Finkel, and Udi Manber <a epub:type="noteref" href="#br0320" id="cf0105" role="doc-noteref">[64]</a>. The tournament tree barrier used in the exercises is credited to John Mellor-Crummey and Michael Scott <a epub:type="noteref" href="#br0620" id="cf0110" role="doc-noteref">[124]</a>. The simple barriers and the static tree barrier are most likely folklore. We learned of the static tree barrier from Beng-Hong Lim. The termination detection barrier and its application to an executor pool are based on a variation suggested by Peter Kessler to an algorithm by Dave Detlefs, Christine Flood, Nir Shavit, and Xiolan Zhang <a epub:type="noteref" href="#br0235" id="cf0115" role="doc-noteref">[47]</a>.</p></section><section><h2 class="h1hd" id="s0045"><a id="st0055"/>18.8 Exercises</h2><p class="textfl" id="p0165"/><div class="boxg1" id="enun0020"><p class="b1num">Exercise 18.1 </p><div><p class="b1textfl" id="p0170"><a href="#f0085" id="cf0120">Fig. 18.14</a> shows how to use barriers to make a parallel prefix computation work on an asynchronous architecture.</p><div class="pageavoid"><figure class="fig" id="f0085"><img alt="Image" height="290" src="images/B9780124159501000288/gr014.jpg" width="256"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.14</span> Parallel prefix computation.</div></figcaption></figure></div><p class="b1text" id="p0175">A <i>parallel prefix</i> computation, given a sequence <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="10" src="images/B9780124159501000288/si7.png" style="vertical-align:middle" width="84"/></span>, of numbers, computes in parallel the partial sums:</p><p class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak" linebreakstyle="after">=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:math></p><div class="showClass"><p class="fig"><img alt="Image" height="69" src="images/B9780124159501000288/si8.png" width="106"/><a id="deq1"/></p></div><p class="b1textfl"> In a synchronous system, where all threads take steps at the same time, there are simple, well-known algorithms for <i>m</i> threads to compute the partial sums in <span class="hiddenClass"><mml:math><mml:mi mathvariant="normal">log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>m</mml:mi></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000288/si9.png" style="vertical-align:middle" width="37"/></span> steps. The computation proceeds in a sequence of rounds, starting at round zero. In round <i>r</i>, if <span class="hiddenClass"><mml:math><mml:mi>i</mml:mi><mml:mo>⩾</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msup></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000288/si10.png" style="vertical-align:middle" width="39"/></span>, thread <i>i</i> reads the value at <span class="hiddenClass"><mml:math><mml:mi>a</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">−</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000288/si11.png" style="vertical-align:middle" width="60"/></span> into a local variable. Next, it adds that value to <span class="hiddenClass"><mml:math><mml:mi>a</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000288/si12.png" style="vertical-align:middle" width="27"/></span>. Rounds continue until <span class="hiddenClass"><mml:math><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:mo>⩾</mml:mo><mml:mi>m</mml:mi></mml:math></span><span><img alt="Image" height="14" src="images/B9780124159501000288/si13.png" style="vertical-align:middle" width="48"/></span>. It is not hard to see that after <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000288/si14.png" style="vertical-align:middle" width="54"/></span> rounds, the array <i>a</i> contains the partial sums.</p><div><ol><li class="b1numlist" id="o0010">1.  What could go wrong if we executed the parallel prefix on <span class="hiddenClass"><mml:math><mml:mi>n</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">&gt;</mml:mo><mml:mi>m</mml:mi></mml:math></span><span><img alt="Image" height="7" src="images/B9780124159501000288/si15.png" style="vertical-align:middle" width="42"/></span> threads?</li><li class="b1numlist" id="o0015">2.  Add one or more barriers to this program to make it work properly in a concurrent setting with <i>n</i> threads. What is the minimum number of barriers that are needed?</li></ol></div><p class="b1textfl"/></div></div><p class="textfl"/><p class="text" id="p0190"><span aria-label="Page 444" epub:type="pagebreak" id="page_444" role="doc-pagebreak"/></p><div class="boxg1" id="enun0025"><p class="b1num">Exercise 18.2 </p><div><p class="b1textfl" id="p0195">Change the sense reversing barrier implementation so that waiting threads call <img alt="Image" height="9" src="images/B9780124159501000288/fx021.jpg" width="26"/>() instead of spinning.</p><div><ul><li class="b1bulllist" id="u0010">•  Give an example of a situation where suspending threads is better than spinning.</li><li class="b1bulllist" id="u0015">•  Give an example of a situation where the other choice is better. <span aria-label="Page 445" epub:type="pagebreak" id="page_445" role="doc-pagebreak"/></li></ul></div><p class="b1textfl"/></div></div><p class="textfl"/><p class="text" id="p0210"/><div class="boxg1" id="enun0030"><p class="b1num">Exercise 18.3 </p><div><p class="b1textfl" id="p0215">Change the tree barrier implementation so that it takes a <img alt="Image" height="9" src="images/B9780124159501000288/fx022.jpg" width="52"/> object whose <img alt="Image" height="6" src="images/B9780124159501000288/fx019.jpg" width="18"/>() method is called once after the last thread arrives at the barrier, but before any thread leaves the barrier.</p></div></div><p class="textfl"/><p class="text" id="p0220"/><div class="boxg1" id="enun0035"><p class="b1num">Exercise 18.4 </p><div><p class="b1textfl" id="p0225">Modify the combining tree barrier so that nodes can use any barrier implementation, not just the sense reversing barrier.</p></div></div><p class="textfl"/><p class="text" id="p0230"/><div class="boxg1" id="enun0040"><p class="b1num">Exercise 18.5 </p><div><p class="b1textfl" id="p0235">A <i>tournament tree barrier</i> (class <img alt="Image" height="9" src="images/B9780124159501000288/fx020.jpg" width="73"/> in <a href="#f0090" id="cf0125">Fig. 18.15</a>) is an alternative tree-structured barrier. Assume there are <i>n</i> threads, where <i>n</i> is a power of 2. The tree is a binary tree consisting of <span class="hiddenClass"><mml:math><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>1</mml:mn></mml:math></span><span><img alt="Image" height="11" src="images/B9780124159501000288/si16.png" style="vertical-align:middle" width="45"/></span> nodes. Each leaf is owned by a single, statically determined, thread. Each node's two children are linked as <i>partners</i>. One partner is statically designated as <i>active</i>, and the other as <i>passive</i>. <a href="#f0095" id="cf0130">Fig. 18.16</a> illustrates the tree structure.</p><div class="pageavoid"><figure class="fig" id="f0090"><img alt="Image" height="503" src="images/B9780124159501000288/gr015.jpg" width="349"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.15</span> The <img alt="Image" height="9" src="images/B9780124159501000288/fx020.jpg" width="73"/> class.</div></figcaption></figure></div><div class="pageavoid"><figure class="fig" id="f0095"><img alt="Image" height="182" src="images/B9780124159501000288/gr016.jpg" width="306"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.16</span> The <img alt="Image" height="9" src="images/B9780124159501000288/fx020.jpg" width="73"/> class: information flow. Nodes are paired statically in active/passive pairs. Threads start at the leaves. Each thread in an active node waits for its passive partner to show up, then it proceeds up the tree. Each passive thread waits for its active partner for notification of completion. Once an active thread reaches the root, all threads have arrived, and notifications flow down the tree in the reverse order.</div></figcaption></figure></div><p class="b1text" id="p0240">Each thread keeps track of the current sense in a thread-local variable. When a thread arrives at a passive node, it sets its active partner's <img alt="Image" height="6" src="images/B9780124159501000288/fx009.jpg" width="32"/> field to the current sense, and spins on its own <img alt="Image" height="6" src="images/B9780124159501000288/fx009.jpg" width="32"/> field until its partner changes that field's value to the current sense. When a thread arrives at an active node, it spins on its <img alt="Image" height="6" src="images/B9780124159501000288/fx009.jpg" width="32"/> field until its passive partner sets it to the current sense. When the field changes, that particular barrier is complete, and the active thread follows the <img alt="Image" height="11" src="images/B9780124159501000288/fx023.jpg" width="38"/> reference to its parent node. Note that an active thread at one level may become passive at the next level. When the root node barrier is complete, notifications percolate down the tree. Each thread moves back down the tree setting its partner's <img alt="Image" height="6" src="images/B9780124159501000288/fx009.jpg" width="32"/> field to the current sense.</p><div><ul><li class="b1bulllist" id="u0020">•  Explain how this barrier slightly improves the combining tree barrier of <a href="#f0040" id="cf0135">Fig. 18.5</a>.</li><li class="b1bulllist" id="u0025">•  The tournament barrier code uses <img alt="Image" height="11" src="images/B9780124159501000288/fx023.jpg" width="38"/> and <img alt="Image" height="11" src="images/B9780124159501000288/fx024.jpg" width="46"/> references to navigate the tree. We could save space by eliminating these fields and keeping all the nodes in a single array with the root at index 0, the root's <span aria-label="Page 446" epub:type="pagebreak" id="page_446" role="doc-pagebreak"/>children at indices 1 and 2, the grandchildren at indices 3–6, and so on. Reimplement the tournament barrier to use indexing arithmetic instead of references to navigate the tree.</li></ul></div><p class="b1textfl"/></div></div><p class="textfl"/><p class="text" id="p0255"/><div class="boxg1" id="enun0045"><p class="b1num">Exercise 18.6 </p><div><p class="b1textfl" id="p0260">The combining tree barrier uses a single thread-local sense field for the entire barrier. Suppose instead we were to associate a thread-local sense with each node as in <a href="#f0100" id="cf0140">Fig. 18.17</a>. Either explain why this implementation is equivalent to the other one, except that it consumes more memory, or give a counterexample showing that it is incorrect.</p><div class="pageavoid"><figure class="fig" id="f0100"><img alt="Image" height="553" src="images/B9780124159501000288/gr017.jpg" width="366"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.17</span> Thread-local tree barrier.</div></figcaption></figure></div></div></div><p class="textfl"/><p class="text" id="p0265"><span aria-label="Page 447" epub:type="pagebreak" id="page_447" role="doc-pagebreak"/></p><div class="boxg1" id="enun0050"><p class="b1num">Exercise 18.7 </p><div><p class="b1textfl" id="p0270">The tree barrier works “bottom-up,” in the sense that barrier completion moves from the leaves up to the root, while wakeup information moves from the root back down to the leaves. <a href="#f0105" id="cs0010">Figs. 18.18</a> and <a href="#f0110">18.19</a> show an alternative design, called a <i>reverse tree barrier</i>, which works just like a tree barrier except for the fact that barrier completion starts at the root and moves down to the leaves. Either sketch an argument why this is correct, perhaps by reduction to the standard tree barrier, or give a counterexample showing why it is incorrect.</p><div class="pageavoid"><figure class="fig" id="f0105"><img alt="Image" height="569" src="images/B9780124159501000288/gr018.jpg" width="356"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.18</span> Reverse tree barrier part 1.</div></figcaption></figure></div><div class="pageavoid"><figure class="fig" id="f0110"><img alt="Image" height="636" src="images/B9780124159501000288/gr019.jpg" width="289"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.19</span> Reverse tree barrier part 2: correct or not?.</div></figcaption></figure></div></div></div><p class="textfl"/><p class="text" id="p0275"><span aria-label="Page 448" epub:type="pagebreak" id="page_448" role="doc-pagebreak"/></p><div class="boxg1" id="enun0055"><p class="b1num">Exercise 18.8 </p><div><p class="b1textfl" id="p0280">Implement an <i>n</i>-thread reusable barrier from an <i>n</i>-wire counting network and a single Boolean variable. Sketch a proof that the barrier works.</p></div></div><p class="textfl"/><p class="text" id="p0285"><span aria-label="Page 449" epub:type="pagebreak" id="page_449" role="doc-pagebreak"/></p><div class="boxg1" id="enun0060"><p class="b1num">Exercise 18.9 </p><div><p class="b1textfl" id="p0290">A <i>dissemination barrier</i> is a symmetric barrier implementation in which threads spin on statically assigned locally cached locations using only loads and stores. As illustrated in <a href="#f0115" id="cf0145">Fig. 18.20</a>, the algorithm runs in a series of rounds. At round <i>r</i>, thread <i>i</i> notifies thread <span class="hiddenClass"><mml:math><mml:mi>i</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">+</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:mspace width="0.25em"/><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">mod</mml:mi></mml:mrow><mml:mspace width="0.25em"/><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000288/si17.png" style="vertical-align:middle" width="97"/></span> (where <i>n</i> is the number of threads) and waits for notification from thread <span class="hiddenClass"><mml:math><mml:mi>i</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:mspace width="0.25em"/><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">mod</mml:mi></mml:mrow><mml:mspace width="0.25em"/><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="15" src="images/B9780124159501000288/si18.png" style="vertical-align:middle" width="97"/></span>.</p><div class="pageavoid"><figure class="fig" id="f0115"><img alt="Image" height="206" src="images/B9780124159501000288/gr020.jpg" width="425"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure 18.20</span> Communication in the dissemination barrier. In each round <i>r</i> a thread <i>i</i> communicates with thread <i>i</i> + 2<sup><i>r</i></sup> (mod <i>n</i>).</div></figcaption></figure></div><p class="b1text" id="p0295">For how many rounds must this protocol run to implement a barrier? What if <i>n</i> is not a power of 2? Justify your answers.</p></div></div><p class="textfl"/><p class="text" id="p0300"/><div class="boxg1" id="enun0065"><p class="b1num">Exercise 18.10 </p><div><p class="b1textfl" id="p0305">Give a reusable implementation of a dissemination barrier in Java.</p><p class="b1text" id="p0310">Hint: Consider keeping track of both the parity and the sense of the current phase.</p></div></div><p class="textfl"/><p class="text" id="p0315"/><div class="boxg1" id="enun0070"><p class="b1num">Exercise 18.11 </p><div><p class="b1textfl" id="p0320">Create a table that summarizes the total number of operations in the static tree, combining tree, and dissemination barriers.</p></div></div><p class="textfl"/><p class="text" id="p0325"/><div class="boxg1" id="enun0075"><p class="b1num">Exercise 18.12 </p><div><p class="b1textfl" id="p0330">Can you devise a “distributed” termination detection algorithm for the executor pool in which threads do not repeatedly update or test a central location for termination, but rather use only local uncontended variables? Variables may be unbounded, but state changes should take constant time (so you cannot parallelize the shared counter).</p><p class="b1text" id="p0335">Hint: Adapt the atomic snapshot algorithm from Chapter <a href="B9780124159501000136.xhtml">4</a>.</p></div></div><p class="textfl"/><p class="text" id="p0340"/><div class="boxg1" id="enun0080"><p class="b1num">Exercise 18.13 </p><div><p class="b1textfl" id="p0345">In the termination detection barrier, the state is set to active before stealing the task; otherwise the stealing thread could be declared inactive; then it would steal a task, and before setting its state back to active, the thread it stole from could become inactive. This would lead to an undesirable situation in which all threads are declared inactive yet the computation continues. Can you devise a terminating executor pool in which the state is set to active only <i>after</i> successfully stealing a task?</p></div></div><p class="textfl"/></section><footer><section epub:type="bibliography" role="doc-bibliography"><div id="bl0470"><h2 class="reftitle" id="st0060">Bibliography</h2><p class="reflist1" epub:type="biblioentry footnote" id="br0235" role="doc-biblioentry">[47] C. Flood, D. Detlefs, N. Shavit, C. Zhang,  Parallel garbage collection for shared memory multiprocessors,   <i>Proc. of the Java TM Virtual Machine Research and Technology Symposium</i>.  2001.</p><p class="reflist1" epub:type="biblioentry footnote" id="br0320" role="doc-biblioentry">[64] D. Hensgen, R. Finkel, U. Manber,  Two algorithms for barrier synchronization,   <cite><i>International Journal of Parallel Programming</i></cite>0885-7458 1988;17(1):1–17.</p><p class="reflist2" epub:type="biblioentry footnote" id="br0620" role="doc-biblioentry">[124] John Mellor-Crummey, Michael Scott,  Algorithms for scalable synchronization on shared-memory multiprocessors,   <cite><i>ACM Transactions on Computer Systems</i></cite> 1991;9(1):21–65.</p><p class="reflist2" epub:type="biblioentry footnote" id="br0840" role="doc-biblioentry">[168] P. Yew, N. Tzeng, D. Lawrie,  Distributing hot-spot addressing in large-scale multiprocessors,   <cite><i>IEEE Transactions on Computers</i></cite> April 1987;C-36(4):388–395.</p></div></section></footer></section></body></html>