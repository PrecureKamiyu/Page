<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML" lang="EN" xml:lang="en"><head><meta content="text/html; charset=utf-8" http-equiv="default-style"/><title>The Art of Multiprocessor Programming</title><link href="Elsevier_eBook.css" rel="stylesheet" type="text/css"/><link href="math.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4f1c4a5b-a3e2-48ff-98f3-ff17812cd57a" name="Adept.expected.resource"/></head><body><section epub:type="appendix" role="doc-appendix"><div id="CN"><a id="c0010tit1"/></div><div aria-label="Page 519" epub:type="pagebreak" id="page_519" role="doc-pagebreak"/><h1 class="fmtitle" id="ti0010">Appendix B: Hardware basics</h1><p class="textfl" id="p0010"> </p><p class="quote" id="sp0040"><i>A novice was trying to fix a broken Lisp machine by turning the power off and on. Knight, seeing what the student was doing spoke sternly: “You cannot fix a machine just by power-cycling it with no understanding of what is going wrong.” Knight turned the machine off and on. The machine worked.</i></p><p class="textfl"/><p class="text" id="p0015">(From “AI Koans,” a collection of jokes popular at MIT in the 1980s.)</p><section><h2 class="h1hd" id="s0010"><a id="st0010"/>B.1 Introduction (and a puzzle)</h2><p class="textfl" id="p0020">You can do a pretty good job of programming a uniprocessor without understanding much about computer architecture, but the same is not true of multiprocessors. You cannot program a multiprocessor effectively unless you know what a multiprocessor <i>is</i>. We illustrate this point by a puzzle. We consider two programs that are logically equivalent, but one is much less efficient than the other. Ominously, the simpler program is the inefficient one. This discrepancy cannot be explained, nor the danger avoided, without a basic understanding of modern multiprocessor architectures.</p><p class="text" id="p0025">Here is the background to the puzzle. Suppose two threads share a resource that can be used by only one thread at a time. To prevent concurrent use, each thread must <i>lock</i> the resource before using it, and <i>unlock</i> it afterward. We study many ways to implement locks in Chapter <a href="B9780124159501000173.xhtml">7</a>. For the puzzle, we consider two simple implementations in which the lock is a single Boolean field. If the field is <i>false</i>, the lock is free, and otherwise it is in use. We manipulate the lock with the <img alt="Image" height="11" src="images/B978012415950100032X/fx001.jpg" width="59"/>(<i>v</i>) method, which atomically swaps its argument <i>v</i> with the field value. To acquire the lock, a thread calls <img alt="Image" height="11" src="images/B978012415950100032X/fx001.jpg" width="59"/><span class="hiddenClass"><mml:math><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="italic">true</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="14" src="images/B978012415950100032X/si1.png" style="vertical-align:middle" width="38"/></span>. If the call returns <i>false</i>, then the lock was free, and the caller succeeded in locking the object. Otherwise, the object was already locked, and the thread must try again later. A thread releases a lock simply by storing <i>false</i> into the Boolean field.</p><p class="text" id="p0030">In <a href="#f0010" id="cf0010">Fig. B.1</a>, the <i>test-and-set</i> (<img alt="Image" height="9" src="images/B978012415950100032X/fx002.jpg" width="46"/>) lock repeatedly calls <img alt="Image" height="11" src="images/B978012415950100032X/fx001.jpg" width="59"/><span class="hiddenClass"><mml:math><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="italic">true</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="14" src="images/B978012415950100032X/si1.png" style="vertical-align:middle" width="38"/></span> (line 4) until it returns <i>false</i>. By contrast, in <a href="#f0015" id="cf0015">Fig. B.2</a>, the <i>test-and-test-and-set</i> lock (<img alt="Image" height="9" src="images/B978012415950100032X/fx003.jpg" width="52"/>) repeatedly reads the lock field (by calling <img alt="Image" height="12" src="images/B978012415950100032X/fx004.jpg" width="70"/> at line 5) until it returns <i>false</i>, and only then calls <img alt="Image" height="11" src="images/B978012415950100032X/fx001.jpg" width="59"/>() (line 6). It is important to understand that reading the lock value is atomic, and applying <img alt="Image" height="11" src="images/B978012415950100032X/fx001.jpg" width="59"/>() to the lock value is atomic, but the combination is not atomic: Between the time a thread reads the lock value and the time it calls <img alt="Image" height="11" src="images/B978012415950100032X/fx001.jpg" width="59"/>(), the lock value may have changed.</p><div class="pageavoid"><figure class="fig" id="f0010"><img alt="Image" height="109" src="images/B978012415950100032X/gr001.jpg" width="281"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure B.1</span> The <img alt="Image" height="9" src="images/B978012415950100032X/fx002.jpg" width="46"/> class.</div></figcaption></figure></div><div class="pageavoid"><figure class="fig" id="f0015"><img alt="Image" height="175" src="images/B978012415950100032X/gr002.jpg" width="255"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure B.2</span> The <img alt="Image" height="9" src="images/B978012415950100032X/fx003.jpg" width="52"/> class.</div></figcaption></figure></div><p class="text" id="p0035"><span aria-label="Page 520" epub:type="pagebreak" id="page_520" role="doc-pagebreak"/>Before you proceed, you should convince yourself that the <img alt="Image" height="9" src="images/B978012415950100032X/fx002.jpg" width="46"/> and <img alt="Image" height="9" src="images/B978012415950100032X/fx003.jpg" width="52"/> algorithms are logically the same. The reason is simple: In the <img alt="Image" height="9" src="images/B978012415950100032X/fx003.jpg" width="52"/> algorithm, reading that the lock is free does not guarantee that the next call to <img alt="Image" height="11" src="images/B978012415950100032X/fx001.jpg" width="59"/>() will succeed, because some other thread may have acquired the lock in the interval between reading the lock and trying to acquire it. So why bother reading the lock before trying to acquire it?</p><p class="text" id="p0040">Here is the puzzle: While the two lock implementations may be logically equivalent, they perform very differently. In a classic 1989 experiment, Anderson measured the time needed to execute a simple test program on several contemporary multiprocessors. He measured the elapsed time for <i>n</i> threads to execute a short critical section one million times. <a href="#f0020" id="cf0020">Fig. B.3</a> shows how long each lock takes, plotted as a function of the number of threads. In a perfect world, both the <img alt="Image" height="9" src="images/B978012415950100032X/fx002.jpg" width="46"/> and <img alt="Image" height="9" src="images/B978012415950100032X/fx003.jpg" width="52"/> curves would be as flat as the ideal curve on the bottom, since each run does the same number of increments. Instead, we see that both curves slope up, indicating that lock-induced delay increases with the number of threads. Curiously, however, the <img alt="Image" height="9" src="images/B978012415950100032X/fx002.jpg" width="46"/> is much slower than the <img alt="Image" height="9" src="images/B978012415950100032X/fx003.jpg" width="52"/> lock, especially as the number of threads increases. Why?</p><div class="pageavoid"><figure class="fig" id="f0020"><img alt="Image" height="246" src="images/B978012415950100032X/gr003.jpg" width="219"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure B.3</span> Schematic performance of a <img alt="Image" height="9" src="images/B978012415950100032X/fx002.jpg" width="46"/>, a <img alt="Image" height="9" src="images/B978012415950100032X/fx003.jpg" width="52"/>, and an ideal lock.</div></figcaption></figure></div><p class="text" id="p0045">This appendix covers much of what you need to know about multiprocessor architecture to write efficient concurrent algorithms and data structures. Along the way, we will explain the divergent curves in <a href="#f0020" id="cf0025">Fig. B.3</a>.</p><p class="text" id="p0050"><span aria-label="Page 521" epub:type="pagebreak" id="page_521" role="doc-pagebreak"/>We are concerned with the following components:</p><div><ul><li class="bulllist" id="u0010">•  The <i>processors</i> are hardware devices that execute software <i>threads</i>. There are typically more threads than processors, and each processor runs a thread for a while, sets it aside, and turns its attention to another thread.</li><li class="bulllist" id="u0015">•  The <i>interconnect</i> is a communication medium that links processors to processors and processors to memory.</li><li class="bulllist" id="u0020">•  The <i>memory</i> is actually a hierarchy of components that store data, ranging from one or more levels of small, fast <i>caches</i> to a large and relatively slow <i>main memory</i>. Understanding how these levels interact is essential to understanding the actual performance of many concurrent algorithms.</li></ul></div><p class="textfl"/><p class="text" id="p0070">From our point of view, one architectural principle drives everything else: <i>Processors and main memory are far apart</i>. It takes a long time for a processor to read a value from memory. It also takes a long time for a processor to write a value to memory, and longer still for the processor to be sure that value has actually been installed in memory. Accessing memory is more like mailing a letter than making a phone call. Almost everything we examine in this appendix is the result of trying to alleviate the long time it takes (“high latency”) to access memory.</p><p class="text" id="p0075">Processor and memory speed change over time, but their <i>relative</i> performance changes slowly. Consider the following analogy. Imagine that it is 1980, and you are in charge of a messenger service in mid-town Manhattan. While cars outperform bicycles on the open road, bicycles outperform cars in heavy traffic, so you choose to use bicycles. Even though the technology behind both bicycles and cars has advanced, the <i>architectural</i> comparison remains the same. Then, as now, if you are designing an urban messenger service, you should use bicycles, not cars.<span aria-label="Page 522" epub:type="pagebreak" id="page_522" role="doc-pagebreak"/></p></section><section><h2 class="h1hd" id="s0015"><a id="st0015"/>B.2 Processors and threads</h2><p class="textfl" id="p0080">A multiprocessor consists of multiple hardware <i>processors</i>, each of which executes a sequential program. When discussing multiprocessor architectures, the basic unit of time is the <i>cycle</i>: the time it takes a processor to fetch and execute a single instruction. In absolute terms, cycle times change as technology advances (from about 10 million cycles per second in 1980 to about 3000 million in 2005), and they vary from one platform to another (processors that control toasters have longer cycles than processors that control web servers). Nevertheless, the relative cost of instructions such as memory access changes slowly when expressed in terms of cycles.</p><p class="text" id="p0085">A <i>thread</i> is a sequential program. While a processor is a hardware device, a thread is a software construct. A processor can run a thread for a while and then set it aside and run another thread, an event known as a <i>context switch</i>. A processor may set aside a thread, or <i>deschedule</i> it, for a variety of reasons. Perhaps the thread has issued a memory request that will take some time to satisfy, or perhaps that thread has simply run long enough, and it is time for another thread to make progress. When a thread is descheduled, it may resume execution on another processor.</p></section><section><h2 class="h1hd" id="s0020"><a id="st0020"/>B.3 Interconnect</h2><p class="textfl" id="p0090">The <i>interconnect</i> is the medium by which processors communicate with the memory and with other processors. There are essentially two kinds of interconnect architectures in use: <i>symmetric multiprocessing</i> (SMP) and <i>nonuniform memory access</i> (NUMA). See <a href="#f0025" id="cf0030">Fig. B.4</a>.</p><div class="pageavoid"><figure class="fig" id="f0025"><img alt="Image" height="118" src="images/B978012415950100032X/gr004.jpg" width="493"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure B.4</span> An SMP architecture with caches on the right and a cacheless NUMA architecture on the left.</div></figcaption></figure></div><p class="text" id="p0095">In an SMP architecture, processors and memory are linked by a <i>bus</i> interconnect, a broadcast medium that acts like a tiny ethernet. Both processors and the main memory have <i>bus controller</i> units in charge of sending and listening for messages broadcast on the bus. (Listening is sometimes called <i>snooping</i>.) SMP architectures are easier to build, but they are not scalable to large numbers of processors because eventually the bus becomes overloaded.</p><p class="text" id="p0100"><span aria-label="Page 523" epub:type="pagebreak" id="page_523" role="doc-pagebreak"/>In a NUMA architecture, a collection of <i>nodes</i> are linked by a point-to-point network, like a tiny local area network. Each node contains one or more processors and a local memory. One node's local memory is accessible to the other nodes, and together, the nodes' memories form a global memory shared by all processors. The NUMA name reflects the fact that a processor can access memory residing on its own node faster than it can access memory residing on other nodes. Networks are more complex than buses, and require more elaborate protocols, but they scale better than buses to large numbers of processors.</p><p class="text" id="p0105">The division between SMP and NUMA architectures is a simplification: For example, some systems have hybrid architectures, where processors within a cluster communicate over a bus, but processors in different clusters communicate over a network.</p><p class="text" id="p0110">From the programmer's point of view, it may not seem important whether the underlying platform is based on a bus, a network, or a hybrid interconnect. It is important, however, to realize that the interconnect is a finite resource shared among the processors. If one processor uses too much of the interconnect's bandwidth, then the others may be delayed.</p></section><section><h2 class="h1hd" id="s0025"><a id="st0025"/>B.4 Memory</h2><p class="textfl" id="p0115">Processors share a <i>main memory</i>, which is a large array of <i>words</i>, indexed by <i>address</i>. The size of a word or an address is platform-dependent, but typically it is either 32 or 64 bits. Simplifying somewhat, a processor reads a value from memory by sending a message containing the desired address to memory. The response message contains the associated <i>data</i>, that is, the contents of memory at that address. A processor writes a value by sending the address and the new data to memory, and the memory sends back an acknowledgment when the new data have been installed.</p></section><section><h2 class="h1hd" id="s0030"><a id="st0030"/>B.5 Caches</h2><p class="textfl" id="p0120">On modern architectures, a main memory access may take hundreds of cycles, so there is a real danger that a processor may spend much of its time just waiting for the memory to respond to requests. Modern systems alleviate this problem by introducing one or more <i>caches</i>: small memories that are situated closer to the processors and are therefore much faster than main memory. These caches are logically situated “between” the processor and the memory: When a processor attempts to read a value from a given memory address, it first looks to see if the value is already in the cache, and if so, it does not need to perform the slower access to memory. If the desired address's value was found, we say the processor <i>hits</i> in the cache, and otherwise it <i>misses</i>. In a similar way, if a processor attempts to write an address that is in the cache, it does not need to perform the slower access to memory. The proportion of requests satisfied in the cache is called the cache <i>hit ratio</i> (or <i>hit rate</i>).</p><p class="text" id="p0125"><span aria-label="Page 524" epub:type="pagebreak" id="page_524" role="doc-pagebreak"/>Caches are effective because most programs display a high degree of <i>locality</i>: If a processor reads or writes a memory address (also called a memory location), then it is likely to read or write the same location again soon. Moreover, if a processor reads or writes a memory location, then it is also likely to read or write <i>nearby</i> locations soon. To exploit this second observation, caches typically operate at a <i>granularity</i> larger than a single word: A cache holds a group of neighboring words called <i>cache lines</i> (sometimes called <i>cache blocks</i>).</p><p class="text" id="p0130">In practice, most processors have two or three levels of caches, called <i>L</i>1, <i>L</i>2, and <i>L</i>3 caches. All but the last (and largest) cache typically reside on the same chip as the processor. An L1 cache typically takes one or two cycles to access. An on-chip L2 may take about 10 cycles to access. The last level cache, whether L2 or L3, typically takes tens of cycles to access. These caches are significantly faster than the hundreds of cycles required to access the memory. Of course, these times vary from platform to platform, and many multiprocessors have even more elaborate cache structures.</p><p class="text" id="p0135">The original proposals for NUMA architectures did not include caches because it was felt that local memory was enough. Later, however, commercial NUMA architectures did include caches. Sometimes the term <i>cache-coherent NUMA</i> (cc-NUMA) is used to mean NUMA architectures with caches. Here, to avoid ambiguity, we use NUMA to include cache-coherence unless we explicitly state otherwise.</p><p class="text" id="p0140">Caches are expensive to build and therefore significantly smaller than the memory: Only a fraction of the memory locations will fit in a cache at the same time. We would therefore like the cache to maintain values of the most highly used locations. This implies that when a location needs to be cached and the cache is full, it is necessary to <i>evict</i> a line, discarding it if it has not been modified, and writing it back to main memory if it has. A <i>replacement policy</i> determines which cache line to replace to make room for a given new location. If the replacement policy is free to replace any line, then we say the cache is <i>fully associative</i>. If, on the other hand, there is only one line that can be replaced, then we say the cache is <i>direct-mapped</i>. If we split the difference, allowing any line from a <i>set</i> of size <i>k</i> to be replaced to make room for a given line, then we say the cache is <i>k-way set associative</i>.</p><section><h3 class="h2hd" id="s0035"><a id="st0035"/>B.5.1 Coherence</h3><p class="textfl" id="p0145"><i>Sharing</i> (or, less politely, <i>memory contention</i>) occurs when one processor reads or writes a memory address that is cached by another. If both processors are reading the data without modifying it, then the data can be cached at both processors. If, however, one processor tries to update the shared cache line, then the other's copy must be <i>invalidated</i> to ensure that it does not read an out-of-date value. In its most general form, this problem is called <i>cache-coherence</i>. The literature contains a variety of very complex and clever cache coherence protocols. Here we review one of the most commonly used, called the <i>MESI</i> protocol (pronounced “messy”) after the names of possible cache line states. (Modern processors tend to use more complex protocols with additional states.) Here are the cache line states:</p><div><ul><li class="bulllist" id="u0025">•  <span aria-label="Page 525" epub:type="pagebreak" id="page_525" role="doc-pagebreak"/><i>Modified</i>: The line has been modified in the cache, and it must eventually be written back to main memory. No other processor has this line cached.</li><li class="bulllist" id="u0030">•  <i>Exclusive</i>: The line has not been modified, and no other processor has this line cached.</li><li class="bulllist" id="u0035">•  <i>Shared</i>: The line has not been modified, and other processors may have this line cached.</li><li class="bulllist" id="u0040">•  <i>Invalid</i>: The line does not contain meaningful data.</li></ul></div><p class="textfl"> We illustrate this protocol by a short example depicted in <a href="#f0030" id="cf0035">Fig. B.5</a>. For simplicity, we assume processors and memory are linked by a bus.</p><div class="pageavoid"><figure class="fig" id="f0030"><img alt="Image" height="279" src="images/B978012415950100032X/gr005.jpg" width="325"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure B.5</span> Example of the MESI cache-coherence protocol's state transitions. (a) Processor <i>A</i> reads data from address <i>a</i>, and stores the data in its cache in the <i>exclusive</i> state. (b) When processor <i>B</i> attempts to read from the same address, <i>A</i> detects the address conflict, and responds with the associated data. Now <i>a</i> is cached at both <i>A</i> and <i>B</i> in the <i>shared</i> state. (c) If <i>B</i> writes to the shared address <i>a</i>, it changes its state to <i>modified</i>, and broadcasts a message warning <i>A</i> (and any other processor that might have those data cached) to set its cache line state to <i>invalid</i>. (d) If <i>A</i> then reads from <i>a</i>, it broadcasts a request, and <i>B</i> responds by sending the modified data both to <i>A</i> and to the main memory, leaving both copies in the <i>shared</i> state.</div></figcaption></figure></div><p class="text" id="p0170">Processor <i>A</i> reads data from address <i>a</i>, and stores the data in its cache in the <i>exclusive</i> state. When processor <i>B</i> attempts to read from the same address, <i>A</i> detects the address conflict, and responds with the associated data. Now <i>a</i> is cached at both <i>A</i> and <i>B</i> in the <i>shared</i> state. If <i>B</i> writes to the shared address <i>a</i>, it changes its state to <i>modified</i>, and broadcasts a message warning <i>A</i> (and any other processor that might have those data cached) to set its cache line state to <i>invalid</i>. If <i>A</i> then reads from <i>a</i>, it broadcasts a request, and <i>B</i> responds by sending the modified data both to <i>A</i> and to the main memory, leaving both copies in the <i>shared</i> state.</p><p class="text" id="p0175"><span aria-label="Page 526" epub:type="pagebreak" id="page_526" role="doc-pagebreak"/><i>False sharing</i> occurs when processors that are accessing logically distinct data nevertheless conflict because the locations they are accessing lie on the same cache line. This observation illustrates a difficult trade-off: Large cache lines are good for locality, but they increase the likelihood of false sharing. The likelihood of false sharing can be reduced by ensuring that data objects that might be accessed concurrently by independent threads lie far enough apart in memory. For example, having multiple threads share a byte array invites false sharing, but having them share an array of double-precision integers is less dangerous.</p></section><section><h3 class="h2hd" id="s0040"><a id="st0040"/>B.5.2 Spinning</h3><p class="textfl" id="p0180">A processor is <i>spinning</i> if it is repeatedly testing some word in memory, waiting for another processor to change it. Depending on the architecture, spinning can have a dramatic effect on overall system performance.</p><p class="text" id="p0185">On an SMP architecture without caches, spinning is a very bad idea. Each time the processor reads the memory, it consumes bus bandwidth without accomplishing any useful work. Because the bus is a broadcast medium, these requests directed to memory may prevent other processors from making progress.</p><p class="text" id="p0190">On a NUMA architecture without caches, spinning may be acceptable if the address in question resides in the processor's local memory. Even though multiprocessor architectures without caches are rare, we will still ask, when we consider a synchronization protocol that involves spinning, whether it permits each processor to spin on its own local memory.</p><p class="text" id="p0195">On an SMP or NUMA architecture with caches, spinning consumes significantly fewer resources. The first time the processor reads the address, it takes a cache miss, and loads the contents of that address into a cache line. Thereafter, as long as those data remain unchanged, the processor simply rereads from its own cache, consuming no interconnect bandwidth, a process known as <i>local spinning</i>. When the cache state changes, the processor takes a single cache miss, observes that the data have changed, and stops spinning.</p></section></section><section><h2 class="h1hd" id="s0045"><a id="st0045"/>B.6 Cache-conscious programming, or the puzzle solved</h2><p class="textfl" id="p0200">We now know enough to explain why the <img alt="Image" height="9" src="images/B978012415950100032X/fx003.jpg" width="52"/> examined in Appendix <a href="#s0010" id="cf0040">B.1</a> outperforms the <img alt="Image" height="9" src="images/B978012415950100032X/fx002.jpg" width="46"/>. Each time the <img alt="Image" height="9" src="images/B978012415950100032X/fx002.jpg" width="46"/> applies <img alt="Image" height="11" src="images/B978012415950100032X/fx001.jpg" width="59"/><span class="hiddenClass"><mml:math><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="italic">true</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="14" src="images/B978012415950100032X/si1.png" style="vertical-align:middle" width="38"/></span> to the lock, it sends a message on the interconnect causing a substantial amount of traffic. In an SMP architecture, the resulting traffic may be enough to saturate the interconnect, delaying all threads, including a thread trying to release the lock, or even threads not contending for the lock. By contrast, while the lock is busy, the <img alt="Image" height="9" src="images/B978012415950100032X/fx003.jpg" width="52"/> spins, reading a locally cached copy of the lock, and producing no interconnect traffic, explaining its improved performance.</p><p class="text" id="p0205"><span aria-label="Page 527" epub:type="pagebreak" id="page_527" role="doc-pagebreak"/>The <img alt="Image" height="9" src="images/B978012415950100032X/fx003.jpg" width="52"/> is still far from ideal. When the lock is released, all its cached copies are invalidated, and all waiting threads call <img alt="Image" height="11" src="images/B978012415950100032X/fx001.jpg" width="59"/><span class="hiddenClass"><mml:math><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="italic">true</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:math></span><span><img alt="Image" height="14" src="images/B978012415950100032X/si1.png" style="vertical-align:middle" width="38"/></span>, resulting in a burst of traffic, smaller than that of the <img alt="Image" height="9" src="images/B978012415950100032X/fx002.jpg" width="46"/>, but nevertheless significant.</p><p class="text" id="p0210">We further discuss the interactions of caches with locking in Chapter <a href="B9780124159501000173.xhtml">7</a>. Here we consider some simple ways to structure data to avoid false sharing.</p><div><ul><li class="bulllist" id="u0045">•  Objects or fields that are accessed independently should be aligned and padded so that they end up on different cache lines.</li><li class="bulllist" id="u0050">•  Keep read-only data separate from data that are modified frequently. For example, consider a list whose structure is constant, but whose elements' value fields change frequently. To ensure that modifications do not slow down list traversals, one could align and pad the value fields so that each one fills up a cache line.</li><li class="bulllist" id="u0055">•  When possible, split an object into thread-local pieces. For example, a counter used for statistics could be split into an array of counters, one per thread, each one residing on a different cache line. Splitting the counter allows each thread to update its own replica, avoiding the invalidation traffic that would be incurred by having a single shared counter.</li><li class="bulllist" id="u0060">•  If a lock protects data that is frequently modified, then keep the lock and the data on distinct cache lines, so that threads trying to acquire the lock do not interfere with the lock-holder's access to the data.</li><li class="bulllist" id="u0065">•  If a lock protects data that are frequently uncontended, then try to keep the lock and the data on the same cache lines, so that acquiring the lock will also load some of the data into the cache.</li></ul></div><p class="textfl"><span aria-label="Page 528" epub:type="pagebreak" id="page_528" role="doc-pagebreak"/></p></section><section><h2 class="h1hd" id="s0050"><a id="st0050"/>B.7 Multicore and multithreaded architectures</h2><p class="textfl" id="p0240">In a <i>multicore</i> architecture, as in <a href="#f0035" id="cf0045">Fig. B.6</a>, multiple processors are placed on the same chip. Each processor on that chip typically has its own L1 cache, but they share a common L2 cache. Processors can communicate efficiently through the shared L2 cache, avoiding the need to go through memory, and to invoke the cumbersome cache-coherence protocol.</p><div class="pageavoid"><figure class="fig" id="f0035"><img alt="Image" height="181" src="images/B978012415950100032X/gr006.jpg" width="235"/><br/><figcaption><div class="figleg" title="figure"><span class="fignum">Figure B.6</span> A multicore SMP architecture. The <i>L</i>2 cache is on chip and shared by all processors while the memory is off-chip.</div></figcaption></figure></div><p class="text" id="p0245">In a <i>multithreaded</i> architecture, a single processor may execute two or more threads at once. Many modern processors have substantial internal parallelism. They can execute instructions out of order, or in parallel (e.g., keeping both fixed and floating-point units busy), or even execute instructions speculatively before branches or data have been computed. To keep hardware units busy, multithreaded processors can mix instructions from multiple streams.</p><p class="text" id="p0250">Modern processor architectures combine multicore with multithreading, where multiple individually multithreaded cores may reside on the same chip. The context switches on some multicore chips are inexpensive and are performed at a very fine granularity, essentially context switching on every instruction. Thus, multithreading serves to hide the high latency of accessing memory: Whenever a thread accesses memory, the processor allows another thread to execute.</p><section><h3 class="h2hd" id="s0055"><a id="st0055"/>B.7.1 Relaxed memory consistency</h3><p class="textfl" id="p0255">When a processor writes a value to memory, that value is kept in the cache and marked as <i>dirty</i>, meaning that it must eventually be written back to main memory. On most modern processors, write requests are not applied to memory when they are issued. Rather, they are collected in a hardware queue called a <i>write buffer</i> (or <i>store buffer</i>), and applied to memory together at a later time. A write buffer provides two benefits. First, it is often more efficient to issue several requests together, a phenomenon called <i>batching</i>. Second, if a thread writes to an address more than once, earlier requests can be discarded, saving a trip to memory, a phenomenon called <i>write absorption</i>.</p><p class="text" id="p0260">The use of write buffers has a very important consequence: The order in which reads and writes are issued to memory is not necessarily the order in which they occur in the program. For example, recall the flag principle of Chapter <a href="B9780124159501000094.xhtml">1</a>, which was crucial to the correctness of mutual exclusion: If two processors each first write their own flag and then read the other's flag location, then one of them will see the other's newly written flag value. With write buffers, this is no longer true: both threads may write, each in its respective write buffer, but these writes might not be applied to the shared memory until after each processor reads the other's flag location in memory. Thus, neither reads the other's flag.</p><p class="text" id="p0265">Compilers make matters even worse. They are good at optimizing performance on single-processor architectures. Often, this optimization involves <i>reordering</i> a thread's reads and writes to memory. Such reordering is invisible for single-threaded programs, but it can have unexpected consequences for multithreaded programs in which threads may observe the order in which writes occur. For example, if one thread fills a buffer with data and then sets an indicator to mark the buffer as full, then concurrent threads may see the indicator set before they see the new data, causing them to read stale values. The erroneous <i>double-checked locking</i> pattern described in Appendix <a href="B9780124159501000318.xhtml">A.3</a> is an example of a pitfall produced by unintuitive aspects of the Java memory model.</p><p class="text" id="p0270"><span aria-label="Page 529" epub:type="pagebreak" id="page_529" role="doc-pagebreak"/>Different architectures provide different guarantees about the extent to which memory reads and writes can be reordered. As a rule, it is better not to rely on such guarantees, and instead to use more expensive techniques, described in the following paragraph, to prevent such reordering.</p><p class="text" id="p0275">Every architecture provides a <i>memory barrier</i> instruction (sometimes called a <i>fence</i>), which forces writes to take place in the order they are issued, but at a price. A memory barrier flushes the write buffer, ensuring that all writes issued before the barrier become visible to the processor that issued the barrier. Memory barriers are often inserted automatically by atomic read–modify–write operations such as <img alt="Image" height="11" src="images/B978012415950100032X/fx001.jpg" width="59"/>(), or by standard concurrency libraries. Thus, explicit use of memory barriers is needed only when processors perform read–write instructions on shared variables outside of critical sections.</p><p class="text" id="p0280">On one hand, memory barriers are expensive (hundreds of cycles, maybe more), and should be used only when necessary. On the other hand, synchronization bugs can be very difficult to track down, so memory barriers should be used liberally, rather than relying on complex platform-specific guarantees about limits to memory instruction reordering.</p><p class="text" id="p0285">The Java language itself allows reads and writes to object fields to be reordered if they occur outside <img alt="Image" height="11" src="images/B978012415950100032X/fx005.jpg" width="79"/> methods or blocks. Java provides a <img alt="Image" height="9" src="images/B978012415950100032X/fx006.jpg" width="53"/> keyword that ensures that reads and writes to a <img alt="Image" height="9" src="images/B978012415950100032X/fx006.jpg" width="53"/> object field that occur outside <img alt="Image" height="11" src="images/B978012415950100032X/fx005.jpg" width="79"/> blocks or methods are not reordered. (The <span class="inlinecode">atomic</span> template provides similar guarantees for C++.) Using this keyword can be expensive, so it should be used only when necessary. We note that in principle, one could use volatile fields to make double-checked locking work correctly, but there would not be much point, since accessing volatile variables requires synchronization anyway.</p><p class="text" id="p0290">Here ends our primer on multiprocessor hardware. We now continue to discuss these architectural concepts in the context of specific data structures and algorithms. A pattern will emerge: The performance of multiprocessor programs is highly dependent on synergy with the underlying hardware.</p></section></section><section><h2 class="h1hd" id="s0060"><a id="st0060"/>B.8 Hardware synchronization instructions</h2><p class="textfl" id="p0295">As discussed in Chapter <a href="B9780124159501000148.xhtml">5</a>, any modern multiprocessor architecture must support powerful synchronization primitives to be universal, that is, to provide concurrent computation's equivalent of a universal Turing machine. It is therefore not surprising that implementations of Java and C++ rely on such specialized hardware instructions (also called hardware primitives) in implementing synchronization, from spin locks and monitors to the most complex lock-free data structures.</p><p class="text" id="p0300">Modern architectures typically provide one of two kinds of universal synchronization primitives. The <i>compare-and-swap</i> (CAS) instruction is supported in architectures by AMD, Intel, and Oracle. It takes three arguments: an address <i>a</i> in memory, an <i>expected</i> value <i>e</i>, and an <i>update</i> value <i>v</i>, and returns a Boolean. It <i>atomically</i> executes the following: If the memory at address <i>a</i> contains the expected value <i>e</i>, <span aria-label="Page 530" epub:type="pagebreak" id="page_530" role="doc-pagebreak"/>write the update value <i>v</i> to that address and return <i>true</i>, otherwise leave the memory unchanged and return <i>false</i>.</p><p class="text" id="p0305">On Intel and AMD architectures, CAS is called <span class="inlinecode">CMPXCHG</span>; on Oracle SPARC systems, it is called <span class="inlinecode">CAS</span>.<sup><a epub:type="noteref" href="#fn001" id="cf0050" role="doc-noteref">1</a></sup> Java's <span class="inlinecode">java.util.concurrent.atomic</span> library provides atomic Boolean, integer, and reference classes that implement CAS by a <img alt="Image" height="12" src="images/B978012415950100032X/fx007.jpg" width="97"/> method. (Because most of our examples are in Java, we often refer to <img alt="Image" height="12" src="images/B978012415950100032X/fx007.jpg" width="97"/> instead of CAS.) The <img alt="Image" height="8" src="images/B978012415950100032X/fx008.jpg" width="35"/> template of C++ provides the same functionality.</p><p class="text" id="p0310">The CAS instruction has one pitfall. Perhaps the most common use of CAS is the following. An application reads value <i>a</i> from a given memory address, and computes a new value <i>c</i> for that location. It intends to store <i>c</i>, but only if the value <i>a</i> at the address has not changed since it was read. One might think that applying a CAS with expected value <i>a</i> and update value <i>c</i> would accomplish this goal. There is a problem: A thread could have overwritten the value <i>a</i> with another value <i>b</i>, and later written <i>a</i> again to the address. The CAS will replace <i>a</i> with <i>c</i>, but the application may not have done what it was intended to do (for example, if the address stores a pointer, the new value <i>a</i> may be the address of a recycled object). This problem is known as the <i>ABA problem</i>, and discussed in detail in Chapter <a href="B9780124159501000203.xhtml">10</a>.</p><p class="text" id="p0315">The other hardware synchronization primitive is a pair of instructions: <i>load-linked</i> and <i>store-conditional</i> (LL/SC). The LL instruction reads from an address <i>a</i>, and a later SC instruction attempts to store a new value at that address. The SC instruction succeeds if the contents of <i>a</i> have not changed since that thread issued the earlier LL instruction. It fails if the contents of <i>a</i> have changed in the interval.</p><p class="text" id="p0320">LL/SC instructions are supported by a number of architectures: Alpha AXP (<span class="inlinecode">ldl_l/stl_c</span>), IBM PowerPC (<span class="inlinecode">lwarx/stwcx</span>) MIPS <span class="inlinecode">ll/sc</span>, and ARM (<span class="inlinecode">ldrex/strex</span>). LL/SC does not suffer from the ABA problem, but in practice there are often severe restrictions on what a thread can do between an LL and the matching SC. A context switch, another LL, or another load or store instruction may cause the SC to fail.</p><p class="text" id="p0325">It is a good idea to use atomic fields and their associated methods sparingly because they are often based on CAS or LL/SC. A CAS or LL/SC instruction takes significantly more cycles to complete than a load or store: It includes a memory barrier and prevents out-of-order execution and various compiler optimizations. The precise cost depends on many factors, and varies not only from one architecture to the next, but also from one application of the instruction to the next within the same architecture. It suffices to say that CAS or LL/SC can be an order of magnitude slower than a simple load or store.<span aria-label="Page 531" epub:type="pagebreak" id="page_531" role="doc-pagebreak"/></p></section><section><h2 class="h1hd" id="s0065"><a id="st0065"/>B.9 Appendix notes</h2><p class="textfl" id="p0330">Tom Anderson <a epub:type="noteref" href="#br0060" id="cf0055" role="doc-noteref">[12]</a> did the classic experiments on spin locks. John Hennessy and David Patterson <a epub:type="noteref" href="#br0315" id="cf0060" role="doc-noteref">[63]</a> give a comprehensive treatment of computer architecture. The MESI protocol is used by Intel's Pentium processor <a epub:type="noteref" href="#br0415" id="cf0065" role="doc-noteref">[83]</a>. The tips on cache-conscious programming are adapted from Benjamin Gamsa, Orran Krieger, Eric Parsons, and Michael Stumm <a epub:type="noteref" href="#br0245" id="cf0070" role="doc-noteref">[49]</a>. Sarita Adve and Karosh Gharachorloo <a epub:type="noteref" href="#br0005" id="cf0075" role="doc-noteref">[1]</a> give an excellent survey of memory consistency models.</p></section><section><h2 class="h1hd" id="s0070"><a id="st0070"/>B.10 Exercises</h2><p class="textfl" id="p0335"/><div class="boxg1" id="enun0010"><p class="b1num">Exercise B.1 </p><div><p class="b1textfl" id="p0340">Thread <i>A</i> must wait for a thread on another processor to change a flag bit in memory. The scheduler can either allow <i>A</i> to spin, repeatedly retesting the flag, or it can deschedule <i>A</i>, allowing some other thread to run. Suppose it takes a total of 10 milliseconds for the operating system to switch a processor from one thread to another. If the operating system deschedules thread <i>A</i> and immediately reschedules it, then it wastes 20 milliseconds. If, instead, <i>A</i> starts spinning at time <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="11" src="images/B978012415950100032X/si2.png" style="vertical-align:middle" width="13"/></span>, and the flag changes at <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="11" src="images/B978012415950100032X/si3.png" style="vertical-align:middle" width="12"/></span>, then the operating system will have wasted <span class="hiddenClass"><mml:math><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></span><span><img alt="Image" height="11" src="images/B978012415950100032X/si4.png" style="vertical-align:middle" width="43"/></span> time doing unproductive work.</p><p class="b1text" id="p0345">A <i>prescient</i> scheduler is one that can predict the future. If it foresees that the flag will change in less than 20 milliseconds, it makes sense to have <i>A</i> spin, wasting less than 20 milliseconds, because descheduling and rescheduling <i>A</i> wastes 20 milliseconds. If, on the other hand, it takes more than 20 milliseconds for the flag to change, it makes sense to replace <i>A</i> with another thread, wasting no more than 20 milliseconds.</p><p class="b1text" id="p0350">Your assignment is to implement a scheduler that never wastes more than <i>twice</i> the time a prescient scheduler would have wasted under the same circumstances.</p></div></div><p class="textfl"/><p class="text" id="p0355"/><div class="boxg1" id="enun0015"><p class="b1num">Exercise B.2 </p><div><p class="b1textfl" id="p0360">Imagine you are a lawyer, paid to make the best case you can for a particular point of view. How would you argue the following claim: “If context switches took negligible time, then processors would not need caches, at least for applications that encompass large numbers of threads”?</p><p class="b1text" id="p0365">Extra credit: Critique your argument.</p></div></div><p class="textfl"/><p class="text" id="p0370"/><div class="boxg1" id="enun0020"><p class="b1num">Exercise B.3 </p><div><p class="b1textfl" id="p0375">Consider a direct-mapped cache with 16 cache lines, indexed 0 to 15, where each cache line encompasses 32 words.</p><div><ul><li class="b1bulllist" id="u0070">•  Explain how to map an address <i>a</i> to a cache line in terms of bit shifting and masking operations. Assume for this question that addresses refer to words, not bytes: address 7 refers to the eighth word in memory.</li><li class="b1bulllist" id="u0075">•  Compute the best and worst possible hit ratios for a program that loops four times through an array of 64 words.</li><li class="b1bulllist" id="u0080">•  Compute the best and worst possible hit ratios for a program that loops four times through an array of 512 words.</li></ul></div><p class="b1textfl"/></div></div><p class="textfl"/><p class="text" id="p0395"/><div class="boxg1" id="enun0025"><p class="b1num">Exercise B.4 </p><div><p class="b1textfl" id="p0400">Consider a direct-mapped cache with 16 cache lines, indexed 0 to 15, where each cache line encompasses 32 words.</p><p class="b1text" id="p0405">Consider a two-dimensional <span class="hiddenClass"><mml:math><mml:mn>32</mml:mn><mml:mo>×</mml:mo><mml:mn>32</mml:mn></mml:math></span><span><img alt="Image" height="11" src="images/B978012415950100032X/si5.png" style="vertical-align:middle" width="53"/></span> array <i>a</i> of words. This array is laid out in memory so that <span class="hiddenClass"><mml:math><mml:mi>a</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:math></span><span><img alt="Image" height="14" src="images/B978012415950100032X/si6.png" style="vertical-align:middle" width="45"/></span> is next to <span class="hiddenClass"><mml:math><mml:mi>a</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:math></span><span><img alt="Image" height="14" src="images/B978012415950100032X/si7.png" style="vertical-align:middle" width="45"/></span>, and so on. Assume the cache is initially empty, but that <span class="hiddenClass"><mml:math><mml:mi>a</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:math></span><span><img alt="Image" height="14" src="images/B978012415950100032X/si6.png" style="vertical-align:middle" width="45"/></span> maps to the first word of cache line 0.</p><p class="b1text" id="p0410"><span aria-label="Page 532" epub:type="pagebreak" id="page_532" role="doc-pagebreak"/>Consider the following <i>column-first</i> traversal:</p><div class="pageavoid"><figure class="fig" id="f0040"><img alt="Image" class="img" height="92" src="images/B978012415950100032X/fx009.jpg" width="275"/></figure></div><p class="textfl"> and the following <i>row-first</i> traversal:</p><div class="pageavoid"><figure class="fig" id="f0045"><img alt="Image" class="img" height="92" src="images/B978012415950100032X/fx010.jpg" width="275"/></figure></div><p class="textfl"> Compare the number of cache misses produced by the two traversals, assuming the oldest cache line is evicted first.</p></div></div><p class="textfl"/><p class="text" id="p0415"/><div class="boxg1" id="enun0030"><p class="b1num">Exercise B.5 </p><div><p class="b1textfl" id="p0420">In the MESI cache-coherence protocol, what is the advantage of distinguishing between exclusive and modified modes?</p><p class="b1text" id="p0425">What is the advantage of distinguishing between exclusive and shared modes?</p></div></div><p class="textfl"/><p class="text" id="p0430"/><div class="boxg1" id="enun0035"><p class="b1num">Exercise B.6 </p><div><p class="b1textfl" id="p0435">Implement the test-and-set and test-and-test-and-set locks shown in <a href="#f0010" id="cs0010">Figs. B.1</a> and <a href="#f0015">B.2</a>, test their relative performance on a multiprocessor, and analyze the results.</p></div></div><p class="textfl"/></section><footer><section epub:type="bibliography" role="doc-bibliography"><div id="bl0140"><h2 class="reftitle" id="st0075">Bibliography</h2><p class="reflist" epub:type="biblioentry footnote" id="br0005" role="doc-biblioentry">[1] Sarita Adve, Kourosh Gharachorloo,  Shared memory consistency models: a tutorial,   <cite><i>Computer</i></cite> 1996;29(12):66–76.</p><p class="reflist1" epub:type="biblioentry footnote" id="br0060" role="doc-biblioentry">[12] Thomas E. Anderson,  The performance of spin lock alternatives for shared-memory multiprocessors,   <cite><i>IEEE Transactions on Parallel and Distributed Systems</i></cite> 1990;1(1):6–16.</p><p class="reflist1" epub:type="biblioentry footnote" id="br0245" role="doc-biblioentry">[49] B. Gamsa, O. Kreiger, E.W. Parsons, M. Stumm,  <i>Performance issues for multiprocessor operating systems</i>. [Technical report] Computer Systems Research Institute, University of Toronto; 1995.</p><p class="reflist1" epub:type="biblioentry footnote" id="br0315" role="doc-biblioentry">[63] J.L. Hennessy, D.A. Patterson,  <i>Computer Architecture: A Quantitative Approach</i>.  Morgan Kaufmann Publishers; 1995.</p><p class="reflist1" epub:type="biblioentry footnote" id="br0415" role="doc-biblioentry">[83]  Intel Corporation,  <i>Pentium Processor User's Manual</i>.  Intel Books; 1993.</p></div></section><section epub:type="rearnotes"><div class="ftnote"><hr/><p class="ftnote1" epub:type="footnote" id="fn001" role="doc-footnote"><sup><a epub:type="noteref" href="#cf0050" role="doc-noteref">1 </a></sup> <a id="np0010"/>“Instead of a Boolean, CAS on SPARC returns the location's prior value, which can be used to retry an unsuccessful CAS. CMPXCHG on Intel's Pentium effectively returns both a Boolean and the prior value.”</p></div></section></footer></section></body></html>