<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Linux Kernel Development, Third Edition</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<div id="filepos1066499" style="height:0pt"></div><h2 class="calibre_4" id="calibre_pb_71"><span class="bold">16. The Page Cache and Page Writeback</span></h2><div class="calibre_5"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos1066678"> </div>
<p class="calibre_2">The Linux kernel implements a disk cache called the <em class="calibre4">page cache</em>. The goal of this cache is to minimize disk I/O by storing data in physical memory that would otherwise require disk access. This chapter deals with the page cache and the process by which changes to the page cache are propagated back to disk, which is called <em class="calibre4">page writeback</em>.</p><div class="calibre_3"> </div>
<p class="calibre_2">Two factors comingle to make disk caches a critical component of any modern operating system. First, disk access is several orders of magnitude slower than memory access—milliseconds versus nanoseconds. Accessing data from memory rather than the disk is much faster, and accessing data from the processor’s L1 or L2 cache is faster still. Second, data accessed once will, with a high likelihood, find itself accessed again in the near future. This principle—that access to a particular piece of data tends to be clustered in time—is called <em class="calibre4">temporal locality</em>, which ensures that if data is cached on its first access, there is a high probability of a cache hit (access to data in the cache) in the near future. Given that memory is so much faster than disk, coupled with the fact that once-used is likely twice-used data, an in-memory cache of the disk is a large performance win.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos1068099"> </div>
<h3 class="calibre_21"><span class="bold">Approaches to Caching</span></h3><div class="calibre_22"> </div>
<p class="calibre_2">The page cache consists of physical pages in RAM, the contents of which correspond to physical blocks on a disk. The size of the page cache is dynamic; it can grow to consume any free memory and shrink to relieve memory pressure. We call the storage device being cached the <em class="calibre4">backing store</em> because the disk stands behind the cache as the source of the canonical version of any cached data. Whenever the kernel begins a read operation—for example, when a process issues the <code class="calibre6"><span class="calibre7">read()</span></code> system call—it first checks if the requisite data is in the page cache. If it is, the kernel can forgo accessing the disk and read the data directly out of RAM. This is called a <em class="calibre4">cache hit</em>. If the data is not in the cache, called a <em class="calibre4">cache miss</em>, the kernel must schedule block I/O operations to read the data off the disk. After the data is read off the disk, the kernel populates the page cache with the data so that any subsequent reads can occur out of the cache. Entire files need not be cached; the page <a id="filepos1069281"></a>cache can hold some files in their entirety while storing only a page or two of other files. What is cached depends on what has been accessed.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos1069474"> </div>
<h4 class="calibre_27"><span class="calibre3">Write Caching</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">This explains how data ends up in the page cache via read operations, but what happens when a process writes to disk, for example via the <code class="calibre6"><span class="calibre7">write()</span></code> system call? Generally speaking, caches can implement one of three different strategies. In the first strategy, called <em class="calibre4">no-write</em>, the cache simply does not cache write operations. A write operation against a piece of data stored in the cache would be written directly to disk, invalidating the cached data and requiring it to be read from disk again on any subsequent read. Caches rarely employ this strategy because it not only fails to cache write operations, but it also makes them costly by invalidating the cache.</p><div class="calibre_3"> </div>
<p class="calibre_2">In the second strategy, a write operation would automatically update both the in-memory cache and the on-disk file. This approach is called a <em class="calibre4">write-through cache</em> because write operations immediately go <em class="calibre4">through</em> the cache to the disk. This approach has the benefit of keeping the cache <em class="calibre4">coherent</em>—synchronized and valid for the backing store—without needing to <em class="calibre4">invalidate</em> it. It is also simple.</p><div class="calibre_3"> </div>
<p class="calibre_2">The third strategy, employed by Linux, is called <em class="calibre4">write-back</em>.<sup class="calibre8"><a id="filepos1070910" href="#filepos1071998">1</a></sup> In a write-back cache, processes perform write operations directly into the page cache. The backing store is not immediately or directly updated. Instead, the written-to pages in the page cache are marked as <em class="calibre4">dirty</em> and are added to a <em class="calibre4">dirty list</em>. Periodically, pages in the dirty list are written back to disk in a process called <em class="calibre4">writeback</em>, bringing the on-disk copy in line with the in-memory cache. The pages are then marked as no longer dirty. The term “dirty” can be confusing because what is actually dirty is not the data in the page cache (which is up to date) but the data on disk (which is out of date). A better term would be <em class="calibre4">unsynchronized</em>. Nonetheless, we say the cache contents, not the invalid disk contents, are dirty. A write-back is generally considered superior to a write-through strategy because by deferring the writes to disk, they can be coalesced and performed in bulk at a later time. The downside is complexity.</p><div class="calibre_3"> </div>
<p class="calibre_2"><sup class="calibre8"><a id="filepos1071998" href="#filepos1070910">1</a></sup>
<em class="calibre4">Some books or operating systems call such a strategy a</em> copy-back <em class="calibre4">or</em> write-behind <em class="calibre4">cache. All three names are synonyms. Linux and other Unix systems use the noun “write-back” to refer to the caching strategy and the verb “writeback” to refer to the action of writing cached data back to the backing store. This book follows that usage.</em></p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos1072486"> </div>
<h4 class="calibre_27"><span class="calibre3">Cache Eviction</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">The final piece to caching is the process by which data is removed from the cache, either to make room for more relevant cache entries or to shrink the cache to make available more RAM for other uses. This process, and the strategy that decides what to remove, is called <em class="calibre4">cache eviction</em>. Linux’s cache eviction works by selecting <em class="calibre4">clean</em> (not dirty) pages and <a id="filepos1072983"></a>simply replacing them with something else. If insufficient clean pages are in the cache, the kernel forces a writeback to make more clean pages available. The hard part is deciding <em class="calibre4">what</em> to evict. The ideal eviction strategy evicts the pages least likely to be used in the future. Of course, knowing what pages are least likely to be accessed requires knowing the future, which is why this hopeful strategy is often referred to as the <em class="calibre4">clairvoyant algorithm</em>. Such a strategy is ideal, but impossible to implement.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos1073563"> </div>
<h5 class="calibre_29"><span class="calibre3">Least Recently Used</span></h5><div class="calibre_24"> </div>
<p class="calibre_2">Cache eviction strategies attempt to approximate the clairvoyant algorithm with what information they have access to. One of the more successful algorithms, particularly for general-purpose page caches, is called <em class="calibre4">least recently used</em>, or <em class="calibre4">LRU</em>. An LRU eviction strategy requires keeping track of when each page is accessed (or at least sorting a list of pages by access time) and evicting the pages with the oldest timestamp (or at the start of the sorted list). This strategy works well because the longer a piece of cached data sits idle, the less likely it is to be accessed in the near future. Least recently used is a great approximation of most likely to be used. However, one particular failure of the LRU strategy is that many files are accessed once and then never again. Putting them at the top of the LRU list is thus not optimal. Of course, as before, the kernel has no way of knowing that a file is going to be accessed only once. But it does know how many times it has been accessed in the past.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos1074759"> </div>
<h5 class="calibre_29"><span class="calibre3">The Two-List Strategy</span></h5><div class="calibre_24"> </div>
<p class="calibre_2">Linux, therefore, implements a modified version of LRU, called the <em class="calibre4">two-list strategy</em>. Instead of maintaining one list, the LRU list, Linux keeps two lists: the <em class="calibre4">active list</em> and the <em class="calibre4">inactive list</em>. Pages on the active list are considered “hot” and are not available for eviction. Pages on the inactive list are available for cache eviction. Pages are placed on the active list only when they are accessed <em class="calibre4">while already residing</em> on the inactive list. Both lists are maintained in a pseudo-LRU manner: Items are added to the tail and removed from the head, as with a queue. The lists are kept in balance: If the active list becomes much larger than the inactive list, items from the active list’s head are moved back to the inactive list, making them available for eviction. The two-list strategy solves the only-used-once failure in a classic LRU and also enables simpler, pseudo-LRU semantics to perform well. This two-list approach is also known as <em class="calibre4">LRU/2</em>; it can be generalized to n-lists, called <em class="calibre4">LRU/n</em>.</p><div class="calibre_3"> </div>
<p class="calibre_2">We now know how the page cache is populated (via reads and writes), how it is synchronized in the face of writes (via writeback), and how old data is evicted to make way for new data (via a two-list strategy). Let’s now consider a real-world scenario to see how the page cache benefits the system. Assume you are working on a large software project—the Linux kernel, perhaps—and have many source files open. As you open and read source code, the files are stored in the page cache. Jumping around from file to file is instantaneous as the data is cached. As you edit the files, saving them appears instantaneous as well because the writes only need to go to memory, not the disk. When you compile the project, the cached files enable the compilation to proceed with far fewer disk accesses, and thus much more quickly. If the entire source tree is too big to fit in <a id="filepos1076890"></a>memory, some of it must be evicted—and thanks to the two-list strategy, any evicted files will be on the inactive list and likely not one of the source files you are directly editing. Later, hopefully when you are not compiling, the kernel will perform page writeback and update the on-disk copies of the source files with any changes you made. This caching results in a dramatic increase in system performance. To see the difference, compare how long it takes to compile your large software project when “cache cold”—say, fresh off a reboot—versus “cache warm.”</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos1077545"> </div>
<h3 class="calibre_21"><span class="bold">The Linux Page Cache</span></h3><div class="calibre_22"> </div>
<p class="calibre_2">The page cache, as its name suggests, is a cache of pages in RAM. The pages originate from reads and writes of regular filesystem files, block device files, and memory-mapped files. In this manner, the page cache contains chunks of recently accessed files. During a page I/O operation, such as <code class="calibre6"><span class="calibre7">read()</span></code>,<sup class="calibre8"><a id="filepos1078009" href="#filepos1078451">2</a></sup> the kernel checks whether the data resides in the page cache. If the data is in the page cache, the kernel can quickly return the requested page from memory rather than read the data off the comparatively slow disk. In the rest of this chapter, we explore the data structures and kernel facilities that maintain Linux’s page cache.</p><div class="calibre_3"> </div>
<p class="calibre_2"><sup class="calibre8"><a id="filepos1078451" href="#filepos1078009">2</a></sup>
<em class="calibre4">As you saw in <a href="index_split_022.html#filepos870716">Chapter 13</a>, “The Virtual Filesystem,” it is not the <code class="calibre6"><span class="calibre7">read()</span></code> and <code class="calibre6"><span class="calibre7">write()</span></code> system calls that perform the actual page I/O operation, but the filesystem-specific methods specified by <code class="calibre6"><span class="calibre7">file-&gt;f_op-&gt;read()</span></code> and <code class="calibre6"><span class="calibre7">file-&gt;f_op-&gt;write()</span></code>.</em></p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos1078989"> </div>
<h4 class="calibre_27"><span class="calibre3">The <code class="calibre6"><span class="calibre7">address_space</span></code> Object</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">A page in the page cache can consist of multiple noncontiguous physical disk blocks.<sup class="calibre8"><a id="filepos1079239" href="#filepos1079664">3</a></sup> Checking the page cache to see whether certain data has been cached is made difficult because of this noncontiguous layout of the blocks that constitute each page. Therefore, it is not possible to index the data in the page cache using only a device name and block number, which would otherwise be the simplest solution.</p><div class="calibre_3"> </div>
<p class="calibre_2"><sup class="calibre8"><a id="filepos1079664" href="#filepos1079239">3</a></sup>
<em class="calibre4">For example, a physical page is 4KB in size on the x86 architecture, whereas a disk block on many filesystems can be as small as 512 bytes. Therefore, 8 blocks might fit in a single page. The blocks need not be contiguous because the files might be laid out all over the disk.</em></p><div class="calibre_3"> </div>
<p class="calibre_2">Furthermore, the Linux page cache is quite general in what pages it can cache. Indeed, the original page cache introduced in System V Release 4 cached only filesystem data. Consequently, the SVR4 page cache used its equivalent of the inode object, called <code class="calibre6"><span class="calibre7">struct vnode</span></code>, to manage the page cache. The Linux page cache aims to cache <em class="calibre4">any</em> page-based object, which includes many forms of files and memory mappings.</p><div class="calibre_3"> </div>
<p class="calibre_2">Although the Linux page cache could work by extending the <code class="calibre6"><span class="calibre7">inode</span></code> structure (discussed in <a href="index_split_022.html#filepos870716">Chapter 13</a>, “The Virtual Filesystem”) to support page I/O operations, such a <a id="filepos1080795"></a>choice would confine the page cache to files. To maintain a generic page cache—one not tied to physical files or the <code class="calibre6"><span class="calibre7">inode</span></code> structure—the Linux page cache uses a new object to manage entries in the cache and page I/O operations. That object is the <code class="calibre6"><span class="calibre7">address_space</span></code> structure. Think of <code class="calibre6"><span class="calibre7">address_space</span></code> as the physical analogue to the virtual <code class="calibre6"><span class="calibre7">vm_area_struct</span></code> introduced in <a href="index_split_024.html#filepos1011741">Chapter 15</a>, “The Process Address Space.” While a single file may be represented by 10 <code class="calibre6"><span class="calibre7">vm_area_struct</span></code> structures (if, say, five processes each <code class="calibre6"><span class="calibre7">mmap()</span></code> it twice), the file has only one <code class="calibre6"><span class="calibre7">address_space</span></code> structure—just as the file may have many virtual addresses but exist only once in physical memory. Like much else in the Linux kernel, <code class="calibre6"><span class="calibre7">address_space</span></code> is misnamed. A better name is perhaps <code class="calibre6"><span class="calibre7">page_cache_entity</span></code> or <code class="calibre6"><span class="calibre7">physical_pages_of_a_file</span></code>.</p><div class="calibre_3"> </div>
<p class="calibre_2">The <code class="calibre6"><span class="calibre7">address_space</span></code> structure is defined in <code class="calibre6"><span class="calibre7">&lt;linux/fs.h&gt;</span></code>:</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00227.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2">The <code class="calibre6"><span class="calibre7">i_mmap</span></code> field is a priority search tree of all shared and private mappings in this address space. A priority search tree is a clever mix of heaps and radix trees.<sup class="calibre8"><a id="filepos1082521" href="#filepos1083043">4</a></sup> Recall from earlier that while a cached file is associated with one <code class="calibre6"><span class="calibre7">address_space</span></code> structure, it can have many <code class="calibre6"><span class="calibre7">vm_area_struct</span></code> structures—a one-to-many mapping from the physical pages to many virtual pages. The <code class="calibre6"><span class="calibre7">i_mmap</span></code> field allows the kernel to efficiently find the mappings associated with this cached file.</p><div class="calibre_3"> </div>
<p class="calibre_2"><sup class="calibre8"><a id="filepos1083043" href="#filepos1082521">4</a></sup>
<em class="calibre4">The kernel implementation is based on the radix priority search tree proposed by Edward M. McCreight in SIAM Journal of Computing, volume 14, number 2, pages 257–276, May 1985.</em></p><div class="calibre_3"> </div>
<p class="calibre_2">There are a total of <code class="calibre6"><span class="calibre7">nrpages</span></code> in the address space.</p><div class="calibre_3"> </div>
<p class="calibre_2">The <code class="calibre6"><span class="calibre7">address_space</span></code> is associated with some kernel object. Normally, this is an inode. If so, the <code class="calibre6"><span class="calibre7">host</span></code> field points to the associated inode. The <code class="calibre6"><span class="calibre7">host</span></code> field is <code class="calibre6"><span class="calibre7">NULL</span></code> if the associated <a id="filepos1083794"></a>object is not an inode—for example, if the <code class="calibre6"><span class="calibre7">address_space</span></code> is associated with the swapper.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos1083974"> </div>
<h4 class="calibre_27"><span class="calibre3"><code class="calibre6"><span class="calibre7">address_space</span></code> Operations</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">The <code class="calibre6"><span class="calibre7">a_ops</span></code> field points to the address space operations table, in the same manner as the VFS objects and their operations tables. The operations table is represented by <code class="calibre6"><span class="calibre7">struct address_space_operations</span></code> and is also defined in <code class="calibre6"><span class="calibre7">&lt;linux/fs.h&gt;</span></code>:</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00228.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2">These function pointers point at the functions that implement page I/O for this cached object. Each backing store describes how it interacts with the page cache via its own <code class="calibre6"><span class="calibre7">address_space_operations</span></code>. For example, the ext3 filesystem defines its operations in <code class="calibre6"><span class="calibre7">fs/ext3/inode.c</span></code>. Thus, these are the functions that manage the page cache, including the most common: reading pages into the cache and updated data in the cache. Thus, the <code class="calibre6"><span class="calibre7">readpage()</span></code> and <code class="calibre6"><span class="calibre7">writepage()</span></code>methods are most important. Let’s look at the steps <a id="filepos1085275"></a>involved in each, starting with a page read operation. First, the Linux kernel attempts to find the request data in the page cache. The <code class="calibre6"><span class="calibre7">find_get_page()</span></code> method is used to perform this check; it is passed an <code class="calibre6"><span class="calibre7">address_space</span></code> and page offset. These values search the page cache for the desired data:</p><div class="calibre_3"> </div>
<p class="calibre_28"><tt class="calibre6"><span class="calibre12">page = find_get_page(mapping, index);</span></tt></p><div class="calibre_22"> </div>
<p class="calibre_2">Here, <code class="calibre6"><span class="calibre7">mapping</span></code> is the given <code class="calibre6"><span class="calibre7">address_space</span></code> and <code class="calibre6"><span class="calibre7">index</span></code> is the desired offset into the file, in pages. (Yes, calling the <code class="calibre6"><span class="calibre7">address_space</span></code> structure <code class="calibre6"><span class="calibre7">mapping</span></code> just furthers the naming confusion. I’m replicating the kernel’s naming for consistency, but I do not condone it.) If the page does not exist in the cache, <code class="calibre6"><span class="calibre7">find_get_page()</span></code> returns <code class="calibre6"><span class="calibre7">NULL</span></code> and a new page is allocated and added to the page cache:</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00229.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2">Finally, the requested data can be read from disk, added to the page cache, and returned to the user:</p><div class="calibre_3"> </div>
<p class="calibre_28"><tt class="calibre6"><span class="calibre12">error = mapping-&gt;a_ops-&gt;readpage(file, page);</span></tt></p><div class="calibre_22"> </div>
<p class="calibre_2">Write operations are a bit different. For file mappings, whenever a page is modified, the VM simply calls</p><div class="calibre_3"> </div>
<p class="calibre_28"><tt class="calibre6"><span class="calibre12">SetPageDirty(page);</span></tt></p><div class="calibre_22"> </div>
<p class="calibre_2">The kernel later writes the page out via the <code class="calibre6"><span class="calibre7">writepage()</span></code> method. Write operations on specific files are more complicated. The generic write path in <code class="calibre6"><span class="calibre7">mm/filemap.c</span></code> performs the following steps:</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00230.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2">First, the page cache is searched for the desired page. If it is not in the cache, an entry is allocated and added. Next, the kernel sets up the write request and the data is copied from user-space into a kernel buffer. Finally, the data is written to disk.</p><div class="calibre_3"> </div>
<p class="calibre_2">Because the previous steps are performed during all page I/O operations, all page I/O is guaranteed to go through the page cache. Consequently, the kernel attempts to satisfy <a id="filepos1088080"></a>all read requests from the page cache. If this fails, the page is read in from disk and added to the page cache. For write operations, the page cache acts as a staging ground for the writes. Therefore, all written pages are also added to the page cache.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos1088384"> </div>
<h4 class="calibre_27"><span class="calibre3">Radix Tree</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">Because the kernel must check for the existence of a page in the page cache before initiating any page I/O, such a check must be quick. Otherwise, the overhead of searching and checking the page cache could nullify any benefits the cache might provide. (At least if the cache hit rate is low—the overhead would have to be awful to cancel out the benefit of retrieving the data from memory in lieu of disk.)</p><div class="calibre_3"> </div>
<p class="calibre_2">As you saw in the previous section, the page cache is searched via the <code class="calibre6"><span class="calibre7">address_space</span></code> object plus an offset value. Each <code class="calibre6"><span class="calibre7">address_space</span></code> has a unique radix tree stored as <code class="calibre6"><span class="calibre7">page_tree</span></code>. A radix tree is a type of binary tree. The radix tree enables quick searching for the desired page, given only the file offset. Page cache searching functions such as <code class="calibre6"><span class="calibre7">find_get_page()</span></code> call <code class="calibre6"><span class="calibre7">radix_tree_lookup()</span></code>, which performs a search on the given tree for the given object.</p><div class="calibre_3"> </div>
<p class="calibre_2">The core radix tree code is available in generic form in <code class="calibre6"><span class="calibre7">lib/radix-tree.c</span></code>. Users of the radix tree need to include <code class="calibre6"><span class="calibre7">&lt;linux/radix-tree.h&gt;</span></code>.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos1089898"> </div>
<h4 class="calibre_27"><span class="calibre3">The Old Page Hash Table</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">Prior to the 2.6 kernel, the page cache was not searched via the radix tree. Instead, a global hash was maintained over all the pages in the system. The hash returned a doubly linked list of entries that hash to the same given value. If the desired page were in the cache, one of the items in the list was the corresponding page. Otherwise, the page was not in the page cache and the hash function returned <code class="calibre6"><span class="calibre7">NULL</span></code>.</p><div class="calibre_3"> </div>
<p class="calibre_2">The global hash had four primary problems:</p><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• A single global lock protected the hash. Lock contention was quite high on even moderately sized machines, and performance suffered as a result.</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• The hash was larger than necessary because it contained all the pages in the page cache, whereas only pages pertaining to the current file were relevant.</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• Performance when the hash lookup failed (that is, the given page was not in the page cache) was slower than desired, particularly because it was necessary to walk the chains off of a given hash value.</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• The hash consumed more memory than other possible solutions.</p></blockquote><div class="calibre_3"> </div>
<p class="calibre_2">The introduction of the radix tree-based page cache in 2.6 solved these issues.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos1091645"> </div>
<h3 class="calibre_21"><span class="bold">The Buffer Cache</span></h3><div class="calibre_22"> </div>
<p class="calibre_2">Individual disk blocks also tie into the page cache, by way of block I/O buffers. Recall from <a href="index_split_023.html#filepos961208">Chapter 14</a>, “The Block I/O Layer,” that a buffer is the in-memory representation of a single physical disk block. Buffers act as descriptors that map pages in memory to <a id="filepos1092066"></a>disk blocks; thus, the page cache also reduces disk access during block I/O operations by both caching disk blocks and buffering block I/O operations until later. This caching is often referred to as the <em class="calibre4">buffer cache</em>, although as implemented it is not a separate cache but is part of the page cache.</p><div class="calibre_3"> </div>
<p class="calibre_2">Block I/O operations manipulate a single disk block at a time. A common block I/O operation is reading and writing inodes. The kernel provides the <code class="calibre6"><span class="calibre7">bread()</span></code> function to perform a low-level read of a single block from disk. Via buffers, disk blocks are mapped to their associated in-memory pages and cached in the page cache.</p><div class="calibre_3"> </div>
<p class="calibre_2">The buffer and page caches were not always unified; doing so was a major feature of the 2.4 Linux kernel. In earlier kernels, there were two separate disk caches: the page cache and the buffer cache. The former cached pages; the latter cached buffers. The two caches were not unified: A disk block could exist in both caches simultaneously. This led to wasted effort synchronizing the two cached copies and memory wasted in duplicating cached items. Today, we have one disk cache: the page cache. The kernel still needs to use buffers, however, to represent disk blocks in memory. Conveniently, the buffers describe the mapping of a block onto a page, which is in the page cache.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos1093563"> </div>
<h3 class="calibre_21"><span class="bold">The Flusher Threads</span></h3><div class="calibre_22"> </div>
<p class="calibre_2">Write operations are deferred in the page cache. When data in the page cache is newer than the data on the backing store, we call that data <em class="calibre4">dirty</em>. Dirty pages that accumulate in memory eventually need to be written back to disk. Dirty page writeback occurs in three situations:</p><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• When free memory shrinks below a specified threshold, the kernel writes dirty data back to disk to free memory because only clean (nondirty) memory is available for eviction. When clean, the kernel can evict the data from the cache and then shrink the cache, freeing up more memory.</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• When dirty data grows older than a specific threshold, sufficiently old data is written back to disk to ensure that dirty data does not remain dirty indefinitely.</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• When a user process invokes the <code class="calibre6"><span class="calibre7">sync()</span></code> and <code class="calibre6"><span class="calibre7">fsync()</span></code> system calls, the kernel performs writeback on demand.</p></blockquote><div class="calibre_3"> </div>
<p class="calibre_2">These three jobs have rather different goals. In fact, two separate kernel threads performed the work in older kernels (see the following section). In 2.6, however, a gang<sup class="calibre8"><a id="filepos1095075" href="#filepos1095253">5</a></sup> of kernel threads, the <em class="calibre4">flusher threads</em>, performs all three jobs.</p><div class="calibre_3"> </div>
<p class="calibre_2"><sup class="calibre8"><a id="filepos1095253" href="#filepos1095075">5</a></sup>
<em class="calibre4">The term “gang” is commonly used in computer science to denote a group of things that can operate in parallel.</em></p><div class="calibre_3"> </div>
<p class="calibre_2">First, the flusher threads need to flush dirty data back to disk when the amount of free memory in the system shrinks below a specified level. The goal of this background write-back is to regain memory consumed by dirty pages when available physical memory is <a id="filepos1095748"></a>low. The memory level at which this process begins is configured by the <code class="calibre6"><span class="calibre7">dirty_background_ratio</span></code> sysctl. When free memory drops below this threshold, the kernel invokes the <code class="calibre6"><span class="calibre7">wakeup_flusher_threads()</span></code> call to wake up one or more flusher threads and have them run the <code class="calibre6"><span class="calibre7">bdi_writeback_all ()</span></code> function to begin writeback of dirty pages. This function takes as a parameter the number of pages to attempt to write back. The function continues writing out data until two conditions are true:</p><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• The specified minimum number of pages has been written out.</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• The amount of free memory is above the <code class="calibre6"><span class="calibre7">dirty_background_ratio</span></code> threshold.</p></blockquote><div class="calibre_3"> </div>
<p class="calibre_2">These conditions ensure that the flusher threads do their part to relieve low-memory conditions. Writeback stops prior to these conditions only if the flusher threads write back <em class="calibre4">all</em> the dirty pages and there is nothing left to do.</p><div class="calibre_3"> </div>
<p class="calibre_2">For its second goal, a flusher thread periodically wakes up (unrelated to low-memory conditions) and writes out old dirty pages. This is performed to ensure that no dirty pages remain in memory indefinitely. During a system failure, because memory is volatile, dirty pages in memory that have not been written to disk are lost. Consequently, periodically synchronizing the page cache with the disk is important. On system boot, a timer is initialized to wake up a flusher thread and have it run the <code class="calibre6"><span class="calibre7">wb_writeback()</span></code> function. This function then writes back all data that was modified longer than <code class="calibre6"><span class="calibre7">dirty_expire_interval</span></code> milliseconds ago. The timer is then reinitialized to expire again in <code class="calibre6"><span class="calibre7">dirty_writeback_interval</span></code> milliseconds. In this manner, the flusher threads periodically wake up and write to disk all dirty pages older than a specified limit.</p><div class="calibre_3"> </div>
<p class="calibre_2">The system administrator can set these values either in <code class="calibre6"><span class="calibre7">/proc/sys/vm</span></code> or via sysctl. <a href="#filepos1098238">Table 16.1</a> lists the variables.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos1098238"> </div>
<p class="calibre_23"><span class="calibre9"><span class="calibre3">Table 16.1. Page Writeback Settings</span></span></p><div class="calibre_24"> </div>
<p class="calibre_23"><img alt="image" src="images/00231.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2"><a id="filepos1098523"></a>The flusher code lives in <code class="calibre6"><span class="calibre7">mm/page-writeback.c</span></code> and <code class="calibre6"><span class="calibre7">mm/backing-dev.c</span></code> and the writeback mechanism lives in <code class="calibre6"><span class="calibre7">fs/fs-writeback.c</span></code>.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos1098801"> </div>
<h4 class="calibre_27"><span class="calibre3">Laptop Mode</span></h4><div class="calibre_24"> </div>
<p class="calibre_2"><em class="calibre4">Laptop mode</em> is a special page writeback strategy intended to optimize battery life by minimizing hard disk activity and enabling hard drives to remain spun down as long as possible. It is configurable via <code class="calibre6"><span class="calibre7">/proc/sys/vm/laptop_mode</span></code>. By default, this file contains a zero and laptop mode is disabled. Writing a one to this file enables laptop mode.</p><div class="calibre_3"> </div>
<p class="calibre_2">Laptop mode makes a single change to page writeback behavior. In addition to performing writeback of dirty pages when they grow too old, the flusher threads also piggyback off any other physical disk I/O, flushing <em class="calibre4">all</em> dirty buffers to disk. In this manner, page writeback takes advantage that the disk was just spun up, ensuring that it will not cause the disk to spin up later.</p><div class="calibre_3"> </div>
<p class="calibre_2">This behavioral change makes the most sense when <code class="calibre6"><span class="calibre7">dirty_expire_interval</span></code> and <code class="calibre6"><span class="calibre7">dirty_writeback_interval</span></code> are set to large values—say, 10 minutes. With writeback so delayed, the disk is spun up infrequently, and when it does spin up, laptop mode ensures that the opportunity is well utilized. Because shutting off the disk drive is a significant source of power savings, laptop mode can greatly improve how long a laptop lasts on battery. The downside is that a system crash or other failure can lose a lot of data.</p><div class="calibre_3"> </div>
<p class="calibre_2">Many Linux distributions automatically enable and disable laptop mode, and modify other writeback tunables, when going on and off battery. This enables a machine to benefit from laptop mode when on battery power and then automatically return to normal page writeback behavior when plugged into AC.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos1100776"> </div>
<h4 class="calibre_27"><span class="calibre3">History: bdflush, kupdated, and pdflush</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">Prior to the 2.6 kernel, the job of the flusher threads was met by two other kernel threads, <em class="calibre4">bdflush</em> and <em class="calibre4">kupdated</em>.</p><div class="calibre_3"> </div>
<p class="calibre_2">The bdflush kernel thread performed background writeback of dirty pages when available memory was low. A set of thresholds was maintained, similar to the flusher threads’, and bdflush was awakened via <code class="calibre6"><span class="calibre7">wakeup_bdflush()</span></code> whenever free memory dropped below those thresholds.</p><div class="calibre_3"> </div>
<p class="calibre_2">Two main differences distinguish bdflush and the current flusher threads. The first, which is discussed in the next section, is that there was always only one bdflush daemon, whereas the number of flusher threads is a function of the number of disk spindles. The second difference is that bdflush was buffer-based; it wrote back dirty buffers. Conversely, the flusher threads are page-based; they write back whole pages. Of course, the pages can correspond to buffers, but the actual I/O unit is a full page and not a single buffer. This is beneficial as managing pages is easier than managing buffers because pages are a more general and common unit.</p><div class="calibre_3"> </div>
<p class="calibre_2">Because bdflush flushes buffers only when memory is low or the number of buffers is too large, the kupdated thread was introduced to periodically write back dirty pages. It served an identical purpose to the <code class="calibre6"><span class="calibre7">wb_writeback()</span></code> function.</p><div class="calibre_3"> </div>
<p class="calibre_2"><a id="filepos1102482"></a>In the 2.6 kernel, bdflush and kupdated gave way to the <em class="calibre4">pdflush threads</em>. Short for <em class="calibre4">page dirty flush</em> (more of those confusing names), the pdflush threads performed similar to the flusher threads of today. The main difference is that the number of pdflush threads is dynamic, by default between two and eight, depending on the I/O load of the system. The pdflush threads are not associated with any specific disk; instead, they are global to all disks in the system. This allows for a simple implementation. The downside is that pdflush can easily trip up on congested disks, and congestion is easy to cause with modern hardware. Moving to per-spindle flushing enables the I/O to perform synchronously, simplifying the congestion logic and improving performance. The flusher threads replaced the pdflush threads in the 2.6.32 kernel. The per-spindle flushing is the main difference; the rest of this section is also applicable to pdflush and thus any 2.6 kernel.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos1103511"> </div>
<h4 class="calibre_27"><span class="calibre3">Avoiding Congestion with Multiple Threads</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">One of the major flaws in the bdflush solution was that bdflush consisted of one thread. This led to possible congestion during heavy page writeback where the single bdflush thread would block on a single congested device queue (the list of I/O requests waiting to submit to disk), whereas other device queues would sit relatively idle. If the system has multiple disks and the associated processing power, the kernel should keep each disk busy. Unfortunately, even with plenty of data needing writeback, bdflush can become stuck handling a single queue and fail to keep all disks saturated. This occurs because the throughput of disks is a finite—and unfortunately comparatively small—number. If only a single thread is performing page writeback, that single thread can easily spend a long time waiting for a single disk because disk throughput is such a limiting quantity. To mitigate this, the kernel needs to multithread page writeback. In this manner, no single device queue can become a bottleneck.</p><div class="calibre_3"> </div>
<p class="calibre_2">The 2.6 kernel solves this problem by enabling multiple flusher threads to exist. Each thread individually flushes dirty pages to disk, allowing different flusher threads to concentrate on different device queues. With the pdflush threads, the number of threads was dynamic, and each thread tried to stay busy grabbing data from the per-superblock dirty list and writing it back to disk. The pdflush approach prevents a single busy disk from starving other disks. This is all good, but what if each pdflush thread were to get hung up writing to the same, congested, queue? In that case, the performance of multiple pdflush threads would not be an improvement over a single thread. The memory consumed, however, would be significantly greater. To mitigate this effect, the pdflush threads employ congestion avoidance: They actively try to write back pages whose queues are not congested. As a result, the pdflush threads spread out their work and refrain from merely hammering on the same busy device.</p><div class="calibre_3"> </div>
<p class="calibre_2">This approach worked fairly well, but the congestion avoidance was not perfect. On modern systems, congestion is easy to cause because I/O bus technology improves at a slower rate than the rest of the computer—processors keep getting faster according to Moore’s Law, but hard drives are only marginally quicker than they were two decades ago. Moreover, aside from pdflush, no other part of the I/O system employs congestion avoidance. Thus, in certain cases pdflush can avoid writing back on a specific disk far longer than desired. With the current flusher threads model, available since 2.6.32, the threads are associated with a block device, so each thread grabs data from its per-block device dirty list and writes it back to its disk. Writeback is thus synchronous and the threads, because there is one per disk, do not need to employ complicated congestion avoidance. This approach improves fairness and decreases the risk of starvation.</p><div class="calibre_3"> </div>
<p class="calibre_2">Because of the improvements in page writeback, starting with the introduction of pdflush and continuing with the flusher threads, the 2.6 kernel can keep many more disks saturated than any earlier kernel. In the face of heavy activity, the flusher threads can maintain high throughput across multiple disks.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos1107136"> </div>
<h3 class="calibre_21"><span class="bold">Conclusion</span></h3><div class="calibre_22"> </div>
<p class="calibre_2">This chapter looked at Linux’s page cache and page writeback. We saw how the kernel performs all page I/O through the page cache and how this page cache, by storing data in memory, significantly improves the performance of the system by reducing the amount of disk I/O. We discussed how writes are maintained in the page cache through a process called write-back caching, which keeps pages “dirty” in memory and defers writing the data back to disk. The flusher “gang” of kernel threads handles this eventual page writeback.</p><div class="calibre_3"> </div>
<p class="calibre_2">Over the last few chapters, we have built a solid understanding of memory and filesystem management. Now let’s segue over to the topic of device drivers and modules to see how the Linux kernel provides a modular and dynamic infrastructure for the run-time insertion and removal of kernel code.</p><div class="calibre_3"> </div>  <div class="mbp_pagebreak" id="calibre_pb_72"></div>
</body></html>
