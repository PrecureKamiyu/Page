<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Part 3</title>
    
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
  <div class="tocheadb">
    <h1 class="tochead" id="heading_id_3"><a id="pgfId-998793"></a><a id="pgfId-998805"></a>14 An introduction to graphs: Finding paths of minimum distance</h1>
  </div>

  <p class="co-summary-head"><a id="pgfId-1014520"></a>This chapter covers</p>

  <ul class="calibre19">
    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1014559"></a>Introducing graphs from a theoretical point of view</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1014560"></a>Learning strategies for implementing graphs</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1014561"></a>Finding the best route for deliveries</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1014562"></a>Introducing search algorithms on graphs: BFS and DFS</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1014563"></a>Using BFS to find the route that traverses the fewest blocks</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1014564"></a>Finding the shortest route with Dijkstra’s minimum distance algorithm</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1014548"></a>Finding the quickest route with A* algorithm for optimal search</li>
  </ul>

  <p class="body"><a id="pgfId-998945"></a>We already described the basics of trees in appendix C and used several kinds of trees in the previous chapters: binary search trees, heaps, k-d trees, and so on. You should now be familiar with them. Graphs could be considered a generalization of trees, although in reality the opposite is true, and it is trees that are a special case of graphs. A tree, in fact, is a connected, acyclic undirected graph. Figure 14.1 shows an example of two graphs, only one of which is a tree. Don’t worry if you are not sure which one, because in this chapter we’ll take a closer look at the meaning of those properties in the definition of trees, and they will help us to better explain graphs’ properties and make sense of these examples.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F1.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1031890"></a>Figure 14.1 Two graphs, only one of which is also a tree. Can you tell which one?</p>

  <p class="body"><a id="pgfId-998992"></a>But to do so we need to follow a meaningful order and so, we first need to give a formal definition of graphs and understand how we can represent them.</p>

  <p class="body"><a id="pgfId-999001"></a>Once we have laid that foundation, we can start modeling interesting problems with graphs and developing algorithms to solve them.</p>

  <p class="body"><a id="pgfId-999010"></a>In particular, here we will focus on the “shortest distance” problem. In chapters 8–11 we developed our example of an e-commerce platform by introducing k-d trees and nearest neighbor search to find the closest hub for deliveries, so that parcels need to travel the least possible distance. Nevertheless, there will still be some road to travel to get our orders to the customers, and our margin gets thinner if we waste time and gas for each delivery.</p>

  <p class="body"><a id="pgfId-999023"></a>Conversely, if we were able to optimize the route fared, we could save some money for each parcel, and a lot of money at scale, on thousands or millions of deliveries.</p>

  <p class="body"><a id="pgfId-999032"></a>In this chapter we will solve this problem, finding the best route to make a single delivery (from a warehouse to the customer’s home), by using graphs; we will tackle the problem at different levels of abstraction, demonstrating how search algorithms such as BFS, Dijkstra, and A* (pronounced “A-star”) work.</p>

  <h2 class="fm-head" id="heading_id_4"><a id="pgfId-999047"></a>14.1 Definitions</h2>

  <p class="body"><a id="pgfId-999059"></a>A <a id="marker-999595"></a>graph G is usually defined in terms of two sets:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-999070"></a>A set of <i class="calibre15">vertices</i><a class="calibre14" id="marker-999599"></a> <code class="fm-code-in-text">V</code>: independent, distinct entities that can appear in any multiplicity. A graph can have 1, 2, 100, or any number of vertices but, in general, graphs don’t support duplicate vertices.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-999089"></a>A set of <i class="calibre15">edges</i><a class="calibre14" id="marker-999603"></a> <code class="fm-code-in-text">E</code> connecting vertices: an edge is defined by a pair of vertices, the first one usually denoted as the <i class="calibre15">source</i> vertex<a class="calibre14" id="marker-999607"></a>, and the second one called the <i class="calibre15">destination</i> vertex<a class="calibre14" id="marker-999611"></a>.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-999114"></a>So, we write <code class="fm-code-in-text">G=(V,E)</code> to make it clear that the graph is made of certain sets of vertices and edges; for instance, the graph in figure 14.2 is formally written as</p>
  <pre class="programlisting">G = ([v<code class="fm-code-in-text2"><sub class="subscript">1</sub></code>, v<code class="fm-code-in-text2"><sub class="subscript">2</sub></code>, v<sub class="calibre25">3</sub>, v<code class="fm-code-in-text2"><sub class="subscript">4</sub></code>], [(v<code class="fm-code-in-text2"><sub class="subscript">1</sub></code>,v<code class="fm-code-in-text2"><sub class="subscript">2</sub></code>),(v<code class="fm-code-in-text2"><sub class="subscript">1</sub></code>,v<sub class="calibre25">3</sub>),(v<code class="fm-code-in-text2"><sub class="subscript">2</sub></code>,v<code class="fm-code-in-text2"><sub class="subscript">4</sub></code>)])<br class="calibre38"/></pre>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F2.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1031939"></a>Figure 14.2 An example of a (directed) graph</p>

  <p class="body"><a id="pgfId-999176"></a>An edge whose source and destination are the same is called a <i class="calibre17">loop</i> (see figure 14.3). <i class="calibre17">Simple graphs</i><a id="marker-999615"></a> can’t have any loops, nor can they have multiple edges between the same pair of nodes. Conversely, <i class="calibre17">multigraphs</i><a id="marker-999619"></a> can have any number of edges between two distinct vertices. Both simple graphs and multigraphs can be extended to permit loops.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F3.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1031979"></a>Figure 14.3 A directed weighted graph, with loops. (Edge labels are omitted for the sake of clarity.)</p>

  <p class="body"><a id="pgfId-999221"></a>We won’t bother with multigraphs in this book; instead we’ll focus on simple graphs, usually without loops.</p>

  <p class="body"><a id="pgfId-999230"></a>We can express the previous definitions more formally. Given the set of edges <code class="fm-code-in-text">E</code></p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-999243"></a>For simple graphs, <code class="fm-code-in-text">E</code> <span class="cambria">⊆</span> <code class="fm-code-in-text">{(x, y) | (x, y)</code> <span class="cambria">∈</span> <code class="fm-code-in-text">V<sup class="superscript1">2</sup></code> <span class="cambria">∧</span> <code class="fm-code-in-text">x</code> <span class="cambria">≠</span> <code class="fm-code-in-text">y}</code></p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-999267"></a>For simple graphs supporting loops, <code class="fm-code-in-text">E</code> <span class="cambria">⊆</span> <code class="fm-code-in-text">V<sup class="superscript1">2</sup></code></p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-999283"></a>It’s also possible to associate a weight to each edge. In this case, the graph is called a <i class="calibre17">weighted graph</i> or, equivalently, a <i class="calibre17">network</i>, where each edge becomes a triplet, and the set of graph’s edges becomes</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-999302"></a>For simple graphs, <code class="fm-code-in-text">E</code> <span class="cambria">⊆</span> <code class="fm-code-in-text">{(x, y, w) | (x, y)</code> <span class="cambria">∈</span> <code class="fm-code-in-text">V<sup class="superscript1">2</sup></code> <span class="cambria">∧</span> <code class="fm-code-in-text">w</code> <span class="cambria">∈</span> <span class="cambria">ℝ</span> <span class="cambria">∧</span> <code class="fm-code-in-text">x</code> <span class="cambria">≠</span> <code class="fm-code-in-text">y}</code></p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-999329"></a>For simple graphs supporting loops, <code class="fm-code-in-text">E</code> <span class="cambria">⊆</span> <code class="fm-code-in-text">V<sup class="superscript1">2 </sup></code>× <span class="cambria"><span class="cambria">ℝ</span></span></p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-999347"></a>Figure 14.3 shows an example of a weighted graph with loops.</p>

  <h3 class="fm-head2" id="heading_id_5"><a id="pgfId-999356"></a>14.1.1 Implementing graphs</h3>

  <p class="body"><a id="pgfId-999370"></a>The <a id="marker-999623"></a>previous section formally defines graphs; however, when we move from theory to practice, we often have to face new issues and cope with constraints.</p>

  <p class="body"><a id="pgfId-999381"></a>As much as mathematical notation makes it clear how we should represent graphs on paper, we need to decide, for instance, what’s the best way to store them into a data structure.</p>

  <p class="body"><a id="pgfId-999390"></a>There are several questions to answer, depending on the context: Should we store labels for vertices and edges, or should we just assign an index to vertices using natural numbers, and enumerate edges following the natural ordering of pairs of indices?</p>

  <p class="body"><a id="pgfId-999399"></a>While storing vertices is relatively easy (using lists, and possibly a dictionary to associate each vertex to its label), there is also another question that goes beyond any context: How should we store edges?</p>

  <p class="body"><a id="pgfId-999410"></a>That question is not as trivial as it might seem: the caveat is that at some point we will want to check to see if there is an edge between two vertices, or maybe find all outgoing edges of a certain vertex. If we just store all edges in a single list, either sorted or unsorted, then we will have to scan the whole list to find out our answers.</p>

  <p class="body"><a id="pgfId-999423"></a>Even with a sorted list, that means accessing <code class="fm-code-in-text">O(log(|E|))</code> elements for the operations in the previous paragraph, and <code class="fm-code-in-text">O(|E|)</code> for listing all edges going into a vertex.</p>

  <p class="body"><a id="pgfId-999436"></a>No surprise, it turns out we can do better. There are two main strategies to store a graph’s edges:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-999454"></a><i class="fm-italics1">Adjacency lists</i>—For each vertex <code class="fm-code-in-text">v</code>, we store a list of the edges <code class="fm-code-in-text">(v, u)</code>, where <code class="fm-code-in-text">v</code> is the source and <code class="fm-code-in-text">u</code>, the destination, is another vertex in <code class="fm-code-in-text">G.</code></p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-999477"></a><i class="fm-italics1">Adjacency matrix</i>—It’s a <code class="fm-code-in-text">|V|×|V|</code> matrix, where the generic cell <code class="fm-code-in-text">(i,j)</code> contains the weight of the edge going from the <code class="fm-code-in-text">i</code>-th vertex to the <code class="fm-code-in-text">j</code>-th vertex (or <code class="fm-code-in-text">true/ false</code><a class="calibre14" id="marker-1018268"></a>, or <code class="fm-code-in-text">1/0</code>, in case of un-weighted graphs, to state the presence/absence of an unweighted edge between those two vertices).</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-999508"></a>Before examining pros and cons of both strategies, let’s illustrate them with an example. Given the graph in figure 14.2, its adjacency list representation is the following dictionary mapping vertices to lists of edges:</p>
  <pre class="programlisting">1 -&gt; [(1,2), (1,3)]
2 -&gt; [(2,4)]
3 -&gt; []
4 -&gt; []</pre>

  <p class="body"><a id="pgfId-999561"></a>The adjacency matrix representation is the following:</p>

  <table border="1" class="contenttable" width="100%">
    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014888"></a> </p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014890"></a><code class="fm-code-in-text2">V<sub class="subscript">1</sub></code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014892"></a><code class="fm-code-in-text2">V<sub class="subscript">2</sub></code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014894"></a><code class="fm-code-in-text2">V<sub class="subscript">3</sub></code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014896"></a><code class="fm-code-in-text2">V<sub class="subscript">4</sub></code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014898"></a><code class="fm-code-in-text2">V<sub class="subscript">1</sub></code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014900"></a>0</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014902"></a>1</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014904"></a>1</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014906"></a>0</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014908"></a><code class="fm-code-in-text2">V<sub class="subscript">2</sub></code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014910"></a>0</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014912"></a>0</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014914"></a>0</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014916"></a>1</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014918"></a><code class="fm-code-in-text2">V<sub class="subscript">3</sub></code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014920"></a>0</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014922"></a>0</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014924"></a>0</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014926"></a>0</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014928"></a><code class="fm-code-in-text2">V<sub class="subscript">4</sub></code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014930"></a>0</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014932"></a>0</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014934"></a>0</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1014936"></a>0</p>
      </td>
    </tr>
  </table>

  <p class="body"><a id="pgfId-999628"></a>As you can see, they are very different. One aspect that stands out immediately is that in the adjacency matrix, most cells are filled with 0s. This stems from the fact that the graph in figure 14.2 only has a few edges, of the many possible.</p>

  <p class="body"><a id="pgfId-999907"></a>We know that because edges are pairs of vertices, in a simple graph the maximum number of edges is <code class="fm-code-in-text">O(|V|<sup class="superscript1">2</sup>)</code>. What’s the minimum number, though?</p>

  <p class="body"><a id="pgfId-999927"></a>It can be anything; a graph can even (hypothetically) have no edges at all. A connected graph, however, must have at least <code class="fm-code-in-text">|V|-1</code> edges.</p>

  <p class="fm-table-caption"><a id="pgfId-1015002"></a>Table 14.1 Weaknesses and strengths of graph representations</p>

  <table border="1" class="contenttable" width="100%">
    <tr class="calibre8">
      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1015008"></a>Operation</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1015010"></a>Adjacency list</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1015012"></a>Adjacency matrix</p>
      </th>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1024335"></a><code class="fm-code-in-text2">Edge insert</code><a id="marker-1024384"></a></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1024337"></a><code class="fm-code-in-text2">O(1)</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1024339"></a><code class="fm-code-in-text2">O(1)</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1015014"></a><code class="fm-code-in-text2">Edge delete</code> <a id="marker-1024399"></a></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1015016"></a><code class="fm-code-in-text2">O(|V|)</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1015018"></a><code class="fm-code-in-text2">O(1)</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1015020"></a><code class="fm-code-in-text2">List of outgoing edges</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1015022"></a><code class="fm-code-in-text2">O(|E|)</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1015024"></a><code class="fm-code-in-text2">O(|V|)</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1015026"></a><code class="fm-code-in-text2">List of ingoing edges</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1015028"></a><code class="fm-code-in-text2">O(|E|)</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1015030"></a><code class="fm-code-in-text2">O(|V|)</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1015032"></a><code class="fm-code-in-text2">Space needed</code><a id="marker-1015051"></a></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1015034"></a><code class="fm-code-in-text2">O(|E|+|V|)</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1015036"></a><code class="fm-code-in-text2">O(|V|<sup class="superscript">2</sup>)</code></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1015038"></a><code class="fm-code-in-text2">Vertex insert</code><a id="marker-1015052"></a></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1015040"></a><code class="fm-code-in-text2">O(1)</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1015042"></a><code class="fm-code-in-text2">O(|V|)</code><a href="#pgfId-1015100"><sup class="footnotenumber1">1</sup></a></p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1015044"></a><code class="fm-code-in-text2">Vertex delete</code><a id="marker-1015053"></a></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1015046"></a><code class="fm-code-in-text2">O(|V|+|E|)<sup class="superscript"><a class="calibre14" href="#pgfId-1015079"><sup class="footnotenumber6">2</sup></a></sup></code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1015048"></a><code class="fm-code-in-text2">O(|V|)</code><a href="#pgfId-1015100"><sup class="footnotenumber1">1</sup></a></p>
      </td>
    </tr>
  </table>

  <p class="body"><a id="pgfId-1000263"></a>Keeping this in mind, we’ll now provide a definition, one that will be handy later in this section:</p>

  <p class="body"><a id="pgfId-1000272"></a>A graph <code class="fm-code-in-text">G=(V,E)</code> is said to be <i class="calibre17">sparse</i> if <code class="fm-code-in-text">|E|=O(|V|)</code>; <code class="fm-code-in-text">G</code> is said to be <i class="calibre17">dense</i> when <code class="fm-code-in-text">|E|=O(|V|<sup class="superscript1">2</sup>)</code>.</p>

  <p class="body"><a id="pgfId-1000299"></a>In other words, sparse graphs have a number of edges comparable to the number of vertices, which are therefore loosely connected to each other, while in dense graphs, each vertex is connected to most of the other vertices.</p>

  <p class="body"><a id="pgfId-1000310"></a>Table 14.1 summarizes the pros and cons of the two different representations of graphs. In a few words, the adjacency list representation is better for sparse graphs because it requires a lot less memory, and because for sparse graphs <code class="fm-code-in-text">|E|</code><span class="cambria">≈</span><code class="fm-code-in-text">|V|</code>, and thus most operations can be performed efficiently.</p>

  <p class="body"><a id="pgfId-1000327"></a>For dense graphs, conversely, since the number of edges is close to the maximum possible, the adjacency matrix representation is more compact and efficient. Moreover, this representation allows more efficient implementation of some algorithms on graphs, such as the search of connected components or transitive closure.</p>

  <p class="body"><a id="pgfId-1000348"></a>In general, when no assumption can be made on the graph, and unless it is otherwise required by the context, the adjacency list representation is preferred because it is more flexible and it supports adding new vertices more <a id="marker-1005398"></a>easily.</p>

  <h3 class="fm-head2" id="heading_id_6"><a id="pgfId-1000363"></a>14.1.2 Graphs as algebraic types</h3>

  <p class="body"><a id="pgfId-1000379"></a>There <a id="marker-1005402"></a>is another aspect of graph representation that is orthogonal to the way we store edges: consistency.</p>

  <p class="body"><a id="pgfId-1000390"></a>To be fair, inconsistencies are much more likely to happen with the adjacency list representation, but they are still possible in certain situations, even using the adjacency matrix.</p>

  <p class="body"><a id="pgfId-1000403"></a>The problem is the following. Regardless of its representation, consider the following graph: <code class="fm-code-in-text">G=([1,2], [(1,2), (1,3), (2,2)])</code>.</p>

  <p class="body"><a id="pgfId-1000420"></a>The graph <code class="fm-code-in-text">G</code> has two vertices, <code class="fm-code-in-text">[1,2]</code>, but it has an edge whose destination is the vertex <code class="fm-code-in-text">“3”</code>. This can happen for any reason; for instance, sloppiness in deleting vertex <code class="fm-code-in-text">“3”</code>, or an error while adding edges.</p>

  <p class="body"><a id="pgfId-1000447"></a>Moreover, the graph has a loop, the edge <code class="fm-code-in-text">(2,2)</code>. What if <code class="fm-code-in-text">G</code> was supposed to be a simple graph without loops?</p>

  <p class="body"><a id="pgfId-1000463"></a>Of course, we can add validation to our <code class="fm-code-in-text">Graph</code> class<a id="marker-1005406"></a>’ methods to prevent these situations, but the data structure itself can’t guarantee that these errors won’t happen.</p>

  <p class="body"><a id="pgfId-1000476"></a>To overcome these limitations, we can define our graphs as an algebraic type;<a href="#pgfId-1008087"><sup class="footnotenumber">3</sup></a> this way, we define graphs as one of the following:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1000489"></a>The empty graphs.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1000502"></a>A singleton, a single vertex with no edges.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1000515"></a>The connection between two graphs <code class="fm-code-in-text">G</code> and <code class="fm-code-in-text">G’</code>. We define one or more edges whose source is in <code class="fm-code-in-text">G</code> and destination is in <code class="fm-code-in-text">G’</code>.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1000538"></a>The union of two graphs <code class="fm-code-in-text">G=(V,E)</code> and <code class="fm-code-in-text">G’=(V’, E’)</code>. We just compute the union of both the vertices and edges set to obtain <code class="fm-code-in-text">G”=(V <span class="cambria">∪</span> V’, E <span class="cambria">∪</span> E’)</code>.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1000562"></a>This representation prevents the inconsistencies we talked about and guarantees that we won’t get malformed graphs, but also allows us to formally define algorithms as transformations on graphs and mathematically prove their correctness.</p>

  <p class="body"><a id="pgfId-1000571"></a>Trying to understand graphs as an algebraic type is a useful exercise to help you gain a deeper understanding of this data structure. At the same time, we need to acknowledge that considering the overhead for these operations, practical uses are limited and mostly relegated to functional languages providing pattern matching on types, such as Scala, Haskell, or <a id="marker-1005410"></a>Clojure.<a href="#pgfId-1008108"><sup class="footnotenumber">4</sup></a></p>

  <h3 class="fm-head2" id="heading_id_7"><a id="pgfId-1000593"></a>14.1.3 Pseudo-code</h3>

  <p class="body"><a id="pgfId-1000607"></a>To <a id="marker-1005414"></a>complete the discussion, listing 14.1 provides an overview of the class that we’ll use for graphs in this book. It uses adjacency lists and models edges and vertices as classes, allowing us to implement different types of graphs by changing the details of these models (for instance, allowing weighted edges).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1015134"></a>Listing 14.1 The <code class="fm-code-in-text">Graph</code> class</p>
  <pre class="programlisting"><b class="strong">class</b> Vertex
  <b class="strong">#type</b> string
  label 
 
<b class="strong">class</b> Edge
  <b class="strong">#type</b> Vertex
  source
  <b class="strong">#type</b> Vertex
  dest
  <b class="strong">#type</b> double
  weight
  <b class="strong">#type</b> string
  label 
 
<b class="strong">class</b> Graph
  <b class="strong">#type</b> List[Vertices]
  vertices
  <b class="strong">#type</b> HashTable[Vertex-&gt;List[Edge]]
  adjacencyList
 
  <b class="strong">function</b> Graph()
    adjacencyList ← <b class="strong">new</b> HashTable()
 
  <b class="strong">function</b> addVertex(v)
    <b class="strong">throw-if</b> v <b class="strong">in</b> vertices
    vertices.insert(v)                                     <span class="fm-combinumeral">❶</span>
    adjacencyList[v] ← []                                  <span class="fm-combinumeral">❷</span>
 
  <b class="strong">function</b> addEdge(v, u, weight=0, label=””)
    <b class="strong">throw-if</b> <b class="strong">not</b> (v <b class="strong">in</b> vertices <b class="strong">and</b> u <b class="strong">in</b> vertices)
    <b class="strong">if</b> areAdjacent(v, u) <b class="strong">then</b>                              <span class="fm-combinumeral">❸</span>
      removeEdge(v, u)                                     <span class="fm-combinumeral">❹</span>
    adjacencyList[v].insert(new Edge(v, u, weight, label)) <span class="fm-combinumeral">❺</span>
 
  <b class="strong">function</b> areAdjacent(v, u)
    <b class="strong">throw-if</b> <b class="strong">not</b> (v <b class="strong">in</b> vertices <b class="strong">and</b> u <b class="strong">in</b> vertices)
    <b class="strong">for</b> e <b class="strong">in</b> adjacencyList[v] <b class="strong">do</b>                           <span class="fm-combinumeral">❻</span>
      <b class="strong">if</b> e.dest == u <b class="strong">then</b>                                  <span class="fm-combinumeral">❼</span>
        <b class="strong">return</b> <b class="strong">true</b>
    <b class="strong">return</b> <b class="strong">false</b></pre>

  <p class="fm-code-annotation"><a id="pgfId-1030391"></a><span class="fm-combinumeral">❶</span> When we add a new vertex, provided it’s not a duplicate, we first need to add it to the vertices list.</p>

  <p class="fm-code-annotation"><a id="pgfId-1030412"></a><span class="fm-combinumeral">❷</span> But we also want to initialize the adjacency list for the new vertex; it will simplify our lives later!</p>

  <p class="fm-code-annotation"><a id="pgfId-1030432"></a><span class="fm-combinumeral">❸</span> Checks if the vertices are adjacent, that is, if there is already an edge from <code class="fm-code-in-text2">v</code> to <code class="fm-code-in-text2">u</code></p>

  <p class="fm-code-annotation"><a id="pgfId-1030449"></a><span class="fm-combinumeral">❹</span> If so, removes the old one first (this method is omitted, but can be derived from <code class="fm-code-in-text2">areAdjacent</code><a id="marker-1030454"></a>)</p>

  <p class="fm-code-annotation"><a id="pgfId-1030467"></a><span class="fm-combinumeral">❺</span> Adds a new edge, created based on the arguments, to the adjacency list for the source vertex</p>

  <p class="fm-code-annotation"><a id="pgfId-1030484"></a><span class="fm-combinumeral">❻</span> Iterates through all edges in the adjacency list for the source vertex</p>

  <p class="fm-code-annotation"><a id="pgfId-1030501"></a><span class="fm-combinumeral">❼</span> As soon as it finds an edge whose destination matches <code class="fm-code-in-text2">u</code>, we can return <code class="fm-code-in-text2">true</code>. If none matches, the vertices are not adjacent.</p>

  <p class="body"><a id="pgfId-1001161"></a>As for concrete implementation, a Java version can be found on the book repo on GitHub,<a href="#pgfId-1008128"><sup class="footnotenumber">5</sup></a> and a JavaScript version is provided by the JsGraphs library; the latter will also implement the algorithms described in the <a id="marker-1005422"></a>next <a id="marker-1005426"></a>sections.</p>

  <h2 class="fm-head" id="heading_id_8"><a id="pgfId-1001180"></a>14.2 Graph properties</h2>

  <p class="body"><a id="pgfId-1001194"></a>As <a id="marker-1005430"></a>we mentioned, graphs are very similar to trees. They are both made of entities (vertices) connected by relations (edges), with a couple of differences:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1001205"></a>In trees, vertices are usually called nodes.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1001218"></a>In trees, edges are somewhat implicit. Since they can only go from a node to its children, it’s more common to talk about parent/children relations than explicitly list edges. Also, because of this, trees are implicitly represented with adjacency lists.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1001234"></a>Furthermore, trees have other peculiar characteristics that make them a strict subset of the whole set of graphs. In particular, any tree is a <i class="calibre17">simple</i><a id="marker-1005434"></a>, <i class="calibre17">undirected</i>, <i class="calibre17">connected,</i> and <i class="calibre17">acyclic</i> graph<a id="marker-1005438"></a>.</p>

  <p class="body"><a id="pgfId-1001260"></a>We have illustrated in the previous section what <i class="calibre17">simple graph</i> means. In fact, a tree cannot have multiple edges between two nodes, nor can it have loops; instead, only a single edge between a node and each of its children is allowed.</p>

  <p class="body"><a id="pgfId-1001275"></a>Let’s now see what the other three properties mean.</p>

  <h3 class="fm-head2" id="heading_id_9"><a id="pgfId-1001284"></a>14.2.1 Undirected</h3>

  <p class="body"><a id="pgfId-1001296"></a>As <a id="marker-1005442"></a><a id="marker-1005446"></a>we mentioned in section 1, a graph is directed when all its edges can be traversed in a single direction, from source (the first vertex in the edge’s pair) to destination.</p>

  <p class="body"><a id="pgfId-1001308"></a>In undirected graphs, conversely, the edges can be traversed in both directions. The difference is shown in figure 14.4.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F4.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032026"></a>Figure 14.4 An undirected graph (A) versus a directed graph with the same vertices (B). The two graphs are not equivalent; in particular, the latter can’t be transformed into an equivalent undirected graph.</p>

  <p class="body"><a id="pgfId-1001336"></a>An undirected graph can easily be represented as a directed graph, by expanding each undirected edge <code class="fm-code-in-text">(u,v)</code> into a couple of directed edges <code class="fm-code-in-text">(u,v)</code> and <code class="fm-code-in-text">(v,u)</code>.</p>

  <p class="body"><a id="pgfId-1001357"></a>Vice versa is usually not true, and many directed graphs can’t be transformed into their undirected isomorphic counterpart.<a href="#pgfId-1008144"><sup class="footnotenumber">6</sup></a></p>

  <p class="body"><a id="pgfId-1001368"></a>So, unless the application context suggests otherwise, representing directed graphs is the least restricting choice.</p>

  <p class="body"><a id="pgfId-1001377"></a>It’s also worth noting that if the adjacency matrix representation is used, undirected graphs can be represented using only half the matrix, because they always have a symmetrical adjacency matrix: <code class="fm-code-in-text">A[u,v] = A[v,u]</code> for every pair of vertices <a id="marker-1005450"></a><a id="marker-1005454"></a><code class="fm-code-in-text">u,v</code>.</p>

  <h3 class="fm-head2" id="heading_id_10"><a id="pgfId-1001396"></a>14.2.2 Connected</h3>

  <p class="body"><a id="pgfId-1001408"></a>A <a id="marker-1005458"></a><a id="marker-1005462"></a>graph is <i class="calibre17">connected</i> if, given any pair of its vertices <code class="fm-code-in-text">(u,v)</code>, there is a sequence of vertices <code class="fm-code-in-text">u, (w<sub class="subscript1">1</sub>, ..., w<sub class="subscript1">K</sub>), v,</code> with <code class="fm-code-in-text">k</code><span class="cambria">≥</span><code class="fm-code-in-text">0</code>, such that there is an edge between any two adjacent vertices in the sequence.</p>

  <p class="body"><a id="pgfId-1001435"></a>For <i class="fm-italics">undirected</i> graphs, this means that in a connected graph, any vertex can be reached by all other vertices, while for <i class="fm-italics">directed</i> graphs<a id="marker-1005466"></a>, it means that each vertex has at least an in-going or an out-going edge.</p>

  <p class="body"><a id="pgfId-1001450"></a>For either kind, it means that the number of edges is at least <code class="fm-code-in-text">|V|-1</code>.</p>

  <p class="body"><a id="pgfId-1001461"></a>Figure 14.5 shows a few examples of connected and disconnected undirected graphs. The notion of connected graph makes the most sense for undirected graphs, while for directed graphs we instead introduce the notion of <i class="calibre17">strongly connected components</i>, illustrated in figure 14.6.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F5.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032068"></a>Figure 14.5 Connected graphs ((A), (B)) versus disconnected graphs ((C), (D))</p>

  <p class="body"><a id="pgfId-1001496"></a>In a <a id="id_Hlk56659999"></a><i class="calibre17">strongly connect component</i><a id="marker-1005470"></a> (<i class="calibre17">SCC</i>), every vertex is reachable from every other vertex; strongly connected components must therefore have cycles (see section 14.2.3).</p>

  <p class="body"><a id="pgfId-1001511"></a>The notion of strongly connected components is particularly important. It allows us to define a graph of the SCCs, which is going to be sensibly smaller than the original graph, and run many algorithms on this graph instead. We can gain a great increase in speed by first examining a graph at a high level, and then (possibly) studying the interaction within each SCC.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F6.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032110"></a>Figure 14.6 Only graph (B) is strongly connected, but graph (C) has two strongly connected components.</p>

  <p class="body"><a id="pgfId-1001548"></a>A tree is usually regarded as an undirected graph, at least in graph theory; but in implementations, each node usually stores the links to its children and only sometimes to its parent. If references to parents are not stored in children, then each edge is not traversable from child to parent, making it a de facto directed <a id="marker-1005474"></a><a id="marker-1005478"></a>edge.</p>

  <h3 class="fm-head2" id="heading_id_11"><a id="pgfId-1001581"></a>14.2.3 Acyclic</h3>

  <p class="body"><a id="pgfId-1001593"></a>A <a id="marker-1005482"></a><a id="marker-1005486"></a><i class="calibre17">cycle</i><a id="marker-1005490"></a>, in a graph, is a non-empty sequence of edges <code class="fm-code-in-text">(u,v<sub class="subscript1">1</sub>), (v<sub class="subscript1">1</sub>,v<sub class="subscript1">2</sub>), ..., (v<sub class="subscript1">K</sub>,u)</code> that starts and ends at the same vertex.</p>

  <p class="body"><a id="pgfId-1001620"></a>An <i class="calibre17">acyclic graph</i><a id="marker-1005494"></a> (shown in figure 14.7) is a graph that has no cycle.</p>

  <p class="body"><a id="pgfId-1001632"></a>Both directed and undirected graphs can have cycles, and as such there is a subset of acyclic graphs that is of special interest: <i class="calibre17">directed acyclic graphs</i><a id="marker-1005498"></a> (<i class="calibre17">DAG</i>s).</p>

  <p class="body"><a id="pgfId-1001646"></a>A DAG has a few interesting properties: there must be at least one vertex that has no incoming edge (otherwise, there would be a cycle); moreover, since it’s acyclic, the set of edges defines a partial ordering on its vertices.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F7.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032152"></a>Figure 14.7 Cyclic graphs ((A) directed, (B) undirected) versus acyclic graphs ((C) and (D))</p>

  <p class="body"><a id="pgfId-1001673"></a>Given a directed acyclic graph <code class="fm-code-in-text">G=(V,E)</code>, in fact, a partial ordering is a relation <span class="cambria">≤</span> such that for any couple of vertices <code class="fm-code-in-text">(u,v)</code>, exactly one of these three conditions will hold:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1001690"></a><code class="fm-code-in-text">u</code> <span class="cambria">≤</span> <code class="fm-code-in-text">v</code>, if there is a path of any number of edges starting from <code class="fm-code-in-text">u</code> and reaching <code class="fm-code-in-text">v.</code></p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1001707"></a><code class="fm-code-in-text">v</code> <span class="cambria">≤</span> <code class="fm-code-in-text">u</code>, if there is a path of any number of edges starting from <code class="fm-code-in-text">v</code> and reaching <code class="fm-code-in-text">u</code>.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1001724"></a><code class="fm-code-in-text">u &lt;&gt; v</code>; they are not comparable because there isn’t any path from <code class="fm-code-in-text">u</code> to <code class="fm-code-in-text">v</code> or vice versa.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1001741"></a>Figure 14.8 shows a couple of generic DAGs and a chain: the latter is the only kind of DAG defining a total ordering on its nodes.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F8.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032194"></a>Figure 14.8 A few examples of DAGs. (A) A connected DAG. (B) A topological sorting for the graph; notice that the output of topological sorting is just a sequence of vertices, and that edges are not considered. Nevertheless, they are shown to demonstrate how they only go left-to-right, if vertices in the topological sorting are listed horizontally (from left to right). (C) A disconnected DAG. (D) A couple of possible topological sorting examples for the graph in (C). (E) A chain graph: for this DAG, there is only one possible topological sorting.</p>

  <p class="body"><a id="pgfId-1022717"></a>The partial ordering on DAGs provides a <i class="calibre17">topological sorting</i><a id="marker-1022729"></a>, an ordering of the vertices such that, for any edge in the graph, the edge’s starting point occurs before its ending point; usually each graph has several equivalent topological orderings. All <i class="calibre17">chains</i><a id="marker-1022731"></a>, like the one in figure 14.8 (E), certainly have a single unique topological sorting, but it’s also possible that a non-chain graph also has a single topological sorting (see figure 14.8 (A–B)).</p>

  <p class="body"><a id="pgfId-1001792"></a>DAGs and topological ordering have many fundamental applications in computer science, spanning from scheduling (at all levels) to resolving symbol dependencies <a id="marker-1022734"></a><a id="marker-1022735"></a>in <a id="marker-1022736"></a>linkers.</p>

  <h2 class="fm-head" id="heading_id_12"><a id="pgfId-1001806"></a>14.3 Graph traversal: BFS and DFS</h2>

  <p class="body"><a id="pgfId-1001822"></a>To <a id="marker-1005522"></a><a id="marker-1005526"></a><a id="marker-1005530"></a>perform searches on a graph, as well as to apply many other algorithms, it is necessary to traverse the graph following its edges.</p>

  <p class="body"><a id="pgfId-1001835"></a>As with trees, there are many possible ways to traverse a graph, depending on the order in which outgoing edges are traversed. In trees, however, it’s always clear where to start the traversal: from the root. By starting from the root, we are always sure that we can traverse a tree and reach all its nodes. In graphs, however, there isn’t a special vertex like a tree’s root, and in general, depending on the vertex chosen the starting point, there might or might not be a sequence of edges that allows us to visit all vertices.</p>

  <p class="body"><a id="pgfId-1001849"></a>In this section, we will focus on simple directed graphs; no other assumption will be made. We are not restricting ourselves to strongly connected graphs, and in general, we don’t have further domain knowledge to choose the vertex from which we should start the search. Consequently, we can’t guarantee that a single traversal will cover all the vertices in the graph. On the contrary, several “restarts” from different starting points are normally needed to visit all the vertices in a graph: we’ll show this while discussing DFS.</p>

  <p class="body"><a id="pgfId-1001864"></a>Initially, however, we will focus on a specific use case: considering the starting point as “given” (externally chosen, without the algorithm knowing much about it) and traversing the graph from it. Based on these assumptions, we will discuss the two most common traversal strategies used on graphs.</p>

  <h3 class="fm-head2" id="heading_id_13"><a id="pgfId-1001873"></a>14.3.1 Optimizing delivery routes</h3>

  <p class="body"><a id="pgfId-1001889"></a>It’s <a id="marker-1005534"></a><a id="marker-1005538"></a><a id="marker-1005542"></a>time to go back to the problem with which we introduced this chapter: we have a single delivery to perform from a source point, the warehouse or factory where goods are stocked, to a single destination, the customer’s address.</p>

  <p class="body"><a id="pgfId-1001902"></a>The hypothesis that we handle deliveries one by one is, obviously, already a simplification. In the general case, it would be too expensive, and delivery companies try to gather together orders from the same warehouse to nearby destinations to spread the costs (gas, the employee’s time, and so on) over several orders.</p>

  <p class="body"><a id="pgfId-1001917"></a>Finding the best route passing through several points, however, is a computationally hard problem;<a href="#pgfId-1008160"><sup class="footnotenumber">7</sup></a> conversely, the best route for the single-source, single-destination case can be found efficiently.</p>

  <p class="body"><a id="pgfId-1001932"></a>We’ll develop a generic solution for this problem incrementally, across this and the next few sections, starting with a further-simplified scenario and removing those simplifications step by step, while presenting more complex algorithms to solve these cases.</p>

  <p class="body"><a id="pgfId-1001941"></a>So, to start our discussion, we need to think about our goal: What’s the “best” route? We can assume, for instance, that the best route is the shortest route, but we could also prefer to find the fastest route or the cheapest one, depending on our requirements.</p>

  <p class="body"><a id="pgfId-1001952"></a>For the moment, let’s assume we want the shortest route. If we simplify the scenario and ignore factors such as traffic, road conditions, and speed limits, then we can hypothesize that the shorter the distance, the faster we can travel it.</p>

  <p class="body"><a id="pgfId-1001973"></a>But even this simplified scenario can be made simpler. Figure 14.9, for instance, shows a portion of a city where roads form a regular grid. This is a common situation in many cities in the United States, while elsewhere in the world it’s not necessarily as common. In Europe, many city centers have a plan originally designed during the Middle Age or even earlier, and roads are far less regular.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F9.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032236"></a>Figure 14.9 An example map: a portion of San Francisco’s downtown, where blocks form a regular grid</p>

  <p class="body"><a id="pgfId-1002010"></a>For the moment we can imagine restricting to this ideal situation, but why do that? Well, because it makes our job easier. If all blocks are the same, and can be approximated with squares (or rectangles whose sides’ proportions are close to squares’), then we don’t have to worry about real distances; we can just count the number of blocks traveled to compute the length of a route. The problem would be trivial to solve . . . if it wasn’t for one-way streets! Once we have a minimum viable solution working for this simplified scenario, we can think about developing it to cover more life-like situations.</p>

  <p class="body"><a id="pgfId-1002035"></a>Figure 14.10 shows the graph that can be built over the map shown in figure 14.9, where we added a vertex at each road intersection, and an edge going from vertex <code class="fm-code-in-text">v</code> to vertex <code class="fm-code-in-text">u</code> means that from the intersection modeled with <code class="fm-code-in-text">v</code> to the one modeled with <code class="fm-code-in-text">u</code>, there is a road that can be traveled in that direction (and not necessarily in the opposite direction, from <code class="fm-code-in-text">u</code> to <code class="fm-code-in-text">v</code>).</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F10.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032278"></a>Figure 14.10 Building a graph on the map in figure 14.8. We added a vertex to each road intersection (some vertices are, however, omitted for better readability) and edges connect adjacent intersections connected by a one-way street (two-way streets are modeled with a couple of edges).</p>

  <p class="body"><a id="pgfId-1002074"></a>If all roads could be traveled in both directions, we would just go west on Market from the warehouse until we crossed 10<code class="fm-code-in-text"><sup class="superscript1">th</sup></code> St., and then south on 10<code class="fm-code-in-text"><sup class="superscript1">th</sup></code> down to our destination.</p>

  <p class="body"><a id="pgfId-1002091"></a>Given the road signs in figure 14.9, however, this is not possible, and we need to take detours. In the next sub-section, we’ll see <a id="marker-1005546"></a><a id="marker-1005550"></a><a id="marker-1005554"></a>how.</p>

  <h3 class="fm-head2" id="heading_id_14"><a id="pgfId-1002108"></a>14.3.2 Breadth first search</h3>

  <p class="body"><a id="pgfId-1002124"></a>Listing 14.2 <a id="marker-1031041"></a>shows the pseudo-code for <a id="id_Hlk56660704"></a>Breadth First Search<a id="marker-1031043"></a> (<i class="fm-italics">BFS</i>), whose goal, as the name suggests, is to widen the search as much as possible, keeping a perimeter of already visited vertices, and expanding this perimeter to the neighboring vertices. This is shown in figure 14.11, where the graph in figure 14.6 (C) is used to demonstrate the first few steps of this algorithm.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F11.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032329"></a>Figure 14.11 BFS in action. In this example, we show a single-source, all-vertices run computing the distance between a source and all vertices in the graph. The algorithm explicitly maintains a queue with the vertices in the frontier, which will be explored next. The vertices are naturally ordered by their distance from the source, so a regular queue can be used, though conceptually the algorithm behaves as if it was using a priority queue. Dashed edges in the first four sub-figures are those edges traversed from source to current vertex (the one at the top of the queue on the left), while the dotted, semi-transparent edges in the bottom figure are those edges that aren’t included in the shortest paths from the source vertex to the rest of the graph.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1016213"></a>Listing 14.2 The <code class="fm-code-in-text">bfs</code> method</p>
  <pre class="programlisting"><b class="strong">function</b> bfs(graph, start, isGoal)             <span class="fm-combinumeral">❶</span>
  queue ← <b class="strong">new</b> Queue()                          <span class="fm-combinumeral">❷</span>
  queue.insert(start)                          <span class="fm-combinumeral">❸</span>
  distances ← <b class="calibre21">new</b> HashTable()                  <span class="fm-combinumeral">❹</span>
  parents ← <b class="calibre21">new</b> HashTable()                    <span class="fm-combinumeral">❺</span>
  <b class="strong">for</b> v <b class="strong">in</b> graph.vertices <b class="strong">do</b>                   <span class="fm-combinumeral">❻</span>
    distances[v] ← <b class="strong">inf</b>
    parents[v] ← <b class="strong">null</b>
  distances[start] ← 0                         <span class="fm-combinumeral">❼</span>
  <b class="strong">while</b> <b class="strong">not</b> queue.empty() <b class="strong">do</b>                   <span class="fm-combinumeral">❽</span>
    v ← queue.dequeue()                        <span class="fm-combinumeral">❾</span>
    <b class="strong">if</b> isGoal(v) <b class="strong">then</b>                          <span class="fm-combinumeral">❿</span>
      <b class="strong">return</b> (v, parents)
    <b class="strong">for</b> e <b class="strong">in</b> graph.adjacencyList[v] <b class="strong">do</b>         <span class="fm-combinumeral">⓫</span>
      u ← e.dest
      <b class="strong">if</b> distances[u] == <b class="strong">inf</b> <b class="strong">then</b>              <span class="fm-combinumeral">⓬</span>
        distances[u] ← distances[v] + 1        <span class="fm-combinumeral">⓭</span>
        parents[u] ← v
        queue.enqueue(u)
   <b class="strong">return</b> (<b class="strong">null</b>, parents)                      <span class="fm-combinumeral">⓮</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1029443"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">bfs</code><a id="marker-1029447"></a> takes a graph, a starting point vertex, and a predicate (<code class="fm-code-in-text2">isGoal</code><a id="marker-1029449"></a>) that takes a vertex and returns <code class="fm-code-in-text2">true</code><a id="marker-1029450"></a> if the goal of the search is reached. Method <code class="fm-code-in-text2">bfs</code> returns a pair with the goal vertex, and a dictionary encoding the shortest paths.</p>

  <p class="fm-code-annotation"><a id="pgfId-1029470"></a><span class="fm-combinumeral">❷</span> Initializes a simple FIFO queue</p>

  <p class="fm-code-annotation"><a id="pgfId-1029487"></a><span class="fm-combinumeral">❸</span> Adds the starting point to the queue, so that it will be extracted on the first iteration</p>

  <p class="fm-code-annotation"><a id="pgfId-1029504"></a><span class="fm-combinumeral">❹</span> Creates a new hash table to keep track of the distance of every vertex from vertex <code class="fm-code-in-text2">start</code>, that is, the minimum number of edges that needs to be traversed to get from vertex <code class="fm-code-in-text2">start</code> to each of the other vertices</p>

  <p class="fm-code-annotation"><a id="pgfId-1029521"></a><span class="fm-combinumeral">❺</span> Also creates another hash table to keep track, for each vertex <code class="fm-code-in-text2">u</code>, of the vertex through which <code class="fm-code-in-text2">u</code> was reached. This dictionary can later be used to reconstruct the path from <code class="fm-code-in-text2">start</code> to the goal.</p>

  <p class="fm-code-annotation"><a id="pgfId-1029538"></a><span class="fm-combinumeral">❻</span> Initializes vertex distances to infinity (or, equivalently, to the largest value that can be stored)</p>

  <p class="fm-code-annotation"><a id="pgfId-1029555"></a><span class="fm-combinumeral">❼</span> For the starting point only, we need to set its distance (from itself) to <code class="fm-code-in-text2">0</code>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1029572"></a><span class="fm-combinumeral">❽</span> Starts a loop, running until the queue is empty. It will run at least once, because of line #3.</p>

  <p class="fm-code-annotation"><a id="pgfId-1029589"></a><span class="fm-combinumeral">❾</span> Dequeues the head of the queue (equivalently, extracts the top of a priority queue). This will become the current vertex.</p>

  <p class="fm-code-annotation"><a id="pgfId-1029606"></a><span class="fm-combinumeral">❿</span> If we reached the goal, we are done; just return current vertex. The function <code class="fm-code-in-text2">isGoal</code><a id="marker-1029611"></a> can abstract the condition checked to find the goal vertices: it can be reaching one or more specific goal vertices, or getting to vertices that satisfy a certain condition.</p>

  <p class="fm-code-annotation"><a id="pgfId-1029624"></a><span class="fm-combinumeral">⓫</span> Iterates over the outgoing edges for current vertex</p>

  <p class="fm-code-annotation"><a id="pgfId-1029641"></a><span class="fm-combinumeral">⓬</span> If this vertex hasn’t been discovered yet (its distance has never been set), then the path with fewer edges from <code class="fm-code-in-text2">start</code> to <code class="fm-code-in-text2">u</code> certainly passes through <code class="fm-code-in-text2">v</code> (because of the way we are expanding the search frontier). Notice that because all edges are assigned distance <code class="fm-code-in-text2">1</code>, this condition is only true the first time we discover a vertex, reaching it in the least number of hops from the source.</p>

  <p class="fm-code-annotation"><a id="pgfId-1029658"></a><span class="fm-combinumeral">⓭</span> Consequently, the first time a vertex is discovered, sets its distance, its parent, and then adds it to the queue so it can later be visited in a future iteration</p>

  <p class="fm-code-annotation"><a id="pgfId-1029675"></a><span class="fm-combinumeral">⓮</span> If the goal is never reached, we need to return <code class="fm-code-in-text2">null</code> or, equivalently, another value that signals that the search failed.</p>

  <p class="body"><a id="pgfId-1002638"></a>At its core, this algorithm keeps a queue of vertices that will be visited next, the so-called <i class="fm-italics">frontier</i>. Vertices are kept in a specific order, so that they are processed from the one closest to the source, to the ones furthest away. The container used to keep track of the vertices in the frontier could be a priority queue, but it would be overkill. Because the metric we use to compute the distance of each vertex from the source is just the number of edges traveled, it turns out that the algorithm naturally discovers vertices in the same order in which they need to be processed. Furthermore, each vertex’s distance can be computed at the time of discovery (the first time each vertex is found while visiting another vertex adjacency list).</p>

  <p class="body"><a id="pgfId-1002680"></a>So, besides the initialization performed in lines #2–9, the core of the BFS algorithm is the loop at line #10: a vertex <code class="fm-code-in-text">v</code> is dequeued and visited. After (optionally) checking whether we reached the search’s goal (if so, we can already return <code class="fm-code-in-text">v</code>), we start exploring <code class="fm-code-in-text">v</code>’s neighborhood (aka its adjacency list, all its outgoing edges). If we find a vertex <code class="fm-code-in-text">u</code> that we hadn’t discovered yet, then we know that its distance from the source will be the distance to get to <code class="fm-code-in-text">v</code> plus 1; therefore we can set the distance for <code class="fm-code-in-text">u</code> and add <code class="fm-code-in-text">u</code> to the tail of the queue.</p>

  <p class="body"><a id="pgfId-1002713"></a>How can we be sure that <code class="fm-code-in-text">u</code> is going to be visited in the right order, that is, after all vertices closer to the source than <code class="fm-code-in-text">u</code> and before the ones further away than <code class="fm-code-in-text">u</code>?</p>

  <p class="body"><a id="pgfId-1002730"></a>This can be proved by induction on the distance of vertices. The base of the induction is the initial case. We add the source vertex with a distance <code class="fm-code-in-text">0</code>. This vertex is the only one at distance <code class="fm-code-in-text">0</code>, and so any other vertex added later can’t be closer than (or as close as) the source.</p>

  <p class="body"><a id="pgfId-1002750"></a>For the inductive step, we assume that our hypothesis is true for all vertices at distance <code class="fm-code-in-text">d-1</code>, and we want to prove it for vertex <code class="fm-code-in-text">v</code> at distance <code class="fm-code-in-text">d</code>; therefore, <code class="fm-code-in-text">v</code> is extracted from the queue after all vertices at distance <code class="fm-code-in-text">d-1</code> and before all vertices at distance <code class="fm-code-in-text">d+1</code>. From this, it follows that none of the vertices in the queue can have distance <code class="fm-code-in-text">d+2</code>, because no vertex at distance <code class="fm-code-in-text">d+1</code> has been visited yet, and we only add vertices to the queue when examining each visited vertex’s adjacency list (so, the distance of a vertex added can only grow by <code class="fm-code-in-text">1</code> unit with respect to its parent). In turn, this guarantees that <code class="fm-code-in-text">u</code> will be visited before any vertex at distance <code class="fm-code-in-text">d+2</code> (or further away) from the source.</p>

  <p class="body"><a id="pgfId-1002784"></a>For the inductive hypothesis, moreover, we are sure that all vertices at distance <code class="fm-code-in-text">d-1</code> are visited before <code class="fm-code-in-text">v</code>, so all vertices at distance <code class="fm-code-in-text">d</code> are already in the queue, and hence they will be visited before <code class="fm-code-in-text">u</code>.</p>

  <p class="body"><a id="pgfId-1002801"></a>This property allows us to use a simple queue instead of a priority queue. Not bad, considering that the former has a <code class="fm-code-in-text">O(1)</code> worst-case running time for enqueuing and dequeuing its elements (while heaps, for example, need <code class="fm-code-in-text">O(log(n))</code> steps for each operation), allowing us to keep the running time of BFS linear. It’s a worst-case <code class="fm-code-in-text">O(|V| + |E|)</code>, linear in the largest between the number of vertices and edges. For connected graphs, this can be simplified to <code class="fm-code-in-text">O(|E|)</code>.</p>

  <p class="body"><a id="pgfId-1002828"></a>We mentioned that checking whether search has reached the goal (line #12) and the whole concept of goal vertex are optional. We can also use BFS to just compute the paths and distances from a single source to all the other vertices in the graph.<a href="#pgfId-1008176"><sup class="footnotenumber">8</sup></a> It’s worth noting that as of today there is no known algorithm that can find the shortest path to a single destination more efficiently than BFS. In other words, it’s asymptotically equivalently expensive to compute the single-source-single-destination shortest path, and the single-source-all-destinations shortest paths.<a href="#pgfId-1008194"><sup class="footnotenumber">9</sup></a></p>

  <p class="body"><a id="pgfId-1002851"></a>If we were interested in all the distances between all the pairs of vertices, we would have better options than running BFS <code class="fm-code-in-text">|V|</code> times (but that’s out of scope <a id="marker-1005582"></a><a id="marker-1005586"></a>here).</p>

  <h3 class="fm-head2" id="heading_id_15"><a id="pgfId-1002865"></a>14.3.3 Reconstructing the path to target</h3>

  <p class="body"><a id="pgfId-1002881"></a>Quite <a id="marker-1005590"></a><a id="marker-1005594"></a>often, besides computing the minimum distance of a certain vertex from a source vertex, we are also interested in discovering the shortest path to that vertex, meaning which edges should be followed, and in which sequence, to get from source to destination.</p>

  <p class="body"><a id="pgfId-1002893"></a>As shown in the bottom part of figure 14.11, we can obtain a tree considering the “parent” relation between visited and discovered vertices: each vertex <code class="fm-code-in-text">u</code> has exactly one parent, the vertex <code class="fm-code-in-text">v</code> that was being visited when <code class="fm-code-in-text">u</code> was discovered (line #15 in listing 14.2).</p>

  <p class="body"><a id="pgfId-1002908"></a>This tree contains all the shortest paths from the source vertex to the other vertices, but the output of the BFS algorithm is just a dictionary, so how can we reconstruct these paths from the <code class="fm-code-in-text">parents</code> container that is returned?</p>

  <p class="body"><a id="pgfId-1002925"></a>Listing 14.3 describes the algorithm. It starts from the goal vertex (the last vertex in the path) and reconstructs the path backward at each step, looking for the parent of the current vertex. In simple graphs, since there is only one edge between an ordered pair of vertices, if we know that we moved from vertex <code class="fm-code-in-text">v</code> to vertex <code class="fm-code-in-text">u</code>, the edge traversed becomes implicit. In multigraphs, we would have to keep track of the actual edge chosen at each step.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1016579"></a>Listing 14.3 The path reconstruction method</p>
  <pre class="programlisting"><b class="strong">function</b> reconstructPath(parents, destination)   <span class="fm-combinumeral">❶</span>
  <b class="strong">if</b> parents[destination] == <b class="strong">null then</b>           <span class="fm-combinumeral">❷</span>
    <b class="strong">return</b> <b class="strong">null</b>
  current ← destination
  path ← [destination]                           <span class="fm-combinumeral">❸</span>
  <b class="strong">while</b> parents[current] != <b class="strong">null do</b>              <span class="fm-combinumeral">❹</span>
    current ← parents[current]                   <span class="fm-combinumeral">❺</span>
    path.insert(current)                         <span class="fm-combinumeral">❻</span>
   <b class="strong">return</b> reverse(path)                          <span class="fm-combinumeral">❼</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1028986"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">reconstructPath</code><a id="marker-1029381"></a> takes the <code class="fm-code-in-text2">parents</code> dictionary produced by <code class="fm-code-in-text2">bfs</code><a id="marker-1029382"></a> and the destination vertex. It returns the path from the source (implicit here, determined by <code class="fm-code-in-text2">parents</code>) to <code class="fm-code-in-text2">destination</code>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1029009"></a><span class="fm-combinumeral">❷</span> Checks that the destination was reachable from the source; otherwise, it returns <code class="fm-code-in-text2">null</code></p>

  <p class="fm-code-annotation"><a id="pgfId-1029026"></a><span class="fm-combinumeral">❸</span> Initializes the path that will be returned, as the list of vertices that will be visited (in reverse order, starting from the destination)</p>

  <p class="fm-code-annotation"><a id="pgfId-1029043"></a><span class="fm-combinumeral">❹</span> Loops until we get to the source, the first vertex whose parent will be <code class="fm-code-in-text2">null</code> (there will be at least one; this is guaranteed by the way <code class="fm-code-in-text2">bfs</code> works)</p>

  <p class="fm-code-annotation"><a id="pgfId-1029063"></a><span class="fm-combinumeral">❺</span> Moves backward through the path, going from current vertex to its parent, that is, the vertex that was visited, during <code class="fm-code-in-text2">bfs</code>, when <code class="fm-code-in-text2">current</code> was discovered</p>

  <p class="fm-code-annotation"><a id="pgfId-1029080"></a><span class="fm-combinumeral">❻</span> Adds this parent to the path</p>

  <p class="fm-code-annotation"><a id="pgfId-1029097"></a><span class="fm-combinumeral">❼</span> Out of the loop, before returning the path we need to reverse it, because right now the list contains vertices from destination to source</p>

  <p class="body"><a id="pgfId-1003184"></a>The code in listing 14.3 assumes that the <code class="fm-code-in-text">parents</code> dictionary is well formed, that the graph is connected, and that there is a path from source to destination. If all of that holds true, there are only two cases where the current vertex can have a <code class="fm-code-in-text">null</code> parent: either <code class="fm-code-in-text">current</code> is the destination vertex (and in that case, it means it wasn’t reachable from the source), or <code class="fm-code-in-text">current</code> is the source vertex.</p>

  <p class="body"><a id="pgfId-1003201"></a>If <code class="fm-code-in-text">parents[destination] != null</code>, in fact, this means that the destination was reached by traversing a path from the source (because of the way BFS works), and it can be proven by induction that there must be a chain of vertices between those two vertices.</p>

  <p class="body"><a id="pgfId-1003214"></a>Let’s see the algorithm in action on the result of the run shown in figure 14.11. <code class="fm-code-in-text">bfs</code>, on that graph, using <code class="fm-code-in-text">v<sub class="subscript1">1</sub></code> as source, returned the following values for parents:</p>
  <pre class="programlisting">[v<sub class="calibre25">1</sub> <span class="cambria">→</span> <code class="fm-code-in-text2">null</code>, <code class="fm-code-in-text2">v<sub class="subscript">2</sub></code> <span class="cambria">→</span> <code class="fm-code-in-text2">v<sub class="subscript">3</sub></code>, <code class="fm-code-in-text2">v<sub class="subscript">3</sub></code> <span class="cambria">→</span> <code class="fm-code-in-text2">v<sub class="subscript">1</sub></code>, <code class="fm-code-in-text2">v<sub class="subscript">4</sub></code> <span class="cambria">→</span> <code class="fm-code-in-text2">v<sub class="subscript">3</sub></code>, <code class="fm-code-in-text2">v<sub class="subscript">5</sub></code> <span class="cambria">→</span> <code class="fm-code-in-text2">v<sub class="subscript">3</sub></code>, <code class="fm-code-in-text2">v<sub class="subscript">6</sub></code> <span class="cambria">→</span> <code class="fm-code-in-text2">v<sub class="subscript">5</sub>]</code>.</pre>

  <p class="body"><a id="pgfId-1003267"></a>If we start at <code class="fm-code-in-text">v<sub class="subscript1">5</sub></code>, for instance, we see that <code class="fm-code-in-text">parents[v<sub class="subscript1">5</sub>]== v<sub class="subscript1">3</sub></code>, and this means that we need to add <code class="fm-code-in-text">v<sub class="subscript1">3</sub></code> to <code class="fm-code-in-text">path</code> after <code class="fm-code-in-text">v<sub class="subscript1">5</sub></code>, and then look at its parent, and so on.</p>

  <p class="body"><a id="pgfId-1003294"></a>Right before line #9, we have <code class="fm-code-in-text">path==[v<sub class="subscript1">5</sub>, v<sub class="subscript1">3</sub>, v<sub class="subscript1">1</sub>]</code>, and reversing it, we get the sequence of vertices we were looking <a id="marker-1005606"></a><a id="marker-1005610"></a>for.</p>

  <h3 class="fm-head2" id="heading_id_16"><a id="pgfId-1003314"></a>14.3.4 Depth first search</h3>

  <p class="body"><a id="pgfId-1003330"></a>BFS, <a id="marker-1005614"></a>as we have seen, uses a clear strategy to traverse a graph. It starts from the closest vertices to a source, and then it propagates in all directions like a wave, in concentric rings: first all the vertices at distance 1, then the next ring, with all the vertices at distance 2, and so on.</p>

  <p class="body"><a id="pgfId-1003349"></a>This strategy is, as we have seen, quite effective when we have to find the shortest paths from a source to the other vertices in a graph; at the same time, of course, this is not the only possible strategy to traverse a graph.</p>

  <p class="body"><a id="pgfId-1003358"></a>A different approach, for instance, is to traverse paths “in depth.” It’s like when you are in a maze looking for the exit: you keep going as far away from the source as possible, choosing your direction at each intersection, until you hit a dead end (in graph terms, until you get to a vertex without any untraveled outgoing edges). At that point, you retrace your steps up to the previous bifurcation (the previous vertex with at least an untraveled edge), and choose a different path.</p>

  <p class="body"><a id="pgfId-1003371"></a>This is the idea behind <i class="calibre17">Depth First Search (DFS)</i>, basically the opposite strategy from BFS. This algorithm, described in listing 14.4, can’t be used to find shortest paths, but it has numerous important applications in graph theory, and also in practice.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1016628"></a>Listing 14.4 The dfs visit of a vertex (and its neighborhood)</p>
  <pre class="programlisting"><b class="strong">function</b> dfs(graph, v, time=0, in_time={}, out_time={})                  <span class="fm-combinumeral">❶</span>
  time ← time + 1                                                        <span class="fm-combinumeral">❷</span>
  in_time[v] ← time                                                      <span class="fm-combinumeral">❸</span>
  <b class="strong">for</b> e <b class="strong">in</b> graph.adjacencyList[v] <b class="strong">do</b>                                     <span class="fm-combinumeral">❹</span>
    u ← e.dest
    <b class="strong">if</b> in_time[u] == <b class="strong">null</b> <b class="strong">then</b>                                           <span class="fm-combinumeral">❺</span>
      (time, in_time, out_time) ← dfs(graph, u, time, in_time, out_time) <span class="fm-combinumeral">❻</span>
  time ← time + 1                                                        <span class="fm-combinumeral">❼</span>
  out_time[v] ← time                                                     <span class="fm-combinumeral">❽</span>
  <b class="strong">return</b> (time, in_time, out_time)                                       <span class="fm-combinumeral">❾</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1028166"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">dfs</code><a id="marker-1028362"></a> takes a graph, a starting point vertex, and a few accessory arguments to keep track of the time of discovery for this vertex, and returns the same arguments updated.</p>

  <p class="fm-code-annotation"><a id="pgfId-1028184"></a><span class="fm-combinumeral">❷</span> Increments the “time” counter. This is needed to keep track of the order of discovery of vertices.</p>

  <p class="fm-code-annotation"><a id="pgfId-1028207"></a><span class="fm-combinumeral">❸</span> Current vertex has just been visited, so record it</p>

  <p class="fm-code-annotation"><a id="pgfId-1028224"></a><span class="fm-combinumeral">❹</span> Iterates over all the outgoing edges of vertex <code class="fm-code-in-text2">v</code></p>

  <p class="fm-code-annotation"><a id="pgfId-1028241"></a><span class="fm-combinumeral">❺</span> If a vertex in-time is still <code class="fm-code-in-text2">null</code>, then it hasn’t been discovered yet (provided proper initialization of this argument). If the edge’s destination vertex <code class="fm-code-in-text2">u</code> hadn’t already been discovered, then we should traverse the edge <code class="fm-code-in-text2">e</code> and visit <code class="fm-code-in-text2">u</code>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1028258"></a><span class="fm-combinumeral">❻</span> Recursively calls <code class="fm-code-in-text2">dfs</code> on vertex <code class="fm-code-in-text2">u</code>, and updates all the auxiliary data, including the <code class="fm-code-in-text2">time</code> counter</p>

  <p class="fm-code-annotation"><a id="pgfId-1028275"></a><span class="fm-combinumeral">❼</span> Once all outgoing edges have been traversed, increments the time by one extra unit (notice that <code class="fm-code-in-text2">time</code> might already have been incremented in the recursive calls)</p>

  <p class="fm-code-annotation"><a id="pgfId-1028292"></a><span class="fm-combinumeral">❽</span> Since we are leaving this vertex for good (we traversed all its outgoing edges), we can set its out-time.</p>

  <p class="fm-code-annotation"><a id="pgfId-1028309"></a><span class="fm-combinumeral">❾</span> Returns updated values for the time</p>

  <p class="body"><a id="pgfId-1003668"></a>Figure 14.12 shows an example of DFS traversal on the same graph we used to illustrate BFS and using the same vertex, <code class="fm-code-in-text">v<sub class="subscript1">1</sub></code>, as a starting point. You can see how the sequence of vertices visited is completely different (and not just because of the way we break ties about which edge to traverse first). The most noticeable detail is, perhaps, that we use a stack instead of a queue to keep track of the next vertices to be visited.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F12.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032444"></a>Figure 14.12 DFS in action. The algorithm implicitly (usually, in recursive implementations—otherwise explicitly) maintains a stack with the next vertices to be traversed. If the stack is kept explicitly, it is also necessary to remember which edges have been travelled.</p>

  <p class="body"><a id="pgfId-1003704"></a>Similarly to BFS, there is no guarantee that a single run of a DFS traversal will visit all vertices in the graph. This is shown in figure 14.13, where instead of starting the traversal from <code class="fm-code-in-text">v<sub class="subscript1">1</sub></code>, we choose to start from <code class="fm-code-in-text">v<sub class="subscript1">4</sub></code>. Because the graph has two strongly connected components, and the first one can’t be reached from <code class="fm-code-in-text">v<sub class="subscript1">4</sub></code>, this unavoidably means that vertices <code class="fm-code-in-text">v<sub class="subscript1">1</sub></code> to <code class="fm-code-in-text">v<sub class="subscript1">3</sub></code> can’t be visited in this traversal.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F13.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032498"></a>Figure 14.13 DFS in action. The starting point is crucial in the graph’s traversal; both DFS and BFS couldn’t reach all vertices, for instance, if the traversal started at vertex <code class="fm-code-in-text">v4</code> (or, for what is worth, <code class="fm-code-in-text">v5</code> or <code class="fm-code-in-text">v6</code>). In this figure, vertices are also marked with the “time” they are processed, and the time they are removed from the stack. These values are relevant for several algorithms.</p>

  <p class="body"><a id="pgfId-1003765"></a>To complete the traversal of the graph, we need to start another traversal in one of the remaining vertices, the ones that haven’t been visited before: this is shown in figure 14.14 where the new traversal starts from vertex <code class="fm-code-in-text">v<sub class="subscript1">3</sub></code>, and avoids the three vertices that hadn’t already been visited in the first run.</p>

  <p class="body"><a id="pgfId-1003783"></a>To complete the discussion, listing 14.5 shows the code to perform a full DFS traversal of a graph, with restarts.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1016658"></a>Listing 14.5 The <code class="fm-code-in-text">dfs</code> traversal of a graph</p>
  <pre class="programlisting"><b class="strong">function</b> dfs(graph)                                <span class="fm-combinumeral">❶</span>
  time ← 0                                         <span class="fm-combinumeral">❷</span>
  in_time ← <b class="strong">null</b> (<span class="cambria">∀</span> v <span class="cambria">ϵ</span> graph)                     <span class="fm-combinumeral">❸</span>
  out_time ← <b class="strong">null</b> (<span class="cambria">∀</span> v <span class="cambria">ϵ</span> graph)
  <b class="strong">for</b> v <b class="strong">in</b> graph.vertices <b class="strong">do</b>                        <span class="fm-combinumeral">❹</span>
    <b class="strong">if</b> in_time[v] == <b class="strong">null</b> <b class="strong">then</b>                      <span class="fm-combinumeral">❺</span>
      (time, in_time, out_time) ←
          dfs(graph, v, time, in_time, out_time)    <span class="fm-combinumeral">❻</span>
  <b class="strong">return</b> (in_time, out_time)                        <span class="fm-combinumeral">❼</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1027581"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">dfs</code><a id="marker-1027585"></a> takes a graph, traverses all of its vertices, and returns two dictionaries with the times at which each vertex was entered and exited.</p>

  <p class="fm-code-annotation"><a id="pgfId-1027606"></a><span class="fm-combinumeral">❷</span> Initializes the “time” counter. This is needed to keep track of the order of discovery of vertices.</p>

  <p class="fm-code-annotation"><a id="pgfId-1027623"></a><span class="fm-combinumeral">❸</span> Initializes the enter and exit times for each vertex to <code class="fm-code-in-text2">null</code></p>

  <p class="fm-code-annotation"><a id="pgfId-1027640"></a><span class="fm-combinumeral">❹</span> Iterates over all the vertices in the graph</p>

  <p class="fm-code-annotation"><a id="pgfId-1027657"></a><span class="fm-combinumeral">❺</span> If a vertex <code class="fm-code-in-text2">v</code>’s in-time is still <code class="fm-code-in-text2">null</code>, then it hasn’t been discovered yet.</p>

  <p class="fm-code-annotation"><a id="pgfId-1027674"></a><span class="fm-combinumeral">❻</span> If so, starts a <code class="fm-code-in-text2">dfs</code> traversal from <code class="fm-code-in-text2">v</code></p>

  <p class="fm-code-annotation"><a id="pgfId-1027691"></a><span class="fm-combinumeral">❼</span> Once all vertices have been visited, returns the enter and exit times</p>

  <p class="body"><a id="pgfId-1004040"></a>It’s also possible to pass a callback to method <code class="fm-code-in-text">dfs</code><a id="marker-1005626"></a>, so that during traversal it can be called on each vertex visited. This can be used for a number of things, from updating the graph (changing vertices’ labels or any other attribute associated), to computing certain arbitrary operations on graphs. All you need to do is pass the callback as an extra argument in listings 14.4 and 14.5 and call the callback as the first thing in listing 14.4.</p>

  <p class="body"><a id="pgfId-1004057"></a>You can also see that in figures 14.13 and 14.14, we also added the “times” at which vertices are visited and exited (after that their whole adjacency list has been traversed). These values are fundamental for several applications, such as computing the topological sorting (for DAGs: just order vertices by reversed out-time), finding cycles (if a neighbor of the currently visited vertex has an in-time, but not an out-time), or computing connected components.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F14.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032553"></a>Figure 14.14 DFS in action. Following up on the example in figure 14.13, we can resume the traversal and get to all the vertices by randomly selecting one of the vertices left, and restarting DFS from it.</p>

  <p class="body"><a id="pgfId-1004098"></a>A final note on performance: as for BFS, this algorithm also has a linear running time <code class="fm-code-in-text">O(|V| + |E|)</code> and requires <code class="fm-code-in-text">O(|V|)</code> recursive calls (or, equivalently, <code class="fm-code-in-text">O(|V|)</code> extra space for the <a id="marker-1005630"></a><a id="marker-1005634"></a>stack).</p>

  <h3 class="fm-head2" id="heading_id_17"><a id="pgfId-1004117"></a>14.3.5 It’s queue vs stack again</h3>

  <p class="body"><a id="pgfId-1004135"></a>When <a id="marker-1005638"></a><a id="marker-1005642"></a><a id="marker-1005646"></a><a id="marker-1005650"></a>we look at these traversal algorithms, the first thing that should be clear is that their purposes, the contexts in which they are applied, are fundamentally different. BFS is used when the source vertex <code class="fm-code-in-text">S</code> is known, and we want to find the shortest path to a certain goal (a specific vertex, all vertices that can be reached from <code class="fm-code-in-text">S</code>, or a certain condition).</p>

  <p class="body"><a id="pgfId-1004155"></a>DFS, however, is mostly used when we need to touch all the vertices, and we don’t care where we start. This algorithm provides great insight into a graph’s structure, and it’s used as a basis for several algorithms, including finding a topological sorting for DAGs, or computing the strongly connected components of a directed graph.</p>

  <p class="body"><a id="pgfId-1004164"></a>The interesting bit about these algorithms i<a id="Current"></a>s that their basic version, performing only the traversal, can be rewritten as the same templated algorithm where newly discovered vertices are added to a container from which, at each iteration, we get the next element. For BFS, this container will be a queue and we’ll process vertices in the order they are discovered, while for DFS, it will be a stack (implicit in the recursive version of this algorithm), and the traversal will try to go as far as possible before backtracking and visit all vertex’s <a id="marker-1005654"></a><a id="marker-1005658"></a><a id="marker-1005662"></a><a id="marker-1005666"></a>neighbors.</p>

  <h3 class="fm-head2" id="heading_id_18"><a id="pgfId-1004184"></a><a id="id_Hlk534499914"></a><a id="id_Toc507447821"></a>14.3.6 Best route to deliver a parcel</h3>

  <p class="body"><a id="pgfId-1004202"></a>Now <a id="marker-1005670"></a><a id="marker-1005674"></a><a id="marker-1005678"></a>that we have seen how BFS works and how to reconstruct the path from a source to a destination, we can go back to our example and apply BFS to the graph in figure 14.10; the result is shown in figure 14.15.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F15.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032593"></a>Figure 14.15 The shortest path to destination, and shortest distances to all the vertices, computed using BFS on the graph in figure 14.10. The shortest path is shown with dashed arrows, while distances are shown next to each vertex.</p>

  <p class="body"><a id="pgfId-1004217"></a>In this case, the shortest path is pretty obvious—it’s the closest possible path to the “straight-line distance” between the source and the destination, and it’s also the path of minimum <i class="calibre17">Manhattan Distance</i><a id="id_Hlk56662143"></a><a id="marker-1005682"></a>.<i class="calibre17"><a href="#pgfId-1008207"><sup class="footnotenumber">10</sup></a></i></p>

  <p class="body"><a id="pgfId-1004232"></a>And yet, by just removing the edge between vertices <code class="fm-code-in-text">v<sub class="subscript1">9</sub></code> and <code class="fm-code-in-text">v<sub class="subscript1">E</sub></code>, the result would have to change completely. Try it out as an exercise to work out the solution for this modified case (manually or by writing your own version of BFS and <a id="marker-1005686"></a><a id="marker-1005690"></a><a id="marker-1005694"></a>running <a id="marker-1005698"></a><a id="marker-1005702"></a>it).</p>

  <h2 class="fm-head" id="heading_id_19"><a id="pgfId-1004276"></a>14.4 Shortest path in weighted graphs: Dijkstra</h2>

  <p class="body"><a id="pgfId-1004295"></a><a id="id_Hlk534499932"></a>Simplifying <a id="marker-1005706"></a><a id="marker-1005710"></a>our scenario allowed us to use a simple and fast algorithm, BFS, to obtain an approximated shortest path for our deliveries. While our simplification works well for modern city centers, such as for downtown San Francisco, it can’t be applied to more generic scenarios. If we need to optimize deliveries for a wider area in San Francisco, or in other cities lacking this regular road structure, approximating distances with the number of blocks traveled doesn’t work well anymore.</p>

  <p class="body"><a id="pgfId-1004313"></a>If we move from San Francisco (or Manhattan) to Dublin’s center, for instance, as figure 14.16 shows, streets don’t have a regular layout anymore, and blocks can greatly vary in size and shape, so we need to take into account the actual distance between each pair of intersections, which won’t be their Manhattan distance anymore.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F16.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032755"></a>Figure 14.16 An example of city map (Dublin’s city center), where the simplifications used to apply BFS for shortest paths wouldn’t be possible</p>

  <h3 class="fm-head2" id="heading_id_20"><a id="pgfId-1004345"></a>14.4.1 Differences with BFS</h3>

  <p class="body"><a id="pgfId-1004357"></a>Like <a id="marker-1032690"></a><a id="marker-1032691"></a><a id="marker-1032692"></a>BFS, Dijkstra’s algorithm takes a graph and a source vertex as input (optionally a goal vertex as well), and computes the minimum distance from the source to the goal (or equivalently, with the same asymptotic running time, to all the other vertices in the graph). Differently than for BFS, though, in Dijkstra’s algorithm the distance between two vertices is measured in terms of edges’ weight. Consider figure 14.17, which shows a directed, weighted graph that models the map shown in figure 14.16.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F17.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032792"></a>Figure 14.17 Overlaying a directed weighted graph over the map in figure 14.16. Edges’ weights are the distances (in meters) between the intersections modeled by the edge’s vertices.</p>

  <p class="body"><a id="pgfId-1004405"></a>In this context, the minimum distance between two vertices <code class="fm-code-in-text">u</code> and <code class="fm-code-in-text">v</code> is the minimum sum, across all paths from <code class="fm-code-in-text">u</code> to <code class="fm-code-in-text">v</code>, of the weights of edges in the path. If there is no such path, that is, if there is no way to go from <code class="fm-code-in-text">u</code> to <code class="fm-code-in-text">v</code>, then the distance between them is considered to be infinite.</p>

  <p class="body"><a id="pgfId-1004450"></a>Figure 14.18 shows, on a simpler example graph, how Dijkstra’s algorithm works. It is similar to BFS, with two main differences:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1004463"></a>The metric used is the sum of weights instead of path lengths.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1004478"></a>Consequently, the container needs to be used to keep track of the next vertices to be visited: we can’t make do with a plain queue anymore; we need a priority queue.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1004494"></a>Everything else, the logic of the algorithm and the auxiliary data used, is similar to BFS. That’s very convenient for us because we can rewrite this algorithm from listing 14.2 with minimal changes. If you think that this similarity is a coincidence, though, hold your breath untill <a id="marker-1005726"></a><a id="marker-1005730"></a><a id="marker-1005734"></a>section 14.5.<a href="#pgfId-1008223"><sup class="footnotenumber">11</sup></a></p>

  <h3 class="fm-head2" id="heading_id_21"><a id="pgfId-1004513"></a>14.4.2 Implementation</h3>

  <p class="body"><a id="pgfId-1004525"></a>Listing 14.6 <a id="marker-1005738"></a><a id="marker-1005742"></a>describes Dijkstra’s algorithm in detail. Comparing it to listing 14.2, you can see how it resembles the BFS algorithm, so much so that we can use the same algorithm shown in listing 14.3 to reconstruct the shortest path for Dijkstra’s as well. Nonetheless, we need to be even more careful about performance in this <a id="marker-1005746"></a><a id="marker-1005750"></a>case.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1016725"></a>Listing 14.6 Dijkstra’s algorithm</p>
  <pre class="programlisting"><b class="calibre21">function</b> dijkstra(graph, start, isGoal)                   <span class="fm-combinumeral">❶</span>
  queue ← <b class="calibre21">new</b> PriorityQueue()                             <span class="fm-combinumeral">❷</span>
  queue.insert(start, 0)                                  <span class="fm-combinumeral">❸</span>
  distances[v] ← <b class="calibre21">inf</b> (<span class="cambria">∀</span> v <span class="cambria">ϵ</span> graph | v &lt;&gt; start)           <span class="fm-combinumeral">❹</span>
  parents[v] ← <b class="calibre21">null</b> (<span class="cambria">∀</span> v <span class="cambria">ϵ</span> graph)                         <span class="fm-combinumeral">❺</span>
  distances[start] ← 0                                    <span class="fm-combinumeral">❻</span>
  <b class="strong">while</b> <b class="strong">not</b> queue.empty() <b class="strong">do</b>                              <span class="fm-combinumeral">❼</span>
    v ← queue.top()                                       <span class="fm-combinumeral">❽</span>
    <b class="strong">if</b> isGoal(v) <b class="strong">then</b>                                     <span class="fm-combinumeral">❾</span>
      <b class="strong">return</b> (v, parents)
    <b class="strong">else</b>
      <b class="strong">for</b> e in graph.adjacencyList[v] <b class="strong">do</b>                  <span class="fm-combinumeral">❿</span>
        u ← e.dest
        <b class="strong">if</b> distances[u] &gt; distances[v] + e.weight <b class="strong">then</b>    <span class="fm-combinumeral">⓫</span>
          distances[u] ← e.weight + distances[v]          <span class="fm-combinumeral">⓬</span>
          parents[u] ← v
          queue.update(u, distances[u])
  <b class="calibre21">return</b> (null, parents)                                  <span class="fm-combinumeral">⓭</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1026667"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">dijkstra</code><a id="marker-1026678"></a> takes a graph, a starting point vertex, and a predicate (<code class="fm-code-in-text2">isGoal</code><a id="marker-1026680"></a>) that takes a vertex and returns <code class="fm-code-in-text2">true</code><a id="marker-1026681"></a> if the goal of the search is reached. Method <code class="fm-code-in-text2">dijkstra</code> returns a pair with the goal vertex (if reached) and a dictionary encoding the shortest paths.</p>

  <p class="fm-code-annotation"><a id="pgfId-1026701"></a><span class="fm-combinumeral">❷</span> Initializes a priority queue. This concretely could be a heap, for instance.</p>

  <p class="fm-code-annotation"><a id="pgfId-1026718"></a><span class="fm-combinumeral">❸</span> Adds the starting point to the priority queue, using its distance (<code class="fm-code-in-text2">0</code>) as its priority, so that it will be extracted on the first iteration</p>

  <p class="fm-code-annotation"><a id="pgfId-1026735"></a><span class="fm-combinumeral">❹</span> Creates a new hash table to keep track of the distance of each vertex from vertex <code class="fm-code-in-text2">start</code>, that is, the sum of the weights of edges that need to be traversed to get from vertex <code class="fm-code-in-text2">start</code> to each other vertex. Initializes the distance to infinity for all vertices in the graph, except for <code class="fm-code-in-text2">start</code>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1026752"></a><span class="fm-combinumeral">❺</span> Also creates another hash table to keep track, for each vertex <code class="fm-code-in-text2">u</code>, of the vertex through which <code class="fm-code-in-text2">u</code> was reached. This dictionary can later be used to reconstruct the path from the <code class="fm-code-in-text2">start</code> to the goal.</p>

  <p class="fm-code-annotation"><a id="pgfId-1026769"></a><span class="fm-combinumeral">❻</span> While we initialized the vertex distances to infinity (or, equivalently, to the largest value that can be stored) for most vertices, for the starting point only, we need to set its distance (from itself) to <code class="fm-code-in-text2">0</code>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1026786"></a><span class="fm-combinumeral">❼</span> Starts a loop, running until the priority queue is empty. It will run at least once because of line #3.</p>

  <p class="fm-code-annotation"><a id="pgfId-1026803"></a><span class="fm-combinumeral">❽</span> Extracts the top of the priority queue. The vertex extracted will become current vertex for this iteration, <code class="fm-code-in-text2">v</code>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1026820"></a><span class="fm-combinumeral">❾</span> If we reached the goal, we are done, and it just returns the current vertex. The function <code class="fm-code-in-text2">isGoal</code> can abstract the condition checked to find the goal: it can reach one or more specific target vertices or get to vertices that satisfy a certain condition.</p>

  <p class="fm-code-annotation"><a id="pgfId-1026837"></a><span class="fm-combinumeral">❿</span> Iterates over the outgoing edges for the current vertex</p>

  <p class="fm-code-annotation"><a id="pgfId-1026854"></a><span class="fm-combinumeral">⓫</span> If vertex <code class="fm-code-in-text2">u</code> hasn’t been visited yet, then maybe its distance from <code class="fm-code-in-text2">start</code> can be made shorter by passing through <code class="fm-code-in-text2">v</code>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1026871"></a><span class="fm-combinumeral">⓬</span> If so, we need to update <code class="fm-code-in-text2">u</code>’s distance, its parent, and then update the queue, so that <code class="fm-code-in-text2">u</code> will be popped from it at the right time (that is, when it becomes the closest vertex to the frontier of visited vertices).</p>

  <p class="fm-code-annotation"><a id="pgfId-1026888"></a><span class="fm-combinumeral">⓭</span> If the goal is never reached, we need to return <code class="fm-code-in-text2">null</code> or, equivalently, another value that signals that search failed. We can still return the <code class="fm-code-in-text2">parents</code> dictionary to reconstruct the shortest paths to all the vertices that were reachable from <code class="fm-code-in-text2">start</code>.</p>

  <h3 class="fm-head2" id="heading_id_22"><a id="pgfId-1005034"></a>14.4.3 Analysis</h3>

  <p class="body"><a id="pgfId-1005046"></a>While <a id="marker-1005766"></a><a id="marker-1005770"></a>in BFS each vertex was added to the (plain) queue and never updated, Dijkstra’s algorithm uses a priority queue to keep track of the closest discovered vertices, and it’s possible that the priority<a href="#pgfId-1008241"><sup class="footnotenumber">12</sup></a> of a vertex changes after it has already been added to the queue.</p>

  <p class="body"><a id="pgfId-1005062"></a>This is due to a fundamental difference. BFS only uses the number of edges traversed as a metric, and if we use the edge’s weight instead, then it’s possible that a path including more edges has a lower weight than another path including fewer edges. For instance, looking at figure 14.18, there are two paths between <code class="fm-code-in-text">v</code><a id="id_Hlk31373885"></a><code class="fm-code-in-text"><sub class="subscript1">1</sub></code> and <code class="fm-code-in-text">v<sub class="subscript1">2</sub></code>; the path <code class="fm-code-in-text">v<sub class="subscript1">1</sub></code><span class="cambria">→</span><code class="fm-code-in-text">v<sub class="subscript1">3</sub></code><span class="cambria">→</span><code class="fm-code-in-text">v<sub class="subscript1">2</sub></code> only traverses <code class="fm-code-in-text">2</code> edges, but its total weight is <code class="fm-code-in-text">8</code>, while the other path, <code class="fm-code-in-text">v<sub class="subscript1">1</sub></code><span class="cambria">→</span><code class="fm-code-in-text">v<sub class="subscript1">3</sub></code><span class="cambria">→</span><code class="fm-code-in-text">v<sub class="subscript1">5</sub></code><span class="cambria">→</span><code class="fm-code-in-text">v<sub class="subscript1">2</sub></code> has length <code class="fm-code-in-text">3</code>, but its total weight is just <code class="fm-code-in-text">5</code>. The second path is longer, and visits one more vertex between <code class="fm-code-in-text">v<sub class="subscript1">3</sub></code> and <code class="fm-code-in-text">v<sub class="subscript1">2</sub></code>, so the distance to <code class="fm-code-in-text">v<sub class="subscript1">2</sub></code> will be initially set to <code class="fm-code-in-text">8</code> (when <code class="fm-code-in-text">v<sub class="subscript1">3</sub></code> is visited), and then updated to <code class="fm-code-in-text">5</code> when it’s <code class="fm-code-in-text">v<sub class="subscript1">5</sub></code>’s turn to be visited.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F18.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032845"></a>Figure 14.18 Running Dijkstra’s algorithm on a directed graph (derived from the example in figure 14.10)</p>

  <p class="body"><a id="pgfId-1005151"></a>Because of this behavior, every time a new vertex is visited, potentially all its neighbors’ priorities can be updated.</p>

  <p class="body"><a id="pgfId-1005160"></a>In turn, this influences the asymptotic performance of Dijkstra’s algorithm, which depends on how efficiently this “update priority” operation can be implemented. In detail, for Dijkstra’s algorithm, we can implement the priority queue as</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1005169"></a>An array (sorted or unsorted, as described in appendix C)</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1005181"></a>A heap</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1005193"></a>A Fibonacci heap</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1005208"></a>The running time using an array for the priority queue is going to be <code class="fm-code-in-text">O(|E|*|V|)</code>, because each vertex will require <code class="fm-code-in-text">O(|V|)</code> operations to update priority or to adjust the queue after extraction.</p>

  <p class="body"><a id="pgfId-1005223"></a>With the remaining two choices, the running time is <code class="fm-code-in-text">O(|V|*log(|V|) + |E|*DQ(|V|))</code> where</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1005234"></a><code class="fm-code-in-text">|V|</code> is the number of vertices on the graph.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1005248"></a><code class="fm-code-in-text">|E|</code> is the number of edges.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1005264"></a><code class="fm-code-in-text">DQ(|V|)</code> is the (average) running time of each “priority update” operation.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1005281"></a>Table 14.2 summarizes the running time of Dijkstra’s algorithm, relating it to the implementation of priority queue used.</p>

  <p class="fm-table-caption"><a id="pgfId-1017092"></a>Table 14.2 Running time of Dijkstra’s algorithm, on a connected graph G=(V,E)</p>

  <table border="1" class="contenttable" width="100%">
    <tr class="calibre8">
      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1017100"></a> </p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1017102"></a>Array</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1017104"></a>Heap</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1017106"></a>Fibonacci Heap</p>
      </th>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1017108"></a><code class="fm-code-in-text2">Running Time</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1017110"></a><code class="fm-code-in-text2">O(|E|*|V|)</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1017112"></a><code class="fm-code-in-text2">O(|E|*log(|V|))</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1017114"></a><code class="fm-code-in-text2">O(|V|*log(|V|) + |E|)<a class="calibre14" href="#pgfId-1017140"><sup class="footnotenumber2">13</sup></a></code></p>
      </td>
    </tr>
  </table>

  <p class="body"><a id="pgfId-1005902"></a>The best theoretical result is obtained with a Fibonacci heap, for which the amortized time to decrease the priority of an element is <code class="fm-code-in-text">O(1)</code>. However, this data structure is complicated to implement and inefficient in practice, so our best bet is using heaps. As we saw in section 2.9, d-way heaps allow us to have a more efficient implementation in <a id="marker-1017199"></a><a id="marker-1017200"></a>practice.</p>

  <h3 class="fm-head2" id="heading_id_23"><a id="pgfId-1005922"></a>14.4.4 Shortest route for deliveries</h3>

  <p class="body"><a id="pgfId-1005938"></a>So <a id="marker-1007952"></a><a id="marker-1007956"></a>far in this section, we’ve discussed how Dijkstra’s algorithm works, how we can implement it, and its performance.</p>

  <p class="body"><a id="pgfId-1005950"></a>Now there is only one thing left to tackle: how we apply this algorithm to our example and find the shortest route to deliver an order to a customer.</p>

  <p class="body"><a id="pgfId-1005959"></a>The good news is that it’s actually straightforward. Once we have created the graph in figure 14.17, we can just apply the algorithm to it (or to the example in figure 14.10, exactly how we did for BFS) and reconstruct the shortest path.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F19.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032887"></a>Figure 14.19 The result of running Dijkstra’s algorithm on the example in figure 14.17. Notice that there are two paths from source (<code class="fm-code-in-text">V<sub class="subscript1">S</sub></code>) to destination (<code class="fm-code-in-text">V<sub class="subscript1">G</sub></code>) whose length is very close; see how hard it would be to spot the shortest (and yet counterintuitive) one. Dashed lines are used for edges in the shortest path, solid lines are used for edges that are traversed, but don’t end up in the shortest path, and finally dotted lines are used for those edges that aren’t even traversed (because, at some point, it’s clear that any path including them would be longer than the shortest path found).</p>

  <p class="body"><a id="pgfId-1005974"></a>The result for this section’s example is shown in figure 14.19, where we computed and showed the shortest distance from the source vertex (<code class="fm-code-in-text">v<sub class="subscript1">S</sub></code>) to every other vertex. Notice that there are some edges, drawn with a thin dotted line, that don’t belong to any shortest path, while the shortest path to our destination, <code class="fm-code-in-text">v<sub class="subscript1">G</sub></code>, is highlighted with thick dashed lines.</p>

  <p class="body"><a id="pgfId-1006022"></a>This example is perfect to illustrate the need of algorithms such as Dijkstra’s because there are two paths between <code class="fm-code-in-text">v<sub class="subscript1">S</sub></code> and <code class="fm-code-in-text">v<sub class="subscript1">G</sub></code> that add up almost to the same distance, and our intuition would likely go for the longest one, because it looks more linear.</p>

  <p class="body"><a id="pgfId-1006040"></a>One final consideration: As we mentioned, applying the algorithm was straightforward, but only because of one property of this graph. It doesn’t have any edge with negative weight. Any such edge would, in fact, violate the assumption behind the algorithm: that if we expand the frontier of visited vertices by choosing the closest unvisited vertex at each iteration, then at the time a vertex is visited, we know its minimum distance from start.</p>

  <p class="body"><a id="pgfId-1006055"></a>This happens because Dijkstra’s algorithm (like BFS) is a <i class="fm-italics">greedy</i> algorithm, a kind of algorithm that can find the solution to a problem by making locally optimal choices. In fact, to decide which vertex to visit next, we only need to take into account the outgoing edges of the vertices we’ve already visited. Greedy algorithms can only be applied to certain problems: having negative edges makes a problem unfit to be solved with any greedy algorithm, because locally-optimal choices won’t be possible anymore.</p>

  <p class="body"><a id="pgfId-1006072"></a>Negative-weight edges might seem counterintuitive, but they are actually quite common. If we measure distances using the gas consumed to travel between two vertices, and the goal is to not end up with an empty tank, then an edge corresponding to a road with a gas station could have a negative weight. Likewise, if we associate a cost to the gas, then an edge that allows us to perform a second delivery or a pick-up could have negative cost because we could earn extra money if we travel it.</p>

  <p class="body"><a id="pgfId-1006089"></a>To cope with negative edges, we need to use <i class="fm-italics">Bellman-Ford</i>’s algorithm<a id="marker-1007960"></a>, an ingenious algorithm that uses the dynamic programming technique to derive a solution that takes into account negative-weight edges. Bellman-Ford’s algorithm is more expensive to run than Dijkstra’s; its running time is <code class="fm-code-in-text">O(|V|*|E|)</code>. Although it can be applied to a broader set of graphs, it too has some limitations: it can’t work with graphs with negative-weight cycles<a href="#pgfId-1008255"><sup class="footnotenumber">14</sup></a> (at the same time, though, it can be used as a test to find <a id="marker-1007964"></a><a id="marker-1007968"></a>such <a id="marker-1007972"></a><a id="marker-1007976"></a>cycles).</p>

  <h2 class="fm-head" id="heading_id_24"><a id="pgfId-1006112"></a><a id="id_Toc507447822"></a>14.5 Beyond Dijkstra’s algorithm: A*</h2>

  <p class="body"><a id="pgfId-1006124"></a>As <a id="marker-1007980"></a><a id="marker-1007984"></a>we have seen, BFS and Dijkstra’s algorithms are very similar to each other; it turns out that they both are a particular case of the A* (pronounced “<i class="calibre17">A-star”</i>) algorithm.</p>

  <p class="body"><a id="pgfId-1006140"></a>This algorithm, shown in listing 14.7, is not just more generic; it improves the performance of Dijkstra’s algorithm in at least two different situations. Before delving into those scenarios, though, let’s focus on the differences between these algorithms and A*.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1017226"></a>Listing 14.7 The A* algorithm</p>
  <pre class="programlisting"><b class="strong">function</b> aStar(graph, start, isGoal, distance, heuristic)     <span class="fm-combinumeral">❶</span>
  queue ← <b class="strong">new</b> PriorityQueue()                                 <span class="fm-combinumeral">❷</span>
  queue.insert(start, 0)                                      <span class="fm-combinumeral">❸</span>
  distances[v] ← <b class="calibre21">inf</b> (<span class="cambria">∀</span> v <span class="cambria">ϵ</span> graph | v &lt;&gt; start)               <span class="fm-combinumeral">❹</span>
  fScore[v] ← <b class="calibre21">inf</b> (<span class="cambria">∀</span> v <span class="cambria">ϵ</span> graph | v &lt;&gt; start)                  <span class="fm-combinumeral">❺</span>
  parents[v] ← <b class="calibre21">null</b> ( v <span class="cambria">ϵ</span> graph)                              <span class="fm-combinumeral">❻</span>
  distances [start] ← 0                                       <span class="fm-combinumeral">❼</span>
  fScore[start] ← heuristic(start)
  <b class="calibre21">while</b>  <b class="strong">not</b> queue.empty() <b class="calibre21">do</b>                                 <span class="fm-combinumeral">❽</span>
    v ← queue.top()                                           <span class="fm-combinumeral">❾</span>
    <b class="strong">if</b> isGoal(v) <b class="strong">then</b>                                         <span class="fm-combinumeral">❿</span>
      <b class="strong">return</b> (v, parents)
    <b class="strong">else</b>
      <b class="strong">for</b> e <b class="strong">in</b> graph.adjacencyList[v] <b class="strong">do</b>                      <span class="fm-combinumeral">⓫</span>
        u ← e.dest
        <b class="strong">if</b> distances[u] &gt; distances[v] + distance(e) <b class="strong">then</b>     <span class="fm-combinumeral">⓬</span>
          distances[u] ← distance(e) + distances[v]           <span class="fm-combinumeral">⓭</span>
          fScore[u] ← distances[u] + heuristic(u)             <span class="fm-combinumeral">⓮</span>
          parents[u] ← v
          queue.update(u, fScore[u])
  <b class="strong">return</b> (<b class="strong">null</b>, parents)                                      <span class="fm-combinumeral">⓯</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1025431"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">aStar</code><a id="marker-1025737"></a> takes: a graph; a vertex; the starting point; a predicate (<code class="fm-code-in-text2">isGoal</code><a id="marker-1025738"></a>) that takes a vertex and returns <code class="fm-code-in-text2">true</code><a id="marker-1025740"></a> if the goal of the search is reached; a function (<code class="fm-code-in-text2">distance</code><a id="marker-1025741"></a>) that takes an edge and returns a float, the distance between its two vertices; a function (<code class="fm-code-in-text2">heuristic</code><a id="marker-1025742"></a>) that takes a vertex <code class="fm-code-in-text2">v</code> and returns a float, an estimate of the distance between <code class="fm-code-in-text2">v</code> and the goal. Method <code class="fm-code-in-text2">aStar</code> returns a pair with the goal vertex (if reached), and a dictionary encoding the shortest paths.</p>

  <p class="fm-code-annotation"><a id="pgfId-1025456"></a><span class="fm-combinumeral">❷</span> Initializes a priority queue. This could be concretely, for instance, a heap.</p>

  <p class="fm-code-annotation"><a id="pgfId-1025473"></a><span class="fm-combinumeral">❸</span> Adds the starting point to the priority queue, using its distance (<code class="fm-code-in-text2">0</code>) as its priority, so that it will be extracted on the first iteration</p>

  <p class="fm-code-annotation"><a id="pgfId-1025490"></a><span class="fm-combinumeral">❹</span> Creates a new hash table to keep track of the distance of each vertex from vertex <code class="fm-code-in-text2">start</code>, that is, the sum of the weight of edges that needs to be traversed to get from vertex <code class="fm-code-in-text2">start</code> to each other vertex. Initializes these distances to infinity for all vertices but vertex <code class="fm-code-in-text2">start</code>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1025507"></a><span class="fm-combinumeral">❺</span> Also creates a new hash table for the f-score of a vertex, capturing the estimated cost to be sustained to reach the goal from <code class="fm-code-in-text2">start</code> in a path passing through a certain vertex. Initializes these values to infinity for all vertices but vertex <code class="fm-code-in-text2">start</code>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1025531"></a><span class="fm-combinumeral">❻</span> Finally, creates another hash table to keep track, for each vertex <code class="fm-code-in-text2">u</code>, of the vertex through which <code class="fm-code-in-text2">u</code> was reached. This dictionary can later be used to reconstruct the path from <code class="fm-code-in-text2">start</code> to the goal.</p>

  <p class="fm-code-annotation"><a id="pgfId-1025548"></a><span class="fm-combinumeral">❼</span> While we initialized the vertex distances to infinity (or, equivalently, to the largest value that can be stored) for most vertices, for the starting point only, we need to set its distance (from itself) to <code class="fm-code-in-text2">0</code>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1025565"></a><span class="fm-combinumeral">❽</span> Starts a loop, running until the priority queue is empty. It will run at least once, because of line #3.</p>

  <p class="fm-code-annotation"><a id="pgfId-1025616"></a><span class="fm-combinumeral">❾</span> Extracts the top of the priority queue. The vertex extracted will become the current vertex for this iteration, <code class="fm-code-in-text2">v</code>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1025580"></a><span class="fm-combinumeral">❿</span> If we reached the goal, we are done, and just return the current vertex. The function <code class="fm-code-in-text2">isGoal</code> can abstract the goal condition checked: it can be reaching one or more specific goal vertices or getting to vertices that satisfy a certain condition.</p>

  <p class="fm-code-annotation"><a id="pgfId-1025633"></a><span class="fm-combinumeral">⓫</span> Iterates over the outgoing edges for current vertex</p>

  <p class="fm-code-annotation"><a id="pgfId-1025650"></a><span class="fm-combinumeral">⓬</span> If vertex <code class="fm-code-in-text2">u</code> hasn’t been visited yet, then maybe its distance from start can be made shorter by passing through <code class="fm-code-in-text2">v</code>.</p>

  <p class="fm-code-annotation"><a id="pgfId-1025667"></a><span class="fm-combinumeral">⓭</span> If so, we need to update its distance and its parent, and then update the queue, so that <code class="fm-code-in-text2">u</code> will be popped from it at the right time (that is, when it becomes the closest vertex to the frontier of visited vertices).</p>

  <p class="fm-code-annotation"><a id="pgfId-1025684"></a><span class="fm-combinumeral">⓮</span> Updates the f-score for <code class="fm-code-in-text2">u</code> combining the distance between <code class="fm-code-in-text2">start</code> and <code class="fm-code-in-text2">u</code> (for which we already have its exact value) and an estimate of the cost of reaching the goal from <code class="fm-code-in-text2">u</code></p>

  <p class="fm-code-annotation"><a id="pgfId-1025701"></a><span class="fm-combinumeral">⓯</span> If the goal is never reached, we need to return <code class="fm-code-in-text2">null</code> or, equivalently, another value that signals that search failed. We can still return the <code class="fm-code-in-text2">parents</code> dictionary to reconstruct the shortest paths to all the vertices that were reachable from <code class="fm-code-in-text2">start</code>.</p>

  <p class="body"><a id="pgfId-1006769"></a>As we can see at line #1 of listing 14.7, this generic definition of A* takes two extra arguments, a distance function, and a heuristic. They both contribute to the computation of the so-called <i class="calibre17">f-score</i> at line #18. This value is a mix of the cost of reaching the current node <code class="fm-code-in-text">u</code> from the source and the expected cost needed in order to reach the goal from <code class="fm-code-in-text">u</code>.</p>

  <p class="body"><a id="pgfId-1006794"></a>By controlling these two arguments, we can obtain either BFS or Dijkstra’s algorithm (or neither). For both of them, the heuristic will need to be a function that is identically equal to <code class="fm-code-in-text">0</code>, something we could write like <code class="fm-code-in-text">lambda(v)</code> <span class="cambria">→</span> <code class="fm-code-in-text">0</code>. Both of these algorithms, in fact, completely disregard any notion of or information about the distance of vertices to goal.</p>

  <p class="body"><a id="pgfId-1006816"></a>For the distance metrics, the situation is different:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1006825"></a>Dijkstra’s algorithm uses the edge’s weight as a distance function, so we need to pass something like <code class="fm-code-in-text">distance = lambda(e)</code> <span class="cambria">→</span> <code class="fm-code-in-text">e.weight</code>.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1006840"></a>BFS only takes into account the number of edges traversed, which is equivalent to considering all edges to have the same weight, identically equal to <code class="fm-code-in-text">1</code>! And thus, we can pass <code class="fm-code-in-text">distance = lambda(e)</code> <span class="cambria">→</span> <code class="fm-code-in-text">1</code>.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1006857"></a>In practice, 99.9% of the times, you’d better directly implement Dijkstra’s algorithm or BFS, and not as a special case of A*. This brings us to a golden rule about keeping things simple that I learned from a great engineer I worked with.</p>

  <p class="fm-callout"><a id="pgfId-1006872"></a><span class="fm-callout-head">Note</span> Do not make your code more generic than needed. You shouldn’t consider writing a generic version of something until you have at least three different variants that could be implemented as minor changes of the same generic code.<a href="#pgfId-1008270"><sup class="footnotenumber1">15</sup></a></p>

  <p class="body"><a id="pgfId-1006894"></a>General purpose code, such as the generic version of A* shown in listing 14.7, usually carries some overhead, for instance, to call methods or lambdas like <code class="fm-code-in-text">distance</code><a id="marker-1008008"></a>, instead of just retrieving an edge’s length, or for BFS, using a priority queue instead of a faster plain queue. It also becomes increasingly hard to maintain and reason about.</p>

  <p class="body"><a id="pgfId-1006914"></a>So, from these considerations, it should be clear that A* hasn’t been developed to provide a generic method to be parameterized. And yet, it can be extremely useful.</p>

  <p class="body"><a id="pgfId-1006923"></a>As we mentioned, in fact, there are at least two good reasons to implement A*, two contexts in which A* provides an advantage over Dijkstra’s.</p>

  <p class="body"><a id="pgfId-1006932"></a>Let’s be clear from the beginning: this is not always true; in the general case, Dijkstra’s algorithm is asymptotically as fast as A* (or the latter might not even be meaningfully applicable).</p>

  <p class="body"><a id="pgfId-1006941"></a>A* gains an advantage only in some contexts where we have extra information that we can somehow use.</p>

  <p class="body"><a id="pgfId-1006950"></a>The first case where we can use A* to drive search faster to the goal is when we have information about the distance from all or some vertices to the goal(s). Figure 14.20 explains this situation better than a thousand words! Notice that in this particular case, the key factor is that the vertices, modeling physical places in the real world, carry extra information with them (their position, which is fixed) that can help estimate their distance to the final goal. This isn’t always true and is usually not the case for generic graphs.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F20.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1032932"></a>Figure 14.20 An example graph where A* provides a significant speedup over Dijkstra’s algorithm. While this is an edge case, in many situations we have some domain knowledge that A* can leverage to prune search branches and get a significant speedup. Notice that here the straight-line distance between vertices, used as a heuristic, as well as the edge weights, are expressed in multiples of a generic unit denoted by <code class="fm-code-in-text">x</code> (it could be meters, or miles, and so on). The numbers in the squares next to each vertex are the distance of that vertex from source, while the numbers in the dashed ellipses are the estimated distance to goal for each vertex, computed as the straight-line distance from that vertex to vertex <code class="fm-code-in-text">v<sub class="subscript1">G</sub></code>.</p>

  <p class="body"><a id="pgfId-1007000"></a>To put it differently, the extra information here doesn’t come from the graph, but from domain knowledge.</p>

  <p class="body"><a id="pgfId-1007009"></a>The good news is somewhat limited though, because there is no a priori guarantee that A* will perform better than Dijkstra’s algorithm. On the contrary, it’s easy to craft an example where A* will always do as badly as Dijkstra’s algorithm. Check out figure 14.21 to get an idea how we can tweak our previous example to fool A*! The key, here and always, is the quality of the extra information captured by the heuristic function: the more reliable and closer to real distance the estimate, the better A* performs.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F21.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1033058"></a>Figure 14.21 An edge case where A* certainly can’t outperform Dijkstra’s algorithm. Both algorithms will visit all vertices (A* always in the same order, Dijkstra’s in a partially random order), before eventually travelling the edge from <code class="fm-code-in-text">v<sub class="subscript1">1</sub></code> to <code class="fm-code-in-text">v<sub class="subscript1">G</sub></code>.</p>

  <h3 class="fm-head2" id="heading_id_25"><a id="pgfId-1007055"></a>14.5.1 How good is A* search?</h3>

  <p class="body"><a id="pgfId-1007073"></a>If <a id="marker-1008012"></a><a id="marker-1008016"></a>we were in a class and this was a live presentation, this should be your follow-up question: Can we craft an example where A* performs consistently worse than Dijkstra’s algorithm?</p>

  <p class="body"><a id="pgfId-1007085"></a>It turns out we can, easily: if we take the example of figure 14.21 and change just the weight of the edge from <code class="fm-code-in-text">v<sub class="subscript1">1</sub></code> to <code class="fm-code-in-text">v<sub class="subscript1">G</sub></code>, setting it to any value smaller than 2, and at the same time we keep the same estimates, then we would be sure that A* would visit every other vertex before getting to the goal, while Dijkstra’s would never visit <code class="fm-code-in-text">v<sub class="subscript1">6</sub></code> and, depending on the order it processes edges out of <code class="fm-code-in-text">v<sub class="subscript1">S</sub></code>, it might also skip vertices <code class="fm-code-in-text">v<sub class="subscript1">2</sub></code> to <code class="fm-code-in-text">v<sub class="subscript1">4</sub></code>.</p>

  <p class="body"><a id="pgfId-1007147"></a>The key for this example is that the heuristic overestimates the distance to goal for <code class="fm-code-in-text">v<sub class="subscript1">1</sub></code> (and a few more vertices): A*, it turns out, minimizes the estimated cost, which can be different from the actual cost, and whenever this estimate is pessimistic, we get into trouble.</p>

  <p class="body"><a id="pgfId-1007159"></a>The previous example, in fact, shows how using the wrong estimates can make the search unnecessarily slower, which is inconvenient but sometimes acceptable. Things could be much worse, however, because we could also find examples where A* returns a solution whose cost is not optimal. In figure 14.22, the estimate for vertex <code class="fm-code-in-text">v<sub class="subscript1">3</sub></code> is bloated, and consequently vertex <code class="fm-code-in-text">v<sub class="subscript1">G</sub></code> is reached through a different path before even visiting <code class="fm-code-in-text">v<sub class="subscript1">3</sub></code>. Remember that once the goal is reached, the search stops and so the wrong (non-optimal) path is returned.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F22.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1033120"></a>Figure 14.22 Another edge case for A*, where the algorithm returns a non-optimal solution. Because the estimate for vertex <code class="fm-code-in-text">v<sub class="subscript1">3</sub></code> is bloated, the goal is reached through a different path before even visiting <code class="fm-code-in-text">v<sub class="subscript1">3</sub></code>; once the goal is reached, the search stops and the wrong (non-optimal) path is returned.</p>

  <p class="body"><a id="pgfId-1007191"></a>While sometimes this can be considered acceptable, in practice, we usually want to avoid it, and we luckily have a way to guarantee this.</p>

  <p class="body"><a id="pgfId-1007204"></a>It can be proved, in fact, that A* is <i class="calibre17">complete <a href="#pgfId-1008285"><sup class="footnotenumber">16</sup></a></i> and <i class="calibre17">optimal</i> when the heuristic function satisfies two conditions: it must be <i class="calibre17">admissible</i> and <i class="calibre17">consistent</i>.</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1007222"></a><i class="calibre15">Admissible (aka optimistic)</i>—Such a heuristic never overestimates the cost to reach the goal.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1007236"></a><i class="calibre15">Consistent</i>—A heuristic is consistent if, given a vertex <code class="fm-code-in-text">v</code> and any of its successor <code class="fm-code-in-text">u</code>, the estimated cost for <code class="fm-code-in-text">u</code> is at most the estimated cost for <code class="fm-code-in-text">v</code>, plus the cost of getting from <code class="fm-code-in-text">v</code> to <code class="fm-code-in-text">u</code>. In formula, <code class="fm-code-in-text">heuristic(u)</code> <span class="cambria">≤</span> <code class="fm-code-in-text">distance(v, u) + heuristic(v)</code>.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1007266"></a>As one of our reviewers of this book suggested, there is an even clearer way to remember the difference between these two conditions: <i class="calibre17">admissible</i> means it’s not overestimating the cost of a path, and <i class="fm-italics">consistent</i> means it’s not overestimating the cost of an edge.</p>

  <p class="body"><a id="pgfId-1007286"></a>When we plan to use A* search, the first thing we need to ensure is that we have a heuristic function that is both admissible and consistent. This condition is necessary and sufficient to ensure the optimal solution will be found.<a href="#pgfId-1008299"><sup class="footnotenumber">17</sup></a></p>

  <p class="body"><a id="pgfId-1007301"></a>What does all of this mean for our problem—delivering goods to customers? Well, to speed up the search for the best route, we can use the “straight-line distance to the customer’s address” as the heuristic. This will guide search, favoring paths that get closer to the goal over those that are directed away from it; in turn, it will require us to visit fewer vertices before reaching the goal.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch14_F23.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1033172"></a>Figure 14.23 Applying A* to the example graph in figure 14.19: while with Dijkstra we would have had to visit all vertices whose distance from source is smaller than <code class="fm-code-in-text">1356</code>, A* reaches the goal faster, and although it still visits some vertices not in the shortest path (highlighted with a shaded background in the table on the right), we can avoid paying a visit to vertices <code class="fm-code-in-text">v<sub class="subscript1">A</sub></code>, <code class="fm-code-in-text">v<sub class="subscript1">B</sub></code>, <code class="fm-code-in-text">v<sub class="subscript1">C</sub></code>, <code class="fm-code-in-text">v<sub class="subscript1">F</sub></code>, <code class="fm-code-in-text">v<sub class="subscript1">N</sub></code>, <code class="fm-code-in-text">v<sub class="subscript1">R</sub></code>.</p>

  <p class="body"><a id="pgfId-1007322"></a>Figure 14.23 shows how A* would find the best route for our previous example, the one to which we applied Dijkstra’s algorithm in figure 14.19. While running Dijkstra’s algorithm, we would have had to visit all vertices whose distance from the source is smaller than 1356 (the total distance of the shortest path from source to goal). A* can reach the goal faster and, although it still visits some vertices not on the shortest path, it doesn’t go through vertices <code class="fm-code-in-text">v<sub class="subscript1">A</sub>, v<sub class="subscript1">B</sub>, v<sub class="subscript1">C</sub>, v<sub class="subscript1">F</sub>, v<sub class="subscript1">N</sub>, v<sub class="subscript1">R</sub></code>, which are instead visited by Dijkstra’s.</p>

  <p class="body"><a id="pgfId-1007395"></a>In this particular example, with A* we can save traversing 6 edges over 23, which is quite good (a 25% save), especially considering that there are two paths with a very close weight, differing by just 2 meters.</p>

  <p class="body"><a id="pgfId-1007404"></a>Considering road distance as edges’ weight and straight-line distance as the heuristic, we can see that straight-line distance is certainly optimistic, because the road distance can never be shorter; at best it can be the same.</p>

  <p class="body"><a id="pgfId-1007413"></a>Straight-line distance is also a consistent heuristic. In fact, if we apply our choices to the condition for consistency, we get</p>
  <pre class="programlisting">straight_line(u, goal) <span class="cambria">≤</span> road_distance(v, u) + straight_distance(v)</pre>

  <p class="body"><a id="pgfId-1007435"></a>which is certainly true, since <code class="fm-code-in-text">straight_line(v,u)</code><span class="cambria">≤</span><code class="fm-code-in-text">road_distance(v,u)</code>, and straight-line distance, being a Euclidean distance, certainly abides by triangle inequality.<a href="#pgfId-1008315"><sup class="footnotenumber">18</sup></a></p>

  <p class="body"><a id="pgfId-1007455"></a>While this condition always holds, the value of the heuristic for a vertex <code class="fm-code-in-text">u</code> can be larger than the value assigned to its parent <code class="fm-code-in-text">v</code>. For example, in figure 14.23, consider vertices <code class="fm-code-in-text">v<sub class="subscript1">E</sub></code> and <code class="fm-code-in-text">v<sub class="subscript1">D</sub></code>: the straight-line distance between vertex <code class="fm-code-in-text">v<sub class="subscript1">D</sub></code> and the goal is larger than its parents’, <code class="fm-code-in-text">v<sub class="subscript1">E</sub></code>’s. This happens because not all vertices are connected by an edge (or rather, two directional edges), and so, for example, from <code class="fm-code-in-text">v<sub class="subscript1">E</sub></code> (which is the closest vertex to the goal), you can reach <code class="fm-code-in-text">v<sub class="subscript1">G</sub></code> only with a detour, by first going through <code class="fm-code-in-text">v<sub class="subscript1">D</sub></code>.</p>

  <p class="body"><a id="pgfId-1007501"></a>It can be proved that if, instead, the graph was fully connected, we would also visit vertices according to their straight-line distance from the goal. In other words, the value of the heuristic would monotonically decrease during the traversal.</p>

  <p class="body"><a id="pgfId-1007514"></a>Then again, most useful graphs are not fully connected, and luckily we don’t need this strict condition for A* to be able to find the optimal solution, but we can make do with a consistent and admissible heuristic.</p>

  <p class="body"><a id="pgfId-1007525"></a>These properties can guarantee that the best solution will be found, but should we also aim to get the most accurate estimate possible? There might be many admissible and consistent heuristics, but does the algorithm find the best route faster if we choose one with a more precise estimate?</p>

  <p class="body"><a id="pgfId-1007536"></a>Needless to say, when we have to compute thousands of routes per hour, using a more efficient search can save a lot of computation, allowing shipments to leave the factory faster and ultimately saving money.</p>

  <p class="body"><a id="pgfId-1007549"></a>Once again, we can get a theoretical guarantee about A* performance: if we fix the heuristic and distance, then for <i class="calibre17">any</i> consistent heuristic, A* will not just be optimal, but also optimally-efficient. This means that no other algorithm is guaranteed to expand fewer nodes than A*.</p>

  <p class="body"><a id="pgfId-1007565"></a>That being said, the closer the estimate is to the actual distance of a vertex to the goal, the faster the algorithm will reach the goal. As an example, try to come up with a better heuristic for the graph in figure 14.23. (Hint: What could be more precise than straight-line distance?)</p>

  <p class="body"><a id="pgfId-1007584"></a>While we are happy to have a way to guarantee that A* will find the optimal solution, we also mentioned that sometimes it is acceptable to settle for a sub-optimal one. Especially when the cost of traversal is high, or there are constraints on the response time, we might want to choose a non-admissible heuristic that guarantees faster <a id="marker-1008020"></a><a id="marker-1008024"></a>convergence.</p>

  <h3 class="fm-head2" id="heading_id_26"><a id="pgfId-1007600"></a>14.5.2 Heuristics as a way to balance real-time data</h3>

  <p class="body"><a id="pgfId-1007620"></a>That <a id="marker-1008028"></a><a id="marker-1008032"></a><a id="marker-1008036"></a>concludes our discussion about optimality of search. We also mentioned there is at least one other scenario where A* can prove itself particularly useful. Let’s briefly explore it.</p>

  <p class="body"><a id="pgfId-1007643"></a>The great power of having this heuristic function is that we can use a different metric with respect to the distance and convey more information about the domain. Heuristics could even combine several data, as long as the value returned by a heuristic is scaled appropriately to make sense when compared to the edge’s distance. For instance, it would make little sense (and cause terrible performance) to use meters for edges’ weights and seconds (or even millimeters) for the heuristic’s estimates; however, we can always scale millimeters to meters or, if the heuristic conveys information about the average time needed to reach the goal from each vertex, then we could multiply it for the roads’ average or minimum speed to obtain a quantity that could then be added to any edge’s weight.</p>

  <p class="body"><a id="pgfId-1007681"></a>But we can also take this to the next level by decoupling even more the purposes of the distance and of the heuristic. Imagine that we are computing the best route on the go, instead of a priori, like a car navigator does.</p>

  <p class="body"><a id="pgfId-1007696"></a>First, we switch our metric from distance to travelling time, and the time needed to traverse a road changes depending on traffic, weather, areas closed to transit during certain hours, and so on.</p>

  <p class="body"><a id="pgfId-1007710"></a>However, the average time needed when following a certain route is known in advance, and we can use that as a compass to balance our live decisions.</p>

  <p class="body"><a id="pgfId-1007719"></a>Suppose, for instance, that now you are also planning deliveries on a larger scale, and you need to ship some goods from Naples to Florence. When you get to Rome, the faster route would be through the motorway passing east of the city, and it usually would take slightly less than three hours. However, your navigator realizes that for the next 10 miles on that route there is heavy traffic, while going around Rome on the west side, the traffic is all clear. If the navigator just used Dijkstra’s algorithm, the next edge to expand would be the shorter time for the next 10 miles, and it would lead you west.</p>

  <p class="body"><a id="pgfId-1007734"></a>Unfortunately, that would add at least one hour to your trip. A* can come to the rescue and balance the short-term advantage of a traffic-free motorway section with the long-term gain of a shorter and usually faster route.</p>

  <p class="body"><a id="pgfId-1007747"></a>Although this example is extremely simplified, you get the point. A* can better balance long-term costs with local choices, and that’s why it has been the state-of-the-art in AI, for example, for pathfinding in video games.<a href="#pgfId-1008332"><sup class="footnotenumber">19</sup></a></p>

  <p class="body"><a id="pgfId-1007762"></a>The next step to improve these navigation algorithms would be considering how requests for shortest path are handled in a vacuum. Especially during rush hours, when many users will request the shortest path between similar locations, suggesting the same route to all of them could lead to unnecessarily high traffic on some roads, and very low traffic on others. Wouldn’t it be great if a more balanced routing system could take into consideration all requests involving the same segments, spread the traffic over several routes, and minimize the congestion due to vehicles sharing the same road?</p>

  <p class="body"><a id="pgfId-1007775"></a>That goal is ambitious, out of scope for this book, and out of reach of classic computers. That’s why quantum developers<a href="#pgfId-1008347"><sup class="footnotenumber">20</sup></a> are working on it, with <a id="marker-1008040"></a><a id="marker-1008044"></a><a id="marker-1008048"></a>encouraging <a id="marker-1008052"></a><a id="marker-1008056"></a>results.</p>

  <h2 class="fm-head" id="heading_id_27"><a id="pgfId-1007797"></a>Summary</h2>

  <ul class="calibre19">
    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1007810"></a>Graph is a data structure that can model many problems, and in general it works well when there are entities connected by some kind of proximity relation.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1007827"></a>While usually the adjacency list representation works better for most problems, for some problems with <i class="calibre15">dense</i> graphs<a class="calibre14" id="marker-1008060"></a> the adjacency matrix is preferable.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1007843"></a>There are many possible ways to explore a graph, but the most common are Breadth First Search<a class="calibre14" id="marker-1008064"></a> (BFS) and Depth First Search<a class="calibre14" id="marker-1008068"></a> (DFS).</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1007867"></a>Dijkstra’s algorithm<a class="calibre14" id="marker-1008072"></a><a class="calibre14" id="marker-1008076"></a> extends BFS when the shortest path needs to be computed in terms of the minimum edges’ weight, not just the minimum number of edges.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1007884"></a>A* provides an improvement on Dijkstra’s when we have extra information besides the edges’ weight, information that can be conveyed into a heuristic estimating the distance from each vertex to the <a class="calibre14" id="marker-1008080"></a>goal.</p>
    </li>
  </ul>
  <hr class="calibre22"/>

  <p class="fm-footnote"><sup class="footnotenumber">1.</sup> <a id="pgfId-1015100"></a>This is an optimistic bound, assuming the adjacency matrix can be resized dynamically. Otherwise, a truer bound is <code class="fm-code-in-text1">O(|V|<sup class="calibre28">2</sup>)</code>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">2.</sup> <a id="pgfId-1015079"></a>We need to check all edges to remove those whose destination has been deleted.</p>

  <p class="fm-footnote"><sup class="footnotenumber">3.</sup> <a id="pgfId-1008087"></a>An algebraic data type is one particular kind of composite type, formed by combining other types, usually with a definition “by induction,” with one or more base types, and operators to combine them. This presentation nicely explains how to define them in C++ and what the benefits are: <span class="fm-hyperlink"><a href="https://www.youtube.com/watch?v=ojZbFIQSdl8">https://www.youtube.com/watch?v=ojZbFIQSdl8</a></span></p>

  <p class="fm-footnote"><sup class="footnotenumber">4.</sup> <a id="pgfId-1008108"></a>For an example with Haskell, see: Mokhov, Andrey. “Algebraic graphs with class (functional pearl).” ACM SIGPLAN Notices. Vol. 52. No. 10. ACM, 2017. A Scala implementation (ongoing, at the time of writing) can be found here: <span class="fm-hyperlink"><a href="https://github.com/algebraic-graphs/scala">https://github.com/algebraic-graphs/scala.</a></span></p>

  <p class="fm-footnote"><sup class="footnotenumber">5.</sup> <a id="pgfId-1008128"></a>See <span class="fm-hyperlink"><a href="https://github.com/mlarocca/AlgorithmsAndDataStructuresInAction#graph">https://github.com/mlarocca/AlgorithmsAndDataStructuresInAction#graph</a></span>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">6.</sup> <a id="pgfId-1008144"></a>Any directed graph where there is an edge (u,v), but not the inverse edge (v,u), can’t be transformed into an isomorphic undirected graph (meaning an undirected graph with the same shape, that is, with the same set of vertices, connected in the same way).</p>

  <p class="fm-footnote"><sup class="footnotenumber">7.</sup> <a id="pgfId-1008160"></a>Have you ever heard of the “traveling salesman problem,” or TSP? That’s one of the most-studied hard problems.</p>

  <p class="fm-footnote"><sup class="footnotenumber">8.</sup> <a id="pgfId-1008176"></a>By either removing the checks for goal or passing a function that always returns false as the isGoal argument.</p>

  <p class="fm-footnote"><sup class="footnotenumber">9.</sup> <a id="pgfId-1008194"></a>Just to be clear, this is different than computing the optimal route through several or all vertices. Single-source-all-destinations only computes the optimal paths from the source to each other vertex taken individually.</p>

  <p class="fm-footnote"><sup class="footnotenumber">10.</sup> <a id="pgfId-1008207"></a>The Manhattan Distance, also known as block distance, is the sum of the absolute difference of the Cartesian coordinates of two points. The name comes from the fact that on the island of Manhattan, most streets have a grid layout, and so the shortest path between two intersections has a distance equal to the sum of the block’s sides.</p>

  <p class="fm-footnote"><sup class="footnotenumber">11.</sup> <a id="pgfId-1008223"></a>Please do <i class="calibre17">not</i> literally hold your breath! And no, not even if you are speed-reading.</p>

  <p class="fm-footnote"><sup class="footnotenumber">12.</sup> <a id="pgfId-1008241"></a>We’ll use a min-heap to store vertices in the frontier, and their distance from source is used as priority.</p>

  <p class="fm-footnote"><sup class="footnotenumber">13.</sup> <a id="pgfId-1017140"></a>Amortized.</p>

  <p class="fm-footnote"><sup class="footnotenumber">14.</sup> <a id="pgfId-1008255"></a>If a graph has a negative-weight cycle, defined as a cycle for which the sum of edges’ weights is negative, then discussing shortest paths could be meaningless. By repeatedly traversing the cycle, in fact, one could get to an arbitrarily low total cost.</p>

  <p class="fm-footnote"><sup class="footnotenumber">15.</sup> <a id="pgfId-1008270"></a>Of course, using patterns like template or strategy will help keep your codebase DRY and easier to maintain, but the point is that implementing general purpose methods or classes will make your code less clean and maintainable, so you need to balance out both aspects.</p>

  <p class="fm-footnote"><sup class="footnotenumber">16.</sup> <a id="pgfId-1008285"></a>Completeness guarantees that only a finite number of nodes will have to be visited before reaching the goal.</p>

  <p class="fm-footnote"><sup class="footnotenumber">17.</sup> <a id="pgfId-1008299"></a>For trees, admissibility alone is a sufficient condition. Can you explain why? Hint: In a tree, how many paths from root to a goal vertex pass through a given node <code class="fm-code-in-text1">u</code>?</p>

  <p class="fm-footnote"><sup class="footnotenumber">18.</sup> <a id="pgfId-1008315"></a>Given three points A, B, and C belonging to a Euclidean space, it always holds true that distance(A,C) <span class="cambria">≤</span> distance(A,B) + distance(B,C).</p>

  <p class="fm-footnote"><sup class="footnotenumber">19.</sup> <a id="pgfId-1008332"></a>Other applications of A*, Dijkstra’s and BFS range from IP routing to graph theory and even garbage collection.</p>

  <p class="fm-footnote"><sup class="footnotenumber">20.</sup> <a id="pgfId-1008347"></a>You can read the details here: <span class="fm-hyperlink"><a href="http://mng.bz/w99B">http://mng.bz/w99B</a></span>. Graph theory is one of those fields that can be revolutionized by quantum computing; for an introduction to practical quantum computing, check out Quantum Computing for Developers, by Johan Vos (Manning Publications, 2021), and Learn Quantum Computing with Python and Q#, by Sarah C. Kaiser and Christopher E. Granade (Manning Publications, 2021).</p>
</body>
</html>
