<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:mbp="Kindle">
  <head>
    <title>C++ Concurrency in Action, Second Edition</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<h2 class="part" id="ch08">Chapter 8. <a id="ch08__title" class="calibre3"></a>Designing concurrent code
      </h2>
      
      <p class="noind"><a id="iddle1148" class="calibre4"></a><i class="calibre6">This chapter covers</i></p>
      
      <p class="calibre19"></p>
      <ul class="calibre21">
         
         <li class="calibre22">Techniques for dividing data between threads</li>
         
         <li class="calibre22">Factors that affect the performance of concurrent code</li>
         
         <li class="calibre22">How performance factors affect the design of data structures</li>
         
         <li class="calibre22">Exception safety in multithreaded code</li>
         
         <li class="calibre22">Scalability</li>
         
         <li class="calibre22">Example implementations of several parallel algorithms</li>
         
      </ul>
      
      <p class="noind">Most of the preceding chapters have focused on the tools you have in your C++ toolbox for writing concurrent code. In <a href="kindle_split_016.html#ch06" class="calibre4">chapters 6</a> and <a href="kindle_split_017.html#ch07" class="calibre4">7</a> we looked at how to use those tools to design basic data structures that are safe for concurrent access by multiple threads.
         Much as a carpenter needs to know more than how to build a hinge or a joint in order to make a cupboard or a table, there’s
         more to designing concurrent code than the design and use of basic data structures. You now need to look at the wider context
         so you can build bigger structures that perform useful work. I’ll be using multithreaded implementations of some of the <a id="iddle1158" class="calibre4"></a><a id="iddle1293" class="calibre4"></a><a id="iddle2532" class="calibre4"></a>C++ Standard Library algorithms as examples, but the same principles apply at all scales of an application.
      </p>
      
      <p class="noind">Just as with any programming project, it’s vital to think carefully about the design of concurrent code. But with multithreaded
         code, there are even more factors to consider than with sequential code. Not only must you think about the usual factors,
         such as encapsulation, coupling, and cohesion (which are amply described in the many books on software design), but you also
         need to consider which data to share, how to synchronize accesses to that data, which threads need to wait for which other
         threads to complete certain operations, and so on.
      </p>
      
      <p class="noind">In this chapter we’ll be focusing on these issues, from the high-level (but fundamental) considerations of how many threads
         to use, which code to execute on which thread, and how this can affect the clarity of the code, to the low-level details of
         how to structure the shared data for optimal performance.
      </p>
      
      <p class="noind">Let’s start by looking at techniques for dividing work between threads.</p>
      
      
      <h3 id="ch08lev1sec1" class="chapter"><a id="ch08lev1sec1__title" class="calibre3"></a>8.1. Techniques for dividing work between threads
      </h3>
      
      <p class="noind">Imagine for a moment that you’ve been tasked with building a house. In order to complete the job, you’ll need to dig the foundation,
         build walls, put in plumbing, add the wiring, and so on. Theoretically, you could do it all yourself with sufficient training,
         but it would probably take a long time, and you’d be continually switching tasks as necessary. Alternatively, you could hire
         a few other people to help out. You now have to choose how many people to hire and decide what skills they need. You could,
         for example, hire a couple of people with general skills and have everybody chip in with everything. You’d still all switch
         tasks as necessary, but now things can be done more quickly because there are more of you.
      </p>
      
      <p class="noind">Alternatively, you could hire a team of specialists: a bricklayer, a carpenter, an electrician, and a plumber, for example.
         Your specialists do whatever their specialty is, so if there’s no plumbing needed, your plumber sits around drinking tea or
         coffee. Things still get done more quickly than before, because there are more of you, and the plumber can put the toilet
         in while the electrician wires up the kitchen, but there’s more waiting around when there’s no work for a particular specialist.
         Even with the idle time, you might find that the work is done faster with specialists than with a team of general handymen.
         Your specialists don’t need to keep changing tools, and they can probably each do their tasks quicker than the generalists
         can. Whether or not this is the case depends on the particular circumstances—you’d have to try it and see.
      </p>
      
      <p class="noind">Even if you hire specialists, you can still choose to hire different numbers of each. It might make sense to have more bricklayers
         than electricians, for example. Also, the makeup of your team and the overall efficiency might change if you had to build
         more than one house. Even though your plumber might not have lots of work to do on any given house, you might have enough
         work to keep him busy all the time if you’re building many houses at once. Also, if you don’t have to pay your specialists
         when <a id="iddle1236" class="calibre4"></a><a id="iddle1290" class="calibre4"></a><a id="iddle1623" class="calibre4"></a><a id="iddle1633" class="calibre4"></a><a id="iddle1674" class="calibre4"></a><a id="iddle1708" class="calibre4"></a><a id="iddle2503" class="calibre4"></a>there’s no work for them to do, you might be able to afford a larger team overall even if you have only the same number of
         people working at any one time.
      </p>
      
      <p class="noind">OK, enough about building; what does all this have to do with threads? Well, with threads the same issues apply. You need
         to decide how many threads to use and what tasks they should be doing. You need to decide whether to have “generalist” threads
         that do whatever work is necessary at any point in time or “specialist” threads that do one thing well, or some combination.
         You need to make these choices whatever the driving reason for using concurrency, and how you do this will have a critical
         effect on the performance and clarity of the code. It’s therefore vital to understand the options so you can make an appropriately
         informed decision when designing the structure of your application. In this section, we’ll look at several techniques for
         dividing the tasks, starting with dividing data between threads before we do any other work.
      </p>
      
      
      <h4 id="ch08lev2sec1" class="calibre23">8.1.1. <a id="ch08lev2sec1__title" class="calibre4"></a>Dividing data between threads before processing begins
      </h4>
      
      <p class="noind">The easiest algorithms to parallelize are simple algorithms, such as <kbd class="calibre17">std::for_each</kbd>, that perform an operation on each element in a data set. In order to parallelize this algorithm, you can assign each element
         to one of the processing threads. How the elements are best divided for optimal performance depends on the details of the
         data structure, as you’ll see later in this chapter when we look at performance issues.
      </p>
      
      <p class="noind">The simplest means of dividing the data is to allocate the first <i class="calibre6">N</i> elements to one thread, the next <i class="calibre6">N</i> elements to another thread, and so on, as shown in <a href="#ch08fig01" class="calibre4">figure 8.1</a>, but other patterns could be used too. No matter how the data is divided, each thread then processes the elements it has
         been assigned without any communication with the other threads until it has completed its processing.
      </p>
      
      
      
      <h5 class="notetitle" id="ch08fig01">Figure 8.1. <a id="ch08fig01__title" class="calibre4"></a>Distributing consecutive chunks of data between threads
      </h5>
      
      <p class="center1"><img alt="" src="08fig01_alt.jpg" class="calibre2"/></p>
      
      
      <p class="noind">This structure will be familiar to anyone who has programmed using the Message Passing Interface (MPI, <a href="http://www.mpi-forum.org/" class="calibre4">http://www.mpi-forum.org/</a>) or OpenMP (<a href="http://www.openmp.org/" class="calibre4">http://www.openmp.org/</a>) frameworks: a task is split into a set of parallel tasks, the worker threads run these tasks independently, and the results
         are combined in a final <i class="calibre6">reduction</i> step. It’s the <a id="iddle1004" class="calibre4"></a><a id="iddle1237" class="calibre4"></a><a id="iddle1291" class="calibre4"></a><a id="iddle1831" class="calibre4"></a><a id="iddle1946" class="calibre4"></a><a id="iddle2533" class="calibre4"></a>approach used by the <kbd class="calibre17">accumulate</kbd> example from <a href="kindle_split_012.html#ch02lev1sec4" class="calibre4">section 2.4</a>; in this case, both the parallel tasks and the final reduction step are accumulations. For a simple <kbd class="calibre17">for_each</kbd>, the final step is a no-op because there are no results to reduce.
      </p>
      
      <p class="noind">Identifying this final step as a reduction is important; a naive implementation such as <a href="kindle_split_012.html#ch02ex09" class="calibre4">listing 2.9</a> will perform this reduction as a final serial step. But this step can often be parallelized as well; <kbd class="calibre17">accumulate</kbd> <i class="calibre6">is</i> a reduction operation, so <a href="kindle_split_012.html#ch02ex09" class="calibre4">listing 2.9</a> could be modified to call itself recursively where the number of threads is larger than the minimum number of items to process
         on a thread, for example. Alternatively, the worker threads could be made to perform some of the reduction steps as each one
         completes its task, rather than spawning new threads each time.
      </p>
      
      <p class="noind">Although this technique is powerful, it can’t be applied to everything. Sometimes the data can’t be divided neatly up front
         because the necessary divisions become apparent only as the data is processed. This is particularly apparent with recursive
         algorithms such as Quicksort; they therefore need a different approach.
      </p>
      
      
      
      <h4 id="ch08lev2sec2" class="calibre23">8.1.2. <a id="ch08lev2sec2__title" class="calibre4"></a>Dividing data recursively
      </h4>
      
      <p class="noind">The Quicksort algorithm has two basic steps: partition the data into items that come before or after one of the elements (the
         pivot) in the final sort order and recursively sort those two “halves.” You can’t parallelize this by dividing the data up
         front, because it’s only by processing the items that you know which “half” they go in. If you’re going to parallelize this
         algorithm, you need to make use of the recursive nature. With each level of recursion there are <i class="calibre6">more</i> calls to the <kbd class="calibre17">quick_sort</kbd> function, because you have to sort both the elements that belong before the pivot and those that belong after it. These recursive
         calls are entirely independent, because they access separate sets of elements, and so are prime candidates for concurrent
         execution. <a href="#ch08fig02" class="calibre4">Figure 8.2</a> shows this recursive division.
      </p>
      
      
      
      <h5 class="notetitle" id="ch08fig02">Figure 8.2. <a id="ch08fig02__title" class="calibre4"></a>Recursively dividing data
      </h5>
      
      <p class="center1"><img alt="" src="08fig02_alt.jpg" class="calibre2"/></p>
      
      
      <p class="noind">In <a href="kindle_split_014.html#ch04" class="calibre4">chapter 4</a>, you saw this implementation. Rather than performing two recursive calls for the higher and lower chunks, you used <kbd class="calibre17">std::async()</kbd> to spawn asynchronous <a id="iddle1938" class="calibre4"></a>tasks for the lower chunk at each stage. By using <kbd class="calibre17">std::async()</kbd>, you ask the C++ Thread Library to decide when to run the task on a new thread and when to run it synchronously.
      </p>
      
      <p class="noind">This is important: if you’re sorting a large set of data, spawning a new thread for each recursion would quickly result in
         a lot of threads. As you’ll see when we look at performance, if you have too many threads, you might slow down the application.
         There’s also a possibility of running out of threads if the data set is large. The idea of dividing the overall task in a
         recursive fashion like this is a good one; you just need to keep a tighter rein on the number of threads. <kbd class="calibre17">std::async()</kbd> can handle this in simple cases, but it’s not the only choice.
      </p>
      
      <p class="noind">One alternative is to use the <kbd class="calibre17">std::thread::hardware_concurrency()</kbd> function to choose the number of threads, as you did with the parallel version of <kbd class="calibre17">accumulate()</kbd> from <a href="kindle_split_012.html#ch02ex09" class="calibre4">listing 2.9</a>. Then, rather than starting a new thread for the recursive calls, you can push the chunk to be sorted onto a thread-safe
         stack, such as one of those described in <a href="kindle_split_016.html#ch06" class="calibre4">chapters 6</a> and <a href="kindle_split_017.html#ch07" class="calibre4">7</a>. If a thread has nothing else to do, either because it has finished processing all its chunks or because it’s waiting for
         a chunk to be sorted, it can take a chunk from the stack and sort that.
      </p>
      
      <p class="noind">The following listing shows a sample implementation that uses this technique. As with most of the examples, this is intended
         to demonstrate an idea rather than being production-ready code. If you’re using a C++17 compiler and your library supports
         it, you’re better off using the parallel algorithms provided by Standard Library, as covered in <a href="kindle_split_020.html#ch10" class="calibre4">chapter 10</a>.
      </p>
      
      
      
      <h5 class="notetitle" id="ch08ex01">Listing 8.1. <a id="ch08ex01__title" class="calibre4"></a>Parallel Quicksort using a stack of pending chunks to sort
      </h5>
      <pre id="PLd0e29723" class="calibre5">template&lt;typename T&gt;
struct sorter                                                  <b class="calibre24"><i class="calibre6">1</i></b>
{
    struct chunk_to_sort
    {
        std::list&lt;T&gt; data;
        std::promise&lt;std::list&lt;T&gt; &gt; promise;
    };
    thread_safe_stack&lt;chunk_to_sort&gt; chunks;                   <b class="calibre24"><i class="calibre6">2</i></b>
    std::vector&lt;std::thread&gt; threads;                          <b class="calibre24"><i class="calibre6">3</i></b>
    unsigned const max_thread_count;
    std::atomic&lt;bool&gt; end_of_data;
    sorter():
        max_thread_count(std::thread::hardware_concurrency()-1),
        end_of_data(false)
    {}
    ~sorter()                                                  <b class="calibre24"><i class="calibre6">4</i></b>
    {
        end_of_data=true;                                      <b class="calibre24"><i class="calibre6">5</i></b>
        for(unsigned i=0;i&lt;threads.size();++i)
        {
            threads[i].join();                                 <b class="calibre24"><i class="calibre6">6</i></b>
        }
    }
    void try_sort_chunk()
    {
        boost::shared_ptr&lt;chunk_to_sort &gt; chunk=chunks.pop();  <b class="calibre24"><i class="calibre6">7</i></b>
        if(chunk)
        {
            sort_chunk(chunk);                                 <b class="calibre24"><i class="calibre6">8</i></b>
        }
    }
    std::list&lt;T&gt; do_sort(std::list&lt;T&gt;&amp; chunk_data)             <b class="calibre24"><i class="calibre6">9</i></b>
    {
        if(chunk_data.empty())
        {
            return chunk_data;
        }
        std::list&lt;T&gt; result;
        result.splice(result.begin(),chunk_data,chunk_data.begin());
        T const&amp; partition_val=*result.begin();
        typename std::list&lt;T&gt;::iterator divide_point=          <b class="calibre24"><i class="calibre6">10</i></b>
            std::partition(chunk_data.begin(),chunk_data.end(),
                           [&amp;](T const&amp; val){return val&lt;partition_val;});
        chunk_to_sort new_lower_chunk;
        new_lower_chunk.data.splice(new_lower_chunk.data.end(),
                                    chunk_data,chunk_data.begin(),
                                    divide_point);
        std::future&lt;std::list&lt;T&gt; &gt; new_lower=
            new_lower_chunk.promise.get_future();
        chunks.push(std::move(new_lower_chunk));               <b class="calibre24"><i class="calibre6">11</i></b>
        if(threads.size()&lt;max_thread_count)                    <b class="calibre24"><i class="calibre6">12</i></b>
        {
            threads.push_back(std::thread(&amp;sorter&lt;T&gt;::sort_thread,this));
        }
        std::list&lt;T&gt; new_higher(do_sort(chunk_data));
        result.splice(result.end(),new_higher);
        while(new_lower.wait_for(std::chrono::seconds(0)) !=
              std::future_status::ready)                       <b class="calibre24"><i class="calibre6">13</i></b>
        {
            try_sort_chunk();                                  <b class="calibre24"><i class="calibre6">14</i></b>
        }
        result.splice(result.begin(),new_lower.get());
        return result;
    }
    void sort_chunk(boost::shared_ptr&lt;chunk_to_sort &gt; const&amp; chunk)
    {
        chunk-&gt;promise.set_value(do_sort(chunk-&gt;data));        <b class="calibre24"><i class="calibre6">15</i></b>
    }
    void sort_thread()
    {
        while(!end_of_data)                                    <b class="calibre24"><i class="calibre6">16</i></b>
        {
            try_sort_chunk();                                  <b class="calibre24"><i class="calibre6">17</i></b>
            std::this_thread::yield();                         <b class="calibre24"><i class="calibre6">18</i></b>
        }
    }
};
template&lt;typename T&gt;
std::list&lt;T&gt; parallel_quick_sort(std::list&lt;T&gt; input)          <b class="calibre24"><i class="calibre6">19</i></b>
{
    if(input.empty())
    {
        return input;
    }
    sorter&lt;T&gt; s;
    return s.do_sort(input);                                  <b class="calibre24"><i class="calibre6">20</i></b>
}</pre>
      
      <p class="noind"><a id="iddle1302" class="calibre4"></a><a id="iddle1319" class="calibre4"></a><a id="iddle1770" class="calibre4"></a><a id="iddle2356" class="calibre4"></a>Here, the <kbd class="calibre17">parallel_quick_sort</kbd> function <b class="calibre24"><i class="calibre6">19</i></b> delegates most of the functionality to the <kbd class="calibre17">sorter</kbd> class <b class="calibre24"><i class="calibre6">1</i></b>, which provides an easy way of grouping the stack of unsorted chunks <b class="calibre24"><i class="calibre6">2</i></b> and the set of threads <b class="calibre24"><i class="calibre6">3</i></b>. The main work is done in the <kbd class="calibre17">do_sort</kbd> member function <b class="calibre24"><i class="calibre6">9</i></b>, which does the usual partitioning of the data <b class="calibre24"><i class="calibre6">10</i></b>. This time, rather than spawning a new thread for one chunk, it pushes it onto the stack <b class="calibre24"><i class="calibre6">11</i></b> and spawns a new thread while you still have processors to spare <b class="calibre24"><i class="calibre6">12</i></b>. Because the lower chunk might be handled by another thread, you then have to wait for it to be ready <b class="calibre24"><i class="calibre6">13</i></b>. In order to help things along (in case you’re the only thread or all the others are already busy), you try to process chunks
         from the stack on this thread while you’re waiting <b class="calibre24"><i class="calibre6">14</i></b>. <kbd class="calibre17">try_sort_chunk</kbd> pops a chunk off the stack <b class="calibre24"><i class="calibre6">7</i></b>, and sorts it <b class="calibre24"><i class="calibre6">8</i></b>, storing the result in the <kbd class="calibre17">promise</kbd>, ready to be picked up by the thread that posted the chunk on the stack <b class="calibre24"><i class="calibre6">15</i></b>.
      </p>
      
      <p class="noind">Your freshly spawned threads sit in a loop trying to sort chunks off the stack <b class="calibre24"><i class="calibre6">17</i></b>, while the <kbd class="calibre17">end_of_data</kbd> flag isn’t set <b class="calibre24"><i class="calibre6">16</i></b>. In between checking, they yield to other threads <b class="calibre24"><i class="calibre6">18</i></b> to give them a chance to put some more work on the stack. This code relies on the destructor of your <kbd class="calibre17">sorter</kbd> class <b class="calibre24"><i class="calibre6">4</i></b> to tidy up these threads. When all the data has been sorted, <kbd class="calibre17">do_sort</kbd> will return (even though the worker threads are still running), so your main thread will return from <kbd class="calibre17">parallel_quick_sort</kbd> <b class="calibre24"><i class="calibre6">20</i></b> and destroy your <kbd class="calibre17">sorter</kbd> object. This sets the <kbd class="calibre17">end_of_data</kbd> flag <b class="calibre24"><i class="calibre6">5</i></b> and waits for the threads to finish <b class="calibre24"><i class="calibre6">6</i></b>. Setting the flag terminates the loop in the thread function <b class="calibre24"><i class="calibre6">16</i></b>.
      </p>
      
      <p class="noind">With this approach you no longer have the problem of unbounded threads that you have with a <kbd class="calibre17">spawn_task</kbd> that launches a new thread, and you’re no longer relying on the C++ Thread Library to choose the number of threads for you,
         as it does with <kbd class="calibre17">std::async()</kbd>. Instead, you limit the number of threads to the value of <kbd class="calibre17">std:: thread::hardware_concurrency()</kbd> in order to avoid excessive task switching. You do, however, have another potential problem: the management of these threads
         and the communication between them add quite a lot of complexity to the code. Also, although the threads are processing separate
         data elements, they all access the stack to add new chunks and to remove chunks for processing. This heavy contention can
         reduce performance, even if you use a lock-free (and hence nonblocking) stack, for reasons you’ll see shortly.
      </p>
      
      <p class="noind">This approach is a specialized version of a <i class="calibre6">thread pool</i>—that’s a set of threads that each take work to do from a list of pending work, do the work, and then go back to the list
         for more. Some of the potential problems with thread pools (including the contention on the work list) and ways of addressing
         them are covered in <a href="kindle_split_019.html#ch09" class="calibre4">chapter 9</a>. <a id="iddle1294" class="calibre4"></a><a id="iddle2450" class="calibre4"></a><a id="iddle2452" class="calibre4"></a><a id="iddle2534" class="calibre4"></a>The problems of scaling your application to multiple processors are discussed in more detail later in this chapter (see <a href="#ch08lev2sec4" class="calibre4">section 8.2.1</a>).
      </p>
      
      <p class="noind">Both dividing the data before processing begins and dividing it recursively presume that the data itself is fixed beforehand,
         and you’re looking at ways of dividing it. This isn’t always the case; if the data is dynamically generated or is coming from
         external input, this approach doesn’t work. In this case, it might make more sense to divide the work by task type rather
         than dividing based on the data.
      </p>
      
      
      
      <h4 id="ch08lev2sec3" class="calibre23">8.1.3. <a id="ch08lev2sec3__title" class="calibre4"></a>Dividing work by task type
      </h4>
      
      <p class="noind">Dividing work between threads by allocating different chunks of data to each thread (whether up front or recursively during
         processing) still rests on the assumption that the threads are going to be doing the same work on each chunk of data. An alternative
         to dividing the work is to make the threads specialists, where each performs a distinct task, just as plumbers and electricians
         perform distinct tasks when building a house. Threads may or may not work on the same data, but if they do, it’s for different
         purposes.
      </p>
      
      <p class="noind">This is the sort of division of work that results from separating concerns with concurrency; each thread has a different task,
         which it carries out independently of other threads. Occasionally other threads may give it data or trigger events that it
         needs to handle, but in general each thread focuses on doing one thing well. In itself, this is basic good design; each piece
         of code should have a single responsibility.
      </p>
      
      
      <h5 class="notetitle" id="ch08lev3sec1"><a id="ch08lev3sec1__title" class="calibre4"></a>Dividing work by task type to separate concerns
      </h5>
      
      <p class="noind">A single-threaded application has to handle conflicts with the single responsibility principle where there are multiple tasks
         that need to be run continuously over a period of time, or where the application needs to be able to handle incoming events
         (such as user key presses or incoming network data) in a timely fashion, even while other tasks are ongoing. In the single-threaded
         world you end up manually writing code that performs a bit of task A, a bit of task B, checks for key presses, checks for
         incoming network packets, and then loops back to perform another bit of task A. This means that the code for task A ends up
         being complicated by the need to save its state and return control to the main loop periodically. If you add too many tasks
         to the loop, things might slow down too much, and the user may find it takes too long to respond to the key press. I’m sure
         you’ve all seen the extreme form of this in action with some application or other: you set it to doing some task, and the
         interface freezes until it has completed the task.
      </p>
      
      <p class="noind">This is where threads come in. If you run each of the tasks in a separate thread, the operating system handles this for you.
         In the code for task A, you can focus on performing the task and not worry about saving state and returning to the main loop
         or how long you spend before doing so. The operating system will automatically save the state and switch to task B or C when
         appropriate, and if the target system has multiple cores or processors, tasks A and B may be able to run concurrently. The
         code for handling the key press or network packet will now be run in a timely fashion, and everybody <a id="iddle1292" class="calibre4"></a><a id="iddle1890" class="calibre4"></a><a id="iddle2449" class="calibre4"></a><a id="iddle2451" class="calibre4"></a><a id="iddle2504" class="calibre4"></a>wins: the user gets timely responses, and you, as the developer, have simpler code because each thread can focus on doing
         operations related directly to its responsibilities, rather than getting mixed up with control flow and user interaction.
      </p>
      
      <p class="noind">That sounds like a nice, rosy vision. Can it be like that? As with everything, it depends on the details. If everything is
         independent, and the threads have no need to communicate with each other, then it can be this easy. Unfortunately, the world
         is rarely like that. These nice background tasks are often doing something that the user requested, and they need to let the
         user know when they’re done by updating the user interface in some manner. Alternatively, the user might want to cancel the
         task, which therefore requires the user interface to somehow send a message to the background task telling it to stop. Both
         these cases require careful thought and design and suitable synchronization, but the concerns are still separate. The user
         interface thread still handles the user interface, but it might have to update it when asked to do so by other threads. Likewise,
         the thread running the background task still focuses on the operations required for that task; it just happens that one of
         them is “allow task to be stopped by another thread.” In neither case do the threads care where the request came from, only
         that it was intended for them and relates directly to their responsibilities.
      </p>
      
      <p class="noind">There are two big dangers with separating concerns with multiple threads. The first is that you’ll end up separating the <i class="calibre6">wrong</i> concerns. The symptoms to check for are that there is a lot of data shared between the threads or the different threads end
         up waiting for each other; both cases boil down to too much communication between threads. If this happens, it’s worth looking
         at the reasons for the communication. If all the communication relates to the same issue, maybe that should be the key responsibility
         of a single thread and extracted from all the threads that refer to it. Alternatively, if two threads are communicating a
         lot with each other but much less with other threads, maybe they should be combined into a single thread.
      </p>
      
      <p class="noind">When dividing work across threads by task type, you don’t have to limit yourself to completely isolated cases. If multiple
         sets of input data require the same <i class="calibre6">sequence</i> of operations to be applied, you can divide the work so each thread performs one stage from the overall sequence.
      </p>
      
      
      
      <h5 class="notetitle" id="ch08lev3sec2"><a id="ch08lev3sec2__title" class="calibre4"></a>Dividing a sequence of tasks between threads
      </h5>
      
      <p class="noind">If your task consists of applying the same sequence of operations to many independent data items, you can use a <i class="calibre6">pipeline</i> to exploit the available concurrency of your system. This is by analogy to a physical pipeline: data flows in at one end
         through a series of operations (pipes) and out at the other end.
      </p>
      
      <p class="noind">To divide the work this way, you create a separate thread for each stage in the pipeline—one thread for each of the operations
         in the sequence. When the operation is completed, the data element is put in a queue to be picked up by the next thread. This
         allows the thread performing the first operation in the sequence to start on the next data element while the second thread
         in the pipeline is working on the first element.
      </p>
      
      <p class="noind"><a id="iddle1167" class="calibre4"></a><a id="iddle1798" class="calibre4"></a>This is an alternative to dividing the data between threads, as described in <a href="#ch08lev2sec1" class="calibre4">section 8.1.1</a>, and is appropriate in circumstances where the input data itself isn’t all known when the operation is started. For example,
         the data might be coming in over a network, or the first operation in the sequence might be to scan a filesystem in order
         to identify files to process.
      </p>
      
      <p class="noind">Pipelines are also good when each operation in the sequence is time-consuming; by dividing the tasks between threads rather
         than the data, you change the performance profile. Suppose you have 20 data items to process on 4 cores, and each data item
         requires 4 steps, which take 3 seconds each. If you divide the data between four threads, then each thread has five items
         to process. Assuming there’s no other processing that might affect the timings, after 12 seconds you’ll have 4 items processed,
         after 24 seconds 8 items processed, and so on. All 20 items will be done after 1 minute. With a pipeline, things work differently.
         Each of your four steps can be assigned to a processing core. Now the first item has to be processed by each core, so it still
         takes the full 12 seconds. Indeed, after 12 seconds you only have 1 item processed, which isn’t as good as with the division
         by data. But once the pipeline is <i class="calibre6">primed</i>, things proceed a bit differently; after the first core has processed the first item, it moves on to the second, so once
         the final core has processed the first item, it can perform its step on the second. You now get 1 item processed every 3 seconds
         rather than having the items processed in batches of 4 every 12 seconds.
      </p>
      
      <p class="noind">The overall time to process the entire batch takes longer because you have to wait nine seconds before the final core starts
         processing the first item. But smoother, more regular processing can be beneficial in some circumstances. Consider, for example,
         a system for watching high-definition digital videos. In order for the video to be watchable, you typically need at least
         25 frames per second and ideally more. Also, the viewer needs these to be evenly spaced to give the impression of continuous
         movement; an application that can decode 100 frames per second is still of no use if it pauses for a second, then displays
         100 frames, then pauses for another second, and displays another 100 frames. On the other hand, viewers are probably happy
         to accept a delay of a couple of seconds when they <i class="calibre6">start</i> watching a video. In this case, parallelizing using a pipeline that outputs frames at a nice steady rate is probably preferable.
      </p>
      
      <p class="noind">Having looked at various techniques for dividing the work between threads, let’s take a look at the factors affecting the
         performance of a multithreaded system and how that can impact your choice of techniques.
      </p>
      
      
      
      
      
      <h3 id="ch08lev1sec2" class="chapter"><a id="ch08lev1sec2__title" class="calibre3"></a>8.2. Factors affecting the performance of concurrent code
      </h3>
      
      <p class="noind">If you’re using concurrency to improve the performance of your code on systems with multiple processors, you need to know
         what factors are going to affect the performance. Even if you’re using multiple threads to separate concerns, you need to
         ensure that this doesn’t adversely affect the performance. Customers won’t thank you if your application runs more slowly
         on their shiny new 16-core machine than it did on their old single-core one.
      </p>
      
      <p class="noind"><a id="iddle1173" class="calibre4"></a><a id="iddle1734" class="calibre4"></a><a id="iddle1808" class="calibre4"></a><a id="iddle2357" class="calibre4"></a>As you’ll see shortly, many factors affect the performance of multithreaded code—even something as simple as changing which
         data elements are processed by each thread (while keeping everything else identical) can have a dramatic effect on performance.
         Without further ado, let’s look at some of these factors, starting with the obvious one: how many processors does your target
         system have?
      </p>
      
      
      <h4 id="ch08lev2sec4" class="calibre23">8.2.1. <a id="ch08lev2sec4__title" class="calibre4"></a>How many processors?
      </h4>
      
      <p class="noind">The number (and structure) of processors is the first big factor that affects the performance of a multithreaded application,
         and it’s a crucial one. In some cases you know exactly what the target hardware is and can design with this in mind, taking
         real measurements on the target system or an exact duplicate. If so, you’re one of the lucky ones; in general, you don’t have
         that luxury. You might be developing on a <i class="calibre6">similar</i> system, but the differences can be crucial. For example, you might be developing on a dual- or quad-core system, but your
         customers’ systems may have one multicore processor (with any number of cores), or multiple single-core processors, or even
         multiple multicore processors. The behavior and performance characteristics of a concurrent program can vary considerably
         under these different circumstances, so you need to think carefully about what the impact may be and test things where possible.
      </p>
      
      <p class="noind">To a first approximation, a single 16-core processor is the same as 4 quad-core processors or 16 single-core processors: in
         each case the system can run 16 threads concurrently. If you want to take advantage of this, your application must have at
         least 16 threads. If it has fewer than 16, you’re leaving processor power on the table (unless the system is running other
         applications too, but we’ll ignore that possibility for now). On the other hand, if you have more than 16 threads ready to
         run (and not blocked, waiting for something), your application will waste processor time switching between the threads, as
         discussed in <a href="kindle_split_011.html#ch01" class="calibre4">chapter 1</a>. When this happens, the situation is called <i class="calibre6">oversubscription</i>.
      </p>
      
      <p class="noind">To allow applications to scale the number of threads in line with the number of threads the hardware can run concurrently,
         the C++11 Standard Thread Library provides <kbd class="calibre17">std::thread::hardware_concurrency()</kbd>. You’ve already seen how that can be used to scale the number of threads to the hardware.
      </p>
      
      <p class="noind">Using <kbd class="calibre17">std::thread::hardware_concurrency()</kbd> directly requires care; your code doesn’t take into account any of the other threads that are running on the system unless
         you explicitly share that information. In the worst-case scenario, if multiple threads call a function that uses <kbd class="calibre17">std::thread::hardware_concurrency()</kbd> for scaling at the same time, there will be huge oversubscription. <kbd class="calibre17">std::async()</kbd> avoids this problem because the library is aware of all calls and can schedule appropriately. Careful use of thread pools
         can also avoid this problem.
      </p>
      
      <p class="noind">But even if you take into account all threads running in your application, you’re still subject to the impact of other applications
         running at the same time. Although the use of multiple CPU-intensive applications simultaneously is rare on single-user systems,
         there are some domains where it’s more common. Systems designed to handle <a id="iddle1095" class="calibre4"></a><a id="iddle1168" class="calibre4"></a><a id="iddle1169" class="calibre4"></a><a id="iddle1208" class="calibre4"></a><a id="iddle1235" class="calibre4"></a><a id="iddle1807" class="calibre4"></a><a id="iddle1927" class="calibre4"></a>this scenario typically offer mechanisms to allow each application to choose an appropriate number of threads, although these
         mechanisms are outside the scope of the C++ Standard. One option is for a facility like <kbd class="calibre17">std::async()</kbd> to take into account the total number of asynchronous tasks run by all applications when choosing the number of threads.
         Another is to limit the number of processing cores that can be used by a given application. I’d expect this limit to be reflected
         in the value returned by <kbd class="calibre17">std::thread::hardware_concurrency()</kbd> on these platforms, although this isn’t guaranteed. If you need to handle this scenario, consult your system documentation
         to see what options are available to you.
      </p>
      
      <p class="noind">One additional twist to this situation is that the ideal algorithm for a problem can depend on the size of the problem compared
         to the number of processing units. If you have a massively parallel system with many processing units, an algorithm that performs
         more operations overall may finish more quickly than one that performs fewer operations, because each processor performs only
         a few operations.
      </p>
      
      <p class="noind">As the number of processors increases, so does the likelihood and performance impact of another problem: that of multiple
         processors trying to access the same data.
      </p>
      
      
      
      <h4 id="ch08lev2sec5" class="calibre23">8.2.2. <a id="ch08lev2sec5__title" class="calibre4"></a>Data contention and cache ping-pong
      </h4>
      
      <p class="noind">If two threads are executing concurrently on different processors and they’re both <i class="calibre6">reading</i> the same data, this usually won’t cause a problem; the data will be copied into their respective caches, and both processors
         can proceed. But if one of the threads <i class="calibre6">modifies</i> the data, this change then has to propagate to the cache on the other core, which takes time. Depending on the nature of
         the operations on the two threads, and the memory orderings used for the operations, this modification may cause the second
         processor to stop in its tracks and wait for the change to propagate through the memory hardware. In terms of CPU instructions,
         this can be a <i class="calibre6">phenomenally</i> slow operation, equivalent to many hundreds of individual instructions, although the exact timing depends primarily on the
         physical structure of the hardware.
      </p>
      
      <p class="noind">Consider the following simple piece of code:</p>
      
      <pre id="PLd0e30324" class="calibre5">std::atomic&lt;unsigned long&gt; counter(0);
void processing_loop()
{
    while(counter.fetch_add(1,std::memory_order_relaxed)&lt;100000000)
    {
        do_something();
    }
}</pre>
      
      <p class="noind">The <kbd class="calibre17">counter</kbd> is global, so any threads that call <kbd class="calibre17">processing_loop()</kbd> are modifying the same variable. Therefore, for each increment the processor must ensure it has an up-to-date copy of <kbd class="calibre17">counter</kbd> in its cache, modify the value, and publish it to other processors. Even though you’re using <kbd class="calibre17">std::memory_order_relaxed</kbd>, so the compiler doesn’t have to synchronize any other data, <kbd class="calibre17">fetch_add</kbd> is a read-modify-write operation and therefore needs to retrieve the most recent value of the variable. If another <a id="iddle1455" class="calibre4"></a><a id="iddle1571" class="calibre4"></a>thread on another processor is running the same code, the data for <kbd class="calibre17">counter</kbd> must therefore be passed back and forth between the two processors and their corresponding caches so that each processor
         has the latest value for <kbd class="calibre17">counter</kbd> when it does the increment. If <kbd class="calibre17">do_something()</kbd> is short enough, or if there are too many processors running this code, the processors might find themselves waiting for
         each other; one processor is ready to update the value, but another processor is currently doing that, so it has to wait until
         the second processor has completed its update and the change has propagated. This situation is called <i class="calibre6">high contention</i>. If the processors rarely have to wait for each other, you have <i class="calibre6">low contention</i>.
      </p>
      
      <p class="noind">In a loop like this one, the data for <kbd class="calibre17">counter</kbd> will be passed back and forth between the caches many times. This is called <i class="calibre6">cache ping-pong</i>, and it can seriously impact the performance of the application. If a processor stalls because it has to wait for a cache
         transfer, it can’t do <i class="calibre6">any</i> work in the meantime, even if there are other threads waiting that could do useful work, so this is bad news for the whole
         application.
      </p>
      
      <p class="noind">You might think that this won’t happen to you; after all, you don’t have any loops like that. Are you sure? What about mutex
         locks? If you acquire a mutex in a loop, your code is similar to the previous code from the point of view of data accesses.
         In order to lock the mutex, another thread must transfer the data that makes up the mutex to its processor and modify it.
         When it’s done, it modifies the mutex again to unlock it, and the mutex data has to be transferred to the next thread to acquire
         the mutex. This transfer time is in addition to any time that the second thread has to wait for the first to release the mutex:
      </p>
      
      <pre id="PLd0e30390" class="calibre5">std::mutex m;
my_data data;
void processing_loop_with_mutex()
{
    while(true)
    {
        std::lock_guard&lt;std::mutex&gt; lk(m);
        if(done_processing(data)) break;
    }
}</pre>
      
      <p class="noind">Now, here’s the worst part: if the data and mutex are accessed by more than one thread, then as you add more cores and processors
         to the system, it becomes more likely that you will get high contention and one processor having to wait for another. If you’re
         using multiple threads to process the same data more quickly, the threads are competing for the data and thus competing for
         the same mutex. The more of them there are, the more likely they’ll try to acquire the mutex at the same time, or access the
         atomic variable at the same time, and so forth.
      </p>
      
      <p class="noind">The effects of contention with mutexes are usually different from the effects of contention with atomic operations for the
         simple reason that the use of a mutex naturally serializes threads at the operating system level rather than at the processor
         level. If you have enough threads ready to run, the operating system can schedule another <a id="iddle1096" class="calibre4"></a><a id="iddle1172" class="calibre4"></a><a id="iddle1359" class="calibre4"></a>thread to run while one thread is waiting for the mutex, whereas a processor stall prevents any threads from running on that
         processor. But it will still impact the performance of those threads that <i class="calibre6">are</i> competing for the mutex; they can only run one at a time, after all.
      </p>
      
      <p class="noind">Back in <a href="kindle_split_013.html#ch03" class="calibre4">chapter 3</a>, you saw how a rarely updated data structure can be protected with a single-writer, multiple-reader mutex (see <a href="kindle_split_013.html#ch03lev2sec12" class="calibre4">section 3.3.2</a>). Cache ping-pong effects can nullify the benefits of this mutex if the workload is unfavorable, because all threads accessing
         the data (even reader threads) still have to modify the mutex itself. As the number of processors accessing the data goes
         up, the contention on the mutex itself increases, and the cache line holding the mutex must be transferred between cores,
         potentially increasing the time taken to acquire and release locks to undesirable levels. There are techniques to ameliorate
         this problem by spreading out the mutex across multiple cache lines, but unless you implement your own mutex, you are subject
         to whatever your system provides.
      </p>
      
      <p class="noind">If this cache ping-pong is bad, how can you avoid it? As you’ll see later in the chapter, the answer ties in nicely with general
         guidelines for improving the potential for concurrency: do what you can to reduce the potential for two threads competing
         for the same memory location.
      </p>
      
      <p class="noind">It’s not quite that simple, though; things never are. Even if a particular memory location is only ever accessed by one thread,
         you can <i class="calibre6">still</i> get cache ping-pong due to an effect known as <i class="calibre6">false sharing</i>.
      </p>
      
      
      
      <h4 id="ch08lev2sec6" class="calibre23">8.2.3. <a id="ch08lev2sec6__title" class="calibre4"></a>False sharing
      </h4>
      
      <p class="noind">Processor caches don’t generally deal in individual memory locations; instead, they deal in blocks of memory called <i class="calibre6">cache lines</i>. These blocks of memory are typically 32 or 64 bytes in size, but the exact details depend on the particular processor model
         being used. Because the cache hardware only deals in cache-line-sized blocks of memory, small data items in adjacent memory
         locations will be in the same cache line. Sometimes this is good: if a set of data accessed by a thread is in the same cache
         line, this is better for the performance of the application than if the same set of data was spread over multiple cache lines.
         But if the data items in a cache line are unrelated and need to be accessed by different threads, this can be a major cause
         of performance problems.
      </p>
      
      <p class="noind">Suppose you have an array of <kbd class="calibre17">int</kbd> values and a set of threads that each access their own entry in the array but do so repeatedly, including updates. Because
         an <kbd class="calibre17">int</kbd> is typically much smaller than a cache line, quite a few of those array entries will be in the same cache line. Consequently,
         even though each thread only accesses its own array entry, the cache hardware <i class="calibre6">still</i> has to play cache ping-pong. Every time the thread accessing entry 0 needs to update the value, ownership of the cache line
         needs to be transferred to the processor running that thread, only to be transferred to the cache for the processor running
         the thread for entry 1 when that thread needs to update its data item. The cache line is shared, even though none of the data
         is, hence the term <i class="calibre6">false sharing</i>. The solution here is to structure the data so that data items to be accessed <a id="iddle1170" class="calibre4"></a><a id="iddle1239" class="calibre4"></a><a id="iddle1812" class="calibre4"></a><a id="iddle2157" class="calibre4"></a><a id="iddle2158" class="calibre4"></a><a id="iddle2427" class="calibre4"></a><a id="iddle2445" class="calibre4"></a>by the same thread are close together in memory (and thus more likely to be in the same cache line), whereas those that are
         to be accessed by separate threads are far apart in memory and thus more likely to be in separate cache lines. You’ll see
         how this affects the design of the code and data later in this chapter. The C++17 standard defines <kbd class="calibre17">std::hardware_destructive_interference_size</kbd> in the header <kbd class="calibre17">&lt;new&gt;</kbd>, which specifies the maximum number of consecutive bytes that may be subject to false sharing for the current compilation
         target. If you ensure that your data is at least this number of bytes apart, then there will be no false sharing.
      </p>
      
      <p class="noind">If having multiple threads access data from the same cache line is bad, how does the memory layout of data accessed by a single
         thread affect things?
      </p>
      
      
      
      <h4 id="ch08lev2sec7" class="calibre23">8.2.4. <a id="ch08lev2sec7__title" class="calibre4"></a>How close is your data?
      </h4>
      
      <p class="noind">Although false sharing is caused by having data accessed by one thread too close to data accessed by another thread, another
         pitfall associated with data layout directly impacts the performance of a single thread on its own. The issue is data proximity:
         if the data accessed by a single thread is spread out in memory, it’s likely that it lies on separate cache lines. On the
         flip side, if the data accessed by a single thread is close together in memory, it’s more likely to lie on the same cache
         line. Consequently, if data is spread out, more cache lines must be loaded from memory onto the processor cache, which can
         increase memory access latency and reduce performance compared to data that’s located close together.
      </p>
      
      <p class="noind">Also, if the data is spread out, there’s an increased chance that a given cache line containing data for the current thread
         also contains data that’s <i class="calibre6">not</i> for the current thread. At the extreme, there’ll be more data in the cache that you don’t care about than data that you do.
         This wastes precious cache space and increases the chance that the processor will experience a cache miss and have to fetch
         a data item from main memory even if it once held it in the cache, because it had to remove the item from the cache to make
         room for another.
      </p>
      
      <p class="noind">Now, this is important with single-threaded code, so why am I bringing it up here? The reason is <i class="calibre6">task switching</i>. If there are more threads than cores in the system, each core is going to be running multiple threads. This increases the
         pressure on the cache, as you try to ensure that different threads are accessing different cache lines in order to avoid false
         sharing. Consequently, when the processor switches threads, it’s more likely to have to reload the cache lines if each thread
         uses data spread across multiple cache lines than if each thread’s data is close together in the same cache line. The C++17
         standard specifies the constant <kbd class="calibre17">std::hardware_constructive_interference_size</kbd>, also in the header <kbd class="calibre17">&lt;new&gt;</kbd>, which is the maximum number of consecutive bytes guaranteed to be on the same cache line (if suitably aligned). If you can
         fit data that is needed together within this number of bytes, it will potentially reduce the number of cache misses.
      </p>
      
      <p class="noind">If there are more threads than cores or processors, the operating system might also choose to schedule a thread on one core
         for one time slice and then on another core <a id="iddle1150" class="calibre4"></a><a id="iddle1171" class="calibre4"></a><a id="iddle1174" class="calibre4"></a><a id="iddle1254" class="calibre4"></a><a id="iddle1735" class="calibre4"></a><a id="iddle2453" class="calibre4"></a><a id="iddle2500" class="calibre4"></a>for the next time slice. This will therefore require transferring the cache lines for that thread’s data from the cache for
         the first core to the cache for the second; the more cache lines that need transferring, the more time-consuming this will
         be. Although operating systems typically avoid this when they can, it does happen and does impact performance.
      </p>
      
      <p class="noind">Task-switching problems are particularly prevalent when lots of threads are ready to run as opposed to waiting. This is an
         issue we’ve already touched on: oversubscription.
      </p>
      
      
      
      <h4 id="ch08lev2sec8" class="calibre23">8.2.5. <a id="ch08lev2sec8__title" class="calibre4"></a>Oversubscription and excessive task switching
      </h4>
      
      <p class="noind">In multithreaded systems, it’s typical to have more threads than processors, unless you’re running on <i class="calibre6">massively parallel</i> hardware. But threads often spend time waiting for external I/O to complete, blocked on mutexes, waiting for condition variables,
         and so forth, so this isn’t a problem. Having the extra threads enables the application to perform useful work rather than
         having processors sitting idle while the threads wait.
      </p>
      
      <p class="noind">This isn’t always a good thing. If you have too many additional threads, there will be more threads ready to run than there
         are available processors, and the operating system will have to start task switching quite heavily in order to ensure they
         all get a fair time slice. As you saw in <a href="kindle_split_011.html#ch01" class="calibre4">chapter 1</a>, this can increase the overhead of the task switching as well as compound any cache problems resulting from lack of proximity.
         Oversubscription can arise when you have a task that repeatedly spawns new threads without limits, as the recursive quick
         sort from <a href="kindle_split_014.html#ch04" class="calibre4">chapter 4</a> did, or where the natural number of threads when you separate by task type is more than the number of processors and the
         work is naturally CPU-bound rather than I/O-bound.
      </p>
      
      <p class="noind">If you’re spawning too many threads because of data division, you can limit the number of worker threads, as you saw in <a href="#ch08lev2sec2" class="calibre4">section 8.1.2</a>. If the oversubscription is due to the natural division of work, there’s not a lot you can do to ameliorate the problem save
         choosing a different division. In that case, choosing the appropriate division may require more knowledge of the target platform
         than you have available and is only worth doing if performance is unacceptable and it can be demonstrated that changing the
         division of work does improve performance.
      </p>
      
      <p class="noind">Other factors can affect the performance of multithreaded code. The cost of cache ping-pong can vary quite considerably between
         two single-core processors and a single dual-core processor, even if they’re the same CPU type and clock speed, for example,
         but these are the major ones that will have a visible impact. Let’s now look at how that affects the design of the code and
         data structures.
      </p>
      
      
      
      
      <h3 id="ch08lev1sec3" class="chapter"><a id="ch08lev1sec3__title" class="calibre3"></a>8.3. Designing data structures for multithreaded performance
      </h3>
      
      <p class="noind">In <a href="#ch08lev1sec1" class="calibre4">section 8.1</a> we looked at various ways of dividing work between threads, and in <a href="#ch08lev1sec2" class="calibre4">section 8.2</a> we looked at various factors that can affect the performance of your code. How can you use this information when designing
         data structures for multithreaded performance? This is a different question than that addressed in <a href="kindle_split_016.html#ch06" class="calibre4">chapters 6</a> and <a href="kindle_split_017.html#ch07" class="calibre4">7</a>, <a id="iddle1022" class="calibre4"></a><a id="iddle1247" class="calibre4"></a><a id="iddle1580" class="calibre4"></a><a id="iddle1709" class="calibre4"></a><a id="iddle2502" class="calibre4"></a>which were about designing data structures that are safe for concurrent access. As you’ve seen in <a href="#ch08lev1sec2" class="calibre4">section 8.2</a>, the layout of the data used by a single thread can have an impact, even if that data isn’t shared with any other threads.
      </p>
      
      <p class="noind">The key things to bear in mind when designing your data structures for multithreaded performance are <i class="calibre6">contention</i>, <i class="calibre6">false sharing</i>, and <i class="calibre6">data proximity</i>. All three of these can have a big impact on performance, and you can often improve things by altering the data layout or
         changing which data elements are assigned to which thread. First off, let’s look at an easy win: dividing array elements between
         threads.
      </p>
      
      
      <h4 id="ch08lev2sec9" class="calibre23">8.3.1. <a id="ch08lev2sec9__title" class="calibre4"></a>Dividing array elements for complex operations
      </h4>
      
      <p class="noind">Suppose you’re doing some heavy-duty math, and you need to multiply two large square matrices together. To multiply matrices,
         you multiply each element in the first <i class="calibre6">row</i> of the first matrix with the corresponding element of the first <i class="calibre6">column</i> of the second matrix and add up the products to give the top-left element of the result. You then repeat this with the second
         row and the first column to give the second element in the first column of the result, and with the first row and second column
         to give the first element in the second column of the result, and so forth. This is shown in <a href="#ch08fig03" class="calibre4">figure 8.3</a>; the highlighting shows that the second row of the first matrix is paired with the third column of the second matrix to give
         the entry in the second row of the third column of the result.
      </p>
      
      
      
      <h5 class="notetitle" id="ch08fig03">Figure 8.3. <a id="ch08fig03__title" class="calibre4"></a>Matrix multiplication
      </h5>
      
      <p class="center1"><img alt="" src="08fig03_alt.jpg" class="calibre2"/></p>
      
      
      <p class="noind">Now let’s assume that these are <i class="calibre6">large</i> matrices with several thousand rows and columns, in order to make it worthwhile to use multiple threads to optimize the multiplication.
         Typically, a nonsparse matrix is represented by a big array in memory, with all the elements of the first row followed by
         all the elements of the second row, and so forth. To multiply your matrices you have three of these huge arrays. In order
         to get optimal performance, you need to pay careful attention to the data access patterns, particularly the writes to the
         third array.
      </p>
      
      <p class="noind">There are many ways you can divide the work between threads. Assuming you have more rows/columns than available processors,
         you could have each thread calculate the values for a number of columns in the result matrix, or have each thread calculate
         the results for a number of rows, or even have each thread calculate the results for a rectangular subset of the matrix.
      </p>
      
      <p class="noind">Back in <a href="#ch08lev2sec6" class="calibre4">sections 8.2.3</a> and <a href="#ch08lev2sec7" class="calibre4">8.2.4</a>, you saw that it’s better to access contiguous elements from an array rather than values all over the place, because this
         reduces cache usage and the chance of false sharing. If you have each thread compute a set of columns, it needs to read every
         value from the first matrix and the values from the corresponding columns in the second matrix, but you only have to write
         the column values. Given that the matrices are stored with the rows contiguous, this means that you’re accessing <i class="calibre6">N</i> elements from the first row, <i class="calibre6">N</i> elements from the second, and so forth (where <i class="calibre6">N</i> is the number of columns you’re processing). Because other threads will be accessing the other elements of each row, it’s
         clear that you ought to be accessing adjacent columns, so the <i class="calibre6">N</i> elements from each row are adjacent, and you minimize false sharing. If the space occupied by your <i class="calibre6">N</i> elements is an exact number of cache lines, there’ll be no false sharing because threads will be working on separate cache
         lines.
      </p>
      
      <p class="noind">On the other hand, if you have each thread compute a set of <i class="calibre6">rows</i>, then it needs to read every value from the <i class="calibre6">second</i> matrix and the values from the corresponding rows of the <i class="calibre6">first</i> matrix, but it only has to write the row values. Because the matrices are stored with the rows contiguous, you’re now accessing
         all elements from <i class="calibre6">N</i> rows. If you again choose adjacent rows, this means that the thread is now the only thread writing to those <i class="calibre6">N</i> rows; it has a contiguous block of memory that’s not touched by any other thread. This is likely an improvement over having
         each thread compute a set of columns, because the only possibility of false sharing is for the last few elements of one block
         with the first few of the next, but it’s worth timing it on the target architecture to confirm.
      </p>
      
      <p class="noind">What about your third option—dividing into rectangular blocks? This can be viewed as dividing into columns and then dividing
         into rows. As such, it has the same false-sharing potential as division by columns. If you can choose the number of columns
         in the block to avoid this possibility, there’s an advantage to rectangular division from the read side: you don’t need to
         read the entirety of either source matrix. You only need to read the values corresponding to the rows and columns of the target
         rectangle. To look at this in concrete terms, consider multiplying two matrices that have 1,000 rows and 1,000 columns. That’s
         1 million elements. If you have 100 processors, they can compute 10 rows each for a nice round 10,000 elements. But to calculate
         the results of those 10,000 elements, they need to access the entirety of the second matrix (1 million elements) plus the
         10,000 elements from the corresponding rows in the first matrix, for a grand total of 1,010,000 elements. On the other hand,
         if they each compute a block of 100 elements by 100 elements (which is still 10,000 elements total), they need to access the
         values from 100 rows of the first matrix (100 x 1,000 = 100,000 elements) and 100 columns of the second matrix (another 100,000).
         This is only 200,000 elements, which is a five-fold reduction in the number of elements read. If you’re reading fewer elements,
         there’s less chance of a cache miss and the potential for greater performance.
      </p>
      
      <p class="noind"><a id="iddle1234" class="calibre4"></a><a id="iddle1252" class="calibre4"></a><a id="iddle2159" class="calibre4"></a><a id="iddle2501" class="calibre4"></a>It may therefore be better to divide the result matrix into small, square or almost-square blocks rather than have each thread
         compute the entirety of a small number of rows. You can adjust the size of each block at runtime, depending on the size of
         the matrices and the available number of processors. As ever, if performance is important, it’s vital to profile various options
         on the target architecture, and check the literature relevant to the field—I make no claim that these are the only or best
         options if you are doing matrix multiplication
      </p>
      
      <p class="noind">Chances are you’re not doing matrix multiplication, so how does this apply to you? The same principles apply to any situation
         where you have large blocks of data to divide between threads; look at all the aspects of the data access patterns carefully,
         and identify the potential causes of performance hits. There may be similar circumstances in your problem domain where changing
         the division of work can improve performance without requiring any change to the basic algorithm.
      </p>
      
      <p class="noind">OK, so we’ve looked at how access patterns in arrays can affect performance. What about other types of data structures?</p>
      
      
      
      <h4 id="ch08lev2sec10" class="calibre23">8.3.2. <a id="ch08lev2sec10__title" class="calibre4"></a>Data access patterns in other data structures
      </h4>
      
      <p class="noind">Fundamentally, the same considerations apply when trying to optimize the data access patterns of other data structures as
         when optimizing access to arrays:
      </p>
      
      <p class="calibre19"></p>
      <ul class="calibre21">
         
         <li class="calibre22">Try to adjust the data distribution between threads so that data that’s close together is worked on by the same thread.</li>
         
         <li class="calibre22">Try to minimize the data required by any given thread.</li>
         
         <li class="calibre22">Try to ensure that data accessed by separate threads is sufficiently far apart to avoid false sharing using <kbd class="calibre17">std::hardware_destructive_interference_size</kbd> as a guide.
         </li>
         
      </ul>
      
      <p class="noind">That’s not easy to apply to other data structures. For example, binary trees are inherently difficult to subdivide in any
         unit other than a subtree, which may or may not be useful, depending on how balanced the tree is and how many sections you
         need to divide it into. Also, the nature of the trees means that the nodes are likely dynamically allocated and thus end up
         in different places on the heap.
      </p>
      
      <p class="noind">Now, having data end up in different places on the heap isn’t a particular problem in itself, but it does mean that the processor
         has to keep more things in cache. This can be beneficial. If multiple threads need to traverse the tree, then they all need
         to access the tree nodes, but if the tree nodes only contain <i class="calibre6">pointers</i> to the real data held at the node, then the processor only has to load the data from memory if it’s needed. If the data is
         being modified by the threads that need it, this can avoid the performance hit of false sharing between the node data itself
         and the data that provides the tree structure.
      </p>
      
      <p class="noind">There’s a similar issue with data protected by a mutex. Suppose you have a simple class that contains a few data items and
         a mutex used to protect accesses from multiple threads. If the mutex and the data items are close together in memory, this
         is ideal for a thread that acquires the mutex; the data it needs may already be in the processor cache, because it was loaded
         in order to modify the mutex. But there’s also a downside: if other threads try to lock the mutex while it’s held by the first
         thread, they’ll need access to that memory. Mutex locks are typically implemented as a read-modify-write atomic operation
         on a memory location within the mutex to try to acquire the mutex, followed by a call to the operating system kernel if the
         mutex is already locked. This read-modify-write operation may cause the data held in the cache by the thread that owns the
         mutex to be invalidated. As far as the mutex goes, this isn’t a problem; that thread isn’t going to touch the mutex until
         it unlocks it. But if the mutex shares a cache line with the data being used by the thread, the thread that owns the mutex
         can take a performance hit because another thread tried to lock the mutex!
      </p>
      
      <p class="noind">One way to test whether this kind of false sharing is a problem is to add huge blocks of padding between the data elements
         that can be concurrently accessed by different threads. For example, you can use
      </p>
      
      <pre id="PLd0e30902" class="calibre5">struct protected_data
{
    std::mutex m;
    char padding[std::hardware_destructive_interference_size];     <b class="calibre24"><i class="calibre6">1</i></b>
    my_data data_to_protect;
};</pre>
      
      <p class="calibre19"></p>
      <ul class="calibre21">
         
         <li class="calibre22"><b class="calibre24"><i class="calibre6">1</i> If std::hardware_destructive_interference_size is not available with your compiler, you could use something like 65536 bytes
               which is likely to be orders of magnitude larger than a cache line</b></li>
         
      </ul>
      
      <p class="noind">to test the mutex contention issue or</p>
      
      <pre id="PLd0e30925" class="calibre5">struct my_data
{
    data_item1 d1;
    data_item2 d2;
    char padding[std::hardware_destructive_interference_size];
};
my_data some_array[256];</pre>
      
      <p class="noind">to test for false sharing of array data. If this improves the performance, you know that false sharing was a problem, and
         you can either leave the padding in or work to eliminate the false sharing in another way by rearranging the data accesses.
      </p>
      
      <p class="noind">There’s more than the data access patterns to consider when designing for concurrency, so let’s look at some of these additional
         considerations.
      </p>
      
      
      
      
      <h3 id="ch08lev1sec4" class="chapter"><a id="ch08lev1sec4__title" class="calibre3"></a>8.4. Additional considerations when designing for concurrency
      </h3>
      
      <p class="noind">So far in this chapter we’ve looked at ways of dividing work between threads, factors affecting performance, and how these
         factors affect your choice of data access patterns and data structures. There’s more to designing code for concurrency than
         that, though. You also need to consider things such as exception safety and scalability. Code is said to be <i class="calibre6">scalable</i> if the performance (whether in terms of reduced speed of execution <a id="iddle1151" class="calibre4"></a><a id="iddle1337" class="calibre4"></a><a id="iddle1740" class="calibre4"></a><a id="iddle1759" class="calibre4"></a>or increased throughput) increases as more processing cores are added to the system. Ideally, the performance increase is
         linear, so a system with 100 processors performs 100 times better than a system with one processor.
      </p>
      
      <p class="noind">Although code can work even if it isn’t scalable—a single-threaded application is certainly not scalable, for example—exception
         safety is a matter of correctness. If your code isn’t exception-safe, you can end up with broken invariants or race conditions,
         or your application might terminate unexpectedly because an operation threw an exception. With this in mind, we’ll look at
         exception safety first.
      </p>
      
      
      <h4 id="ch08lev2sec11" class="calibre23">8.4.1. <a id="ch08lev2sec11__title" class="calibre4"></a>Exception safety in parallel algorithms
      </h4>
      
      <p class="noind">Exception safety is an essential aspect of good C++ code, and code that uses concurrency is no exception. In fact, parallel
         algorithms often require that you take more care with exceptions than normal sequential algorithms. If an operation in a sequential
         algorithm throws an exception, the algorithm only has to worry about ensuring that it tidies up after itself to avoid resource
         leaks and broken invariants; it can merrily allow the exception to propagate to the caller for them to handle. By contrast,
         in a parallel algorithm many of the operations will be running on separate threads. In this case, the exception can’t be allowed
         to propagate because it’s on the wrong call stack. If a function spawned on a new thread exits with an exception, the application
         is terminated.
      </p>
      
      <p class="noind">As a concrete example, let’s revisit the <kbd class="calibre17">parallel_accumulate</kbd> function from <a href="kindle_split_012.html#ch02ex09" class="calibre4">listing 2.9</a>, which is reproduced here.
      </p>
      
      
      
      <h5 class="notetitle" id="ch08ex02">Listing 8.2. <a id="ch08ex02__title" class="calibre4"></a>A naive parallel version of <kbd class="calibre17">std::accumulate</kbd> (from <a href="kindle_split_012.html#ch02ex09" class="calibre4">listing 2.9</a>)
      </h5>
      <pre id="PLd0e31017" class="calibre5">template&lt;typename Iterator,typename T&gt;
struct accumulate_block
{
    void operator()(Iterator first,Iterator last,T&amp; result)
    {
        result=std::accumulate(first,last,result);              <b class="calibre24"><i class="calibre6">1</i></b>
    }
};
template&lt;typename Iterator,typename T&gt;
T parallel_accumulate(Iterator first,Iterator last,T init)
{
    unsigned long const length=std::distance(first,last);       <b class="calibre24"><i class="calibre6">2</i></b>
    if(!length)
        return init;
    unsigned long const min_per_thread=25;
    unsigned long const max_threads=
        (length+min_per_thread-1)/min_per_thread;
    unsigned long const hardware_threads=
        std::thread::hardware_concurrency();
    unsigned long const num_threads=
        std::min(hardware_threads!=0?hardware_threads:2,max_threads);
    unsigned long const block_size=length/num_threads;
    std::vector&lt;T&gt; results(num_threads);                        <b class="calibre24"><i class="calibre6">3</i></b>
    std::vector&lt;std::thread&gt;  threads(num_threads-1);           <b class="calibre24"><i class="calibre6">4</i></b>
    Iterator block_start=first;                                 <b class="calibre24"><i class="calibre6">5</i></b>
    for(unsigned long i=0;i&lt;(num_threads-1);++i)
    {
        Iterator block_end=block_start;                         <b class="calibre24"><i class="calibre6">6</i></b>
        std::advance(block_end,block_size);
        threads[i]=std::thread(                                 <b class="calibre24"><i class="calibre6">7</i></b>
            accumulate_block&lt;Iterator,T&gt;(),
            block_start,block_end,std::ref(results[i]));
        block_start=block_end;                                  <b class="calibre24"><i class="calibre6">8</i></b>
    }
    accumulate_block&lt;Iterator,T&gt;()(
        block_start,last,results[num_threads-1]);               <b class="calibre24"><i class="calibre6">9</i></b>
    std::for_each(threads.begin(),threads.end(),
        std::mem_fn(&amp;std::thread::join));
    return std::accumulate(results.begin(),results.end(),init); <b class="calibre24"><i class="calibre6">10</i></b>
}</pre>
      
      <p class="noind"><a id="iddle1336" class="calibre4"></a><a id="iddle1741" class="calibre4"></a><a id="iddle1866" class="calibre4"></a><a id="iddle2340" class="calibre4"></a><a id="iddle2537" class="calibre4"></a>Now let’s go through and identify the places where an exception can be thrown: anywhere where you call a function you know
         can throw or you perform an operation on a user-defined type that may throw.
      </p>
      
      <p class="noind">First up, you have the call to <kbd class="calibre17">distance</kbd> <b class="calibre24"><i class="calibre6">2</i></b>, which performs operations on the user-supplied iterator type. Because you haven’t yet done any work, and this is on the
         calling thread, it’s fine. Next up, you have the allocation of the <kbd class="calibre17">results</kbd> vector <b class="calibre24"><i class="calibre6">3</i></b> and the <kbd class="calibre17">threads</kbd> vector <b class="calibre24"><i class="calibre6">4</i></b>. Again, these are on the calling thread, and you haven’t done any work or spawned any threads, so this is fine. If the construction
         of <kbd class="calibre17">threads</kbd> throws, the memory allocated for <kbd class="calibre17">results</kbd> will have to be cleaned up, but the destructor will take care of that for you.
      </p>
      
      <p class="noind">Skipping over the initialization of <kbd class="calibre17">block_start</kbd> <b class="calibre24"><i class="calibre6">5</i></b>, because that’s similarly safe, you come to the operations in the thread-spawning loop, <b class="calibre24"><i class="calibre6">6</i></b>, <b class="calibre24"><i class="calibre6">7</i></b>, and <b class="calibre24"><i class="calibre6">8</i></b>. Once you’ve been through the creation of the first thread at <b class="calibre24"><i class="calibre6">7</i></b>, you’re in trouble if you throw any exceptions; the destructors of your new <kbd class="calibre17">std::thread</kbd> objects will call <kbd class="calibre17">std::terminate</kbd> and abort your program. This isn’t a good place to be.
      </p>
      
      <p class="noind">The call to <kbd class="calibre17">accumulate_block</kbd> <b class="calibre24"><i class="calibre6">9</i></b>, can potentially throw, with similar consequences; your thread objects will be destroyed and call <kbd class="calibre17">std::terminate</kbd>. On the other hand, the final call to <kbd class="calibre17">std::accumulate</kbd> <b class="calibre24"><i class="calibre6">10</i></b> can throw without causing any hardship, because all the threads have been joined by this point.
      </p>
      
      <p class="noind">That’s it for the main thread, but there’s more: the calls to <kbd class="calibre17">accumulate_block</kbd> on the new threads might throw at <b class="calibre24"><i class="calibre6">1</i></b>. There aren’t any <kbd class="calibre17">catch</kbd> blocks, so this exception will be left unhandled and cause the library to call <kbd class="calibre17">std::terminate()</kbd> to abort the application.
      </p>
      
      <p class="noind">In case it’s not glaringly obvious, this code isn’t exception-safe.</p>
      
      
      <h5 class="notetitle" id="ch08lev3sec3"><a id="ch08lev3sec3__title" class="calibre4"></a>Adding exception safety
      </h5>
      
      <p class="noind">OK, so we’ve identified all the possible <kbd class="calibre17">throw</kbd> points and the nasty consequences of exceptions. What can you do about it? Let’s start by addressing the issue of the exceptions
         thrown on your new threads.
      </p>
      
      <p class="noind"><a id="iddle2212" class="calibre4"></a>You encountered the tool for this job in <a href="kindle_split_014.html#ch04" class="calibre4">chapter 4</a>. If you look carefully at what you’re trying to achieve with new threads, it’s apparent that you’re trying to calculate a
         result to return while allowing for the possibility that the code might throw an exception. This is <i class="calibre6">precisely</i> what the combination of <kbd class="calibre17">std::packaged_task</kbd> and <kbd class="calibre17">std::future</kbd> is designed for. If you rearrange your code to use <kbd class="calibre17">std::packaged_task</kbd>, you end up with the following code.
      </p>
      
      
      
      <h5 class="notetitle" id="ch08ex03">Listing 8.3. <a id="ch08ex03__title" class="calibre4"></a>A parallel version of <kbd class="calibre17">std::accumulate</kbd> using <kbd class="calibre17">std::packaged_task</kbd></h5>
      <pre id="PLd0e31253" class="calibre5">template&lt;typename Iterator,typename T&gt;
struct accumulate_block
{
    T operator()(Iterator first,Iterator last)                          <b class="calibre24"><i class="calibre6">1</i></b>
    {
        return std::accumulate(first,last,T());                         <b class="calibre24"><i class="calibre6">2</i></b>
    }
};
template&lt;typename Iterator,typename T&gt;
T parallel_accumulate(Iterator first,Iterator last,T init)
{
    unsigned long const length=std::distance(first,last);
    if(!length)
        return init;
    unsigned long const min_per_thread=25;
    unsigned long const max_threads=
        (length+min_per_thread-1)/min_per_thread;
    unsigned long const hardware_threads=
        std::thread::hardware_concurrency();
    unsigned long const num_threads=
        std::min(hardware_threads!=0?hardware_threads:2,max_threads);
    unsigned long const block_size=length/num_threads;
    std::vector&lt;std::future&lt;T&gt; &gt; futures(num_threads-1);               <b class="calibre24"><i class="calibre6">3</i></b>
    std::vector&lt;std::thread&gt; threads(num_threads-1);
    Iterator block_start=first;
    for(unsigned long i=0;i&lt;(num_threads-1);++i)
    {
        Iterator block_end=block_start;
        std::advance(block_end,block_size);
        std::packaged_task&lt;T(Iterator,Iterator)&gt; task(                 <b class="calibre24"><i class="calibre6">4</i></b>
            accumulate_block&lt;Iterator,T&gt;());
        futures[i]=task.get_future();                                  <b class="calibre24"><i class="calibre6">5</i></b>
        threads[i]=std::thread(std::move(task),block_start,block_end); <b class="calibre24"><i class="calibre6">6</i></b>
        block_start=block_end;
    }
    T last_result=accumulate_block&lt;Iterator,T&gt;()(block_start,last);    <b class="calibre24"><i class="calibre6">7</i></b>
    std::for_each(threads.begin(),threads.end(),
        std::mem_fn(&amp;std::thread::join));
    T result=init;                                                     <b class="calibre24"><i class="calibre6">8</i></b>
    for(unsigned long i=0;i&lt;(num_threads-1);++i)
    {
        result+=futures[i].get();                                      <b class="calibre24"><i class="calibre6">9</i></b>
    }
    result += last_result;                                            <b class="calibre24"><i class="calibre6">10</i></b>
    return result;
}</pre>
      
      <p class="noind"><a id="iddle1423" class="calibre4"></a><a id="iddle1500" class="calibre4"></a>The first change is that the function call operator of <kbd class="calibre17">accumulate_block</kbd> now returns the result directly, rather than taking a reference to somewhere to store it <b class="calibre24"><i class="calibre6">1</i></b>. You’re using <kbd class="calibre17">std::packaged_task</kbd> and <kbd class="calibre17">std::future</kbd> for the exception safety, so you can use it to transfer the result too. This does require that you explicitly pass a default-constructed
         <kbd class="calibre17">T</kbd> in the call to <kbd class="calibre17">std::accumulate</kbd> <b class="calibre24"><i class="calibre6">2</i></b>, rather than reusing the supplied <kbd class="calibre17">result</kbd> value, but that’s a minor change.
      </p>
      
      <p class="noind">The next change is that rather than having a vector of results, you have a vector of <kbd class="calibre17">futures</kbd> <b class="calibre24"><i class="calibre6">3</i></b> to store an <kbd class="calibre17">std::future&lt;T&gt;</kbd> for each spawned thread. In the thread-spawning loop, you first create a task for <kbd class="calibre17">accumulate_block</kbd> <b class="calibre24"><i class="calibre6">4</i></b>. <kbd class="calibre17">std::packaged_task&lt;T(Iterator, Iterator)&gt;</kbd> declares a task that takes two <kbd class="calibre17">Iterator</kbd>s and returns a <kbd class="calibre17">T</kbd>, which is what your function does. You then get the future for that task <b class="calibre24"><i class="calibre6">5</i></b> and run that task on a new thread, passing in the start and end of the block to process <b class="calibre24"><i class="calibre6">6</i></b>. When the task runs, the result will be captured in the future, as will any exception thrown.
      </p>
      
      <p class="noind">Because you’ve been using futures, you don’t have a result array, so you must store the result from the final block in a variable
         <b class="calibre24"><i class="calibre6">7</i></b>, rather than in a slot in the array. Also, because you have to get the values out of the futures, it’s now simpler to use
         a basic <kbd class="calibre17">for</kbd> loop rather than <kbd class="calibre17">std::accumulate</kbd>, starting with the supplied initial value <b class="calibre24"><i class="calibre6">8</i></b> and adding in the result from each future <b class="calibre24"><i class="calibre6">9</i></b>. If the corresponding task threw an exception, this will have been captured in the future and will now be thrown again by
         the call to <kbd class="calibre17">get()</kbd>. Finally, you add the result from the last block <b class="calibre24"><i class="calibre6">10</i></b> before returning the overall result to the caller.
      </p>
      
      <p class="noind">So, that’s removed one of the potential problems: exceptions thrown in the worker threads are rethrown in the main thread.
         If more than one of the worker threads throws an exception, only one will be propagated, but that’s not too big a deal. If
         it matters, you can use something like <kbd class="calibre17">std::nested_exception</kbd> to capture all the exceptions and throw that instead.
      </p>
      
      <p class="noind">The remaining problem is the leaking threads if an exception is thrown between when you spawn the first thread and when you’ve
         joined with them all. The simplest solution is to catch any exceptions, join with the threads that are still <kbd class="calibre17">joinable()</kbd>, and rethrow the exception:
      </p>
      
      <pre id="PLd0e31417" class="calibre5">try
{
    for(unsigned long i=0;i&lt;(num_threads-1);++i)
    {
        // ... as before
    }
    T last_result=accumulate_block&lt;Iterator,T&gt;()(block_start,last);
    std::for_each(threads.begin(),threads.end(),
        std::mem_fn(&amp;std::thread::join));
}
catch(...)
{
    for(unsigned long i=0;i&lt;(num_thread-1);++i)
    {
        if(threads[i].joinable())
            thread[i].join();
    }
    throw;
}</pre>
      
      <p class="noind"><a id="iddle2489" class="calibre4"></a>Now this works. All the threads will be joined, no matter how the code leaves the block. But <kbd class="calibre17">try</kbd>-<kbd class="calibre17">catch</kbd> blocks are ugly, and you have duplicate code. You’re joining the threads both in the “normal” control flow <i class="calibre6">and</i> in the <kbd class="calibre17">catch</kbd> block. Duplicate code is rarely a good thing, because it means more places to change. Instead, let’s extract this out into
         the destructor of an object; it is, after all, the idiomatic way of cleaning up resources in C++. Here’s your class:
      </p>
      
      <pre id="PLd0e31447" class="calibre5">class join_threads
{
    std::vector&lt;std::thread&gt;&amp; threads;
public:
    explicit join_threads(std::vector&lt;std::thread&gt;&amp; threads_):
        threads(threads_)
    {}
    ~join_threads()
    {
        for(unsigned long i=0;i&lt;threads.size();++i)
        {
            if(threads[i].joinable())
                threads[i].join();
        }
    }
};</pre>
      
      <p class="noind">This is similar to your <kbd class="calibre17">thread_guard</kbd> class from <a href="kindle_split_012.html#ch02ex03" class="calibre4">listing 2.3</a>, except it’s extended for the whole vector of threads. You can then simplify your code as follows.
      </p>
      
      
      
      <h5 class="notetitle" id="ch08ex04">Listing 8.4. <a id="ch08ex04__title" class="calibre4"></a>An exception-safe parallel version of <kbd class="calibre17">std::accumulate</kbd></h5>
      <pre id="PLd0e31470" class="calibre5">template&lt;typename Iterator,typename T&gt;
T parallel_accumulate(Iterator first,Iterator last,T init)
{
    unsigned long const length=std::distance(first,last);
    if(!length)
        return init;
    unsigned long const min_per_thread=25;
    unsigned long const max_threads=
        (length+min_per_thread-1)/min_per_thread;
    unsigned long const hardware_threads=
        std::thread::hardware_concurrency();
    unsigned long const num_threads=
        std::min(hardware_threads!=0?hardware_threads:2,max_threads);
    unsigned long const block_size=length/num_threads;
    std::vector&lt;std::future&lt;T&gt; &gt; futures(num_threads-1);
    std::vector&lt;std::thread&gt; threads(num_threads-1);
    join_threads joiner(threads);                        <b class="calibre24"><i class="calibre6">1</i></b>
    Iterator block_start=first;
    for(unsigned long i=0;i&lt;(num_threads-1);++i)
    {
        Iterator block_end=block_start;
        std::advance(block_end,block_size);
        std::packaged_task&lt;T(Iterator,Iterator)&gt; task(
            accumulate_block&lt;Iterator,T&gt;());
        futures[i]=task.get_future();
        threads[i]=std::thread(std::move(task),block_start,block_end);
        block_start=block_end;
    }
    T last_result=accumulate_block&lt;Iterator,T&gt;()(block_start,last);
    T result=init;
    for(unsigned long i=0;i&lt;(num_threads-1);++i)
    {
        result+=futures[i].get();                        <b class="calibre24"><i class="calibre6">2</i></b>
    }
    result += last_result;
    return result;
}</pre>
      
      <p class="noind"><a id="iddle1339" class="calibre4"></a><a id="iddle1742" class="calibre4"></a><a id="iddle1867" class="calibre4"></a><a id="iddle1947" class="calibre4"></a>Once you’ve created your container of threads, you create an instance of your new class <b class="calibre24"><i class="calibre6">1</i></b> to join with all the threads on exit. You can then remove your explicit join loop, safe in the knowledge that the threads
         will be joined however the function exits. Note that the calls to <kbd class="calibre17">futures[i].get()</kbd> <b class="calibre24"><i class="calibre6">2</i></b> will block until the results are ready, so you don’t need to have explicitly joined with the threads at this point. This
         is unlike the original from <a href="#ch08ex02" class="calibre4">listing 8.2</a>, where you needed to have joined with the threads to ensure that the <kbd class="calibre17">results</kbd> vector was correctly populated. Not only do you get exception-safe code, but your function is shorter because you’ve extracted
         the join code into your new (reusable) class.
      </p>
      
      
      
      <h5 class="notetitle" id="ch08lev3sec4"><a id="ch08lev3sec4__title" class="calibre4"></a>Exception safety with std::async()
      </h5>
      
      <p class="noind">Now that you’ve seen what’s required for exception safety when explicitly managing the threads, let’s take a look at the same
         thing done with <kbd class="calibre17">std::async()</kbd>. As you’ve already seen, in this case the library takes care of managing the threads for you, and any threads spawned are
         completed when the future is ready. The key thing to note for exception safety is that if you destroy the future without waiting
         for it, the destructor will wait for the thread to complete. This neatly avoids the problem of leaked threads that are still
         executing and holding references to the data. The next listing shows an exception-safe implementation using <kbd class="calibre17">std::async()</kbd>.
      </p>
      
      
      
      <h5 class="notetitle" id="ch08ex05">Listing 8.5. <a id="ch08ex05__title" class="calibre4"></a>An exception-safe parallel version of <kbd class="calibre17">std::accumulate</kbd> using <kbd class="calibre17">std::async</kbd></h5>
      <pre id="PLd0e31561" class="calibre5">template&lt;typename Iterator,typename T&gt;
T parallel_accumulate(Iterator first,Iterator last,T init)
{
    unsigned long const length=std::distance(first,last);              <b class="calibre24"><i class="calibre6">1</i></b>
    unsigned long const max_chunk_size=25;
    if(length&lt;=max_chunk_size)
    {
        return std::accumulate(first,last,init);                      <b class="calibre24"><i class="calibre6">2</i></b>
    }
    else
    {
    Iterator mid_point=first;
        std::advance(mid_point,length/2);                             <b class="calibre24"><i class="calibre6">3</i></b>
        std::future&lt;T&gt; first_half_result=
            std::async(parallel_accumulate&lt;Iterator,T&gt;,               <b class="calibre24"><i class="calibre6">4</i></b>
                       first,mid_point,init);
        T second_half_result=parallel_accumulate(mid_point,last,T()); <b class="calibre24"><i class="calibre6">5</i></b>
        return first_half_result.get()+second_half_result;            <b class="calibre24"><i class="calibre6">6</i></b>
    }
}</pre>
      
      <p class="noind"><a id="iddle1018" class="calibre4"></a><a id="iddle1157" class="calibre4"></a><a id="iddle1338" class="calibre4"></a><a id="iddle1882" class="calibre4"></a>This version uses a recursive division of the data rather than pre-calculating the division of the data into chunks, but it’s
         a whole lot simpler than the previous version, and it’s still exception-safe. As before, you start by finding the length of
         the sequence <b class="calibre24"><i class="calibre6">1</i></b>, and if it’s smaller than the maximum chunk size, you resort to calling <kbd class="calibre17">std::accumulate</kbd> directly <b class="calibre24"><i class="calibre6">2</i></b>. If there are more elements than your chunk size, you find the midpoint <b class="calibre24"><i class="calibre6">3</i></b> and then spawn an asynchronous task to handle that half <b class="calibre24"><i class="calibre6">4</i></b>. The second half of the range is handled with a direct recursive call <b class="calibre24"><i class="calibre6">5</i></b>, and then the results from the two chunks are added together <b class="calibre24"><i class="calibre6">6</i></b>. The library ensures that the <kbd class="calibre17">std::async</kbd> calls make use of the hardware threads that are available without creating an overwhelming number of threads. Some of the
         “asynchronous” calls will be executed synchronously in the call to <kbd class="calibre17">get()</kbd> <b class="calibre24"><i class="calibre6">6</i></b>.
      </p>
      
      <p class="noind">The beauty of this is that not only can it take advantage of the hardware concurrency, but it’s also trivially exception-safe.
         If an exception is thrown by the recursive call <b class="calibre24"><i class="calibre6">5</i></b>, the future created from the call to <kbd class="calibre17">std::async</kbd> <b class="calibre24"><i class="calibre6">4</i></b> will be destroyed as the exception propagates. This will in turn wait for the asynchronous task to finish, avoiding a dangling
         thread. On the other hand, if the asynchronous call throws, this is captured by the future, and the call to <kbd class="calibre17">get()</kbd> <b class="calibre24"><i class="calibre6">6</i></b> will rethrow the exception.
      </p>
      
      <p class="noind">What other considerations do you need to take into account when designing concurrent code? Let’s look at <i class="calibre6">scalability</i>. How much does the performance improve if you move your code to a system with more processors?
      </p>
      
      
      
      
      <h4 id="ch08lev2sec12" class="calibre23">8.4.2. <a id="ch08lev2sec12__title" class="calibre4"></a>Scalability and Amdahl’s law
      </h4>
      
      <p class="noind"><i class="calibre6">Scalability</i> is all about ensuring that your application can take advantage of additional processors in the system it’s running on. At
         one extreme you have a single-threaded application that’s completely unscalable; even if you add 100 processors to your system,
         the performance will remain unchanged. At the other extreme you have something like the SETI@Home (<a href="http://setiathome.ssl.berkeley.edu/" class="calibre4">http://setiathome.ssl.berkeley.edu/</a>) project, which is designed to take advantage of thousands of additional processors (in the form of individual computers
         added to the network by users) as they become available.
      </p>
      
      <p class="noind"><a id="iddle1456" class="calibre4"></a>For any given multithreaded program, the number of threads that are performing useful work will vary as the program runs.
         Even if every thread is doing useful work for the entirety of its existence, the application may initially have only one thread,
         which will then have the task of spawning all the others. But even that’s a highly unlikely scenario. Threads often spend
         time waiting for each other or waiting for I/O operations to complete.
      </p>
      
      <p class="noind">Every time one thread has to wait for something (whatever that something is), unless there’s another thread ready to take
         its place on the processor, you have a processor sitting idle that could be doing useful work.
      </p>
      
      <p class="noind">A simplified way of looking at this is to divide the program into “serial” sections where only one thread is doing any useful
         work and “parallel” sections where all the available processors are doing useful work. If you run your application on a system
         with more processors, the “parallel” sections will theoretically be able to complete more quickly, because the work can be
         divided between more processors, whereas the “serial” sections will remain serial. Under such a simplified set of assumptions,
         you can therefore estimate the potential performance gain to be achieved by increasing the number of processors: if the “serial”
         sections constitute a fraction, <i class="calibre6">fs</i>, of the program, then the performance gain, <i class="calibre6">P</i>, from using <i class="calibre6">N</i> processors can be estimated as
      </p>
      
      
      
      <p class="center1"><img alt="" src="f0278-01.jpg" class="calibre2"/></p>
      
      
      <p class="noind">This is <i class="calibre6">Amdahl’s law</i>, which is often cited when talking about the performance of concurrent code. If everything can be parallelized, so the serial
         fraction is 0, the speedup is <i class="calibre6">N</i>. Alternatively, if the serial fraction is one-third, even with an infinite number of processors you’re not going to get a
         speedup of more than 3.
      </p>
      
      <p class="noind">But this paints a naive picture, because tasks are rarely infinitely divisible in the way that would be required for the equation
         to hold, and it’s also rare for everything to be CPU-bound in the way that’s assumed. As you’ve seen, threads may wait for
         many things while executing.
      </p>
      
      <p class="noind">One thing that’s clear from Amdahl’s law is that when you’re using concurrency for performance, it’s worth looking at the
         overall design of the application to maximize the potential for concurrency and ensure that there’s always useful work for
         the processors to be doing. If you can reduce the size of the “serial” sections or reduce the potential for threads to wait,
         you can improve the potential for performance gains on systems with more processors. Alternatively, if you can provide more
         data for the system to process, and thus keep the parallel sections primed with work, you can reduce the serial fraction and
         increase the performance gain, <i class="calibre6">P</i>.
      </p>
      
      <p class="noind">Scalability is about <i class="calibre6">reducing the time it takes to perform an action or increasing the amount of data that can be processed in a given time</i> as more processors are added. Sometimes these are equivalent (you can process more data if each element is processed faster),
         but not always. Before choosing the techniques to use for dividing work between <a id="iddle1152" class="calibre4"></a><a id="iddle1231" class="calibre4"></a><a id="iddle1508" class="calibre4"></a><a id="iddle2522" class="calibre4"></a>threads, it’s important to identify which of these aspects of scalability are important to you.
      </p>
      
      <p class="noind">I mentioned at the beginning of this section that threads don’t always have useful work to do. Sometimes they have to wait
         for other threads, or for I/O to complete, or for something else. If you give the system something useful to do during this
         wait, you can effectively “hide” the waiting.
      </p>
      
      
      
      <h4 id="ch08lev2sec13" class="calibre23">8.4.3. <a id="ch08lev2sec13__title" class="calibre4"></a>Hiding latency with multiple threads
      </h4>
      
      <p class="noind">For most of the discussions of the performance of multithreaded code, we’ve been assuming that the threads are running “flat
         out” and always have useful work to do when they’re running on a processor. This is not true; in application code, threads
         frequently block while waiting for something. For example, they may be waiting for some I/O to complete, waiting to acquire
         a mutex, waiting for another thread to complete some operation and notify a condition variable or populate a future, or even
         sleeping for a period of time.
      </p>
      
      <p class="noind">Whatever the reason for the waits, if you have only as many threads as there are physical processing units in the system,
         having blocked threads means you’re wasting CPU time. The processor that would otherwise be running a blocked thread is instead
         doing nothing. Consequently, if you know that one of your threads is likely to spend a considerable portion of its time waiting
         around, you can make use of that spare CPU time by running one or more additional threads.
      </p>
      
      <p class="noind">Consider a virus-scanner application, which divides the work across threads using a pipeline. The first thread searches the
         filesystem for files to check and puts them in a queue. Meanwhile, another thread takes filenames from the queue, loads the
         files, and scans them for viruses. You know that the thread searching the filesystem for files to scan is definitely going
         to be I/O-bound, so you make use of the “spare” CPU time by running an additional scanning thread. You’d then have one file-searching
         thread and as many scanning threads as there are physical cores or processors in the system. Because the scanning thread may
         also have to read significant portions of the files off the disk in order to scan them, it might make sense to have even more
         scanning threads. But at some point there’ll be too many threads, and the system will slow down again as it spends more and
         more time task switching, as described in <a href="#ch08lev2sec8" class="calibre4">section 8.2.5</a>.
      </p>
      
      <p class="noind">As ever, this is an optimization, so it’s important to measure performance before and after any change in the number of threads;
         the optimal number of threads will be highly dependent on the nature of the work being done and the percentage of time the
         thread spends waiting.
      </p>
      
      <p class="noind">Depending on the application, it might be possible to use up this spare CPU time without running additional threads. For example,
         if a thread is blocked because it’s waiting for an I/O operation to complete, it might make sense to use asynchronous I/O
         if that’s available, and then the thread can perform other useful work while the I/O is performed in the background. In other
         cases, if a thread is waiting for another thread to perform an operation, then rather than blocking, the waiting thread might
         <a id="iddle1153" class="calibre4"></a><a id="iddle1163" class="calibre4"></a><a id="iddle1328" class="calibre4"></a><a id="iddle1393" class="calibre4"></a><a id="iddle1427" class="calibre4"></a><a id="iddle1803" class="calibre4"></a><a id="iddle1864" class="calibre4"></a><a id="iddle1930" class="calibre4"></a>be able to perform that operation itself, as you saw with the lock-free queue in <a href="kindle_split_017.html#ch07" class="calibre4">chapter 7</a>. In an extreme case, if a thread is waiting for a task to be completed and that task hasn’t yet been started by any thread,
         the waiting thread might perform the task in entirety itself or another task that’s incomplete. You saw an example of this
         in <a href="#ch08ex01" class="calibre4">listing 8.1</a>, where the <kbd class="calibre17">sort</kbd> function repeatedly tries to sort outstanding chunks as long as the chunks it needs are not yet sorted.
      </p>
      
      <p class="noind">Rather than adding threads to ensure that all available processors are being used, sometimes it pays to add threads to ensure
         that external events are handled in a timely manner to increase the <i class="calibre6">responsiveness</i> of the system.
      </p>
      
      
      
      <h4 id="ch08lev2sec14" class="calibre23">8.4.4. <a id="ch08lev2sec14__title" class="calibre4"></a>Improving responsiveness with concurrency
      </h4>
      
      <p class="noind">Most modern graphical user interface frameworks are <i class="calibre6">event-driven</i>; the user performs actions on the user interface by pressing keys or moving the mouse, which generate a series of events
         or messages that the application then handles. The system may also generate messages or events on its own. In order to ensure
         that all events and messages are correctly handled, the application typically has an event loop that looks like this:
      </p>
      
      <pre id="PLd0e31899" class="calibre5">while(true)
{
    event_data event=get_event();
    if(event.type==quit)
        break;
    process(event);
}</pre>
      
      <p class="noind">Obviously, the details of the API will vary, but the structure is generally the same: wait for an event, do whatever processing
         is necessary to handle it, and then wait for the next one. If you have a single-threaded application, this can make long-running
         tasks hard to write, as described in <a href="#ch08lev2sec3" class="calibre4">section 8.1.3</a>. In order to ensure that user input is handled in a timely manner, <kbd class="calibre17">get_event()</kbd> and <kbd class="calibre17">process()</kbd> must be called with reasonable frequency, whatever the application is doing. This means that either the task must periodically
         suspend itself and return control to the event loop, or the <kbd class="calibre17">get_event()/process()</kbd> code must be called from within the code at convenient points. Either option complicates the implementation of the task.
      </p>
      
      <p class="noind">By separating the concerns with concurrency, you can put the lengthy task on a whole new thread and leave a dedicated GUI
         thread to process the events. The threads can then communicate through simple mechanisms rather than having to somehow mix
         the event-handling code in with the task code. The following listing shows a simple outline for this separation.
      </p>
      
      
      
      <h5 class="notetitle" id="ch08ex06">Listing 8.6. <a id="ch08ex06__title" class="calibre4"></a>Separating GUI thread from task thread
      </h5>
      <pre id="PLd0e31928" class="calibre5">std::thread task_thread;
std::atomic&lt;bool&gt; task_cancelled(false);
void gui_thread()
{
    while(true)
    {
        event_data event=get_event();
        if(event.type==quit)
            break;
        process(event);
    }
}
void task()
{
    while(!task_complete() &amp;&amp; !task_cancelled)
    {
        do_next_operation();
    }
    if(task_cancelled)
    {
        perform_cleanup();
    }
    else
    {
        post_gui_event(task_complete);
    }
}
void process(event_data const&amp; event)
{
    switch(event.type)
    {
    case start_task:
        task_cancelled=false;
        task_thread=std::thread(task);
        break;
    case stop_task:
        task_cancelled=true;
        task_thread.join();
        break;
    case task_complete:
        task_thread.join();
        display_results();
        break;
    default:
        //...
    }
}</pre>
      
      <p class="noind">By separating the concerns in this way, the user thread is always able to respond to the events in a timely fashion, even
         if the task takes a long time. This responsiveness is often key to the user experience when using an application; applications
         that completely lock up whenever a particular operation is being performed (whatever that may be) are inconvenient to use.
         By providing a dedicated event-handling thread, the GUI can handle GUI-specific messages (such as resizing or repainting the
         window) without interrupting the execution of the time-consuming processing, while still passing on the relevant messages
         where they <i class="calibre6">do</i> affect the long-running task.
      </p>
      
      <p class="noind"><a id="iddle1149" class="calibre4"></a><a id="iddle1155" class="calibre4"></a><a id="iddle1753" class="calibre4"></a><a id="iddle2358" class="calibre4"></a><a id="iddle2490" class="calibre4"></a>So far in this chapter you’ve had a thorough look at the issues that need to be considered when designing concurrent code.
         Taken as a whole, these can be quite overwhelming, but as you get used to working with your “multithreaded programming hat”
         on, most of them will become second nature. If these considerations are new to you, hopefully they’ll become clearer as you
         look at how they impact some concrete examples of multithreaded code.
      </p>
      
      
      
      
      <h3 id="ch08lev1sec5" class="chapter"><a id="ch08lev1sec5__title" class="calibre3"></a>8.5. Designing concurrent code in practice
      </h3>
      
      <p class="noind">When designing concurrent code for a particular task, the extent to which you’ll need to consider each of the issues described
         previously will depend on the task. To demonstrate how they apply, we’ll look at the implementation of parallel versions of
         three functions from the C++ Standard Library. This will give you a familiar basis on which to build, while providing a platform
         for looking at the issues. As a bonus, we’ll also have usable implementations of the functions, which could be used to help
         with parallelizing a larger task.
      </p>
      
      <p class="noind">I’ve primarily selected these implementations to demonstrate particular techniques rather than to be state-of-the-art implementations;
         more advanced implementations that make better use of the available hardware concurrency may be found in the academic literature
         on parallel algorithms or in specialist multithreading libraries such as Intel’s Threading Building Blocks (<a href="http://threadingbuildingblocks.org/" class="calibre4">http://threadingbuildingblocks.org/</a>).
      </p>
      
      <p class="noind">Conceptually, the simplest parallel algorithm is a parallel version of <kbd class="calibre17">std::for_each</kbd>, so we’ll start with that.
      </p>
      
      
      <h4 id="ch08lev2sec15" class="calibre23">8.5.1. <a id="ch08lev2sec15__title" class="calibre4"></a>A parallel implementation of std::for_each
      </h4>
      
      <p class="noind"><kbd class="calibre17">std::for_each</kbd> is simple in concept; it calls a user-supplied function on every element in a range in turn. The big difference between a
         parallel implementation and the sequential <kbd class="calibre17">std::for_each</kbd> is the order of the function calls. <kbd class="calibre17">std::for_each</kbd> calls the function with the first element in the range, then the second, and so on, whereas with a parallel implementation
         there’s no guarantee as to the order in which the elements will be processed, and they may (indeed, we hope they <i class="calibre6">will</i>) be processed concurrently.
      </p>
      
      <p class="noind">To implement a parallel version of this, you need to divide the range into sets of elements to process on each thread. You
         know the number of elements in advance, so you can divide the data before processing begins (<a href="#ch08lev2sec1" class="calibre4">section 8.1.1</a>). We’ll assume that this is the only parallel task running, so you can use <kbd class="calibre17">std::thread::hardware_concurrency()</kbd> to determine the number of threads. You also know that the elements can be processed entirely independently, so you can use
         contiguous blocks to avoid false sharing (<a href="#ch08lev2sec6" class="calibre4">section 8.2.3</a>).
      </p>
      
      <p class="noind">This algorithm is similar in concept to the parallel version of <kbd class="calibre17">std::accumulate</kbd> described in <a href="#ch08lev2sec11" class="calibre4">section 8.4.1</a>, but rather than computing the sum of each element, you merely have to apply the specified function. Although you might imagine
         this would greatly simplify the code, because there’s no result to return, if you want to pass on exceptions to the caller,
         you still need to use the <kbd class="calibre17">std::packaged_task</kbd> and <kbd class="calibre17">std:: future</kbd> mechanisms to transfer the exception between threads. A sample implementation is shown here.
      </p>
      
      
      
      <h5 class="notetitle" id="ch08ex07">Listing 8.7. <a id="ch08ex07__title" class="calibre4"></a>A parallel version of <kbd class="calibre17">std::for_each</kbd></h5>
      <pre id="PLd0e32059" class="calibre5">template&lt;typename Iterator,typename Func&gt;
void parallel_for_each(Iterator first,Iterator last,Func f)
{
    unsigned long const length=std::distance(first,last);
    if(!length)
        return;
    unsigned long const min_per_thread=25;
    unsigned long const max_threads=
        (length+min_per_thread-1)/min_per_thread;
    unsigned long const hardware_threads=
        std::thread::hardware_concurrency();
    unsigned long const num_threads=
        std::min(hardware_threads!=0?hardware_threads:2,max_threads);
    unsigned long const block_size=length/num_threads;
    std::vector&lt;std::future&lt;void&gt; &gt; futures(num_threads-1);    <b class="calibre24"><i class="calibre6">1</i></b>
    std::vector&lt;std::thread&gt; threads(num_threads-1);
    join_threads joiner(threads);
    Iterator block_start=first;
    for(unsigned long i=0;i&lt;(num_threads-1);++i)
    {
        Iterator block_end=block_start;
        std::advance(block_end,block_size);
        std::packaged_task&lt;void(void)&gt; task(                   <b class="calibre24"><i class="calibre6">2</i></b>
            [=]()
            {
                std::for_each(block_start,block_end,f);
            });
        futures[i]=task.get_future();
        threads[i]=std::thread(std::move(task));               <b class="calibre24"><i class="calibre6">3</i></b>
        block_start=block_end;
    }
    std::for_each(block_start,last,f);
    for(unsigned long i=0;i&lt;(num_threads-1);++i)
    {
        futures[i].get();                                      <b class="calibre24"><i class="calibre6">4</i></b>
    }
}</pre>
      
      <p class="noind">The basic structure of the code is identical to that of <a href="#ch08ex04" class="calibre4">listing 8.4</a>, which is unsurprising. The key difference is that the <kbd class="calibre17">futures</kbd> vector stores <kbd class="calibre17">std::future&lt;void&gt;</kbd> <b class="calibre24"><i class="calibre6">1</i></b>, because the worker threads don’t return a value, and a simple lambda function that invokes the function <kbd class="calibre17">f</kbd> on the range from <kbd class="calibre17">block_start</kbd> to <kbd class="calibre17">block_end</kbd> is used for the task <b class="calibre24"><i class="calibre6">2</i></b>. This avoids having to pass the range into the thread constructor <b class="calibre24"><i class="calibre6">3</i></b>. Because the worker threads don’t return a value, the calls to <kbd class="calibre17">futures[i].get()</kbd> <b class="calibre24"><i class="calibre6">4</i></b> provide a means of retrieving any exceptions thrown on the worker threads; if you don’t want to pass on the exceptions, you
         could omit this.
      </p>
      
      <p class="noind"><a id="iddle1154" class="calibre4"></a><a id="iddle1752" class="calibre4"></a><a id="iddle2144" class="calibre4"></a>Just as your parallel implementation of <kbd class="calibre17">std::accumulate</kbd> could be simplified using <kbd class="calibre17">std::async</kbd>, so can your <kbd class="calibre17">parallel_for_each</kbd>. This implementation follows.
      </p>
      
      
      
      <h5 class="notetitle" id="ch08ex08">Listing 8.8. <a id="ch08ex08__title" class="calibre4"></a>A parallel version of <kbd class="calibre17">std::for_each</kbd> using <kbd class="calibre17">std::async</kbd></h5>
      <pre id="PLd0e32166" class="calibre5">template&lt;typename Iterator,typename Func&gt;
void parallel_for_each(Iterator first,Iterator last,Func f)
{
    unsigned long const length=std::distance(first,last);
    if(!length)
        return;
    unsigned long const min_per_thread=25;
    if(length&lt;(2*min_per_thread))
    {
        std::for_each(first,last,f);                       <b class="calibre24"><i class="calibre6">1</i></b>
    }
    else
    {
        Iterator const mid_point=first+length/2;
        std::future&lt;void&gt; first_half=                      <b class="calibre24"><i class="calibre6">2</i></b>
            std::async(&amp;parallel_for_each&lt;Iterator,Func&gt;,
                       first,mid_point,f);
        parallel_for_each(mid_point,last,f);               <b class="calibre24"><i class="calibre6">3</i></b>
        first_half.get();                                  <b class="calibre24"><i class="calibre6">4</i></b>
    }
}</pre>
      
      <p class="noind">As with your <kbd class="calibre17">std::async</kbd>-based <kbd class="calibre17">parallel_accumulate</kbd> from <a href="#ch08ex05" class="calibre4">listing 8.5</a>, you split the data recursively rather than before execution, because you don’t know how many threads the library will use.
         As before, you divide the data in half at each stage, running one half asynchronously <b class="calibre24"><i class="calibre6">2</i></b> and the other directly <b class="calibre24"><i class="calibre6">3</i></b>, until the remaining data is too small to be worth dividing, in which case you defer to <kbd class="calibre17">std::for_each</kbd> <b class="calibre24"><i class="calibre6">1</i></b>. Again, the use of <kbd class="calibre17">std::async</kbd> and the <kbd class="calibre17">get()</kbd> member function of <kbd class="calibre17">std::future</kbd> <b class="calibre24"><i class="calibre6">4</i></b> provides the exception propagation semantics.
      </p>
      
      <p class="noind">Let’s move on from algorithms that must perform the same operation on each element (of which there are several; <kbd class="calibre17">std::count</kbd> and <kbd class="calibre17">std::replace</kbd> spring to mind, for starters) to a slightly more complicated example in the shape of <kbd class="calibre17">std::find</kbd>.
      </p>
      
      
      
      <h4 id="ch08lev2sec16" class="calibre23">8.5.2. <a id="ch08lev2sec16__title" class="calibre4"></a>A parallel implementation of std::find
      </h4>
      
      <p class="noind"><kbd class="calibre17">std::find</kbd> is a useful algorithm to consider next because it’s one of several algorithms that can complete without every element having
         been processed. For example, if the first element in the range matches the search criterion, there’s no need to examine any
         other elements. As you’ll see shortly, this is an important property for performance, and it has direct consequences for the
         design of the parallel implementation. It’s a particular example of how data access patterns can affect the design of your
         code (<a href="#ch08lev2sec10" class="calibre4">section 8.3.2</a>). Other algorithms in this category include <kbd class="calibre17">std::equal</kbd> and <kbd class="calibre17">std::any_of</kbd>.
      </p>
      
      <p class="noind">If you and your partner were searching for an old photograph through the boxes of keepsakes in your attic, you wouldn’t let
         them continue searching if you found the <a id="iddle2213" class="calibre4"></a>photograph. Instead, you’d let them know you’d found the photograph (perhaps by shouting, “Found it!”), so that they could
         stop searching and move on to something else. The nature of many algorithms requires that they process every element, so they
         have no equivalent to shouting, “Found it!” For algorithms such as <kbd class="calibre17">std::find</kbd>, the ability to complete “early” is an important property and not something to squander. You therefore need to design your
         code to make use of it—to interrupt the other tasks in some way when the answer is known, so that the code doesn’t have to
         wait for the other worker threads to process the remaining elements.
      </p>
      
      <p class="noind">If you don’t interrupt the other threads, the serial version may outperform your parallel implementation, because the serial
         algorithm can stop searching and return once a match is found. If, for example, the system can support four concurrent threads,
         each thread will have to examine one quarter of the elements in the range, and your naive parallel implementation would take
         approximately one quarter of the time a single thread would take to check every element. If the matching element lies in the
         first quarter of the range, the sequential algorithm will return first, because it doesn’t need to check the remainder of
         the elements.
      </p>
      
      <p class="noind">One way in which you can interrupt the other threads is by making use of an atomic variable as a flag and checking the flag
         after processing every element. If the flag is set, one of the other threads has found a match, so you can cease processing
         and return. By interrupting the threads in this way, you preserve the property that you don’t have to process every value
         and improve the performance compared to the serial version in more circumstances. The downside to this is that atomic loads
         can be slow operations, so this can impede the progress of each thread.
      </p>
      
      <p class="noind">Now you have two choices as to how to return the values and how to propagate any exceptions. You can use an array of futures,
         <kbd class="calibre17">std::packaged_task</kbd>, for transferring the values and exceptions, and then process the results back in the main thread; or you can use <kbd class="calibre17">std::promise</kbd> to set the final result directly from the worker threads. It all depends on how you want to handle exceptions from the worker
         threads. If you want to stop on the first exception (even if you haven’t processed all elements), you can use <kbd class="calibre17">std::promise</kbd> to set both the value and the exception. On the other hand, if you want to allow the other workers to keep searching, you
         can use <kbd class="calibre17">std::packaged_task</kbd>, store all the exceptions, and then rethrow one of them if a match isn’t found.
      </p>
      
      <p class="noind">In this case I’ve opted to use <kbd class="calibre17">std::promise</kbd> because the behavior matches that of <kbd class="calibre17">std::find</kbd> more closely. One thing to watch out for here is the case where the element being searched for isn’t in the supplied range.
         You therefore need to wait for all the threads to finish <i class="calibre6">before</i> getting the result from the future. If you block on the future, you’ll be waiting forever if the value isn’t there. The result
         is shown here.
      </p>
      
      
      
      <h5 class="notetitle" id="ch08ex09">Listing 8.9. <a id="ch08ex09__title" class="calibre4"></a>An implementation of a parallel find algorithm
      </h5>
      <pre id="PLd0e32317" class="calibre5">template&lt;typename Iterator,typename MatchType&gt;
Iterator parallel_find(Iterator first,Iterator last,MatchType match)
{
    struct find_element                                               <b class="calibre24"><i class="calibre6">1</i></b>
    {
        void operator()(Iterator begin,Iterator end,
                        MatchType match,
                        std::promise&lt;Iterator&gt;* result,
                        std::atomic&lt;bool&gt;* done_flag)
        {
            try
            {
                for(;(begin!=end) &amp;&amp; !done_flag-&gt;load();++begin)      <b class="calibre24"><i class="calibre6">2</i></b>
                {
                    if(*begin==match)
                    {
                        result-&gt;set_value(begin);                     <b class="calibre24"><i class="calibre6">3</i></b>
                        done_flag-&gt;store(true);                       <b class="calibre24"><i class="calibre6">4</i></b>
                        return;
                    }
                }
            }
            catch(...)                                                <b class="calibre24"><i class="calibre6">5</i></b>
            {
                try
                {
                    result-&gt;set_exception(std::current_exception());  <b class="calibre24"><i class="calibre6">6</i></b>
                    done_flag-&gt;store(true);
                }
                catch(...)                                            <b class="calibre24"><i class="calibre6">7</i></b>
                {}
            }
        }
    };
    unsigned long const length=std::distance(first,last);
    if(!length)
        return last;
    unsigned long const min_per_thread=25;
    unsigned long const max_threads=
        (length+min_per_thread-1)/min_per_thread;
    unsigned long const hardware_threads=
        std::thread::hardware_concurrency();
    unsigned long const num_threads=
        std::min(hardware_threads!=0?hardware_threads:2,max_threads);
    unsigned long const block_size=length/num_threads;
    std::promise&lt;Iterator&gt; result;                                    <b class="calibre24"><i class="calibre6">8</i></b>
    std::atomic&lt;bool&gt; done_flag(false);                               <b class="calibre24"><i class="calibre6">9</i></b>
    std::vector&lt;std::thread&gt; threads(num_threads-1);
    {                                                                 <b class="calibre24"><i class="calibre6">10</i></b>
        join_threads joiner(threads);
        Iterator block_start=first;
        for(unsigned long i=0;i&lt;(num_threads-1);++i)
        {
            Iterator block_end=block_start;
            std::advance(block_end,block_size);
            threads[i]=std::thread(find_element(),                    <b class="calibre24"><i class="calibre6">11</i></b>
                                   block_start,block_end,match,
                                   &amp;result,&amp;done_flag);
            block_start=block_end;
        }
        find_element()(block_start,last,match,&amp;result,&amp;done_flag);    <b class="calibre24"><i class="calibre6">12</i></b>
    }
    if(!done_flag.load())                                             <b class="calibre24"><i class="calibre6">13</i></b>
    {
        return last;
    }
    return result.get_future().get();                                 <b class="calibre24"><i class="calibre6">14</i></b>
}</pre>
      
      <p class="noind"><a id="iddle1368" class="calibre4"></a><a id="iddle1762" class="calibre4"></a>The main body of <a href="#ch08ex09" class="calibre4">listing 8.9</a> is similar to the previous examples. This time, the work is done in the function call operator of the local <kbd class="calibre17">find_element</kbd> class <b class="calibre24"><i class="calibre6">1</i></b>. This loops through the elements in the block it’s been given, checking the flag at each step <b class="calibre24"><i class="calibre6">2</i></b>. If a match is found, it sets the final result value in the promise <b class="calibre24"><i class="calibre6">3</i></b>, and then sets the <kbd class="calibre17">done_flag</kbd> <b class="calibre24"><i class="calibre6">4</i></b> before returning.
      </p>
      
      <p class="noind">If an exception is thrown, this is caught by the catchall handler <b class="calibre24"><i class="calibre6">5</i></b>, and you try to store the exception in the promise <b class="calibre24"><i class="calibre6">6</i></b> before setting the <kbd class="calibre17">done_flag</kbd>. Setting the value on the promise might throw an exception if the promise is already set, so you catch and discard any exceptions
         that happen here <b class="calibre24"><i class="calibre6">7</i></b>.
      </p>
      
      <p class="noind">This means that if a thread calling <kbd class="calibre17">find_element</kbd> either finds a match or throws an exception, all other threads will see <kbd class="calibre17">done_flag</kbd> set and will stop. If multiple threads find a match or throw at the same time, they’ll race to set the result in the promise.
         But this is a benign race condition; whichever succeeds is nominally “first” and therefore an acceptable result.
      </p>
      
      <p class="noind">Back in the main <kbd class="calibre17">parallel_find</kbd> function itself, you have the promise <b class="calibre24"><i class="calibre6">8</i></b> and flag <b class="calibre24"><i class="calibre6">9</i></b> used to stop the search, both of which are passed in to the new threads along with the range to search <b class="calibre24"><i class="calibre6">11</i></b>. The main thread also uses <kbd class="calibre17">find_element</kbd> to search the remaining elements <b class="calibre24"><i class="calibre6">12</i></b>. As already mentioned, you need to wait for all threads to finish before you check the result, because there might not be
         any matching elements. You do this by enclosing the thread launching-and-joining code in a block <b class="calibre24"><i class="calibre6">10</i></b> so all threads are joined when you check the flag to see whether a match was found <b class="calibre24"><i class="calibre6">13</i></b>. If a match was found, you can get the result or throw the stored exception by calling <kbd class="calibre17">get()</kbd> on the <kbd class="calibre17">std::future&lt;Iterator&gt;</kbd> you can get from the promise <b class="calibre24"><i class="calibre6">14</i></b>.
      </p>
      
      <p class="noind">Again, this implementation assumes that you’re going to be using all available hardware threads or that you have some other
         mechanism to determine the number of threads to use for the upfront division of work between threads. As before, you can use
         <kbd class="calibre17">std::async</kbd> and recursive data division to simplify your implementation, while using the automatic scaling facility of the C++ Standard
         Library. An implementation of <kbd class="calibre17">parallel_find</kbd> using <kbd class="calibre17">std::async</kbd> is shown in the following listing.
      </p>
      
      
      
      <h5 class="notetitle" id="ch08ex10">Listing 8.10. <a id="ch08ex10__title" class="calibre4"></a>An implementation of a parallel find algorithm using <kbd class="calibre17">std::async</kbd></h5>
      <pre id="PLd0e32510" class="calibre5">template&lt;typename Iterator,typename MatchType&gt;                            <b class="calibre24"><i class="calibre6">1</i></b>
Iterator parallel_find_impl(Iterator first,Iterator last,MatchType match,
                            std::atomic&lt;bool&gt;&amp; done)
{
    try
    {
        unsigned long const length=std::distance(first,last);
        unsigned long const min_per_thread=25;                           <b class="calibre24"><i class="calibre6">2</i></b>
        if(length&lt;(2*min_per_thread))                                    <b class="calibre24"><i class="calibre6">3</i></b>
        {
            for(;(first!=last) &amp;&amp; !done.load();++first)                  <b class="calibre24"><i class="calibre6">4</i></b>
            {
                if(*first==match)
                {
                    done=true;                                           <b class="calibre24"><i class="calibre6">5</i></b>
                    return first;
                }
            }
            return last;                                                 <b class="calibre24"><i class="calibre6">6</i></b>
        }
        else
        {
            Iterator const mid_point=first+(length/2);                   <b class="calibre24"><i class="calibre6">7</i></b>
            std::future&lt;Iterator&gt; async_result=
                std::async(&amp;parallel_find_impl&lt;Iterator,MatchType&gt;,      <b class="calibre24"><i class="calibre6">8</i></b>
                           mid_point,last,match,std::ref(done));
            Iterator const direct_result=
                    parallel_find_impl(first,mid_point,match,done);      <b class="calibre24"><i class="calibre6">9</i></b>
            return (direct_result==mid_point)?
                async_result.get():direct_result;                        <b class="calibre24"><i class="calibre6">10</i></b>
        }
    }
    catch(...)
    {
        done=true;                                                       <b class="calibre24"><i class="calibre6">11</i></b>
        throw;
    }
}
template&lt;typename Iterator,typename MatchType&gt;
Iterator parallel_find(Iterator first,Iterator last,MatchType match)
{
    std::atomic&lt;bool&gt; done(false);
    return parallel_find_impl(first,last,match,done);                    <b class="calibre24"><i class="calibre6">12</i></b>
}</pre>
      
      <p class="noind">The desire to finish early if you find a match means that you need to introduce a flag that is shared between all threads
         to indicate that a match has been found. This therefore needs to be passed in to all recursive calls. The simplest way to
         achieve this is by delegating to an implementation function <b class="calibre24"><i class="calibre6">1</i></b>, which takes an additional parameter—a reference to the <kbd class="calibre17">done</kbd> flag, which is passed in from the main entry point <b class="calibre24"><i class="calibre6">12</i></b>.
      </p>
      
      <p class="noind">The core implementation then proceeds along familiar lines. In common with many of the implementations here, you set a minimum
         number of items to process on a single thread <b class="calibre24"><i class="calibre6">2</i></b>; if you can’t cleanly divide into two halves of at least that size, you run everything on the current thread <b class="calibre24"><i class="calibre6">3</i></b>. The algorithm is a simple loop through the specified range, looping until you reach the end of the range or the <kbd class="calibre17">done</kbd> flag is set <b class="calibre24"><i class="calibre6">4</i></b>. If you do find a match, the <kbd class="calibre17">done</kbd> flag is set before returning <b class="calibre24"><i class="calibre6">5</i></b>. If you stop <a id="iddle1763" class="calibre4"></a>searching either because you got to the end of the list, or because another thread set the <kbd class="calibre17">done</kbd> flag, you return <kbd class="calibre17">last</kbd> to indicate that no match was found here <b class="calibre24"><i class="calibre6">6</i></b>.
      </p>
      
      <p class="noind">If the range can be divided, you first find the midpoint <b class="calibre24"><i class="calibre6">7</i></b> before using <kbd class="calibre17">std::async</kbd> to run the search in the second half of the range <b class="calibre24"><i class="calibre6">8</i></b>, being careful to use <kbd class="calibre17">std::ref</kbd> to pass a reference to the <kbd class="calibre17">done</kbd> flag. In the meantime, you can search in the first half of the range by doing a direct recursive call <b class="calibre24"><i class="calibre6">9</i></b>. Both the asynchronous call and the direct recursion may result in further subdivisions if the original range is big enough.
      </p>
      
      <p class="noind">If the direct search returned <kbd class="calibre17">mid_point</kbd>, then it failed to find a match, so you need to get the result of the asynchronous search. If no result was found in that
         half, the result will be <kbd class="calibre17">last</kbd>, which is the correct return value to indicate that the value was not found <b class="calibre24"><i class="calibre6">10</i></b>. If the “asynchronous” call was deferred rather than truly asynchronous, it will run here in the call to <kbd class="calibre17">get()</kbd>; in these circumstances, the search of the top half of the range is skipped if the search in the bottom half was successful.
         If the asynchronous search is running on another thread, the destructor of the <kbd class="calibre17">async_result</kbd> variable will wait for the thread to complete, so you don’t have any leaking threads.
      </p>
      
      <p class="noind">As before, the use of <kbd class="calibre17">std::async</kbd> provides you with exception safety and exception-propagation features. If the direct recursion throws an exception, the future’s
         destructor will ensure that the thread running the asynchronous call has terminated before the function returns, and if the
         asynchronous call throws, the exception is propagated through the <kbd class="calibre17">get()</kbd> call <b class="calibre24"><i class="calibre6">10</i></b>. The use of a <kbd class="calibre17">try</kbd>/<kbd class="calibre17">catch</kbd> block around the whole thing is only there to set the <kbd class="calibre17">done</kbd> flag on an exception and ensure that all threads terminate quickly if an exception is thrown <b class="calibre24"><i class="calibre6">11</i></b>. The implementation would still be correct without it but would keep checking elements until every thread was finished.
      </p>
      
      <p class="noind">A key feature that both implementations of this algorithm share with the other parallel algorithms you’ve seen is that there’s
         no longer the guarantee that items are processed in the sequence that you get from <kbd class="calibre17">std::find</kbd>. This is essential if you’re going to parallelize the algorithm. You can’t process elements concurrently if the order matters.
         If the elements are independent, it doesn’t matter for things like <kbd class="calibre17">parallel_for_each</kbd>, but it means that your <kbd class="calibre17">parallel_find</kbd> might return an element toward the end of the range even when there’s a match toward the beginning, which might be surprising
         if you’re not expecting it.
      </p>
      
      <p class="noind">OK, so you’ve managed to parallelize <kbd class="calibre17">std::find</kbd>. As I stated at the beginning of this section, there are other similar algorithms that can complete without processing every
         data element, and the same techniques can be used for those. We’ll also look further at the issue of interrupting threads
         in <a href="kindle_split_019.html#ch09" class="calibre4">chapter 9</a>.
      </p>
      
      <p class="noind">To complete our trio of examples, we’ll go in a different direction and look at <kbd class="calibre17">std::partial_sum</kbd>. This algorithm doesn’t get a lot of press, but it’s an interesting algorithm to parallelize and highlights some additional
         design choices.
      </p>
      
      
      
      
      <h4 id="ch08lev2sec17" class="calibre23">8.5.3. <a id="ch08lev2sec17__title" class="calibre4"></a>A parallel implementation of std::partial_sum
      </h4>
      
      <p class="noind"><a id="iddle1156" class="calibre4"></a><a id="iddle1675" class="calibre4"></a><a id="iddle1754" class="calibre4"></a><a id="iddle2220" class="calibre4"></a><kbd class="calibre17">std::partial_sum</kbd> calculates the running totals in a range, so each element is replaced by the sum of that element and all the elements prior
         to it in the original sequence. Thus the sequence 1, 2, 3, 4, 5 becomes 1, (1+2)=3, (1+2+3)=6, (1+2+3+4)=10, (1+2+3+4+5)=15.
         This is interesting to parallelize because you can’t just divide the range into chunks and calculate each chunk independently.
         For example, the initial value of the first element needs to be added to every other element.
      </p>
      
      <p class="noind">One approach to determining the partial sum of a range is to calculate the partial sum of individual chunks and then add the
         resulting value of the last element in the first chunk onto the elements in the next chunk, and so forth. If you have the
         elements 1, 2, 3, 4, 5, 6, 7, 8, 9 and you’re splitting into three chunks, you get {1, 3, 6}, {4, 9, 15}, {7, 15, 24} in the
         first instance. If you then add 6 (the sum for the last element in the first chunk) onto the elements in the second chunk,
         you get {1, 3, 6}, {10, 15, 21}, {7, 15, 24}. Then you add the last element of the second chunk (21) onto the elements in
         the third and final chunk to get the final result: {1, 3, 6}, {10, 15, 21}, {28, 36, 55}.
      </p>
      
      <p class="noind">As well as the original division into chunks, the addition of the partial sum from the previous block can also be parallelized.
         If the last element of each block is updated first, the remaining elements in a block can be updated by one thread while a
         second thread updates the next block, and so forth. This works well when there are many more elements in the list than processing
         cores, because each core has a reasonable number of elements to process at each stage.
      </p>
      
      <p class="noind">If you have a lot of processing cores (as many or more than the number of elements), this doesn’t work so well. If you divide
         the work among the processors, you end up working in pairs of elements at the first step. Under these conditions, this forward
         propagation of results means that many processors are left waiting, so you need to find some work for them to do. You can
         then take a different approach to the problem. Rather than doing the full forward propagation of the sums from one chunk to
         the next, you do a partial propagation: first sum adjacent elements as before, but then add those sums to those two elements
         away, then add the next set of results to the results from four elements away, and so forth. If you start with the same initial
         nine elements, you get 1, 3, 5, 7, 9, 11, 13, 15, 17 after the first round, which gives you the final results for the first
         two elements. After the second you then have 1, 3, 6, 10, 14, 18, 22, 26, 30, which is correct for the first four elements.
         After round three you have 1, 3, 6, 10, 15, 21, 28, 36, 44, which is correct for the first eight elements, and finally after
         round four you have 1, 3, 6, 10, 15, 21, 28, 36, 45, which is the final answer. Although there are more total steps than in
         the first approach, there’s greater scope for parallelism if you have many processors; each processor can update one entry
         with each step.
      </p>
      
      <p class="noind">Overall, the second approach takes log2(<i class="calibre6">N</i>) steps of approximately <i class="calibre6">N</i> operations (one per processor), where <i class="calibre6">N</i> is the number of elements in the list. This compares to the first algorithm where each thread has to perform <i class="calibre6">N</i>/<i class="calibre6">k</i> operations for the initial partial sum of the chunk allocated to it and then further <i class="calibre6">N</i>/<i class="calibre6">k</i> operations to do the <a id="iddle1570" class="calibre4"></a>forward propagation, where <i class="calibre6">k</i> is the number of threads. Thus the first approach is <i class="calibre6">O</i>(<i class="calibre6">N</i>), whereas the second is <i class="calibre6">O</i>(<i class="calibre6">N</i> log(<i class="calibre6">N</i>)) in terms of the total number of operations. But if you have as many processors as list elements, the second approach requires
         only log(<i class="calibre6">N</i>) operations per processor, whereas the first serializes the operations when <i class="calibre6">k</i> gets large, because of the forward propagation. For small numbers of processing units, the first approach will therefore
         finish faster, whereas for massively parallel systems, the second will finish faster. This is an extreme example of the issues
         discussed in <a href="#ch08lev2sec4" class="calibre4">section 8.2.1</a>.
      </p>
      
      <p class="noind">Anyway, efficiency issues aside, let’s look at some code. The following listing shows the first approach.</p>
      
      
      
      <h5 class="notetitle" id="ch08ex11">Listing 8.11. <a id="ch08ex11__title" class="calibre4"></a>Calculating partial sums in parallel by dividing the problem
      </h5>
      <pre id="PLd0e32836" class="calibre5">template&lt;typename Iterator&gt;
void parallel_partial_sum(Iterator first,Iterator last)
{
    typedef typename Iterator::value_type value_type;

    struct process_chunk                                                 <b class="calibre24"><i class="calibre6">1</i></b>
    {
        void operator()(Iterator begin,Iterator last,
                        std::future&lt;value_type&gt;* previous_end_value,
                        std::promise&lt;value_type&gt;* end_value)
        {
            try
            {
                Iterator end=last;
                ++end;
                std::partial_sum(begin,end,begin);                       <b class="calibre24"><i class="calibre6">2</i></b>
                if(previous_end_value)                                   <b class="calibre24"><i class="calibre6">3</i></b>
                {
                    value_type&amp; addend=previous_end_value-&gt;get();        <b class="calibre24"><i class="calibre6">4</i></b>
                    *last+=addend;                                       <b class="calibre24"><i class="calibre6">5</i></b>
                    if(end_value)
                    {
                        end_value-&gt;set_value(*last);                     <b class="calibre24"><i class="calibre6">6</i></b>
                    }
                    std::for_each(begin,last,[addend](value_type&amp; item)  <b class="calibre24"><i class="calibre6">7</i></b>
                                  {
                                      item+=addend;
                                  });
                }
                else if(end_value)
                {
                    end_value-&gt;set_value(*last);                         <b class="calibre24"><i class="calibre6">8</i></b>
                }
            }
            catch(...)                                                   <b class="calibre24"><i class="calibre6">9</i></b>
            {
                if(end_value)
                {
                    end_value-&gt;set_exception(std::current_exception());  <b class="calibre24"><i class="calibre6">10</i></b>
                }
                else
                {
                    throw;                                              <b class="calibre24"><i class="calibre6">11</i></b>
                }
            }
        }
    };
    unsigned long const length=std::distance(first,last);
    if(!length)
        return;
    unsigned long const min_per_thread=25;                              <b class="calibre24"><i class="calibre6">12</i></b>
    unsigned long const max_threads=
        (length+min_per_thread-1)/min_per_thread;
    unsigned long const hardware_threads=
        std::thread::hardware_concurrency();
    unsigned long const num_threads=
        std::min(hardware_threads!=0?hardware_threads:2,max_threads);
    unsigned long const block_size=length/num_threads;
    typedef typename Iterator::value_type value_type;
    std::vector&lt;std::thread&gt; threads(num_threads-1);                    <b class="calibre24"><i class="calibre6">13</i></b>
    std::vector&lt;std::promise&lt;value_type&gt; &gt;
         end_values(num_threads-1);                                     <b class="calibre24"><i class="calibre6">14</i></b>
    std::vector&lt;std::future&lt;value_type&gt; &gt;
         previous_end_values;                                           <b class="calibre24"><i class="calibre6">15</i></b>
    previous_end_values.reserve(num_threads-1);                         <b class="calibre24"><i class="calibre6">16</i></b>
    join_threads joiner(threads);
    Iterator block_start=first;
    for(unsigned long i=0;i&lt;(num_threads-1);++i)
    {
        Iterator block_last=block_start;
        std::advance(block_last,block_size-1);                          <b class="calibre24"><i class="calibre6">17</i></b>
        threads[i]=std::thread(process_chunk(),                         <b class="calibre24"><i class="calibre6">18</i></b>
                               block_start,block_last,
                               (i!=0)?&amp;previous_end_values[i-1]:0,
                               &amp;end_values[i]);
        block_start=block_last;
        ++block_start;                                                  <b class="calibre24"><i class="calibre6">19</i></b>
        previous_end_values.push_back(end_values[i].get_future());      <b class="calibre24"><i class="calibre6">20</i></b>
    }
    Iterator final_element=block_start;
    std::advance(final_element,std::distance(block_start,last)-1);      <b class="calibre24"><i class="calibre6">21</i></b>
    process_chunk()(block_start,final_element,                          <b class="calibre24"><i class="calibre6">22</i></b>
                    (num_threads&gt;1)?&amp;previous_end_values.back():0,
                    0);
}</pre>
      
      <p class="noind">In this instance, the general structure is the same as with the previous algorithms, dividing the problem into chunks, with
         a minimum chunk size per thread <b class="calibre24"><i class="calibre6">12</i></b>. In this case, as well as the vector of threads <b class="calibre24"><i class="calibre6">13</i></b>, you have a vector of promises <b class="calibre24"><i class="calibre6">14</i></b>, which is used to store the value of the last element in the chunk, and a vector of futures <b class="calibre24"><i class="calibre6">15</i></b>, which is used to retrieve the last value from the previous chunk. You can reserve the space for the futures <b class="calibre24"><i class="calibre6">16</i></b> to avoid a reallocation while spawning threads, because you know how many you’re going to have.
      </p>
      
      <p class="noind"><a id="iddle1013" class="calibre4"></a><a id="iddle1461" class="calibre4"></a><a id="iddle1755" class="calibre4"></a><a id="iddle1797" class="calibre4"></a><a id="iddle1804" class="calibre4"></a><a id="iddle2418" class="calibre4"></a>The main loop is the same as before, except this time you want the iterator that <i class="calibre6">points to</i> the last element in each block, rather than being the usual one past the end <b class="calibre24"><i class="calibre6">17</i></b>, so that you can do the forward propagation of the last element in each range. The processing is done in the <kbd class="calibre17">process_chunk</kbd> function object, which we’ll look at shortly; the start and end iterators for this chunk are passed in as arguments alongside
         the future for the end value of the previous range (if any) and the promise to hold the end value of this range <b class="calibre24"><i class="calibre6">18</i></b>.
      </p>
      
      <p class="noind">After you’ve spawned the thread, you can update the block start, remembering to advance it past that last element <b class="calibre24"><i class="calibre6">19</i></b>, and store the future for the last value in the current chunk into the vector of futures so it will be picked up next time
         around the loop <b class="calibre24"><i class="calibre6">20</i></b>.
      </p>
      
      <p class="noind">Before you process the final chunk, you need to get an iterator for the last element <b class="calibre24"><i class="calibre6">21</i></b>, which you can pass in to <kbd class="calibre17">process_chunk</kbd> <b class="calibre24"><i class="calibre6">22</i></b>. <kbd class="calibre17">std::partial_sum</kbd> doesn’t return a value, so you don’t need to do anything once the final chunk has been processed. The operation is complete
         once all the threads have finished.
      </p>
      
      <p class="noind">OK, now it’s time to look at the <kbd class="calibre17">process_chunk</kbd> function object that does all the work <b class="calibre24"><i class="calibre6">1</i></b>. You start by calling <kbd class="calibre17">std::partial_sum</kbd> for the entire chunk, including the final element <b class="calibre24"><i class="calibre6">2</i></b>, but then you need to know if you’re the first chunk or not <b class="calibre24"><i class="calibre6">3</i></b>. If you are <i class="calibre6">not</i> the first chunk, then there was a <kbd class="calibre17">previous_end_value</kbd> from the previous chunk, so you need to wait for that <b class="calibre24"><i class="calibre6">4</i></b>. In order to maximize the parallelism of the algorithm, you then update the last element first <b class="calibre24"><i class="calibre6">5</i></b>, so you can pass the value on to the next chunk (if there is one) <b class="calibre24"><i class="calibre6">6</i></b>. Once you’ve done that, you can use <kbd class="calibre17">std::for_each</kbd> and a simple lambda function <b class="calibre24"><i class="calibre6">7</i></b>, to update all the remaining elements in the range.
      </p>
      
      <p class="noind">If there was <i class="calibre6">not</i> a <kbd class="calibre17">previous_end_value</kbd>, you’re the first chunk, so you can update the <kbd class="calibre17">end_value</kbd> for the next chunk (again, if there is one—you might be the only chunk) <b class="calibre24"><i class="calibre6">8</i></b>.
      </p>
      
      <p class="noind">Finally, if any of the operations threw an exception, you catch it <b class="calibre24"><i class="calibre6">9</i></b> and store it in the promise <b class="calibre24"><i class="calibre6">10</i></b> so it will propagate to the next chunk when it tries to get the previous end value <b class="calibre24"><i class="calibre6">4</i></b>. This will propagate all exceptions into the final chunk, which then rethrows <b class="calibre24"><i class="calibre6">11</i></b>, because you know you’re running on the main thread.
      </p>
      
      <p class="noind">Because of the synchronization between the threads, this code isn’t readily amenable to rewriting with <kbd class="calibre17">std::async</kbd>. The tasks wait on results made available partway through the execution of other tasks, so all tasks must be running concurrently.
      </p>
      
      <p class="noind">With the block-based, forward-propagation approach out of the way, let’s look at the second approach to computing the partial
         sums of a range.
      </p>
      
      
      <h5 class="notetitle" id="ch08lev3sec5"><a id="ch08lev3sec5__title" class="calibre4"></a>Implementing the incremental pairwise algorithm for partial sums
      </h5>
      
      <p class="noind">This second approach to calculating the partial sums by adding elements increasingly further away works best where your processors
         can execute the additions in lockstep. In this case, no further synchronization is necessary because all the intermediate
         results can be propagated directly to the next processor that needs them. But in practice, you rarely have these systems to
         work with, except for those cases where a single <a id="iddle1063" class="calibre4"></a><a id="iddle1917" class="calibre4"></a><a id="iddle1921" class="calibre4"></a>processor can execute the same instruction across a small number of data elements simultaneously with so-called Single-Instruction/Multiple-Data
         (SIMD) instructions. Therefore, you must design your code for the general case and explicitly synchronize the threads at each
         step.
      </p>
      
      <p class="noind">One way to do this is to use a <i class="calibre6">barrier</i>—a synchronization mechanism that causes threads to wait until the required number of threads has reached the barrier. Once
         all the threads have reached the barrier, they’re all unblocked and may proceed. The C++11 Thread Library doesn’t offer this
         facility directly, so you have to design one yourself.
      </p>
      
      <p class="noind">Imagine a roller coaster at the fairground. If there’s a reasonable number of people waiting, the fairground staff will ensure
         that every seat is filled before the roller coaster leaves the platform. A barrier works the same way: you specify up front
         the number of “seats,” and threads have to wait until all the “seats” are filled. Once there are enough waiting threads, they
         can all proceed; the barrier is reset and starts waiting for the next batch of threads. Often, this construct is used in a
         loop, where the same threads come around and wait until next time. The idea is to keep the threads in lockstep, so one thread
         doesn’t run away in front of the others and get out of step. For an algorithm such as this one, that would be disastrous,
         because the runaway thread would potentially modify data that was still being used by other threads or use data that hadn’t
         been correctly updated yet.
      </p>
      
      <p class="noind">The following listing shows a simple implementation of a barrier.</p>
      
      
      
      <h5 class="notetitle" id="ch08ex12">Listing 8.12. <a id="ch08ex12__title" class="calibre4"></a>A simple barrier class
      </h5>
      <pre id="PLd0e33167" class="calibre5">class barrier
{
    unsigned const count;
    std::atomic&lt;unsigned&gt; spaces;
    std::atomic&lt;unsigned&gt; generation;
public:
    explicit barrier(unsigned count_):             <b class="calibre24"><i class="calibre6">1</i></b>
        count(count_),spaces(count),generation(0)
    {}
    void wait()
    {
        unsigned const my_generation=generation;   <b class="calibre24"><i class="calibre6">2</i></b>
        if(!--spaces)                              <b class="calibre24"><i class="calibre6">3</i></b>
        {
            spaces=count;                          <b class="calibre24"><i class="calibre6">4</i></b>
            ++generation;                          <b class="calibre24"><i class="calibre6">5</i></b>
        }
        else
        {
            while(generation==my_generation)       <b class="calibre24"><i class="calibre6">6</i></b>
                std::this_thread::yield();         <b class="calibre24"><i class="calibre6">7</i></b>
        }
    }
};</pre>
      
      <p class="noind"><a id="iddle1300" class="calibre4"></a><a id="iddle2132" class="calibre4"></a><a id="iddle2623" class="calibre4"></a>With this implementation, you construct a <kbd class="calibre17">barrier</kbd> with the number of “seats” <b class="calibre24"><i class="calibre6">1</i></b>, which is stored in the <kbd class="calibre17">count</kbd> variable. Initially, the number of <kbd class="calibre17">spaces</kbd> at the barrier is equal to this count. As each thread waits, the number of <kbd class="calibre17">spaces</kbd> is decremented <b class="calibre24"><i class="calibre6">3</i></b>. When it reaches zero, the number of spaces is reset back to <kbd class="calibre17">count</kbd> <b class="calibre24"><i class="calibre6">4</i></b>, and the <kbd class="calibre17">generation</kbd> is increased to signal to the other threads that they can continue <b class="calibre24"><i class="calibre6">5</i></b>. If the number of free <kbd class="calibre17">spaces</kbd> does not reach zero, you have to wait. This implementation uses a simple spin lock <b class="calibre24"><i class="calibre6">6</i></b>, checking the <kbd class="calibre17">generation</kbd> against the value you retrieved at the beginning of <kbd class="calibre17">wait()</kbd> <b class="calibre24"><i class="calibre6">2</i></b>. Because the <kbd class="calibre17">generation</kbd> is only updated when all the threads have reached the barrier <b class="calibre24"><i class="calibre6">5</i></b>, you <kbd class="calibre17">yield()</kbd> while waiting <b class="calibre24"><i class="calibre6">7</i></b>, so the waiting thread doesn’t hog the CPU in a busy wait.
      </p>
      
      <p class="noind">When I said this implementation was simple, I meant it: it uses a spin wait, so it’s not ideal for cases where threads are
         likely to be waiting a long time, and it doesn’t work if there’s more than <kbd class="calibre17">count</kbd> threads that can potentially call <kbd class="calibre17">wait()</kbd> at any one time. If you need to handle either of those scenarios, you must use a more robust (but more complex) implementation
         instead. I’ve also stuck to sequentially consistent operations on the atomic variables, because that makes everything easier
         to reason about, but you could potentially relax some of the ordering constraints. This global synchronization is expensive
         on massively parallel architectures, because the cache line holding the barrier state must be shuttled between all the processors
         involved (see the discussion of cache ping-pong in <a href="#ch08lev2sec5" class="calibre4">section 8.2.2</a>), so you must take great care to ensure that this is the best choice here. If your C++ Standard Library provides the facilities
         from the Concurrency TS, you could use <kbd class="calibre17">std::experimental::barrier</kbd> here. See <a href="kindle_split_014.html#ch04" class="calibre4">chapter 4</a> for details.
      </p>
      
      <p class="noind">This is what you need here; you have a fixed number of threads that need to run in a lockstep loop. Well, it’s <i class="calibre6">almost</i> a fixed number of threads. As you may remember, the items at the beginning of the list acquire their final values after a
         couple of steps. This means that either you have to keep those threads looping until the entire range has been processed,
         or you need to allow your barrier to handle threads dropping out and decreasing <kbd class="calibre17">count</kbd>. I opted for the latter option because it avoids having threads doing unnecessary work, looping until the final step is done.
      </p>
      
      <p class="noind">This means you have to change <kbd class="calibre17">count</kbd> to be an atomic variable, so you can update it from multiple threads without external synchronization:
      </p>
      
      <pre id="PLd0e33318" class="calibre5">std::atomic&lt;unsigned&gt; count;</pre>
      
      <p class="noind">The initialization remains the same, but now you have to explicitly <kbd class="calibre17">load()</kbd> from <kbd class="calibre17">count</kbd> when you reset the number of <kbd class="calibre17">spaces</kbd>:
      </p>
      
      <pre id="PLd0e33336" class="calibre5">spaces=count.load();</pre>
      
      <p class="noind">These are all the changes that you need on the <kbd class="calibre17">wait()</kbd> front; now you need a new member function to decrement <kbd class="calibre17">count</kbd>. Let’s call it <kbd class="calibre17">done_waiting()</kbd>, because a thread is declaring that it is done with waiting:
      </p>
      
      
      <pre id="PLd0e33357" class="calibre5">void done_waiting()
{
    --count;                    <b class="calibre24"><i class="calibre6">1</i></b>
    if(!--spaces)               <b class="calibre24"><i class="calibre6">2</i></b>
    {
        spaces=count.load();    <b class="calibre24"><i class="calibre6">3</i></b>
        ++generation;
    }
}</pre>
      
      <p class="noind">The first thing you do is decrement the <kbd class="calibre17">count</kbd> <b class="calibre24"><i class="calibre6">1</i></b> so that the next time <kbd class="calibre17">spaces</kbd> is reset it reflects the new lower number of waiting threads. Then you need to decrease the number of free <kbd class="calibre17">spaces</kbd> <b class="calibre24"><i class="calibre6">2</i></b>. If you don’t do this, the other threads will be waiting forever, because <kbd class="calibre17">spaces</kbd> was initialized to the old, larger value. If you’re the last thread through on this batch, you need to reset the counter
         and increase the <kbd class="calibre17">generation</kbd> <b class="calibre24"><i class="calibre6">3</i></b>, as you do in <kbd class="calibre17">wait()</kbd>. The key difference here is that if you’re the last thread in the batch, you don’t have to wait.
      </p>
      
      <p class="noind">You’re now ready to write your second implementation of partial sum. At each step, every thread calls <kbd class="calibre17">wait()</kbd> on the barrier to ensure the threads step through together, and once each thread is done, it calls <kbd class="calibre17">done_waiting()</kbd> on the barrier to decrement the count. If you use a second buffer alongside the original range, the barrier provides all
         the synchronization you need. At each step, the threads read from either the original range or the buffer and write the new
         value to the corresponding element of the other. If the threads read from the original range on one step, they read from the
         buffer on the next, and vice versa. This ensures there are no race conditions between the reads and writes by separate threads.
         Once a thread has finished looping, it must ensure that the correct final value has been written to the original range. The
         following listing pulls this all together.
      </p>
      
      
      
      <h5 class="notetitle" id="ch08ex13">Listing 8.13. <a id="ch08ex13__title" class="calibre4"></a>A parallel implementation of <kbd class="calibre17">partial_sum</kbd> by pairwise updates
      </h5>
      <pre id="PLd0e33426" class="calibre5">struct barrier
{
    std::atomic&lt;unsigned&gt; count;
    std::atomic&lt;unsigned&gt; spaces;
    std::atomic&lt;unsigned&gt; generation;
    barrier(unsigned count_):
        count(count_),spaces(count_),generation(0)
    {}
    void wait()
    {
        unsigned const gen=generation.load();
        if(!--spaces)
        {
            spaces=count.load();
            ++generation;
        }
        else
        {
            while(generation.load()==gen)
            {
                std::this_thread::yield();
            }
        }
    }
    void done_waiting()
    {
        --count;
        if(!--spaces)
        {
            spaces=count.load();
            ++generation;
        }
    }
};
template&lt;typename Iterator&gt;
void parallel_partial_sum(Iterator first,Iterator last)
{
    typedef typename Iterator::value_type value_type;
    struct process_element                                     <b class="calibre24"><i class="calibre6">1</i></b>
    {
        void operator()(Iterator first,Iterator last,
                        std::vector&lt;value_type&gt;&amp; buffer,
                        unsigned i,barrier&amp; b)
        {
            value_type&amp; ith_element=*(first+i);
            bool update_source=false;

            for(unsigned step=0,stride=1;stride&lt;=i;++step,stride*=2)
            {
                value_type const&amp; source=(step%2)?             <b class="calibre24"><i class="calibre6">2</i></b>
                    buffer[i]:ith_element;
                value_type&amp; dest=(step%2)?
                    ith_element:buffer[i];
                value_type const&amp; addend=(step%2)?             <b class="calibre24"><i class="calibre6">3</i></b>
                    buffer[i-stride]:*(first+i-stride);
                dest=source+addend;                            <b class="calibre24"><i class="calibre6">4</i></b>
                update_source=!(step%2);
                b.wait();                                      <b class="calibre24"><i class="calibre6">5</i></b>
            }
            if(update_source)                                  <b class="calibre24"><i class="calibre6">6</i></b>
            {
                ith_element=buffer[i];
            }
            b.done_waiting();                                  <b class="calibre24"><i class="calibre6">7</i></b>
        }
    };
    unsigned long const length=std::distance(first,last);
    if(length&lt;=1)
        return;
    std::vector&lt;value_type&gt; buffer(length);
    barrier b(length);
    std::vector&lt;std::thread&gt; threads(length-1);                <b class="calibre24"><i class="calibre6">8</i></b>
    join_threads joiner(threads);
    Iterator block_start=first;
    for(unsigned long i=0;i&lt;(length-1);++i)
    {
        threads[i]=std::thread(process_element(),first,last,   <b class="calibre24"><i class="calibre6">9</i></b>
                               std::ref(buffer),i,std::ref(b));
    }
    process_element()(first,last,buffer,length-1,b);           <b class="calibre24"><i class="calibre6">10</i></b>
}</pre>
      
      <p class="noind">The overall structure of this code is probably becoming familiar by now. You have a class with a function call operator (<kbd class="calibre17">process_element</kbd>) for doing the work <b class="calibre24"><i class="calibre6">1</i></b>, which you run on a bunch of threads <b class="calibre24"><i class="calibre6">9</i></b> stored in a vector <b class="calibre24"><i class="calibre6">8</i></b>, and which you also call from the main thread <b class="calibre24"><i class="calibre6">10</i></b>. The key difference this time is that the number of threads is dependent on the number of items in the list rather than on
         <kbd class="calibre17">std::thread::hardware_concurrency</kbd>. As I said already, unless you’re on a massively parallel machine where threads are cheap, this is probably a bad idea, but
         it shows the overall structure. It would be possible to have fewer threads, with each thread handling several values from
         the source range, but there will come a point where there are sufficiently few threads that this is less efficient than the
         forward-propagation algorithm.
      </p>
      
      <p class="noind">The key work is done in the function call operator of <kbd class="calibre17">process_element</kbd>. At each step, you either take the <i class="calibre6">i</i>th element from the original range or the <i class="calibre6">i</i>th element from the buffer <b class="calibre24"><i class="calibre6">2</i></b> and add it to the value <kbd class="calibre17">stride</kbd> elements prior <b class="calibre24"><i class="calibre6">3</i></b>, storing it in the buffer if you started in the original range or back in the original range if you started in the buffer
         <b class="calibre24"><i class="calibre6">4</i></b>. You then wait on the barrier <b class="calibre24"><i class="calibre6">5</i></b> before starting the next step. You’ve finished when the <kbd class="calibre17">stride</kbd> takes you off the start of the range, in which case you need to update the element in the original range if your final result
         was stored in the buffer <b class="calibre24"><i class="calibre6">6</i></b>. Finally, you tell the barrier that you’re <kbd class="calibre17">done_waiting()</kbd> <b class="calibre24"><i class="calibre6">7</i></b>.
      </p>
      
      <p class="noind">Note that this solution isn’t exception-safe. If an exception is thrown in <kbd class="calibre17">process_element</kbd> on one of the worker threads, it will terminate the application. You could deal with this by using <kbd class="calibre17">std::promise</kbd> to store the exception, as you did for the <kbd class="calibre17">parallel_find</kbd> implementation from <a href="#ch08ex09" class="calibre4">listing 8.9</a>, or even using <kbd class="calibre17">std::exception_ptr</kbd> protected by a mutex.
      </p>
      
      <p class="noind">That concludes our three examples. Hopefully, they’ve helped to crystallize some of the design considerations highlighted
         in <a href="#ch08lev1sec1" class="calibre4">sections 8.1</a>, <a href="#ch08lev1sec2" class="calibre4">8.2</a>, <a href="#ch08lev1sec3" class="calibre4">8.3</a>, and <a href="#ch08lev1sec4" class="calibre4">8.4</a>, and have demonstrated how these techniques can be brought to bear in real code.
      </p>
      
      
      
      
      
      <h3 id="ch08lev1sec6" class="chapter"><a id="ch08lev1sec6__title" class="calibre3"></a>Summary
      </h3>
      
      <p class="noind">We’ve covered quite a lot of ground in this chapter. We started with various techniques for dividing work between threads,
         such as dividing the data beforehand or using a number of threads to form a pipeline. We then looked at the issues surrounding
         the performance of multithreaded code from a low-level perspective, with a look at false sharing and data contention before
         moving on to how the patterns of data access can affect the performance of a bit of code. We then looked at additional considerations
         in the design of concurrent code, such as exception safety and scalability. Finally, we ended with a number of examples of
         parallel algorithm implementations, each of which highlighted particular issues that can occur when designing multithreaded
         code.
      </p>
      
      <p class="noind">One item that has cropped up a couple of times in this chapter is the idea of a thread pool—a preconfigured group of threads
         that run tasks assigned to the pool. Quite a lot of thought goes into the design of a good thread pool, so we’ll look at some
         of the issues in the next chapter, along with other aspects of advanced thread management.
      </p>
      
      
      
      
      <div class="calibre13" id="calibre_pb_28"></div>
</body></html>
