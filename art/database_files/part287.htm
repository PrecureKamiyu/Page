<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>15.8  Query Processing in Memory</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part286.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part288.htm">下一个 &gt;</a></p><p class="s65" style="padding-top: 7pt;padding-left: 72pt;text-indent: 0pt;text-align: left;">15.8  <span style=" color: #00AEEF;">Query Processing in Memory</span></p><p style="padding-top: 12pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">The query processing algorithms that we have described so far focus on minimizing</p><p class="s42" style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">I/O <span class="s43">cost. In this section, we discuss extensions to the query processing techniques that</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">help minimize memory access costs by using cache-conscious query processing algo- rithms and query compilation. We then discuss query processing with column-oriented storage. The algorithms we describe in this section give signiﬁcant beneﬁts for memory resident data; they are also very useful with disk-resident data, since they can speed up processing once data has been brought into the in-memory buﬀer.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">15.8.1 Cache-Conscious Algorithms</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">When data is resident in memory, access is much faster than if data were resident on magnetic disks, or even <span class="s44">SSDs</span>. However, it must be kept in mind that data already in <span class="s44">CPU </span>cache can be accessed as much as 100 times faster than data in memory. Modern <span class="s44">CPU</span>s have several levels of cache. Commonly used <span class="s44">CPU</span>s today have an L1 cache of size around 64 kilobytes, with a latency of about 1 nanosecond, an L2 cache of size around 256 kilobytes, with a latency of around 5 nanoseconds, and an L3 cache of having a size of around 10 megabytes, with a latency of 10 to 15 nanoseconds. In contrast, reading data in memory results in a latency of around 50 to 100 nanoseconds. For simplicity in the rest of this section we ignore the diﬀerence between the L1, L2 and L3 cache levels, and assume that there is only a single cache level.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">As we saw in Section 14.4.7, the speed diﬀerence between cache memory and main memory, and the fact that data are transferred between main memory and cache in units of a <i>cache-line </i>(typically about 64 bytes), results in a situation where the relationship between cache and main memory is not dissimilar to the relationship between main memory and disk (although with smaller speed diﬀerences). But there is a diﬀerence: while the contents of the main memory buﬀers disk-based data are controlled by the database system, <span class="s44">CPU </span>cache is controlled by the algorithms built into the computer hardware. Thus, the database system cannot directly control what is kept in cache.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">However, query processing algorithms can be designed in a way that the makes the best use of cache, to optimize performance. Here are some ways this can be done:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s39" style="padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">To sort a relation that is in-memory, we use the external merge-sort algorithm, with the run size chosen such that the run ﬁts into the cache; assuming we focus on the L3 cache, each run should be a few megabytes in size. We then use an in-memory sorting algorithm on each run; since the run ﬁts in cache, cache misses are likely to be minimal when the run is sorted. The sorted runs (all of which are in memory) are then merged. Merging is cache eﬃcient, since access to the runs is sequential: when a particular word is accessed from memory, the cache line that is fetched will contain the words that would be accessed next from that run.</span></p><p style="padding-left: 107pt;text-indent: 15pt;text-align: justify;">To sort a relation larger than memory, we can use external sort-merge with much larger run sizes, but use the in-memory merge-sort technique we just de- scribed to perform the in-memory sort of the large runs.</p><p class="s39" style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Hash-join requires probing of an index on the build relation. If the build relation ﬁts in memory, an index could be built on the whole relation; however, cache hits during probe can be maximized by partitioning the relations into smaller pieces</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 139pt;text-indent: 0pt;text-align: justify;">such that each partition of the build-relation along with the index ﬁts in the cache. Each partition is processed separately, with a build and a probe phase; since the build partition and its index ﬁt in cache, cache misses are minimized during the build as well as the probe phase.</p><p style="padding-left: 139pt;text-indent: 15pt;text-align: justify;">For relations larger than memory, the ﬁrst stage of hash-join should partition the two relations such that for each partition, the partitions of the two relations together ﬁt in memory. The technique just described can then be used to perform the hash join on each of these partitions, after fetching the contents into memory.</p><p class="s39" style="padding-top: 3pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Attributes in a tuple can be arranged such that attributes that tend to be accessed together are laid out consecutively. For example, if a relation is often used for aggre- gation, those attributes used as group by attributes, and those that are aggregated upon, can be stored consecutively. As a result, if there is a cache miss on one at- tribute, the cache line that is fetched would contain attributes that are likely to be used immediately.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 17pt;text-align: right;">Cache-aware algorithms are of increasing importance in modern database systems, since memory sizes are often large enough that much of the data is memory-resident. In cases where the requisite data item is not in cache, there is a processing <span class="s63">stall </span>while the data item is retrieved from memory and loaded into cache. In order to con- tinue to make use of the core that made the request resulting in the stall, the operating system maintains multiple threads of execution on which a core may work. Parallel query processing algorithms, which we study in Chapter 22 can use multiple threads running on a single <span class="s44">CPU </span>core; if one thread is stalled, another can start execution so</p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">the <span class="s44">CPU </span>core is utilized better.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">15.8.2 Query Compilation</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">With data resident in memory, <span class="s44">CPU </span>cost becomes the bottleneck, and minimizing <span class="s44">CPU </span>cost can give signiﬁcant beneﬁts. Traditional databases query processors act as inter- preters that execute a query plan. However, there is a signiﬁcant overhead due to inter- pretation: for example, to access an attribute of a record, the query execution engine may repeatedly look up the relation meta-data to ﬁnd the oﬀset of the attribute within the record, since the same code must work for all relations. There is also signiﬁcant overhead due to function calls that are performed for each record processed by an operation.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">To avoid overhead due to interpretation, modern main-memory databases com- pile query plans into machine code or intermediate level byte-code. For example, the compiler can compute the oﬀset of an attribute at compile time, and generate code where the oﬀset is a constant. The compiler can also combine the code for multiple functions in a way that minimizes function calls. With these, and other related opti- mizations, compiled code has been found to execute faster, by up to a factor of 10, than interpreted code.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;"><a name="bookmark332">15.8.3 Column-Oriented Storage</a><a name="bookmark353">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">In Section 13.6, we saw that in data-analytic applications, only a few attributes of a large schema may be needed, and that in such cases, storing a relation by column instead of by row may be advantageous. Selection operations on a single attribute (or small number of attributes) have signiﬁcantly lower cost in a column store since only the relevant attributes need to be accessed. However, since accessing each attribute requires its own data access, the cost of retrieving many attributes is higher and may incur additional seeks if data are stored on disk.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Because column stores permit eﬃcient access to many values for a given attribute at once, they are well suited to exploit the vector-processing capabilities of modern processors. This capability allows certain operations (such as comparisons and aggre- gations) to be performed in a parallel on multiple attribute values. When compiling query plans to machine code, the compiler can generate vector-processing instructions supported by the processor.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part286.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part288.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
