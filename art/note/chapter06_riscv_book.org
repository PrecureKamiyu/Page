#+title: Memory From RISCV book
* Chapter six: Memory Heirarchy

The chapter is about /Memory Heirarchy/. You should already know a
little about it if you have read /CSAPP/.

Note that the note is based on the side notes. You need to check both side notes and books. 

** Intro 
- /principle of locality/ 
  - /temporal locality/: like frequently-accessed data
  - /spatial locality/: like an array

- /memory heirarchy/: something like a pyramid.

-----------------

- /Blocks/: Cache memory is organized into a series of blocks, also known as lines 1. Each block contains a certain amount of data and has its own block address 1. The size of the block and the overall capacity of the cache are two important factors that affect the performance of the cache memory 1. The spatial aspect suggests that instead of fetching just one item from the main memory to the cache, it is useful to fetch several items that reside at adjacent addresses as well 2.
- /hit/, /hit rate/: if the block is the block that we want, then we hit. We could also miss, that is, not to hit. If we miss, we have to access lower level to find the block that we want, and send it to the cache. Hit rate is *the fraction of memory accesses found in the upper level*.  Hit rate is the measure of the performance of the cache.
- /hit time/, /miss penalty/: Hit time is the time we need to access data when hit. When we miss, we have to retrieve data from lower level. The extra time we spend is the miss penalty. 

#+BEGIN_NOTE
miss penalty: The time required to fetch a block into a level of
the memory hierarchy from the lower level, including the time to
*access the block*, *transmit it from one level to the other*, *insert it
in the level that experienced the miss*, and then *pass the block to the
requestor*.
#+END_NOTE

----------

Here is a self check: 

Which of the following statements are generally true?
1. Memory hierarchies take advantage of temporal locality.
2. On a read, the value returned depends on which blocks are in the cache.
3. Most of the cost of the memory hierarchy is at the highest level.
4. Most of the capacity of the memory hierarchy is at the lowest level.

And here is the answer:
1. True. Memory hierarchies take advantage of temporal locality by keeping recently accessed data in the faster, higher levels of the hierarchy so that future accesses to that data can be faster.
2. True. On a read, the value returned depends on whether the requested data is present in the cache or not. If it is present, the value is returned quickly from the cache. If it is not present, the value must be fetched from a lower level of the memory hierarchy, which takes longer.
3. False. Most of the cost of the memory hierarchy is at the lowest level (usually secondary storage), which is typically much larger and more expensive than the higher levels (such as cache and main memory).
4. True. Most of the capacity of the memory hierarchy is at the lowest level (usually secondary storage), which is typically much larger than the higher levels (such as cache and main memory).

** Memory Technologies

- /DRAM/ and /SRAM/: /D/ stands for *dynamic*, which is to say that the value it stored have to be refreshed every once a while or it will fade away. /S/ stands for *static*, saying that the value will not fade away.
- /Flash Memory/: widely used in cell phone. The store unit can be worn out, that is to say, if you use the unit too often, it will not working. To avoid the exact problem, Flash Memory can spread the writes to the flash memory. The controller may slow down the performance a little bit, but worth it. The controller is always needed, unless you have an external controller. Also, the controller can used to avoid the badly-manufactured part of the flash memory.
- /Magnetic Disk/: with a big capacity, whose structure is introduced in later section.

---------------

The difference of /DRAM/ and /SRAM/: Answered by *Google Bard*

Static RAM (SRAM) is a type of random access memory (RAM) that stores data in flip-flops. Flip-flops are electronic circuits that can store a single bit of data, either a 0 or a 1. As long as power is applied to the SRAM, the data stored in the flip-flops will be maintained indefinitely.

Dynamic RAM (DRAM) is a type of random access memory (RAM) that stores data in capacitors. Capacitors are electronic components that can store an electric charge. The amount of charge stored in a capacitor can be used to represent a 0 or a 1. However, the charge in a capacitor will slowly leak away over time. Therefore, DRAM must be refreshed periodically to prevent the data from being lost.
The main difference between SRAM and DRAM is that SRAM stores data in flip-flops, while DRAM stores data in capacitors. This difference in storage mechanism has several implications:

- Basic Unit: SRAM use /flip-flops/ and DRAM use /capacitors/. A filp-flop consists of six transistors maybe, and a capacitor is made of one or two transistors.
- Speed: SRAM is faster than DRAM because it does not need to be refreshed periodically.
- Cost: SRAM is more expensive than DRAM because it requires more transistors per bit of storage.
- Density: DRAM is denser than SRAM because it can store more data in the same amount of space.
- Refresh rate: SRAM does not need refreshing, while that DRAM need refreshing.

-------

- /two-level coding structure/: we refresh a row every time, leaving time for we to access.

In the context of DRAM (Dynamic Random Access Memory), a row refers to a set of memory cells that share a common word line. A word line is an electrical line that selects a row of cells in the memory array. When a row is selected, all the cells in that row can be accessed simultaneously.

If a word is 32 bits long and there are 1024 words, then there would be 1024 rows and each row would have 32 bits. Each bit in a row would correspond to one memory cell. 

- /buffer, bank, and the structure/

A DRAM has a *buffer* (if it has one bank), the buffer is used to hold a row of data. It works like SRAM. A *row* is loaded to the buffer for the sake of temporal locality. There is a buffer for one *bank*. A DRAM can have many banks. So DRAM has many buffers available. For more information please check the book.

There is the quote to the book: 

#+begin_quote
The row organization that helps with refresh also helps with
performance. To improve performance, DRAMs buffer rows for repeated
access. The buffer acts like an SRAM; by changing the address, random
bits can be accessed in the buffer until the next row access. This
capability improves the access time significantly, since the access
time to bits in the row is much lower. Making the chip wider also
improves the memory bandwidth of the chip. When the row is in the
buffer, it can be transferred by successive addresses at whatever the
width of the DRAM is (typically 4, 8, or 16 bits), or by specifying a
block transfer and the starting address within the buffer.

Sustaining that much bandwidth requires clever organization inside the
DRAM.  Instead of just a faster row buffer, the DRAM can be internally
organized to read or write from multiple banks, with each having its
own row buffer. Sending an address to several banks permits them all
to read or write simultaneously. For example, with four banks, there
is just one access time and then accesses rotate between the four
banks to supply four times the bandwidth. This rotating access scheme
is called *address interleaving*.
#+end_quote
End of the Quote.

---------
- /SDRAM, in a burst/

/S/ stands for synchronize. It is to say, a clock is added to DRAM to deal with synchronization problem. Let us see what *New Bing* said: 

#+begin_quote
SDRAM (Synchronous Dynamic Random Access Memory) is able to transfer data in a burst without having to specify additional address bits by using a clock to synchronize the data transfers. Once the initial address is specified, the clock is used to transfer the successive data in the burst. The clock generates a series of pulses at regular intervals, and each pulse triggers the transfer of the next piece of data in the burst. This means that the data can be transferred more quickly and efficiently without having to specify additional address bits for each piece of data in the burst.
#+end_quote

End of the Quote.

And there is actually a weird question: what is "in a burst"? It seems that "a burst transfer" is different to "a block transfer". A burst is like ejaculation---it starts and just goes on till the end. A block is like I don't know, 搬砖? Anyway, this is what *NB* said: 

#+begin_quote
A burst transfer refers to the transfer of a group of data sequentially in rapid succession. In the context of synchronous DRAMs (Dynamic Random Access Memory), this means that once the initial address is specified, the successive data in the burst can be transferred without having to specify additional address bits. Instead, the clock is used to transfer the successive data in the burst.

A block transfer, on the other hand, refers to the transfer of a block of data from one location to another. The difference between a burst and a block transfer is that a burst transfer transfers data sequentially in rapid succession, while a block transfer transfers a block of data as a whole.
#+end_quote
End of the Quote.
--------- 
- /flash memory, wear leveling/

I shall just copy the text, since it is not long.

#+begin_quote
*Flash memory* is a type of electrically erasable programmable read-only memory
(EEPROM).

Unlike disks and DRAM, but like other EEPROM technologies, /writes can wear out
flash memory bits/. To cope with such limits, most flash products include a controller
to /spread the writes/ by remapping blocks that have been written many times to less
trodden blocks. This technique is called *wear leveling*. With wear leveling, personal
mobile devices are very unlikely to exceed the write limits in the flash. Such wear
leveling lowers the potential performance of flash, but it is needed unless higher-level 
software monitors block wear. Flash controllers that perform wear leveling can
also improve yield by mapping out memory cells that were manufactured incorrectly.
#+end_quote
End of the Quote. "trodden" is "tread".

-------------
- /DIMM/

You may skip the part. This part seems not so important. And here the text that is processed by *NB*:
#+begin_quote
A DIMM (Dual In-line Memory Module) is a small circuit board that contains one or several dynamic random-access memory (DRAM) chips. DIMMs are commonly used to add memory to personal computers, workstations, printers, and servers. They are designed to be 64 bits wide, enabling fast data transfer.

The text you provided explains that memory for servers is commonly sold on DIMMs. These modules typically contain 4-16 DRAMs and are normally organized to be 8 bytes wide for server systems. A DIMM using DDR4-3200 SDRAMs could transfer at a rate of 8 × 3200 = 25,600 megabytes per second. Such DIMMs are named after their bandwidth: PC25600.

Since a DIMM can have many DRAM chips and only a portion of them are used for a particular transfer, the term “memory rank” is used to refer to the subset of chips in a DIMM that share common address lines.
#+end_quote
End of the Quote. I have no idea what is "memory rank".

--------------
- /Magnetic Disk/

Let us just skip this part
--------------
** The Basic of Caches
*** A Small Intro
- /Cache/: cache is word referring to the safe space. Just like in the example, you table where there are books that you need is the cache. Here we called the space that is between the memory and the process as *cache*.

- /The structure of Cache/: Cache is a smaller space between the memory and processor. When the processor want to access some data that is in the memory it does not directly access to the memory. Instead, it access to the cache. If the cache does hold the data (hit), the speed is considerably enhanced.

- /block/: A block has many bytes (or just one). Block size is very critical to the performance of the cache.

- /direct mapped/:we use the address of the *block* to determine the position in the cache. And usually we know that the address in the cache is:
\[
\rm (Block\ Address) \bmod{(Number\ of\ blocks\ in\ the\ cache)}
\]

*** Accessing to the Cache
- /cache index field/: the index of the block in cache.
- /tag field/: the tag is used to identify the blocks. For example, we have block address 
- /valid bit/: There is a valid bit every one block to indicate if the block are valid.

-----------

- /calculation of the field/
Three steps:

1. We find the block address: \( {\rm Block\ Address =  (Address ) } /2 ^{m+2}\), that is we get rid of the low \(m+2\) bits.
2. We find the lower \(n\) bits of the block address. \( {\rm Index = (Block \ Address) }\bmod{2^n} \), that is we get the lower \(n\) bits. that is the index.
3. We find the higher \( 64 - m -  2 - n \) bits of block address, which is the /tag field/.

----------- 
- /calculate the bits in the cache/

You shall see the example on the book.

-----------
- /Block size/

It is wrong that the bigger the block size, the better. While big block size may enhance hit rate, it also enhance the miss penalty.
You shall check for the book. Here is the quote:
#+begin_quote
Larger blocks exploit spatial locality to lower miss rates. As Figure 5.11 shows,
increasing the block size usually decreases the miss rate. The miss rate may go up
eventually if the block size becomes a significant fraction of the cache size, because
the number of blocks that can be held in the cache will become small, and there will
be a great deal of competition for those blocks. As a result, a block will be bumped
out of the cache before many of its words are accessed. Stated alternatively, spatial
locality among the words in a block decreases with a very large block; consequently,
the benefits to the miss rate become smaller.

A more serious problem associated with just increasing the block size is that the
cost of a miss rises. The miss penalty is determined by the time required to fetch
the block from the next lower level of the hierarchy and load it into the cache. The
time to fetch the block has two parts: the latency to the first word and the transfer
time for the rest of the block. Clearly, unless we change the memory system, the
transfer time—and hence the miss penalty—will likely increase as the block size
expands. Furthermore, the improvement in the miss rate starts to decrease as the
blocks become larger. The result is that the increase in the miss penalty overwhelms
the decrease in the miss rate for blocks that are too large, and cache performance
thus decreases. Of course, if we design the memory to transfer larger blocks more
efficiently, we can increase the block size and obtain further improvements in cache
performance. We discuss this topic in the next section.
#+end_quote

*** Handling Cache Misses
- /Handling miss/

Four steps when miss happens.
*** Handling Writes
- /Consistent and Inconsistent/
- /Write Through/
  - /Write Queue/
- /Write Back/
-------
Elaboration: WHAT THE FUCK?
-------
- /An Example of the Intrinsity FastMATH Processor/

*** Summary of the Section

I will just copy the summary parts.

#+begin_quote
We began the previous section by examining the simplest of caches: a direct-mapped
cache with a one-word block. In such a cache, both hits and misses are simple, since
a word can go in exactly one location and there is a separate tag for every word. To
keep the cache and memory consistent, a write-through scheme can be used, so
that every write into the cache also causes memory to be updated. The alternative
to write-through is a write-back scheme that copies a block back to memory when
it is replaced; we’ll discuss this scheme further in upcoming sections.

To take advantage of spatial locality, a cache must have a block size larger than
one word. The use of a bigger block decreases the miss rate and improves the
efficiency of the cache by reducing the amount of tag storage relative to the amount
of data storage in the cache. Although a larger block size decreases the miss rate, it
can also increase the miss penalty. If the miss penalty increased linearly with the
block size, larger blocks could easily lead to lower performance.

To avoid performance loss, the bandwidth of main memory is increased to
transfer cache blocks more efficiently. Common methods for increasing bandwidth
external to the DRAM are making the memory wider and interleaving. DRAM
designers have steadily improved the interface between the processor and memory
to increase the bandwidth of burst mode transfers to reduce the cost of larger cache
block sizes.
#+end_quote

End of the Quote. Check yourself:

The speed of the memory system affects the designer’s decision on the size of
the cache block. Which of the following cache designer guidelines is generally
valid?
1. The shorter the memory latency, the smaller the cache block
2. The shorter the memory latency, the larger the cache block
3. The higher the memory bandwidth, the smaller the cache block
4. The higher the memory bandwidth, the larger the cache block

Here is the answer from *NB*:
#+begin_quote
Memory latency refers to the time it takes to access data from memory. When memory latency is short, it takes less time to transfer data from memory to the cache. This means that larger cache blocks can be used without significantly impacting performance. Larger cache blocks can take advantage of spatial locality by fetching more data that is likely to be accessed in the near future.

Memory bandwidth refers to the amount of data that can be transferred from memory to the cache in a given amount of time. When memory bandwidth is high, more data can be transferred from memory to the cache in a given amount of time. This means that larger cache blocks can be used without significantly impacting performance. Larger cache blocks can take advantage of spatial locality by fetching more data that is likely to be accessed in the near future.
#+end_quote

** Measuring and Improving Cache Performance

