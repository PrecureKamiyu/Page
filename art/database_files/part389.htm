<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>21.6  Distributed File Systems</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part388.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part390.htm">下一个 &gt;</a></p><p class="s65" style="padding-left: 72pt;text-indent: 0pt;text-align: left;">21.6  <span style=" color: #00AEEF;">Distributed File Systems</span></p><p style="padding-top: 12pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">A <span class="s63">distributed file system </span>stores ﬁles across a large collection of machines while giving a single-ﬁle-system view to clients. As with any ﬁle system, there is a system of ﬁle names and directories, which clients can use to identify and access ﬁles. Clients do not need to bother about where the ﬁles are stored.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 17pt;text-align: justify;">The goal of ﬁrst-generation distributed ﬁle systems was to allow client machines to access ﬁles stored on one or more ﬁle servers. In contrast, later-generation distributed ﬁle systems, which we focus on, address distribution of ﬁle blocks across a very large number of nodes. Such distributed ﬁle systems can store very large amounts of data and support very large numbers of concurrent clients. A landmark system in this context was the Google File System (GFS), developed in the early 2000s, which saw widespread use within Google. The open-source Hadoop File System (HDFS) is based on the GFS architecture and is now very widely used.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Distributed ﬁle systems are generally designed to eﬃciently store large ﬁles whose sizes range from tens of megabytes to hundreds of gigabytes or more. However, they are designed to store moderate numbers of such ﬁles, of the order of millions; they are typically not designed to stores billions of diﬀerent ﬁles. In contrast, the parallel data storage systems we have seen earlier are designed to store very large numbers (billions or more) of data items, whose size can range from small (tens of bytes) to medium (a few megabytes).</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">As in parallel data storage systems, the data in a distributed ﬁle system are stored across a number of nodes. Since ﬁles can be much larger than data items in a data storage system, ﬁles are broken up into multiple blocks. The blocks of a single ﬁle can be partitioned across multiple machines. Further, each ﬁle block is replicated across multiple (typically three) machines, so that a machine failure does not result in the ﬁle becoming inaccessible.</p><p style="padding-left: 106pt;text-indent: 0pt;text-align: justify;">File systems typically support two kinds of <i>metadata</i>:</p><p class="s63" style="padding-top: 10pt;padding-left: 113pt;text-indent: -16pt;text-align: left;">1. <span class="p">A directory system, which allows a hierarchical organization of ﬁles into directo- ries and subdirectories, and</span></p><p class="s63" style="padding-top: 6pt;padding-left: 113pt;text-indent: -17pt;text-align: left;">2. <span class="p">A mapping from a ﬁle name to the sequence of identiﬁers of blocks that store the actual data in each ﬁle.</span></p><p style="padding-top: 10pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">In the case of a centralized ﬁle system, the block identiﬁers help locate blocks in a storage device such as a disk. In the case of a distributed ﬁle system, in addition to providing a block identiﬁer, the ﬁle system must provide the location (node identiﬁer) where the block is stored; in fact, due to replication, the ﬁle system provides a set of node identiﬁers along with each block identiﬁer.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">In the rest of this section, we describe the organization of the Hadoop File System (HDFS), which is shown in Figure 21.7; the architecture of HDFS is derived from that of the Google File System (GFS). The nodes (machines) which store data blocks in HDFS are called <span class="s63">datanodes</span>. Blocks have an associated ID, and datanodes map the block ID to a location in their local ﬁle system where the block is stored.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The ﬁle system metadata too can be partitioned across many nodes, but unless carefully architected, this could lead to bad performance. GFS and HDFS took a sim- pler and more pragmatic approach of storing the ﬁle system metadata at a single node, called the <span class="s63">namenode </span>in HDFS.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="4" alt="image" src="Image_3159.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="4" alt="image" src="Image_3160.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="422" height="493" alt="image" src="Image_3161.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="4" alt="image" src="Image_3162.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="4" alt="image" src="Image_3163.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="4" alt="image" src="Image_3164.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="4" alt="image" src="Image_3165.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="61" height="51" alt="image" src="Image_3166.png"/></span></p><p class="s469" style="text-indent: 0pt;line-height: 9pt;text-align: left;">Metadata Ops</p><p style="text-indent: 0pt;text-align: left;"/><p class="s469" style="text-indent: 0pt;line-height: 9pt;text-align: left;">Client</p><p style="text-indent: 0pt;text-align: left;"/><p class="s469" style="text-indent: 0pt;line-height: 9pt;text-align: left;">Block Read</p><p style="text-indent: 0pt;text-align: left;"/><p class="s469" style="text-indent: 0pt;line-height: 9pt;text-align: left;">DataNodes</p><p style="text-indent: 0pt;text-align: left;"/><p class="s469" style="text-indent: 0pt;line-height: 9pt;text-align: left;">Blocks</p><p style="text-indent: 0pt;text-align: left;"/><p class="s469" style="text-indent: 0pt;line-height: 9pt;text-align: left;">Client</p><p style="text-indent: 0pt;text-align: left;"/><p class="s469" style="text-indent: 0pt;line-height: 9pt;text-align: left;">Block Write</p><p style="text-indent: 0pt;text-align: left;"/><p class="s469" style="text-indent: 0pt;line-height: 9pt;text-align: left;">Replication</p><p style="text-indent: 0pt;text-align: left;"/><p class="s469" style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;text-align: center;">BackupNode</p><p class="s118" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Metadata (name, replicas, ...)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s473" style="text-indent: 0pt;line-height: 2pt;text-align: left;"><span><img width="3" height="3" alt="image" src="Image_3167.png"/></span>	<span><img width="3" height="3" alt="image" src="Image_3168.png"/></span></p><p class="s469" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: center;">NameNode</p><p class="s118" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: center;">Metadata (name, replicas, ...)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s469" style="padding-top: 2pt;padding-left: 256pt;text-indent: 0pt;text-align: left;">Rack 1                Rack 2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 165pt;text-indent: 0pt;text-align: left;">Figure 21.7 <span class="s74">Hadoop Distributed File System (HDFS) architecture</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Since all metadata reads have to go to the namenode, if a disk access were required to satisfy a metadata read, the number of requests that could be satisﬁed per second would be very small. To ensure acceptable performance, HDFS namenodes cache the entire metadata in memory; the size of memory then becomes a limiting factor in the number of ﬁles and blocks that the ﬁle system can manage. To reduce the memory size, HDFS uses very large block sizes (typically 64 MB) to reduce the number of blocks that the namenode must track for each ﬁle. Despite this, the limited amount of main memory on most machines constrains namenodes to support only a limited number of</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">ﬁles (of the order of millions). However, with main-memory sizes of many gigabytes, and block sizes of tens of megabytes, an <span class="s44">HDFS </span>system can comfortably handle many petabytes of data.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">In any system with a large number of datanodes, datanode failures are a frequent occurrence. To deal with datanode failures, data blocks must be replicated to multiple datanodes. If a datanode fails, the block can still be read from one of the other datan- odes that stores a replica of the block. Replication to three datanodes is widely used to provide high availability, without paying too high a storage overhead.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">We now consider how a ﬁle open and read request is satisﬁed with HDFS. First, the client contacts the namenode, with the name of the ﬁle. The namenode ﬁnds the list of IDs of blocks containing the ﬁle data and returns to the client the list of block IDs, along with the set of nodes that contain replicas of each of the blocks. The client then contacts any one of the replicas for each block of the ﬁle, sending it the ID of the block, to retrieve the block. In case that particular replica does not respond, the client can contact any of the other replicas.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">To satisfy a write request, the client ﬁrst contacts the namenode, which allocates blocks, and decides which datanodes should store replicas of each block. The metadata are recorded at the namenode and sent back to the client. The client then writes the block to all the replicas. As an optimization to reduce network traﬃc, HDFS implemen- tations may choose to store two replicas in the same rack; in that case, the block write is performed to one replica, which then copies the data to the second replica on the same rack. When all the replicas have processed the write of a block, an acknowledgment is sent to the client.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Replication introduces the problem of consistency of data across the replicas in case the ﬁle is updated. As an example, suppose one of the replicas of a data block is updated, but due to system failure, another replica does not get updated; then the system could end up with inconsistent states across the replicas. And what value is read would depend on which replica is accessed, which is not an acceptable situation.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">While data storage systems in general need to deal with consistency, using tech- niques that we study in Chapter 23, some distributed ﬁle systems such as HDFS take a diﬀerent approach: namely, not allowing updates. In other words, a ﬁle can be ap- pended to, but data that are written cannot be updated. As each block of the ﬁle is written, the block is copied to the replicas. The ﬁle cannot be read until it is <i>closed</i>, that is, all data have been written to the ﬁle, and the blocks have been written suc- cessfully at all their replicas. The model of writing data to a ﬁle is sometimes referred to as <span class="s63">write-once-read-many </span>access model. Others such as GFS allow updates and de- tect certain inconsistent states caused by failures while writing to replicas; however, transactional (atomic) updates are not supported.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The restriction that ﬁles cannot be updated, but can only be appended to, is not a problem for many applications of a distributed ﬁle system. Applications that require updates should use a data-storage system that supports updates instead of using a dis- tributed ﬁle system.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part388.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part390.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
