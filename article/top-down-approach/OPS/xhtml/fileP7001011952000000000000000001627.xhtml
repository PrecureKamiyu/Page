<?xml version="1.0" encoding="utf-8"?><html xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:svg="http://www.w3.org/2000/svg" xmlns:epub="http://www.idpf.org/2007/ops" xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" epub:prefix="index: http://www.index.com/"><head>
<meta name="dcterms.conformsTo" content="PXE Basic 1.0"></meta>
<meta name="generator" content="PXE Tools version 1.39.109"></meta>
<!--Created by pxe.pl for standard version PXE Basic 1.0,data-profile-product=standard by PXE Tools 1.39.109, partial=false-->
<title>3.6 Principles of Congestion Control</title><link rel="alternate stylesheet" type="text/css" title="sepia" href="../css/sepia.css"></link><link rel="alternate stylesheet" type="text/css" title="night" href="../css/night.css"></link><link rel="stylesheet" type="text/css" title="day" href="../css/main.css"></link><link rel="stylesheet" type="text/css" title="day" href="../css/print.css"></link>
<script src="js/format_lg_obj.js"></script>
</head><body epub:type="bodymatter">
<section id="P7001011952000000000000000001627" class="level1"><header><h1 class="title" id="P700101195200000000000000000A4AE" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4AE" epub:type="title"><span class="number">3.6</span> Principles of Congestion Control</h1></header>
<p id="P700101195200000000000000000A4AF" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4AF">In the previous sections, we examined both the general principles and specific TCP mechanisms used to provide for a reliable data transfer service in the face of packet loss. We mentioned earlier that, in practice, such loss typically results from the overflowing of router buffers as the network becomes congested. Packet retransmission thus treats a symptom of network congestion (the loss of a specific transport-layer segment) but does not treat the cause of network congestion—too many sources attempting to send data at too high a rate. To treat the cause of network congestion, mechanisms are needed to throttle senders in the face of network congestion.</p>
<p id="P700101195200000000000000000A4B0" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4B0">In this section, we consider the problem of congestion control in a general context, seeking to understand why congestion is a bad thing, how network congestion is manifested in the performance received by upper-layer applications, and various approaches that can be taken to avoid, or react to, network congestion. This more general study of congestion control is appropriate since, as with reliable data transfer, it is high on our “top-ten” list of fundamentally important problems in networking. The following section contains a detailed study of TCP’s congestion-control algorithm.</p>
<section id="P700101195200000000000000000162B" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000162B" class="level2"><header><h1 class="title" id="P700101195200000000000000000A4B1" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4B1" epub:type="title"><span class="number">3.6.1</span> The Causes and the Costs of Congestion</h1></header>
<p id="P700101195200000000000000000A4B2" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4B2">Let’s begin our general study of congestion control by examining three increasingly complex scenarios in which congestion occurs. In each case, we’ll look at why <span class="pagebreak" title="262" id="P700101195200000000000000000162E" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000162E" epub:type="pagebreak" role="doc-pagebreak"></span>congestion occurs in the first place and at the cost of congestion (in terms of resources not fully utilized and poor performance received by the end systems). We’ll not (yet) focus on how to react to, or avoid, congestion but rather focus on the simpler issue of understanding what happens as hosts increase their transmission rate and the network becomes congested.</p>
<section id="P700101195200000000000000000162F" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000162F" class="level3"><header><h1 class="title" id="P700101195200000000000000000A4B3" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4B3" epub:type="title">Scenario 1: Two Senders, a Router with Infinite Buffers</h1></header>
<p id="P700101195200000000000000000A4B4" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4B4">We begin by considering perhaps the simplest congestion scenario possible: Two hosts (A and B) each have a connection that shares a single hop between source and destination, as shown in <a class="xref" href="#P7001011952000000000000000001635" data-foobar="1"><span class="label">Figure</span> <span class="number">3.43</span></a>.</p>
<p id="P700101195200000000000000000A4B5" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4B5">Let’s assume that the application in Host A is sending data into the connection (for example, passing data to the transport-level protocol via a socket) at an average rate of λ<sub>in</sub> bytes/sec. These data are original in the sense that each unit of data is sent into the socket only once. The underlying transport-level protocol is a simple one. Data is encapsulated and sent; no error recovery (for example, retransmission), flow control, or congestion control is performed. Ignoring the additional overhead due to adding transport- and lower-layer header information, the rate at which Host A offers traffic to the router in this first scenario is thus λ<sub>in</sub> bytes/sec. Host B operates in a similar manner, and we assume for simplicity that it too is sending at a rate of λ<sub>in</sub> bytes/sec. Packets from Hosts A and B pass through a router and over a shared outgoing link of capacity <i>R</i>. The router has buffers that allow it to store incoming packets when the packet-arrival rate exceeds the outgoing link’s capacity. In this first scenario, we assume that the router has an infinite amount of buffer space.</p>
<p id="P700101195200000000000000000A4B6" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4B6"><a class="xref" href="#P700101195200000000000000000163C" data-foobar="1"><span class="label">Figure</span> <span class="number">3.44</span></a> plots the performance of Host A’s connection under this first scenario. The left graph plots the <span class="keyword" id="P7001011952000000000000000001634" data-uri="M03_KURO4140_07_SE_C03.xhtml#P7001011952000000000000000001634"><b>per-connection throughput</b></span> (number of bytes per</p>
<figure id="P7001011952000000000000000001635" class="figure" data-uri="M03_KURO4140_07_SE_C03.xhtml#P7001011952000000000000000001635">
<img alt="Illustration of congestion scenario one: two connections sharing a single hop with infinite buffers." height="310" width="790" aria-describedby="P7001011952000000000000000001639" id="P700101195200000000000000000A4B7" data-uri="P700101195200000000000000000556F" src="../images/4055103043.png"></img>
<figcaption id="P700101195200000000000000000A4B8" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4B8"><header><h1 class="title" id="P700101195200000000000000000A4B9" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4B9" epub:type="title"><span class="label">Figure </span><span class="number">3.43</span> Congestion scenario 1: Two connections sharing a single hop with infinite buffers</h1></header>

</figcaption>
</figure><div class="longdesc" id="P7001011952000000000000000001639" data-uri="M03_KURO4140_07_SE_C03.xhtml#P7001011952000000000000000001639" aria-hidden="false"><a class="xref" aria-hidden="false" href="../longalt/la_4055103043.xhtml#la_4055103043"><span class="label">Description</span></a></div>
<figure id="P700101195200000000000000000163C" class="figure" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000163C">
<img alt="Illustration of congestion scenario 1: Throughput and delay as a function of host sending rate." height="250" width="266" aria-describedby="P7001011952000000000000000001641" id="P700101195200000000000000000A4BC" data-uri="P7001011952000000000000000005570" src="../images/4055103044.png"></img>
<figcaption id="P700101195200000000000000000A4BD" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4BD"><header><h1 class="title" id="P700101195200000000000000000A4BE" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4BE" epub:type="title"><span class="pagebreak" title="263" id="P7001011952000000000000000001640" data-uri="M03_KURO4140_07_SE_C03.xhtml#P7001011952000000000000000001640" epub:type="pagebreak" role="doc-pagebreak"></span><span class="label">Figure </span><span class="number">3.44</span> Congestion scenario 1: Throughput and delay as a function of host sending rate</h1></header>

</figcaption>
</figure><div class="longdesc" id="P7001011952000000000000000001641" data-uri="M03_KURO4140_07_SE_C03.xhtml#P7001011952000000000000000001641" aria-hidden="false"><a class="xref" aria-hidden="false" href="../longalt/la_4055103044.xhtml#la_4055103044"><span class="label">Description</span></a></div>
<figure id="P7001011952000000000000000001644" class="figure" data-uri="M03_KURO4140_07_SE_C03.xhtml#P7001011952000000000000000001644">
<img alt="Illustration of congestion scenario 1: Throughput and delay as a function of host sending rate." height="252" width="242" aria-describedby="P7001011952000000000000000001646" id="P700101195200000000000000000A4C1" data-uri="P7001011952000000000000000005571" src="../images/4055103044a.png"></img>
<details class="longdesc" id="P7001011952000000000000000001646" data-uri="M03_KURO4140_07_SE_C03.xhtml#P7001011952000000000000000001646">
<summary><span class="label">Description</span></summary>
<p id="P700101195200000000000000000A4C2" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4C2">In this graph, the x-axis again is labeled (lambda)in and extends to a point labeled "R/2." The y-axis is labeled "Delay" and contains no indicated measurements. A blue line again originates from the intersection of the x- and y-axes, though this time it arcs sharply upwards; it never intersects with R/2 on the x-axis, but ends up running parallel to it.</p>
</details>
</figure>
<p class="continued" id="P700101195200000000000000000A4C3" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4C3">second at the receiver) as a function of the connection-sending rate. For a sending rate between 0 and <i>R</i>/2, the throughput at the receiver equals the sender’s sending rate—everything sent by the sender is received at the receiver with a finite delay. When the sending rate is above <i>R</i>/2, however, the throughput is only <i>R</i>/2. This upper limit on throughput is a consequence of the sharing of link capacity between two connections. The link simply cannot deliver packets to a receiver at a steady-state rate that exceeds <i>R</i>/2. No matter how high Hosts A and B set their sending rates, they will each never see a throughput higher than <i>R</i>/2.</p>
<p id="P700101195200000000000000000A4C4" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4C4">Achieving a per-connection throughput of <i>R</i>/2 might actually appear to be a good thing, because the link is fully utilized in delivering packets to their destinations. The right-hand graph in <a class="xref" href="#P700101195200000000000000000163C" data-foobar="1"><span class="label">Figure</span> <span class="number">3.44</span></a>, however, shows the consequence of operating near link capacity. As the sending rate approaches <i>R</i>/2 (from the left), the average delay becomes larger and larger. When the sending rate exceeds <i>R</i>/2, the average number of queued packets in the router is unbounded, and the average delay between source and destination becomes infinite (assuming that the connections operate at these sending rates for an infinite period of time and there is an infinite amount of buffering available). Thus, while operating at an aggregate throughput of near <i>R</i> may be ideal from a throughput standpoint, it is far from ideal from a delay standpoint. <i>Even in this (extremely) idealized scenario, we’ve already found one cost of a congested network—large queuing delays are experienced as the packet-arrival rate nears the link capacity.</i></p>
</section>
<section id="P700101195200000000000000000164A" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000164A" class="level3"><header><h1 class="title" id="P700101195200000000000000000A4C5" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4C5" epub:type="title">Scenario 2: Two Senders and a Router with Finite Buffers</h1></header>
<p id="P700101195200000000000000000A4C6" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4C6">Let’s now slightly modify scenario 1 in the following two ways (see <a class="xref" href="#P700101195200000000000000000164E" data-foobar="1"><span class="label">Figure</span> <span class="number">3.45</span></a>). First, the amount of router buffering is assumed to be finite. A consequence of this real-world assumption is that packets will be dropped when arriving to an already-full buffer. Second, we assume that each connection is reliable. If a packet containing<span class="pagebreak" title="264" id="P700101195200000000000000000164D" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000164D" epub:type="pagebreak" role="doc-pagebreak"></span></p>
<figure id="P700101195200000000000000000164E" class="figure" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000164E">
<img alt="Illustration of scenario 2: Two hosts (with retransmissions) and a router with finite buffers." height="363" width="790" aria-describedby="P7001011952000000000000000001652" id="P700101195200000000000000000A4C7" data-uri="P7001011952000000000000000005572" src="../images/4055103045.png"></img>
<figcaption id="P700101195200000000000000000A4C8" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4C8"><header><h1 class="title" id="P700101195200000000000000000A4C9" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4C9" epub:type="title"><span class="label">Figure </span><span class="number">3.45</span> Scenario 2: Two hosts (with retransmissions) and a router with finite buffers</h1></header>

</figcaption>
</figure><div class="longdesc" id="P7001011952000000000000000001652" data-uri="M03_KURO4140_07_SE_C03.xhtml#P7001011952000000000000000001652" aria-hidden="false"><a class="xref" aria-hidden="false" href="../longalt/la_4055103045.xhtml#la_4055103045"><span class="label">Description</span></a></div>
<p class="continued" id="P700101195200000000000000000A4CD" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4CD">a transport-level segment is dropped at the router, the sender will eventually retransmit it. Because packets can be retransmitted, we must now be more careful with our use of the term <i>sending rate</i>. Specifically, let us again denote the rate at which the application sends original data into the socket by λ<sub>in</sub> bytes/sec. The rate at which the transport layer sends segments (containing original data <i>and</i> retransmitted data) into the network will be denoted <span class="inlineequation"><m:math display="inline" alttext="" data-uri="" altimg-width="25" altimg-height="17" altimg="../images/ch03math30.png"><m:mrow><m:msub><m:msup><m:mi>λ</m:mi><m:mo>′</m:mo></m:msup><m:mrow><m:mtext>in</m:mtext></m:mrow></m:msub></m:mrow></m:math></span> bytes/sec. <span class="inlineequation"><m:math display="inline" alttext="" data-uri="" altimg-width="25" altimg-height="17" altimg="../images/ch03math31.png"><m:mrow><m:msub><m:msup><m:mi>λ</m:mi><m:mo>′</m:mo></m:msup><m:mrow><m:mtext>in</m:mtext></m:mrow></m:msub></m:mrow></m:math></span> is sometimes referred to as the <span class="keyword" id="P7001011952000000000000000001657" data-uri="M03_KURO4140_07_SE_C03.xhtml#P7001011952000000000000000001657"><b>offered load</b></span> to the network.</p>
<p id="P700101195200000000000000000A4CE" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4CE">The performance realized under scenario 2 will now depend strongly on how retransmission is performed. First, consider the unrealistic case that Host A is able to somehow (magically!) determine whether or not a buffer is free in the router and thus sends a packet only when a buffer is free. In this case, no loss would occur, λ<sub>in</sub> would be equal to <span class="inlineequation"><m:math display="inline" alttext="" data-uri="" altimg-width="25" altimg-height="17" altimg="../images/ch03math32.png"><m:mrow><m:msub><m:msup><m:mi>λ</m:mi><m:mo>′</m:mo></m:msup><m:mrow><m:mtext>in</m:mtext></m:mrow></m:msub></m:mrow></m:math></span>, and the throughput of the connection would be equal to λ<sub>in</sub>. This case is shown in <a class="xref" href="#P700101195200000000000000000165B" data-foobar="1"><span class="label">Figure</span> <span class="number">3.46(a</span></a>). From a throughput standpoint, performance is ideal—everything that is sent is received. Note that the average host sending rate cannot exceed <i>R</i>/2 under this scenario, since packet loss is assumed never to occur.</p>
<p id="P700101195200000000000000000A4CF" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4CF">Consider next the slightly more realistic case that the sender retransmits only when a packet is known for certain to be lost. (Again, this assumption is a bit of a stretch. However, it is possible that the sending host might set its timeout large enough to be virtually assured that a packet that has not been acknowledged has been lost.) In this case, the performance might look something like that shown in <a class="xref" href="#P700101195200000000000000000165B" data-foobar="1"><span class="label">Figure</span> <span class="number">3.46(b)</span></a>. To appreciate what is happening here, consider the case that the offered load, <span class="inlineequation"><m:math display="inline" alttext="" data-uri="" altimg-width="25" altimg-height="17" altimg="../images/ch03math33.png"><m:mrow><m:msub><m:msup><m:mi>λ</m:mi><m:mo>′</m:mo></m:msup><m:mrow><m:mtext>in</m:mtext></m:mrow></m:msub></m:mrow></m:math></span> (the rate of original data transmission plus retransmissions), equals <i>R/2</i>. According to <a class="xref" href="#P700101195200000000000000000165B" data-foobar="1"><span class="label">Figure</span> <span class="number">3.46(b)</span></a>, at this value of the offered load, the rate at which data<span class="pagebreak" title="265" id="P700101195200000000000000000165A" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000165A" epub:type="pagebreak" role="doc-pagebreak"></span></p>
<figure id="P700101195200000000000000000165B" class="figure" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000165B">
<img alt="Illustration of scenario 2 performance with finite buffers." height="274" width="867" aria-describedby="P700101195200000000000000000165F" id="P700101195200000000000000000A4D0" data-uri="P7001011952000000000000000005573" src="../images/4055103046.png"></img>
<figcaption id="P700101195200000000000000000A4D1" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4D1"><header><h1 class="title" id="P700101195200000000000000000A4D2" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4D2" epub:type="title"><span class="label">Figure </span><span class="number">3.46</span> Scenario 2 performance with finite buffers</h1></header>

</figcaption>
</figure><div class="longdesc" id="P700101195200000000000000000165F" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000165F" aria-hidden="false"><a class="xref" aria-hidden="false" href="../longalt/la_4055103046.xhtml#la_4055103046"><span class="label">Description</span></a></div>
<p class="continued" id="P700101195200000000000000000A4D5" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4D5">are delivered to the receiver application is <i>R</i>/3. Thus, out of the 0.5<i>R</i> units of data transmitted, 0.333<i>R</i> bytes/sec (on average) are original data and 0.166<i>R</i> bytes/sec (on average) are retransmitted data. <i>We see here another cost of a congested network—the sender must perform retransmissions in order to compensate for dropped (lost) packets due to buffer overflow.</i></p>
<p id="P700101195200000000000000000A4D6" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4D6">Finally, let us consider the case that the sender may time out prematurely and retransmit a packet that has been delayed in the queue but not yet lost. In this case, both the original data packet and the retransmission may reach the receiver. Of course, the receiver needs but one copy of this packet and will discard the retransmission. In this case, the work done by the router in forwarding the retransmitted copy of the original packet was wasted, as the receiver will have already received the original copy of this packet. The router would have better used the link transmission capacity to send a different packet instead. <i>Here then is yet another cost of a congested network—unneeded retransmissions by the sender in the face of large delays may cause a router to use its link bandwidth to forward unneeded copies of a packet.</i> <a class="xref" href="#P700101195200000000000000000165B" data-foobar="1"><span class="label">Figure</span> <span class="number">3.46 (c)</span></a> shows the throughput versus offered load when each packet is assumed to be forwarded (on average) twice by the router. Since each packet is forwarded twice, the throughput will have an asymptotic value of <i>R</i>/4 as the offered load approaches <i>R/</i>2.</p>
</section>
<section id="P7001011952000000000000000001664" data-uri="M03_KURO4140_07_SE_C03.xhtml#P7001011952000000000000000001664" class="level3"><header><h1 class="title" id="P700101195200000000000000000A4D7" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4D7" epub:type="title">Scenario 3: Four Senders, Routers with Finite Buffers, and Multihop Paths</h1></header>
<p id="P700101195200000000000000000A4D8" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4D8">In our final congestion scenario, four hosts transmit packets, each over overlapping two-hop paths, as shown in <a class="xref" href="#P7001011952000000000000000001668" data-foobar="1"><span class="label">Figure</span> <span class="number">3.47</span></a>. We again assume that each host uses a timeout/retransmission mechanism to implement a reliable data transfer service, that all hosts have the same value of λ<sub>in</sub>, and that all router links have capacity <i>R</i> bytes/sec.<span class="pagebreak" title="266" id="P7001011952000000000000000001667" data-uri="M03_KURO4140_07_SE_C03.xhtml#P7001011952000000000000000001667" epub:type="pagebreak" role="doc-pagebreak"></span></p>
<figure id="P7001011952000000000000000001668" class="figure" data-uri="M03_KURO4140_07_SE_C03.xhtml#P7001011952000000000000000001668">
<img alt="Illustration of four senders, routers with finite buffers, and multihop paths." height="594" width="723" aria-describedby="P700101195200000000000000000166C" id="P700101195200000000000000000A4D9" data-uri="P7001011952000000000000000005574" src="../images/4055103047.png"></img>
<figcaption id="P700101195200000000000000000A4DA" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4DA"><header><h1 class="title" id="P700101195200000000000000000A4DB" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4DB" epub:type="title"><span class="label">Figure </span><span class="number">3.47</span> Four senders, routers with finite buffers, and multihop paths</h1></header>

</figcaption>
</figure><div class="longdesc" id="P700101195200000000000000000166C" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000166C" aria-hidden="false"><a class="xref" aria-hidden="false" href="../longalt/la_4055103047.xhtml#la_4055103047"><span class="label">Description</span></a></div>
<p id="P700101195200000000000000000A4DE" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4DE">Let’s consider the connection from Host A to Host C, passing through routers R1 and R2. The A–C connection shares router R1 with the D–B connection and shares router R2 with the B–D connection. For extremely small values of λ<sub>in</sub>, buffer overflows are rare (as in congestion scenarios 1 and 2), and the throughput approximately equals the offered load. For slightly larger values of λ<sub>in</sub>, the corresponding throughput is also larger, since more original data is being transmitted into the network and delivered to the destination, and overflows are still rare. Thus, for small values of λ<sub>in</sub>, an increase in λ<sub>in</sub> results in an increase in λ<sub>out</sub>.</p>
<p id="P700101195200000000000000000A4DF" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4DF">Having considered the case of extremely low traffic, let’s next examine the case that λ<sub>in</sub> (and hence <span class="inlineequation"><m:math display="inline" alttext="" data-uri="" altimg-width="25" altimg-height="17" altimg="../images/ch03math34.png"><m:mrow><m:msub><m:msup><m:mi>λ</m:mi><m:mo>′</m:mo></m:msup><m:mrow><m:mtext>in</m:mtext></m:mrow></m:msub></m:mrow></m:math></span>) is extremely large. Consider router R2. The A–C traffic arriving to router R2 (which arrives at R2 after being forwarded from R1) can have an arrival rate at R2 that is at most <i>R,</i> the capacity of the link from R1 to R2, regardless of the value of λ<sub>in</sub>. If <span class="inlineequation"><m:math display="inline" alttext="" data-uri="" altimg-width="25" altimg-height="17" altimg="../images/ch03math35.png"><m:mrow><m:msub><m:msup><m:mi>λ</m:mi><m:mo>′</m:mo></m:msup><m:mrow><m:mtext>in</m:mtext></m:mrow></m:msub></m:mrow></m:math></span> is extremely large for all connections (including the<span class="pagebreak" title="267" id="P7001011952000000000000000001671" data-uri="M03_KURO4140_07_SE_C03.xhtml#P7001011952000000000000000001671" epub:type="pagebreak" role="doc-pagebreak"></span></p>
<figure id="P7001011952000000000000000001672" class="figure" data-uri="M03_KURO4140_07_SE_C03.xhtml#P7001011952000000000000000001672">
<img alt="Illustration of scenario 3 performance with finite buffers and multihop paths." height="265" width="374" aria-describedby="P7001011952000000000000000001676" id="P700101195200000000000000000A4E0" data-uri="P7001011952000000000000000005575" src="../images/4055103048.png"></img>
<figcaption id="P700101195200000000000000000A4E1" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4E1"><header><h1 class="title" id="P700101195200000000000000000A4E2" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4E2" epub:type="title"><span class="label">Figure </span><span class="number">3.48</span> Scenario 3 performance with finite buffers and multihop paths</h1></header>

</figcaption>
</figure><div class="longdesc" id="P7001011952000000000000000001676" data-uri="M03_KURO4140_07_SE_C03.xhtml#P7001011952000000000000000001676" aria-hidden="false"><a class="xref" aria-hidden="false" href="../longalt/la_4055103048.xhtml#la_4055103048"><span class="label">Description</span></a></div>
<p class="continued" id="P700101195200000000000000000A4E4" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4E4">B–D connection), then the arrival rate of B–D traffic at R2 can be much larger than that of the A–C traffic. Because the A–C and B–D traffic must compete at router R2 for the limited amount of buffer space, the amount of A–C traffic that successfully gets through R2 (that is, is not lost due to buffer overflow) becomes smaller and smaller as the offered load from B–D gets larger and larger. In the limit, as the offered load approaches infinity, an empty buffer at R2 is immediately filled by a B–D packet, and the throughput of the A–C connection at R2 goes to zero. This, in turn, <i>implies that the A–C end-to-end throughput goes to zero</i> in the limit of heavy traffic. These considerations give rise to the offered load versus throughput tradeoff shown in <a class="xref" href="#P7001011952000000000000000001672" data-foobar="1"><span class="label">Figure</span> <span class="number">3.48</span></a>.</p>
<p id="P700101195200000000000000000A4E5" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4E5">The reason for the eventual decrease in throughput with increasing offered load is evident when one considers the amount of wasted work done by the network. In the high-traffic scenario outlined above, whenever a packet is dropped at a second-hop router, the work done by the first-hop router in forwarding a packet to the second-hop router ends up being “wasted.” The network would have been equally well off (more accurately, equally bad off) if the first router had simply discarded that packet and remained idle. More to the point, the transmission capacity used at the first router to forward the packet to the second router could have been much more profitably used to transmit a different packet. (For example, when selecting a packet for transmission, it might be better for a router to give priority to packets that have already traversed some number of upstream routers.) <i>So here we see yet another cost of dropping a packet due to congestion—when a packet is dropped along a path, the transmission capacity that was used at each of the upstream links to forward that packet to the point at which it is dropped ends up having been wasted.</i></p>
</section>
</section>
<section id="P700101195200000000000000000167A" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000167A" class="level2"><header><h1 class="title" id="P700101195200000000000000000A4E6" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4E6" epub:type="title"><span class="pagebreak" title="268" id="P700101195200000000000000000167C" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000167C" epub:type="pagebreak" role="doc-pagebreak"></span><span class="number">3.6.2</span> Approaches to Congestion Control</h1></header>
<p id="P700101195200000000000000000A4E7" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4E7">In <a class="xref" href="fileP700101195200000000000000000168F.xhtml#P700101195200000000000000000168F" data-foobar="7"><span class="label">Section</span> <span class="number">3.7</span></a>, we’ll examine TCP’s specific approach to congestion control in great detail. Here, we identify the two broad approaches to congestion control that are taken in practice and discuss specific network architectures and congestion-control protocols embodying these approaches.</p>
<p id="P700101195200000000000000000A4E8" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4E8">At the highest level, we can distinguish among congestion-control approaches by whether the network layer provides explicit assistance to the transport layer for congestion-control purposes:</p>
<ul id="P700101195200000000000000000A4E9" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4E9">
<li id="P700101195200000000000000000A4EA" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4EA"><p id="P700101195200000000000000000A4EB" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4EB"><span class="leadin">End-to-end congestion control.</span> In an end-to-end approach to congestion control, the network layer provides no explicit support to the transport layer for congestion-control purposes. Even the presence of network congestion must be inferred by the end systems based only on observed network behavior (for example, packet loss and delay). We’ll see shortly in <a class="xref" href="fileP700101195200000000000000000168F.xhtml#P7001011952000000000000000001744" data-foobar="7"><span class="label">Section</span> <span class="number">3.7.1</span></a> that TCP takes this end-to-end approach toward congestion control, since the IP layer is not required to provide feedback to hosts regarding network congestion. TCP segment loss (as indicated by a timeout or the receipt of three duplicate acknowledgments) is taken as an indication of network congestion, and TCP decreases its window size accordingly. We’ll also see a more recent proposal for TCP congestion control that uses increasing round-trip segment delay as an indicator of increased network congestion</p></li>
<li id="P700101195200000000000000000A4EC" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4EC"><p id="P700101195200000000000000000A4ED" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4ED"><span class="leadin">Network-assisted congestion control.</span> With network-assisted congestion control, routers provide explicit feedback to the sender and/or receiver regarding the congestion state of the network. This feedback may be as simple as a single bit indicating congestion at a link – an approach taken in the early IBM SNA <a class="biblioref" href="fileP70010119520000000000000000037E0.xhtml#P7001011952000000000000000003BFD" data-foobar="7">[Schwartz 1982]</a>, DEC DECnet <a class="biblioref" href="fileP70010119520000000000000000037E0.xhtml#P70010119520000000000000000039B6" data-foobar="7">[Jain 1989</a>; <a class="biblioref" href="fileP70010119520000000000000000037E0.xhtml#P7001011952000000000000000003AC8" data-foobar="7">Ramakrishnan 1990]</a> architectures, and ATM <a class="biblioref" href="fileP70010119520000000000000000037E0.xhtml#P7001011952000000000000000003841" data-foobar="7">[Black 1995]</a> network architectures. More sophisticated feedback is also possible. For example, in <span class="keyword" id="P7001011952000000000000000001684" data-uri="M03_KURO4140_07_SE_C03.xhtml#P7001011952000000000000000001684"><b>ATM Available Bite Rate (ABR)</b></span> congestion control, a router informs the sender of the maximum host sending rate it (the router) can support on an outgoing link. As noted above, the Internet-default versions of IP and TCP adopt an end-to-end approach towards congestion control. We’ll see, however, in <a class="xref" href="fileP700101195200000000000000000168F.xhtml#P7001011952000000000000000001762" data-foobar="7"><span class="label">Section</span> <span class="number">3.7.2</span></a> that, more recently, IP and TCP may also optionally implement network-assisted congestion control.</p></li>
</ul>
<p id="P700101195200000000000000000A4EE" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4EE">For network-assisted congestion control, congestion information is typically fed back from the network to the sender in one of two ways, as shown in <a class="xref" href="#P7001011952000000000000000001687" data-foobar="1"><span class="label">Figure</span> <span class="number">3.49</span></a>. Direct feedback may be sent from a network router to the sender. This form of notification typically takes the form of a choke packet (essentially saying, “I’m congested!”). The second and more common form of notification occurs when a router marks/updates a field in a packet flowing from sender to receiver to indicate congestion. Upon receipt of a marked packet, the receiver then notifies the sender of the congestion indication. This latter form of notification takes a full round-trip time.<span class="pagebreak" title="269" id="P7001011952000000000000000001686" data-uri="M03_KURO4140_07_SE_C03.xhtml#P7001011952000000000000000001686" epub:type="pagebreak" role="doc-pagebreak"></span></p>
<figure id="P7001011952000000000000000001687" class="figure" data-uri="M03_KURO4140_07_SE_C03.xhtml#P7001011952000000000000000001687">
<img alt="Illustration of two feedback pathways for network-indicated congestion information." height="348" width="649" aria-describedby="P700101195200000000000000000168B" id="P700101195200000000000000000A4EF" data-uri="P7001011952000000000000000005576" src="../images/4055103049.png"></img>
<figcaption id="P700101195200000000000000000A4F0" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4F0"><header><h1 class="title" id="P700101195200000000000000000A4F1" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000A4F1" epub:type="title"><span class="label">Figure </span><span class="number">3.49</span> Two feedback pathways for network-indicated congestion information</h1></header>

</figcaption>
</figure><div class="longdesc" id="P700101195200000000000000000168B" data-uri="M03_KURO4140_07_SE_C03.xhtml#P700101195200000000000000000168B" aria-hidden="false"><a class="xref" aria-hidden="false" href="../longalt/la_4055103049.xhtml#la_4055103049"><span class="label">Description</span></a></div>
</section>
</section></body></html>