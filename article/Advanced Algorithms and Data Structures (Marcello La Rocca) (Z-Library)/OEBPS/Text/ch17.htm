<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>17</title>
    
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
  <div class="tocheadb">
    <h1 class="tochead" id="heading_id_2"><a id="pgfId-998529"></a><a id="pgfId-998541"></a>17 Simulated annealing: Optimization beyond local minima</h1>
  </div>

  <p class="co-summary-head"><a id="pgfId-1010131"></a>This chapter covers</p>

  <ul class="calibre19">
    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1010175"></a>Introducing simulated annealing</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1010176"></a>Using simulated annealing to improve delivery schedules</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1010177"></a>Presenting a primer on the traveling salesman problem</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1010178"></a>Using simulated annealing for minimum crossing embeddings</li>

    <li class="co-summary-bullet"><a class="calibre14" id="pgfId-1010164"></a>Using an algorithm based on simulated annealing to draw graphs nicely</li>
  </ul>

  <p class="body"><a id="pgfId-998634"></a>If you have read chapters 15 and 16, you should by now be familiar with graph embeddings and optimization problems. In the previous chapter, in particular, we explained how to reformulate graph embedding as an optimization problem, and we introduced <i class="calibre17">gradient descent</i><a id="marker-1001679"></a>, an optimization technique that can be used to find (near-)optimal solutions to this category of problems. In particular, we discussed two solutions to the problem, seen as <i class="calibre17">crossing-number optimization</i><a id="marker-1001683"></a> and as a <i class="calibre17">force-directed graph drawing</i><a id="marker-1001687"></a>; gradient descent is particularly suitable for the latter, while particularly bad for the former.</p>

  <p class="body"><a id="pgfId-998661"></a>One issue we have seen with gradient descent is that it tends to get stuck in local minima, which is pretty much the last thing we would want, considering that we often have to deal with cost functions having lots of local peaks.</p>

  <p class="body"><a id="pgfId-998670"></a>We have already discussed one workaround for this issue, running gradient descent several times using random-restart to choose a different starting point each time.</p>

  <p class="body"><a id="pgfId-998679"></a>Still, even with this technique, each iteration of gradient descent is pretty much doomed (in the best-case scenario) to end in the closest local minimum along the steepest path from the starting point. It would be great, instead, if even a single run could have some non-null<a href="#pgfId-1005253"><sup class="footnotenumber">1</sup></a> probability to move over a local minimum and find a better solution.</p>

  <p class="body"><a id="pgfId-998695"></a>In this chapter, we’ll discuss an algorithm that does exactly that, and more. It also overcomes some of the constraints that limit the applicability of gradient descent. After reading this chapter, you will have learned about <i class="calibre17">simulated annealing</i><a id="marker-1001691"></a>, a powerful optimization technique, how to apply it to some of the hardest problems on graphs, and how to weigh pros and cons of choosing it over other optimization techniques.</p>

  <p class="body"><a id="pgfId-998715"></a>Simulated annealing is quite powerful as is: among the algorithms we have described, it’s the only one that’s able to balance narrow and broad search, exploring a large portion of the problem space and managing to make local progresses.</p>

  <p class="body"><a id="pgfId-998724"></a>Moreover, it’s been around since the 1970s, and that explains why it’s one of the favorite heuristics used in optimization. During the last 20–30 years, new optimization techniques have been developed, such as <i class="calibre17">genetic algorithms</i><a id="marker-1001695"></a> (which we’ll discuss in the next chapter), and <i class="calibre17">artificial immune systems</i><a id="id_Hlk56841124"></a><a id="marker-1001699"></a> (AIS), which use different biology-inspired approaches to speed up convergence, but the adoption ratio for simulated annealing remains high, and recently it has also been revived with<a id="id_Hlk56841161"></a> <i class="calibre17">quantum annealing</i><a id="marker-1001703"></a>.</p>

  <p class="body"><a id="pgfId-998752"></a>Last but not least, you’ll improve your ability to plan deliveries over several destinations by learning about the <i class="calibre17">traveling salesman problem</i> (TSP<a id="marker-1014610"></a> for short) and how to find near-optimal solutions in a reasonable time. Continuing our discussion on optimizing logistics for our imaginary e-commerce company, we will move from planning a single trip for each delivery to optimizing the route of a delivery truck across several cities, so that it can be loaded once at the company’s warehouse and make several deliveries without going back.</p>

  <h2 class="fm-head" id="heading_id_3"><a id="pgfId-998774"></a>17.1 Simulated annealing</h2>

  <p class="body"><a id="pgfId-998788"></a>In chapter 16 we introduced local optimization techniques as a way to improve over mere random sampling algorithms. We discussed gradient descent at length and alluded to a randomized optimization technique. If you remember our analogy to marble races, gradient descent would always move a marble (our solution) along the steepest path on the track (the landscape produced by the cost function), but stop in valleys. Figure 17.1 contrasts the gradient descent approach with the random search local optimization called <i class="calibre17">Hill descent.<a href="#pgfId-1005265"><sup class="footnotenumber">2</sup></a></i><a id="id_Hlk56841234"></a><a id="marker-1001711"></a> In this case, the algorithm picks a direction randomly and checks if by making a short (possibly random) step in that direction we get to an improvement. If so, it moves to the new solution; otherwise, no change is made, and another attempt is performed at the next iteration. As we also saw in chapter 16, for gradient descent it is, in theory, possible to step over local minima, if the learning rate is large enough; however, this is also unlikely to happen.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F1.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1020153"></a>Figure 17.1 Gradient descent versus local random optimization. While for gradient descent, steps are proportional to the curve’s steepness, the random steps are . . . just random (within a reasonable radius). That’s why it’s easier for the random algorithm to step over a local minimum, or even get out of it (technically, both are possible for gradient descent as well, as we saw in chapter 16, depending on the learning rate and shape of the cost function).</p>

  <p class="body"><a id="pgfId-998845"></a>Be warned, though, that in figure 17.1 we are not showing all the failed attempts where local random optimization tried the wrong direction: overall, getting to the same point will need more steps than gradient descent, because instead of going in the direction of maximum change, we are wandering around randomly. This is more apparent looking at a 2-D domain/3-D surface, like the one shown in figure 17.2.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F2.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1020202"></a>Figure 17.2 Gradient descent versus local random optimization on a cost function of two parameters. Can you guess which one is going to take more steps?</p>

  <p class="body"><a id="pgfId-998883"></a>As it is, this approach shares some of the same issues we discussed in chapter 16 about gradient descent. It is likely to get stuck in local minima and plateaus. And, even worse, it’s taking a slower way to get there.</p>

  <p class="body"><a id="pgfId-998896"></a>On the plus side, it also has some advantages. First and foremost, we can release the constraint about the cost function being differentiable, and can actually even ignore the definition of the function, as long as we have a way to compute it.<a href="#pgfId-1005288"><sup class="footnotenumber">3</sup></a> This is especially useful with functions such as the edge crossing of an embedding, a step function which is not differentiable in the points where its value abruptly changes and has a derivative identically equal to zero elsewhere.</p>

  <p class="body"><a id="pgfId-998913"></a>The obvious downside, as shown in figure 17.2, is that the random optimization is going to require many more steps than gradient descent, because it will take long detours. Even worse, since it will not take the direction where the function decreases faster, it will likely end up in a different spot than gradient descent. It’s impossible to predict where, and whether it will find a better or worse final value. To cope with this, the only strategy is perseverance: increasing the number of runs and storing the best result across them is likely to bring to a satisfying result (the more runs, the better the expected solution).</p>

  <p class="body"><a id="pgfId-998936"></a>Finally, there are issues faced in gradient descent that not even the randomized method can shake off.</p>

  <p class="body"><a id="pgfId-998945"></a>For instance, while it’s possible that the random optimization algorithm gets out of a local minimum, it’s not guaranteed. Since both algorithms are greedy,<a href="#pgfId-1005304"><sup class="footnotenumber">4</sup></a> they only move from the current position when they find a domain point to which a lower cost corresponds; it’s possible that the range of the random steps is not wide enough to get it out of a pit. You can see this in figure 17.1, where the “leap” the algorithm has to make to get itself out of a local minimum is pretty large compared to the other updates. If such a large delta is even allowed (depending on how the algorithm is configured through its hyper-parameters), it will likely take a lot of random attempts before it generates a step that is both in the right direction and sufficiently far away.</p>

  <h3 class="fm-head2" id="heading_id_4"><a id="pgfId-998966"></a>17.1.1 Sometimes you need to climb up to get to the bottom</h3>

  <p class="body"><a id="pgfId-998990"></a>An example of an even more troubling configuration for the local optimization algorithm is shown in figure 17.3. The gap between the current minimum where the randomized algorithm is stuck and the next point in the cost function’s landscape with a lower value (the closest improvement) is way too far away to get there in a single step. What we would need is for the algorithm to be able to say, fine, it doesn’t matter if I’m in a minimum; I’ll climb over this hill and see if the next valley (or the one after the next) is deeper.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F3.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1020363"></a>Figure 17.3 A limitation that the random local optimization algorithm shares with gradient descent is that it only moves downhill. If the next position with a better value for the cost function is too far, the algorithm won’t be able to reach it in a single step, and so it will be stuck.</p>

  <p class="body"><a id="pgfId-999023"></a>Of course, there are many ways to do that. For instance, we could store somewhere the best solution we have found and keep exploring, moving past a local minimum. Another option could be deciding that sometimes a step uphill is fine. We can plan it systematically, every few steps, or probabilistically, accepting a step uphill with a certain probability.</p>

  <p class="body"><a id="pgfId-999036"></a>Simulated annealing uses the latter approach; while in its original formulation it doesn’t keep track of the best solutions found, this can be easily added.</p>

  <p class="body"><a id="pgfId-999049"></a>This heuristic<a href="#pgfId-1005318"><sup class="footnotenumber">5</sup></a> takes its name from a technique used in metallurgy, <i class="calibre17">annealing</i><a id="marker-1001715"></a>, that consists of repeated cycles where a material is heated up and then controllably cooled to improve its strength: something like a blacksmith forging an iron sword by quenching it in cold water, but in a more controlled way (and without water!).</p>

  <p class="body"><a id="pgfId-999063"></a>Simulated annealing does something quite similar. In its simplest version it consists of a single cooling phase, but variants exist that alternate phases where the temperature has risen, with others when the system has cooled.</p>

  <p class="body"><a id="pgfId-999076"></a>The system’s temperature, in turn, is directly connected to the energy of the simulated system and the probability that it is allowed to transition to a higher-energy state (in other words, to a solution for which the cost function takes higher values).</p>

  <p class="body"><a id="pgfId-999085"></a>Figure 17.4 shows a possible path on a 3-D surface that resembles the progress of simulated annealing optimization. This time, the algorithm is capable of getting itself out of a local minimum, even by taking a counter intuitive step uphill: that’s the difference compared to greedy algorithms (including the ones summarized in section 17.1) and the true advantage of this technique.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F4.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1020522"></a>Figure 17.4 The ups and downs of a possible path followed by simulated annealing optimization. Be warned, this is an artificial example. It’s likely that in real runs, in the initial phases, the algorithm would move all over the place, while later, when the temperature drops, it will converge toward one of the minima.</p>

  <p class="body"><a id="pgfId-999117"></a>The path shown in figure 17.4, however, though possible,<a href="#pgfId-1005341"><sup class="footnotenumber">6</sup></a> is not typical of simulated annealing. A more common path would be much more chaotic, and would have made the figure a bit too messy.</p>

  <p class="body"><a id="pgfId-999134"></a>The algorithm, in real applications, is more likely to initially jump back and forth, exploring several areas of the landscape, and often moving uphill. Then, as cooling progresses, it will be less likely to transition to higher positions, and at the same time we can directly or indirectly reduce the length of the random steps. Overall, after the initial exploration phase, it should enter a fine-tuning phase.</p>

  <p class="body"><a id="pgfId-999147"></a>An example of the whole process is shown in figure 17.5. As you can see, I was not exaggerating about how chaotic it looks!</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F5.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1020405"></a>Figure 17.5 A more realistic example of a possible simulated annealing path to the minimum for a function of a single parameter. Initially steps are larger and the probability of accepting a worse result is higher. As the cooling progresses, fine-tuning kicks-in, with smaller steps only (or mostly) towards better results.</p>

  <h3 class="fm-head2" id="heading_id_5"><a id="pgfId-999183"></a><a id="id_Hlk40973885"></a>17.1.2 Implementation</h3>

  <p class="body"><a id="pgfId-999195"></a>So, <a id="marker-1020500"></a><a id="marker-1020501"></a>enough talking about how cool this algorithm is; let’s look at some pseudo-code. Listing 17.1 presents the algorithm, and a JavaScript implementation, provided as part of the JsGraphs library, is linked on the book’s repo on GitHub.<a href="#pgfId-1005353"><sup class="footnotenumber">7</sup></a></p>

  <p class="fm-code-listing-caption"><a id="pgfId-1010308"></a>Listing 17.1 A generic implementation of simulated annealing</p>
  <pre class="programlisting"><b class="strong">function</b> simulatedAnnealing(C, P0, T0, acceptance, maxSteps)     <span class="fm-combinumeral">❶</span>
  <b class="strong">for</b> k <b class="strong">in</b> {1..maxSteps} <b class="strong">do</b>                                      <span class="fm-combinumeral">❷</span>
    T ← temperature(T0, k, maxSteps)                             <span class="fm-combinumeral">❸</span>
    P ← randomStep(P0.clone(), T)                                <span class="fm-combinumeral">❹</span>
    <b class="strong">if</b> acceptance(C(P), C(P0), T) &gt; randomFloat(0,1) <b class="strong">then</b>        <span class="fm-combinumeral">❺</span>
      P0 ← P    
  <b class="strong">return</b> P0</pre>

  <p class="fm-code-annotation"><a id="pgfId-1012788"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">simulatedAnnealing</code> <a id="marker-1019265"></a>takes a cost function <code class="fm-code-in-text2">C</code>, a starting point <code class="fm-code-in-text2">P0</code>, a starting temperature <code class="fm-code-in-text2">T0</code>, a probability function <code class="fm-code-in-text2">acceptance</code>,<a id="marker-1019267"></a> and the maximum number of iterations to perform, <code class="fm-code-in-text2">maxSteps</code><a id="marker-1019268"></a>. It returns a point in the domain: ideally the point where <code class="fm-code-in-text2">C</code> has the smallest value (either locally or globally).</p>

  <p class="fm-code-annotation"><a id="pgfId-1012822"></a><span class="fm-combinumeral">❷</span> Starts the iteration, running the main cycle at most <code class="fm-code-in-text2">maxSteps</code> times</p>

  <p class="fm-code-annotation"><a id="pgfId-1012853"></a><span class="fm-combinumeral">❸</span> Sets the temperature of the system, based on the initial temperature and current iteration</p>

  <p class="fm-code-annotation"><a id="pgfId-1012924"></a><span class="fm-combinumeral">❹</span> Creates a new point <code class="fm-code-in-text2">P</code> in the domain, to which the system should transition</p>

  <p class="fm-code-annotation"><a id="pgfId-1012893"></a><span class="fm-combinumeral">❺</span> If the probability of transitioning from <code class="fm-code-in-text2">P0</code> to <code class="fm-code-in-text2">P</code> is higher than a random floating-point number, drawn between <code class="fm-code-in-text2">0</code> and <code class="fm-code-in-text2">1</code>, then updates current state to <code class="fm-code-in-text2">P</code></p>

  <p class="body"><a id="pgfId-999411"></a>The algorithm looks beautifully simple, doesn’t it? And quite concise, although, as always, we try to present generic methods as templates, and when possible, we abstract as many sub-routines as possible into helper methods that can later be implemented according to the context.</p>

  <p class="body"><a id="pgfId-999422"></a>In this case, there are three of them; in the next section we’ll discuss both the function computing how the temperature evolves and the one giving the probability of acceptance of a transition. Now, let’s discuss function <code class="fm-code-in-text">randomStep</code><a id="marker-1001739"></a>, which allows us to build the next tentative solution to which the algorithm could transition.</p>

  <p class="body"><a id="pgfId-999438"></a>This function needs, obviously, to be domain-dependent: the size of the problem space and the type of solutions will determine how we can change the current solution (a point in the problem space). For the graph-embedding problem, for instance, we can randomly move each vertex along both axes, within the maximum area in which the graph must be embedded.</p>

  <p class="body"><a id="pgfId-999447"></a>But as you can see at line #4, we added a dependency on the temperature for this function!</p>

  <p class="body"><a id="pgfId-999456"></a>We mentioned in the previous section that you can adjust the length of the random step directly or indirectly. The short answer is that you want to be careful if you do it directly, but it’s an option. To see why, we first have to better explain in detail the other two functions and understand why simulated annealing <a id="marker-1001743"></a><a id="marker-1001747"></a>works.</p>

  <h3 class="fm-head2" id="heading_id_6"><a id="pgfId-999476"></a>17.1.3 Why simulated annealing works</h3>

  <p class="body"><a id="pgfId-999494"></a><a id="id_Hlk40975196"></a>If <a id="marker-1001751"></a><a id="marker-1001755"></a>you feel that the way simulated annealing works is slightly counterintuitive, well, you are not alone. After all, if the algorithm can make large steps, and it can also move to worse positions without making any use of any knowledge of the cost function (either previous or acquired while running), how do we know it will end up in the area of the global minimum and not get stuck in some sort of funnel to a local minimum when we start reducing temperature? Well, we don’t. But with most landscapes, if we run the algorithm long enough and find the right configuration, in practice it will get pretty close to the global optimum and outmatch gradient descent.</p>

  <p class="body"><a id="pgfId-999511"></a>There are lots of “ifs” in that proposition. The truth is that, like most heuristic algorithms, it works well in practice when it’s in the hands of someone who knows how to tune it properly and has enough computing time to make it run for lots of iterations.</p>

  <p class="body"><a id="pgfId-999524"></a>Still, if that’s the case, then simulated annealing is a great alternative for scenarios where gradient descent would suffer. It’s not that one is better than the other; like all tools, there are problems for which gradient descent works better, and others where it would be hard or impossible to apply it. But let’s see in detail why it does work.</p>

  <p class="body"><a id="pgfId-999550"></a>The key to the algorithm is the probabilistic mechanism that allows the algorithm to move uphill (lines #2–5), toward worse values for the cost function, and in particular the function that spits out the probability of accepting a positive delta, depending on temperature and on the magnitude of this delta.</p>

  <p class="body"><a id="pgfId-999559"></a>Assuming that at a given iteration, when temperature has the value <code class="fm-code-in-text">T</code>, the algorithm attempts to transition from current point <code class="fm-code-in-text">P<sub class="subscript1">0</sub></code> to a point <code class="fm-code-in-text">P</code>, then we can express this probability function <code class="fm-code-in-text">A(P<sub class="subscript1">0</sub>, P, T)</code> as</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F_EQ1.png"/></p>

  <p class="body"><a id="pgfId-999589"></a>where <code class="fm-code-in-text">C(P)</code> is the cost of solution <code class="fm-code-in-text">P</code> (similarly for <code class="fm-code-in-text">P<sub class="subscript1">0</sub></code>), and <code class="fm-code-in-text">k</code> is a constant that must be calibrated on the initial temperature and max delta in the cost function, so that in the initial phase of the algorithm (when the system’s temperature is close to the initial temperature), the probability of acceptance of a positive delta is close to 1 for any two states.</p>

  <p class="body"><a id="pgfId-999607"></a>Typically, the probability of transitioning to a lower-energy state is set to 1, so that such a transition is always allowed in any phase of the simulation.</p>

  <p class="body"><a id="pgfId-999616"></a>But of course, this is not the only way to define this function. It is, however, a typical definition for the acceptance probability, because it stems directly from the metallurgic analogy: this formula is directly inspired by the <a id="id_Hlk56842157"></a>Boltzmann distribution that measures the probability that a system is in a state with a certain energy and temperature.<a href="#pgfId-1005366"><sup class="footnotenumber">8</sup></a> But instead of an absolute value for the energy, for simulated annealing we consider a variation from a lower to a higher energy state.</p>

  <p class="body"><a id="pgfId-999649"></a>Now, what’s the effect of this probability distribution on the single step of the algorithm? Let’s consider the case where the amplitude of the update step in the problem space is not constrained (so we could even move between opposite corners of the domain) and take a look at figure 17.6.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F6.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1020566"></a>Figure 17.6 The effect of temperature on the probability of transitioning to a higher-energy state, and in turn on the annealing algorithm. (A) Initially, the temperature is high enough that any transition is allowed. At this stage, the algorithm behaves like random sampling (without storing the best result) and explores a large section of the domain. (B) When the system cools down, transitions to higher states become unlikely, and above some threshold they become so unlikely as to be considered forbidden. Transitions to lower states are always allowed. (C) When we are close to the end of the annealing simulation, the probability of moving to higher-energy states is so slim as to be negligible; the system can only move to better solutions.</p>

  <p class="body"><a id="pgfId-999658"></a>Initially, when the temperature of the system is high (figure 17.6 (A)), and by construction the probability distribution should allow almost any update (no matter how bad it seems), the algorithm can move all over the landscape and even get out of local minima, even if this means that it might abandon an optimal position for one of the worst negative peaks. Actually, at this stage, it is even easy for the algorithm to walk away from good solutions.</p>

  <p class="body"><a id="pgfId-999708"></a>This mimics high-energy systems, where particles (or molecules) move chaotically in all directions.</p>

  <p class="body"><a id="pgfId-999717"></a>As the system cools down, the distribution changes (figure 17.7 (B)). Going uphill becomes less likely, and some positions above a certain delta (with respect to their cost) become completely unreachable.</p>

  <p class="body"><a id="pgfId-999730"></a>Finally, when temperature gets close to the <code class="fm-code-in-text">T<sub class="subscript1">LOW</sub></code>, the halting temperature (or equivalently, as we’ll see, when we are close to the maximum number of iterations), going uphill becomes so unlikely as to be basically forbidden, and the algorithm can only transition to lower-energy states. It behaves like <i class="calibre17">Hill descent</i><a id="marker-1001759"></a>.<a href="#pgfId-1005387"><sup class="footnotenumber">9</sup></a> Looking at figure 17.6 (C), however, you can see that there is a difference, and transitions to points far away in the domain are allowed if they correspond to a lower cost. This means the algorithm can still get out of local minima (and it’s hopefully even probable that it will!) and converge to a local optimum.</p>

  <p class="body"><a id="pgfId-999750"></a>Notice that the acceptance or rejection of a transition to a new state is not related to the distance of the new state from the current one in the problem space. It’s only based on the energy levels of the two states, which in turn are given by (or at least proportional to) the value of the cost function in those states.</p>

  <p class="body"><a id="pgfId-1015102"></a>Even more importantly, after the very initial stages where basically all transitions are allowed, then the further uphill a new state is, the less likely it becomes for the transition to be accepted. This is why the algorithm works well, because it progressively encourages transitions toward areas where cost is lower (see figure 17.6 (B–C)), while not limiting the search to the neighborhood of the current position in the problem space.</p>

  <p class="body"><a id="pgfId-999781"></a>Why do transitions uphill work? Because when going uphill, it might be able to go over a cliff and reach a deeper valley. While it might seem counterintuitive on a 2-D chart, this becomes even more relevant in high-dimensional space.</p>

  <p class="body"><a id="pgfId-999792"></a>Similar to the metallurgic process, however, cooling down the system with the right pace is instrumental to the quality of the final result.</p>

  <p class="body"><a id="pgfId-999801"></a>And, of course, being a merely stochastic process, it also needs some “luck”; especially toward the end, many random steps will produce transitions uphill, and as such will be rejected. But if we try hard enough, for many iterations, chances are that a positive change will be randomly found.</p>

  <p class="body"><a id="pgfId-999814"></a>And that’s the strength of the algorithm, and, at the same time, its weakness. We can reach an improvement, but since we discard progressively more and more attempted updates, we’ll need several iterations (technically: a lot of them!) and, of course, a good function for the random steps to probe the problem space.</p>

  <p class="body"><a id="pgfId-999827"></a>This makes the algorithm particularly slow and resource-consuming, especially if iterations (generating a random point and evaluating the cost function) are expensive to <a id="marker-1001763"></a><a id="marker-1001767"></a>compute.</p>

  <h3 class="fm-head2" id="heading_id_7"><a id="pgfId-999839"></a>17.1.4 Short-range vs long-range transitions</h3>

  <p class="body"><a id="pgfId-999855"></a>Now, <a id="marker-1001771"></a><a id="marker-1001775"></a>again, the question arises: Do we want to progressively limit the range of transitions, and in turn how far the algorithm can move at each step in the domain space?</p>

  <p class="body"><a id="pgfId-999867"></a>We could make the length of the maximum update step dependent on the temperature parameter <code class="fm-code-in-text">T</code>, and as such, it would shrink in time.</p>

  <p class="body"><a id="pgfId-999878"></a>But the interesting part is that even if we keep the same maximum step length for the whole process, there is some kind of indirect reduction while the temperature cools. As shown in figure 17.6, the filtering actually happens on the co-domain of the cost function, but the indirect effect is limiting the domain to a subset of the original problem space; and, in turn, the effect is the same as water running down funnels—it gets channeled and velocity slows down (as pressure builds up).</p>

  <p class="body"><a id="pgfId-999897"></a>If we further restrict the acceptable transitions based on proximity along the problem space, on one hand we get a greater number of attempted updates in the area surrounding the current point, which could be good if we are close to the global minimum, because it would speed up convergence.</p>

  <p class="body"><a id="pgfId-999910"></a>On the other hand, we would likely lose the ability to get out of local minima, which is the best reason to use simulated annealing in the first place.</p>

  <p class="body"><a id="pgfId-999919"></a>Therefore, one “safe” solution is to implement the <code class="fm-code-in-text">randomStep</code><a id="marker-1001779"></a> method as pure, random sampling; however, this means that we will need a lot of iterations to find a transition downhill, and the algorithm will keep bouncing between valleys without focusing on fine-tuning. An interesting compromise could be increasing the probability of small steps, but still allowing far-reaching updates, and even trying them every few iterations. This can be done in combination with shrinking the range of the local-search (the small steps) as the temperature cools down.</p>

  <p class="body"><a id="pgfId-999937"></a>The last thing we need to discuss is how to update the temperature. Similar to the acceptance probability, there are several possible viable options for this function—there are no restrictions on it.</p>

  <p class="body"><a id="pgfId-999948"></a>Typically, however, geometric (aka exponential) decay is used, with the temperature value updated not every iteration, but after some interval (for instance, every 1000 iterations or so).</p>

  <p class="body"><a id="pgfId-999957"></a>The mathematical formula for this function is</p>

  <p class="fm-equation"><i class="calibre17">T</i><sub class="subscript"><i class="calibre15">i</i></sub> = <span class="cambria">α</span><i class="calibre17">T</i><sub class="subscript"><i class="calibre15">i</i> – 1</sub>, 0 &lt; <span class="cambria">α</span> &lt; 1</p>

  <p class="body"><a id="pgfId-999975"></a>The temperature at iteration <code class="fm-code-in-text">i</code> is a fraction of the temperature at iteration <code class="fm-code-in-text">i-1;</code> <span class="cambria">α</span>, which must be between <code class="fm-code-in-text">0</code> and <code class="fm-code-in-text">1</code>, controls how much cooler the temperature gets between two iterations.</p>

  <p class="body"><a id="pgfId-999994"></a>Now, you might have to fiddle a bit with the value of <span class="cambria">α</span> and the interval between temperature updates to tune them and get the best results. In general, exponential decay slows down quickly at the beginning and slowly from halfway to the end. Figure 17.7 shows a few examples of how the ease of the slowdown depend on the choice of <span class="cambria">α</span>.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F7.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1020621"></a>Figure 17.7 To understand how the parameter <span class="cambria">α</span> controls cooling in simulated annealing, let’s look at the exponential decay function: <code class="fm-code-in-text">f(i)=T<sub class="subscript1">i</sub>=</code> <span class="cambria">α</span><code class="fm-code-in-text">*f(i-1),</code><span class="cambria">∀</span> <code class="fm-code-in-text">i&gt;0</code>. In this chart, we set <code class="fm-code-in-text">f(0)=T<sub class="subscript1">0</sub>=100</code> and show how the function’s shape changes with the rate <span class="cambria">α</span>, which controls how fast the function goes to zero. Why is it called exponential decay, you ask? Because <code class="fm-code-in-text">T<sub class="subscript1">1</sub>=</code> <span class="cambria">α</span><code class="fm-code-in-text">T<sub class="subscript1">0</sub></code>, <code class="fm-code-in-text">T<sub class="subscript1">2</sub>=</code> <span class="cambria">α</span><code class="fm-code-in-text">T<sub class="subscript1">1</sub>=</code> <span class="cambria">α</span><code class="fm-code-in-text"><sup class="superscript1">2</sup>T<sub class="subscript1">0</sub></code>, and in general <code class="fm-code-in-text">T<sub class="subscript1">n</sub>=</code> <span class="cambria">α</span><code class="fm-code-in-text"><sup class="superscript1">n</sup>T<sub class="subscript1">0</sub></code>. If <span class="cambria">α</span> is between <code class="fm-code-in-text">0</code> and <code class="fm-code-in-text">1</code>, <span class="cambria">α</span><code class="fm-code-in-text"><sup class="superscript1">n</sup></code> and in turn <code class="fm-code-in-text">T<sub class="subscript1">n</sub></code> get progressively (exponentially) smaller.</p>

  <p class="body"><a id="pgfId-1000100"></a>In practice, <span class="cambria">α≈</span><code class="fm-code-in-text">0.98</code> is usually a safe bet for an initial choice (then, you can take it from there and tune <a id="marker-1001783"></a><a id="marker-1001787"></a>it).</p>

  <h3 class="fm-head2" id="heading_id_8"><a id="pgfId-1000114"></a>17.1.5 Variants</h3>

  <p class="body"><a id="pgfId-1000126"></a>As <a id="marker-1001791"></a><a id="marker-1001795"></a>we have seen, simulated annealing—despite being an indisputably powerful tool—also has some flaws, the most concerning of which is its slow speed in converging due to its stochastic nature.</p>

  <p class="body"><a id="pgfId-1000146"></a>And so, like for all algorithms, over time many variants have been studied to remedy its shortcomings and make it even better.</p>

  <p class="body"><a id="pgfId-1000155"></a>We already alluded to a trivial modification that we could add: storing the best solution found across all iterations. It might happen, in fact, especially on large, multi-dimensional problem spaces, that during the initial high-energy phase we serendipitously land in proximity to a global minimum, then move uphill afterward, and never manage to get back to such a good result again. How likely this is to happen depends on many factors, such as the shape of the landscape (the cost function) and whether the right configuration for the parameters was found. In any case, remembering the best result ever can be an easy win.</p>

  <p class="body"><a id="pgfId-1000172"></a>Pushing this consideration a little further, <i class="calibre17">simulated annealing with restart</i> stores one or a few of the best results found across iterations, and when it gets stuck, it moves to one of these previous (and advantageous) positions, resetting the descent or just moving to a different area of the problem space.</p>

  <p class="body"><a id="pgfId-1000186"></a>We have mentioned, if you remember, that despite being lower with respect to greedy algorithms, the probability of getting stuck in local minima is still not <code class="fm-code-in-text">null</code>.</p>

  <p class="body"><a id="pgfId-1000195"></a>In particular, this event can be concurrently caused by a few factors:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1000204"></a>Non-optimal choice of the algorithm’s parameters. Cooling becomes, for instance, too fast, and the algorithm gets stuck away from global minimum.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1000221"></a>Bad luck. As we said, it’s a stochastic algorithm, after all, and it might reach the optimum too soon and never be able to go back once the system cools down.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1000238"></a>The update step (more likely so). While we saw that random sampling causes the fewest constraints toward other areas in the problem space with lower energy, it will slow down local convergence (possibly too much to be acceptable).</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1000255"></a>Sometimes it’s possible to have update rules that allow long steps, while other times (as we’ll see), it might be easier to implement only small steps, which means that those long leaps to different areas that could bring the algorithm outside of local minima are more rare.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1000269"></a>When some or all of these happen, using random restart could save the day.</p>

  <p class="body"><a id="pgfId-1000278"></a>Another issue with simulated annealing is that tuning its parameters can be tricky and annoying. In <i class="calibre17">adaptive simulated annealing</i><a id="marker-1001799"></a> the algorithm parameters, <code class="fm-code-in-text">k</code> and <span class="cambria">α</span>, are automatically adjusted as the algorithm progresses, or depending on the trend in the energy level. The latter uses mechanisms borrowed from thermodynamics, allowing even increases in temperature (simulating cycles of cooling down and warming <a id="marker-1001803"></a><a id="marker-1001807"></a>up).</p>

  <h3 class="fm-head2" id="heading_id_9"><a id="pgfId-1000309"></a>17.1.6 Simulated annealing vs gradient descent: Which one should I use?</h3>

  <p class="body"><a id="pgfId-1000333"></a>We <a id="marker-1001811"></a><a id="marker-1001815"></a><a id="marker-1001819"></a>have seen that simulated annealing is slower than gradient descent to get to minima: the latter takes the fastest route, so it’s hard to beat on a clear course.</p>

  <p class="body"><a id="pgfId-1000348"></a>The caveat is that gradient descent requires a differentiable cost function and gets easily stuck in local minima.</p>

  <p class="body"><a id="pgfId-1000357"></a>Whenever the cost function is not differentiable or step-shaped, as with the minimum-crossing-embedding problem, or the problem space is discrete (for problems like the TSP, which we’ll describe later in this chapter), then simulated annealing is preferred over gradient descent.</p>

  <p class="body"><a id="pgfId-1000369"></a>Likewise, for problems where we have flexible requirements about running time and the quality of the solution, simulated annealing might still be preferable to gradient descent. Keep in mind that simulated annealing is a <i class="calibre17">Monte Carlo algorithm</i><a id="marker-1001823"></a>, and so (as we discuss in appendix F) it returns a sub-optimal solution, whose quality increases with the quantity of time we allot for the algorithm to run.</p>

  <p class="body"><a id="pgfId-1000391"></a>When, instead, we have guarantees about the differentiability and shape of the function (for instance, if we are sure we have a bowl-shaped function, as with linear/ logistic regression, and so on), then we want to take advantage of the superpowers of gradient descent.</p>

  <p class="body"><a id="pgfId-1000408"></a>Are there cases where simulation annealing is best avoided?</p>

  <p class="body"><a id="pgfId-1000417"></a>Obviously, as we have discussed, if a problem doesn’t admit near-optimal solutions, but rather demands the best possible one, then simulated annealing is not the best tool in our belt.</p>

  <p class="body"><a id="pgfId-1000426"></a>The shape of the cost function matters as well. When the cost function has narrow valleys, the algorithm will have a low probability of finding them, and if they hold the global minima, it’s unlikely simulated annealing will converge to a near-optimal solution.</p>

  <p class="body"><a id="pgfId-1000441"></a>Finally, as we will see in our examples, an important requirement is that the cost function should be easily calculated for a new candidate solution (possibly allowing us to directly compute the delta based only on the difference with the current solution). Since this cost is computed at each iteration, a computationally intensive cost function will slow down the optimization, forcing us to run the algorithm for <a id="marker-1001827"></a><a id="marker-1001831"></a><a id="marker-1001835"></a>fewer iterations.</p>

  <h2 class="fm-head" id="heading_id_10"><a id="pgfId-1000460"></a>17.2 Simulated annealing + traveling salesman</h2>

  <p class="body"><a id="pgfId-1000478"></a><a id="marker-1001839"></a><a id="marker-1001843"></a><a id="marker-1001847"></a>I hope you have found the discussion about simulated annealing interesting so far. We learned about a tremendously useful tool, and now it’s time to put it into practice; luckily, we have just the perfect application for it!</p>

  <p class="body"><a id="pgfId-1000495"></a>Do you remember our e-commerce company? We left it in chapter 14 dealing with deliveries, optimizing routes for single-destination deliveries inside town.</p>

  <p class="body"><a id="pgfId-1000506"></a>As we mentioned, planning individual deliveries from a warehouse to customers for each order is uneconomic and unrealistic.</p>

  <p class="body"><a id="pgfId-1000517"></a>This doesn’t mean that what we learned in chapter 14 about optimizing routes with Dijkstra’s and <i class="calibre17">A*</i> algorithms was useless. It’s quite the opposite: that’s some fine-grained optimization that we can always perform for a single delivery, going from the generic <code class="fm-code-in-text">i</code>-th destination to the next one. Since we will have to compute these routes on the fly, it’s even more important to use an efficient algorithm.</p>

  <p class="body"><a id="pgfId-1000541"></a>And yet, since to amortize delivery costs and stay on the market we need to load each truck with several shipments (and possibly load each truck to maximum capacity) and have it go out on daily tours with several deliveries, finding the optimum path from a source to a destination is not enough anymore.</p>

  <p class="body"><a id="pgfId-1000554"></a>In this section, we’ll focus on optimizing the route of a delivery truck, assuming its load (and in turn its destinations) are already fixed. Once we have improved this phase, we still have another, higher-level optimization in front of us: assigning the deliveries to the trucks to minimize the distance (or travel time, or, more likely, cost) across all trucks and all deliveries. But we need to take one step at a time, so in this chapter we’ll focus on the following problem: given a list of cities, each connected to each other by roads, find the optimal tour, aka a sequence of the cities such that we move from each city to the next one, at some cost (for instance, the distance between the two cities) and eventually return to the first city while keeping the total cost minimal.</p>

  <p class="body"><a id="pgfId-1000565"></a>Figure 17.8 illustrates this situation: you can see 10 real and DC Universe cities, the connections between each pair of cities, and—highlighted—the shortest tour touching each city exactly once.</p>

  <p class="body"><a id="pgfId-1000580"></a>Now, we are considering a single delivery per city just to make the example clearer, with all steps being at the same scale. Nothing changes, however, if instead we have multiple deliveries in each city. Because the intra-city distances will be smaller than the ones between different cities, deliveries in the same city will be naturally clustered together. It is also likely possible for the problem to be optimized at different levels in two steps: first, find the best order in which cities should be visited, and then, within each city, compute the best route, using the same algorithm.<a href="#pgfId-1005401"><sup class="footnotenumber">10</sup></a> But these are just low-level details.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F8.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1020663"></a>Figure 17.8 An example map, mixing the DC Universe’s cities and other real ones, with some distances: because this is a complete graph with close to fifty edges, we are only showing a few numbers, almost only for the edges in the best tour (the thicker edges).</p>

  <p class="body"><a id="pgfId-1000630"></a>The abstract formulation of this riddle is a well-known computer science problem known as the traveling salesman problem (<i class="calibre17">TSP</i><a id="marker-1001851"></a>). As you might have guessed, it’s a difficult puzzle to solve, in particular a <i class="calibre17">NP-complete</i><a id="id_Hlk56849864"></a> problem<a id="marker-1001855"></a>; this implies it’s both <i class="calibre17">NP-Hard</i> and in <i class="calibre17">NP</i>—the former meaning that there is no known deterministic algorithm that can solve it in polynomial time, while the latter means that if we have a candidate solution, we can check it in polynomial time.</p>

  <p class="body"><a id="pgfId-1000660"></a>Informally, we can say that we expect any deterministic algorithm that solves the TSP will need exponential time (although we can’t be sure, because we don’t have an answer to the <i class="calibre17">P vs NP</i> problem).</p>

  <p class="body"><a id="pgfId-1000671"></a>The first consequence for us of the TSP being NP-Hard is that we can’t assume we’ll be able to solve an instance of this problem on the fly, on a driver’s mobile phone, or even a laptop computer (unless the number of cities to deliver to is small; but for the 10 cities in figure 17.8, there are already 10! ~ 3.6 million possible sequences). We need computer power, and planning ahead, and possibly even computing ahead, to reuse the results as much as possible.</p>

  <p class="body"><a id="pgfId-1000688"></a>Even with a supercomputer at our service, in fact, the running time of an exact algorithm just grows too fast. With 15 cities, there are ~1.3 trillion possible solutions, which becomes <code class="fm-code-in-text">2.4e18</code> (2 billion of billions) with 20 cities. Even assuming that we can find an algorithm running in exponential time (which is asymptotically better than factorial), we couldn’t handle more than ~40 different deliveries (probably far fewer).</p>

  <h3 class="fm-head2" id="heading_id_11"><a id="pgfId-1000711"></a>17.2.1 Exact vs approximated solutions</h3>

  <p class="body"><a id="pgfId-1000727"></a>The <a id="marker-1001859"></a><a id="marker-1001863"></a>last consideration we make is that we can afford to compromise on the optimal solution. We can be fine using a near-optimal solution; we don’t need the best possible. For instance, if we add a few miles over a total of 1,000 miles per trip, it feels acceptable, while doubling the total distance of the tour would be quite expensive, and making it 10 times costlier would be a disaster.</p>

  <p class="body"><a id="pgfId-1000749"></a>This is not true for every context, and there are situations where we need the absolute best because even a small difference is translated into a large cost, or maybe it could help save lives. In surgery simulations, for instance, a small error can have dire consequences, as you can imagine.</p>

  <p class="body"><a id="pgfId-1000764"></a>But since we are okay with sub-optimal solutions, this means we can use a heuristic to get an acceptable solution within a reasonable time.</p>

  <p class="body"><a id="pgfId-1000773"></a>Several heuristics have been developed specifically for TSP. For instance, for graphs whose distances obey the triangle inequality (like ours), a class of algorithms using the <i class="calibre17">minimum spanning tree</i><a id="marker-1001867"></a> of a graph (see section 2.8.2) and running in <i class="calibre17">linearithmic</i><a id="marker-1001871"></a> time, <code class="fm-code-in-text">O(n*log(n))</code>, can guarantee a solution whose cost is at most twice the minimum cost, and on average<a href="#pgfId-1005417"><sup class="footnotenumber">11</sup></a> between just 15 and 20% higher than the shortest tour.</p>

  <p class="body"><a id="pgfId-1000796"></a>We want to try a different way, though. This is clearly an optimization problem, so why not try to tackle it with our new shining tool, simulated annealing?</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre14" id="pgfId-1020703"></a>Is using simulated annealing the right choice for my project?</p>

    <p class="fm-sidebar-text"><a id="pgfId-1020704"></a>A word of caution is necessary here. We have mentioned Maslow’s hammer law a few times throughout this book, and this configures like a case where we need to think carefully before choosing which tool to employ. The risk, as you know by now, is being tempted to use a hammer (simulated annealing) when a screwdriver could work better.</p>

    <p class="fm-sidebar-text"><a id="pgfId-1020705"></a>So, before deciding if simulated annealing is worth implementing, we need to ask ourselves a few questions, such as these: Do we have the skills inside our team/company to develop and tune this algorithm? Would we have better skills for another solution? What’s the difference in effort needed between these solutions? What’s the benefit of one versus the other?</p>
  </div>

  <p class="body"><a id="pgfId-1000863"></a>Simulated annealing can potentially bring us a better solution than the average provided by the MST heuristic; it can even lead us to global optimum. Moreover, let’s assume we don’t have expertise on the specific problem inside the company: overall, it might be worth trying simulated annealing, which is high-level and potentially could be reused for other optimization problems in the future.</p>

  <p class="body"><a id="pgfId-1000878"></a>When it comes to the cost function, we are in luck. It naturally stems from the problem’s very definition: it’s the sum of the edges between adjacent vertices in the sequence (including between its last and first entries, of <a id="marker-1001875"></a><a id="marker-1001879"></a>course).</p>

  <h3 class="fm-head2" id="heading_id_12"><a id="pgfId-1000894"></a>17.2.2 Visualizing cost</h3>

  <p class="body"><a id="pgfId-1000908"></a>One <a id="marker-1001883"></a><a id="marker-1001887"></a>nice thing about the solutions to the TSP is that the problem space is the set of all the possible permutations of a graph’s vertices, and since each sequence can be mapped to an integer, we have a way to show a nice 2-D chart for the cost function, and even see how the algorithm progresses. The problem space, of course, is huge when the number of vertices grows, as it appears clearly in figure 17.9, where we have to zoom in on a small portion of the domain to be able to distinguish the landscape of the cost function.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F9.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1020734"></a>Figure 17.9 The cost landscape for the TSP applied to the graph in figure 17.8 (A). The cost function is displayed for the whole domain; it’s hard to make sense of it! 17.8 (B) Zooming in on the first few hundreds of permutations, we can see that there are several local minima.</p>

  <p class="body"><a id="pgfId-1000949"></a>To provide a clearer view and description of the process, we need to keep the set of cities small: for instance, we can restrict to the six DC Universe cities in figure 17.8, obtaining the graph shown in figure 17.10, for which there are just 720 possible permutations of the vertices. This produces the landscape shown in figure 17.11, which is far less clogged than what we saw in figure 17.9.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F10.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1020776"></a>Figure 17.10 Solving TSP for a sub-graph of the <code class="fm-code-in-text">K<sub class="subscript1">10</sub></code> graph shown in figure 17.8. The figure highlights the <code class="fm-code-in-text">K<sub class="subscript1">6</sub></code> complete graph formed by the cities belonging to the DC Universe, and the best solution found. The rest of the vertices/edges of the original graph are grayed out for clarity.</p>

  <p class="body"><a id="pgfId-1000987"></a>Now, since we have the full landscape, you might point out that we could just brute-force search it and find the best solution, and you’d be right; we have evaluated the cost of each single permutation to draw the chart in figure 17.11, so we can just extract its minimum. As shown in the figure, the best solution is the sequence:</p>
  <pre class="programlisting">[New Carthage, Happy Harbor, Gotham City, Metropolis, Opal City, Civic City]</pre>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F11.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1020818"></a>Figure 17.11 The cost landscape for TSP applied to the complete graph <code class="fm-code-in-text">K6</code> as shown in figure 17.10. Besides the actual values, notice some patterns repeating.</p>

  <p class="body"><a id="pgfId-1001033"></a>The point is that we wouldn’t be able to do that for larger instances. With the full graph in figure 17.8, for example, it already takes several minutes to generate all 3.6 million possible <a id="marker-1001891"></a><a id="marker-1001895"></a>permutations!</p>

  <h3 class="fm-head2" id="heading_id_13"><a id="pgfId-1001051"></a>17.2.3 Pruning the domain</h3>

  <p class="body"><a id="pgfId-1001065"></a>The <a id="marker-1001899"></a><a id="marker-1001903"></a>reason we wanted to show you the cost function on the full domain of the sub-problem with six vertices is because this chart can teach us a lot. For instance, we can see there are many local minima that appear to have the same cost. Can you guess why that is?</p>

  <p class="body"><a id="pgfId-1001082"></a>As always, take a couple of minutes (if you’d like) to think about the answer, before moving on to the solution.</p>

  <p class="body"><a id="pgfId-1001091"></a>To answer the question, consider these permutations of the cities in the graph:</p>
  <pre class="programlisting">[Opal City, Civic City, New Carthage, Happy Harbor, Gotham City, Metropolis]
[New Carthage, Happy Harbor, Gotham City, Metropolis, Opal City, Civic City]</pre>

  <p class="body"><a id="pgfId-1001119"></a>What’s the difference between these two permutations, in terms of solutions? Since we are considering closed tours (from the first city, through all other cities, and then back to the start), they are completely identical, except the second one is shifted left by two cities. In fact, the cost of the two tours is the same (because they involve the same edges).</p>

  <p class="body"><a id="pgfId-1001136"></a>There are six equivalent sequences that can be derived from the one we gave as a solution, one for each city used as a starting point.</p>

  <p class="body"><a id="pgfId-1001149"></a>We can therefore fix in advance the city from which the tour starts, knowing that this won’t affect the result, but it will cut the number of permutations we need to examine. From 720 we go down to only 120—not bad, and it will be an even better gain for larger graphs.</p>

  <p class="body"><a id="pgfId-1001164"></a>Moreover, consider that this will work well with the specific instance of the problem we need to solve for our e-commerce company. We always need to start (and end) our tours at the warehouse, where the goods to deliver are loaded on the trucks.</p>

  <p class="body"><a id="pgfId-1001177"></a>If we set, for instance, <code class="fm-code-in-text">New Carthage</code> as the starting point, there might still be several equally good solutions (if multiple sub-paths have the same total cost), but if the graph is undirected there will only be, at most, two equivalent solutions:</p>
  <pre class="programlisting">[New Carthage, Happy Harbor, Gotham City, Metropolis, Opal City, Civic City]
[New Carthage, Civic City, Opal City, Metropolis, Gotham City, Happy Harbor]</pre>

  <p class="body"><a id="pgfId-1001208"></a>That’s because if edges have the same cost in both directions, we can travel through a simple cycle (a tour) in either direction (in figure 17.10, clockwise or counter-clockwise).</p>

  <p class="body"><a id="pgfId-1001217"></a>But we don’t mind having two possible global solutions, so there is no further action we should take <a id="marker-1001907"></a><a id="marker-1001911"></a>here.</p>

  <h3 class="fm-head2" id="heading_id_14"><a id="pgfId-1001231"></a>17.2.4 State transitions</h3>

  <p class="body"><a id="pgfId-1001245"></a>Now <a id="marker-1001915"></a><a id="marker-1001919"></a>it’s time to translate this constraint into code. Luckily for us, that’s not too difficult, given how we designed the simulated annealing algorithm in listing 17.1. We need to pass the definition of the function that computes the transition to the next state, and right there, we can always set the first vertex of the sequence to the same value. We also have the cost function figured out, so we are ready to implement all the missing pieces.</p>

  <p class="body"><a id="pgfId-1001265"></a>Listing 17.2 starts by exploring the cost function, which, as we have discussed, is the sum of the weights of the edges between two adjacent vertices in the permutation. Still, while computing this value we need to be careful about a couple of details. First and foremost, we assume that the input graph has an edge between every pair of vertices. If this isn’t true, we need to check for it and return a special value (for instance, infinity, or any large-enough weight) to basically guarantee any solution including missing edges will be naturally discarded. Another alternative could be checking the solutions when transitions are computed and making sure that those with missing edges between adjacent vertices will be discarded early.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1011215"></a>Listing 17.2 Cost function for TSP</p>
  <pre class="programlisting"><b class="strong">function</b> tspTourCost(graph, P)                                       <span class="fm-combinumeral">❶</span>
  cost ← 0
  <b class="strong">for</b> k <b class="strong">in</b> {0..|P|} <b class="strong">do</b>                                               <span class="fm-combinumeral">❷</span>
    cost ← cost + graph.edgeBetween(P[k], P[(k+1)%|P|]).weight       <span class="fm-combinumeral">❸</span>
  <b class="strong">return</b> cost</pre>

  <p class="fm-code-annotation"><a id="pgfId-1013291"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">tspTourCost</code><a id="marker-1019041"></a> takes a graph and a candidate solution <code class="fm-code-in-text2">P</code> (a point in the problem space, that is, a permutation of the list of vertices in the graph) and computes the cost of the solution as the sum of the weight of all edges in the candidate tour. It’s assumed the graph has an edge between every pair of vertices (or, alternatively, that the methods return a special value, like <code class="fm-code-in-text2">inf</code>, if there isn’t one).</p>

  <p class="fm-code-annotation"><a id="pgfId-1013333"></a><span class="fm-combinumeral">❷</span> Cycles through the sequence</p>

  <p class="fm-code-annotation"><a id="pgfId-1013370"></a><span class="fm-combinumeral">❸</span> Checks the edge between <code class="fm-code-in-text2">k</code>-th vertex in the sequence and its successor, the one at position (<code class="fm-code-in-text2">k+1</code>) modulo the length of the sequence (so that when it gets to the last element of the list, it will circle back and its successor will be the first vertex)</p>

  <p class="body"><a id="pgfId-1001413"></a>The other detail we would like to highlight is that we need to wrap around the array, because we have to add the cost of the edge between the last and first vertices in the sequence (the edge that closes the tour). This can be handled in many ways, using modulo being the most succinct, while the most efficient would be treating this last edge as a separate case outside the <code class="fm-code-in-text">for</code> loop.</p>

  <p class="body"><a id="pgfId-1001424"></a>When it comes to the transitions to a new solution, we have a few options:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1001433"></a><i class="calibre15">Swapping adjacent vertices</i>—After selecting a random position in the sequence, we would swap the vertex at the given index with its successor. For instance, the solution <code class="fm-code-in-text">[1,2,3,4]</code> could change into <code class="fm-code-in-text">[2,1,3,4]</code> or <code class="fm-code-in-text">[1,3,2,4],</code> and so on. This transition corresponds to a local search, where only the immediate neighborhood of the current solution is explored; this raises the probability that fine tuning leads to improvement, but makes leaps and getting out of local minima harder.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1001458"></a><i class="calibre15">Swapping any pair of vertices</i>—Both positions for the vertices to swap are chosen at random, and they are simply swapped: <code class="fm-code-in-text">[1,2,3,4]</code> can also become <code class="fm-code-in-text">[3,2,1,4]</code>, which was not allowed by the previous operator. These kinds of transitions allow medium-range search, although the two solutions are still quite close to each other.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1001480"></a><i class="calibre15">Generating a new sequence at random</i>—This allows us to leap across the whole domain at any stage of the algorithm, making it less likely to lead to fine-grain improvements toward a local minimum in the final stage of the simulation, because relatively few attempts will be made in the neighborhood of current solutions. At the same time, it will also leave the door open for distant leaps to any area of the domain, whenever a better solution is (randomly) found.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1001504"></a>And, of course, many more intermediate options are available, such as doing a fixed or random number of swaps per transition or moving a whole segment of the solution elsewhere.</p>

  <p class="body"><a id="pgfId-1001515"></a>Which one of these works best? That’s a good question, which is hard to answer from a theoretical point. Let’s try them out on our <code class="fm-code-in-text">K10</code> graph, as shown in figure 17.8, and see which one gives the best average result. We will perform 1,000 simulations, each running for the same number of steps and with the same values for <code class="fm-code-in-text">k</code> and <span class="cambria">α</span>, and compare the average cost for the solutions. We’ll also assume that each sequence starts with <code class="fm-code-in-text">“New Carthage”</code>, so that we cut duplicate solutions by a factor 10. Results are summed up in table 17.1.</p>

  <p class="body"><a id="pgfId-1001544"></a>Of course, the domain this time is still huge, ~3.6 million permutations, but not so large that we can’t afford brute-force (armed with some patience; it takes a while); therefore, we know that the best possible solution has cost <code class="fm-code-in-text">1625</code> (as shown in fig-ure 17.8):</p>
  <pre class="programlisting">[“New Carthage”, “Syracuse”, “Buffalo”, “Pittsburgh”, “Harrisburg”, 
<span class="fm-code-continuation-arrow">➥</span> “Opal City”, “Metropolis”, “Gotham City”, “Civic City”]</pre>

  <p class="fm-table-caption"><a id="pgfId-1011294"></a>Table 17.1 Average cost for the best solution found by simulated annealing with different algorithms for transitions</p>

  <table border="1" class="contenttable" width="100%">
    <colgroup class="calibre1">
      <col class="calibre2" span="1" width="50%"/>
      <col class="calibre2" span="1" width="50%"/>
    </colgroup>

    <tr class="calibre8">
      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1011298"></a>Operation</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1011300"></a>Mean cost <code class="fm-code-in-text">(</code><span class="cambria">α</span><code class="fm-code-in-text">=0.98, T0=200</code> and <code class="fm-code-in-text">k=1000</code>)</p>
      </th>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011302"></a><code class="fm-code-in-text2">Adjacent Swaps</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011304"></a>1937.291</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011306"></a><code class="fm-code-in-text2">Random Swaps</code><a id="marker-1011313"></a><a id="marker-1011314"></a></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011308"></a>1683.563</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011310"></a><code class="fm-code-in-text2">Random Permutation</code><a id="marker-1011315"></a></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011312"></a>1831.886</p>
      </td>
    </tr>
  </table>

  <p class="body"><a id="pgfId-1001946"></a>It’s interesting that the best result is obtained with the medium-range search, while the worst one is given by the local search. This means that with the configuration used, the local search gets stuck in local optima, while the random permutation is too erratic and fails to obtain local convergence in the final stage of the algorithm, when the temperature gets low.</p>

  <p class="body"><a id="pgfId-1002072"></a>We must stress that with different parameters, this could change completely. Take, for instance, <span class="cambria">α</span>, the decaying factor: with a difference choice, the cooling process would be slower, so we wonder, could this allow the local search to work better? Let’s try it out; the results are in table 17.2.</p>

  <p class="fm-table-caption"><a id="pgfId-1011390"></a>Table 17.2 Average cost, with different temperature decay rate</p>

  <table border="1" class="contenttable" width="100%">
    <colgroup class="calibre1">
      <col class="calibre2" span="1" width="25%"/>
      <col class="calibre2" span="1" width="25%"/>
      <col class="calibre2" span="1" width="25%"/>
      <col class="calibre2" span="1" width="25%"/>
    </colgroup>

    <tr class="calibre8">
      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1011398"></a>Operation</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1011400"></a>Mean cost (<span class="cambria">α</span><code class="fm-code-in-text">=0.97</code>)</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1011402"></a>Mean cost (<span class="cambria">α</span><code class="fm-code-in-text">=0.98</code>)</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1011404"></a>Mean cost (<span class="cambria">α</span><code class="fm-code-in-text">=0.99</code>)</p>
      </th>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011406"></a><code class="fm-code-in-text2">Adjacent Swaps</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011408"></a>1972.502</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011410"></a>1937.291</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011412"></a>1868.701</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011414"></a><code class="fm-code-in-text2">Random Swaps</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011416"></a>1692.044</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011418"></a>1683.563</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011420"></a>1668.248</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011422"></a><code class="fm-code-in-text2">Random Permutation</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011424"></a>1816.658</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011426"></a>1831.886</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011428"></a>1913.416</p>
      </td>
    </tr>
  </table>

  <p class="body"><a id="pgfId-1002109"></a>Increasing the temperature decay rate from 0.97 to 0.99 means that the cooling is performed more slowly and uniformly (you can refer to figure 17.7 to visualize the decay curves). In turn, this seems to help when we use only local search around the current solution, while making thing worse when used for full-domain searches. The medium-range search performed through random vertex swaps gets even closer to the best cost, on average.</p>

  <p class="body"><a id="pgfId-1002306"></a>These results show a couple more things that are worth highlighting:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1002315"></a>Even on a small case (10 vertices) and running thousands of iterations, we get arbitrarily close to the best solution, but we don’t always get the best result. That’s a calculated risk with heuristics.</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1002331"></a>Finding the best configuration for an optimization algorithm requires time, experience, and sometimes a bit of luck.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1002345"></a>We also tried larger values for alpha, such as <span class="cambria">α</span><code class="fm-code-in-text">=0.995</code>, which are not shown in the table, partly because they lead to poor results. Probably the cooling becomes too slow and the algorithm gets closer to random sampling (this suspicion is corroborated by the fact that with a smaller value for <code class="fm-code-in-text">k</code>, the normalizing Boltzmann constant, the deterioration in the results smoothens).</p>

  <p class="body"><a id="pgfId-1002363"></a>To conclude this section on TSP, listing 17.3 shows a method that implements a random transition from current state to the next one, by combining all three methods discussed so far. An implementation of these methods for library JsGraphs can be found on GitHub.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1011487"></a>Listing 17.3 Random transition function for TSP</p>
  <pre class="programlisting"><b class="strong">function</b> randomStep(P)              <span class="fm-combinumeral">❶</span>
  which ← randomFloat(0,1)          <span class="fm-combinumeral">❷</span>
  <b class="calibre21">if</b> which &lt; 0.1 <b class="calibre21">then</b>               <span class="fm-combinumeral">❸</span>
    i ← randomInt(0, |P|)           <span class="fm-combinumeral">❹</span>
    swap(i, (i+1) % |P|)            <span class="fm-combinumeral">❹</span>
  <b class="calibre21">elsif</b> which &lt; 0.8 then
    i ← randomInt(0, |P|)           <span class="fm-combinumeral">❺</span>
    j ← randomInt(0, |P|)
    swap(i, j)
  <b class="calibre21">else</b>
    P ← randomPermutation(P)        <span class="fm-combinumeral">❻</span>
  <b class="calibre21">return</b> P    </pre>

  <p class="fm-code-annotation"><a id="pgfId-1013417"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">randomStep</code><a id="marker-1018640"></a> takes a candidate solution <code class="fm-code-in-text2">P</code> (a point in the problem space, that is, a permutation of the list of vertices in the graph) and computes a transition to a new neighboring state.</p>

  <p class="fm-code-annotation"><a id="pgfId-1013455"></a><span class="fm-combinumeral">❷</span> Chooses a random value between <code class="fm-code-in-text2">0</code> and <code class="fm-code-in-text2">1</code> (excluded)</p>

  <p class="fm-code-annotation"><a id="pgfId-1013486"></a><span class="fm-combinumeral">❸</span> Based on the number chosen and some static probabilities, decides which transition to apply</p>

  <p class="fm-code-annotation"><a id="pgfId-1013596"></a><span class="fm-combinumeral">❹</span> Swaps two adjacent vertices</p>

  <p class="fm-code-annotation"><a id="pgfId-1013631"></a><span class="fm-combinumeral">❺</span> Swaps two arbitrary vertices. Notice that the two indices might in theory be equal. Rather than overcomplicating the code, we can risk a collision, which just means no swap is performed. Especially for large lists, chances are low enough anyway.</p>

  <p class="fm-code-annotation"><a id="pgfId-1013664"></a><span class="fm-combinumeral">❻</span> Replaces current state with a random new state (a random permutation)</p>

  <p class="body"><a id="pgfId-1002615"></a>How does this method comparatively perform? Table 17.3 compares it to the three “pure” solutions: it manages to get the best of all the possible strategies and drives the average cost down with all choices for <span class="cambria">α</span>.</p>

  <p class="fm-table-caption"><a id="pgfId-1011765"></a>Table 17.3 Comparing average solution cost found using an ensemble of original methods</p>

  <table border="1" class="contenttable" width="100%">
    <colgroup class="calibre1">
      <col class="calibre2" span="1" width="25%"/>
      <col class="calibre2" span="1" width="25%"/>
      <col class="calibre2" span="1" width="25%"/>
      <col class="calibre2" span="1" width="25%"/>
    </colgroup>

    <tr class="calibre8">
      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1011773"></a>Operation</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1011775"></a>Mean cost (<span class="cambria">α</span><code class="fm-code-in-text">=0.97</code>)</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1011777"></a>Mean cost (<span class="cambria">α</span><code class="fm-code-in-text">=0.98</code>)</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1011779"></a>Mean cost (<span class="cambria">α</span><code class="fm-code-in-text">=0.99</code>)</p>
      </th>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011781"></a><code class="fm-code-in-text2">Adjacent Swaps</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011783"></a>1972.502</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011785"></a>1937.291</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011787"></a>1868.701</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011789"></a><code class="fm-code-in-text2">Random Swaps</code><a id="marker-1011812"></a><a id="marker-1011813"></a></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011791"></a>1692.044</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011793"></a>1683.563</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011795"></a>1668.248</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011797"></a><code class="fm-code-in-text2">Random Permutation</code><a id="marker-1011814"></a></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011799"></a>1816.658</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011801"></a>1831.886</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011803"></a>1913.416</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011805"></a><code class="fm-code-in-text2">Ensemble Step</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011807"></a>1683.966</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011809"></a>1672.494</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1011811"></a>1660.904</p>
      </td>
    </tr>
  </table>

  <p class="body"><a id="pgfId-1002642"></a>This seems to suggest that an ensemble method, with the right ratio between the long and distant local searches, can leverage the strengths of both types of transition heuristics.</p>

  <p class="body"><a id="pgfId-1002890"></a>The best result, with the ensemble method, is obtained with a larger decay rate, so with a more uniform cooling process.</p>

  <p class="body"><a id="pgfId-1002899"></a>Figure 17.12 shows the algorithm in action: how cost evolves while cooling the system. While initially the cost is fluctuating, the oscillation becomes progressively narrower and, once the temperature is low enough, search is channeled toward the global minimum.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F12.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1020860"></a>Figure 17.12 A run of simulated annealing for K10 TSP. It takes a while, but finally finds the path to global minimum.</p>

  <p class="body"><a id="pgfId-1002927"></a>We haven’t explored all the possible values for <span class="cambria">α</span>, <code class="fm-code-in-text">k</code>, <code class="fm-code-in-text">T<sub class="subscript1">0</sub></code>, and even the relative probabilities of applying each of the three transition operators within the ensemble method could be further tuned. To be more systematic, we should build a few more examples with different graphs, and then write a small piece of code that changes one parameter at a time and records the mean or median cost found. Try it out using the code included in JsGraph library<a id="marker-1004545"></a> as a starting point; it could be a nice <a id="marker-1004549"></a><a id="marker-1004553"></a>exercise.</p>

  <h3 class="fm-head2" id="heading_id_15"><a id="pgfId-1002960"></a>17.2.5 Adjacent vs random swaps</h3>

  <p class="body"><a id="pgfId-1002976"></a>The <a id="marker-1004557"></a><a id="marker-1004561"></a>last point I’d like to briefly discuss is why it seems that swapping random vertices works better than swapping adjacent pairs.</p>

  <p class="body"><a id="pgfId-1002988"></a>One factor that contributes to this can be seen with the following example, illustrated in figure 17.13: it shows a case where the local-search transition heuristic, swapping only adjacent pairs, might struggle. First, we have an example of a swap between two random vertices (17.13 (A)) that grants a good improvement in the cost function. This transition will be always accepted, regardless of the temperature of the system.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F13.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1020902"></a>Figure 17.13 An example of why the local search transition heuristic, swapping only adjacent pairs, might struggle. In the top half, a swap between two random vertices (A) is performed. There is a good improvement in the cost function, so this transition will be always accepted. When only swaps between adjacent pairs are allowed (B), several pejorative moves (two, in this case, but it can be made an arbitrarily long sequence) must be accepted before getting to the same configuration as (A). This might happen in the early stages of the cooling process, but as we get to the end of the cooling cycle it becomes unlikely.</p>

  <p class="body"><a id="pgfId-1003024"></a>When only swaps between adjacent pairs are allowed (17.13 (B)), in this example several pejorative moves (two, in this case, but it can be made an arbitrarily long sequence) must be accepted before getting to the same final configuration shown in figure 17.13 (A). Accepting a sequence of transitions uphill might happen in the early stages of the cooling process, but as we get to the end of the cooling cycle, it becomes progressively less likely for a sequence of bad moves to be accepted. Hence, by using only narrow search, the algorithm might find it impossible to improve an intermediate result.</p>

  <p class="body"><a id="pgfId-1003035"></a>The consideration that the acceptance rate for sequences of moves exponentially decays with the temperature can also suggests a different approach for our <code class="fm-code-in-text">randomStep</code> method<a id="marker-1004565"></a>—passing the temperature along, and making narrow search more likely to happen in the early stages (when the temperature is higher), while applying an inverse dependence for long search probability, which could then be more likely in late stages.</p>

  <p class="body"><a id="pgfId-1003052"></a>I encourage you to try it out by changing the code<a href="#pgfId-1005432"><sup class="footnotenumber">12</sup></a> cloned from JsGraphs’s GitHub repo. Getting your hands dirty is a great way to become familiar with simulated <a id="marker-1004569"></a><a id="marker-1004573"></a>annealing.</p>

  <h3 class="fm-head2" id="heading_id_16"><a id="pgfId-1003073"></a>17.2.6 Applications of TSP</h3>

  <p class="body"><a id="pgfId-1003085"></a>Besides <a id="marker-1004577"></a><a id="marker-1004581"></a>our issue with delivery routes, efficient TSP approximation algorithms can benefit a lot of real-world scenarios.</p>

  <p class="body"><a id="pgfId-1003097"></a>The story goes that one of the pioneers of research about the TSP was motivated, in the 1940s, by the need to optimize bus routes to pick children up for school.</p>

  <p class="body"><a id="pgfId-1003106"></a>Later, this problem was applied pervasively to the logistic of most services that involve planning tours, from mail delivery to farming.</p>

  <p class="body"><a id="pgfId-1003115"></a>In time, cable companies started to use it to improve scheduling of service calls, and more recently (and more relevantly to our field), it has become a must to make the automated process of drilling holes in and soldering printed circuit boards<a id="marker-1004585"></a> (<i class="calibre17">PCB</i>) more efficient, and even to aid some processes in <a id="marker-1004589"></a><a id="marker-1004593"></a>genome <a id="marker-1004597"></a><a id="marker-1004601"></a><a id="marker-1004605"></a>sequencing.</p>

  <h2 class="fm-head" id="heading_id_17"><a id="pgfId-1003135"></a>17.3 Simulated annealing and graph embedding</h2>

  <p class="body"><a id="pgfId-1003153"></a>To <a id="marker-1004609"></a><a id="marker-1004613"></a><a id="marker-1004617"></a>solve the TSP, we were dealing with a graph while ignoring its embedding,<a href="#pgfId-1005448"><sup class="footnotenumber">13</sup></a> the only thing that mattered was the distance between pair of vertices.</p>

  <p class="body"><a id="pgfId-1003169"></a>In the last couple of chapters, we focused on abstract graphs and finding meaningful ways to embed them in the plane.</p>

  <p class="body"><a id="pgfId-1003178"></a>If you recall, when we presented simulated annealing, we said it could work well with discrete cost functions, and even step-shaped cost functions. Those, as we have seen, are some of the situations where it’s advisable to prefer simulated annealing over gradient descent.</p>

  <p class="body"><a id="pgfId-1003191"></a>Therefore, how could we close this chapter without trying to use simulated annealing to crack the <i class="calibre17">minimum edge crossing</i><a id="marker-1004621"></a> problem?</p>

  <h3 class="fm-head2" id="heading_id_18"><a id="pgfId-1003203"></a>17.3.1 <a id="Current"></a> Minimum edge crossing</h3>

  <p class="body"><a id="pgfId-1003219"></a>As <a id="marker-1016007"></a><a id="marker-1016008"></a>always, to apply our template for simulated annealing (shown in listing 17.1) to a concrete problem, we need to specify two functions: the cost function and the update step.</p>

  <p class="body"><a id="pgfId-1003231"></a>The former is often implied by the problem, and so it is here, for the basic version of the problem: we just count how many edges intersect.</p>

  <p class="body"><a id="pgfId-1003240"></a>The latter, the update step, leaves more leeway to the algorithm designer. Should we just move a single vertex randomly? By how much? Should we swap the positions of two vertices? Should edges be taken into consideration during the update?</p>

  <p class="body"><a id="pgfId-1003253"></a>The one advice I can give in these situations is to start small, get something simple working, and then try to add new ideas and validate them by measuring whether or not they help convergence. This is what we have done with the TSP by first developing the methods performing a single action, measuring their effectiveness, and then combining them in a random ensemble.</p>

  <p class="body"><a id="pgfId-1003262"></a>To make things more concrete, we’ll focus on finding an embedding close to the rectilinear crossing number<a id="marker-1004633"></a> (rcn) for complete graph <code class="fm-code-in-text">K<sub class="subscript1">8</sub></code>. If you remember Guy’s conjecture, which we provided in chapter 15, it gives us an exact formula for this graph’s crossing number, which is 18. In that same chapter, however, we also learned that the rectilinear crossing number for complete graphs can be and usually is larger than its crossing number;<a href="#pgfId-1005463"><sup class="footnotenumber">14</sup></a> in this case, <code class="fm-code-in-text">rcn(K<sub class="subscript1">8</sub>)</code> is <code class="fm-code-in-text">19</code>.</p>

  <p class="body"><a id="pgfId-1003301"></a>So, let’s start with a simple step that takes a vertex and slightly moves it within a certain range. After fiddling with the range of updates for vertices, we chose to update both <code class="fm-code-in-text">x</code> and <code class="fm-code-in-text">y</code> separately, by choosing two random deltas within 10% of the drawing area’s width and height respectively. Smaller ranges made the algorithm too slow, while with larger values the algorithm behaved erratically. Then, running simulated annealing with <code class="fm-code-in-text">k=0.1,</code> <span class="cambria">α</span><code class="fm-code-in-text">=0.97, T<sub class="subscript1">0</sub>=200</code> and <code class="fm-code-in-text">maxSteps=500</code>, the average <code class="fm-code-in-text">rcn</code> over 100 runs was <code class="fm-code-in-text">21.904</code>. Not bad, but not particularly good.</p>

  <p class="body"><a id="pgfId-1003335"></a>Two considerations must be made. First, we kept the number of steps low (if you recall, we used up to 10,000 for TSP in the last section). The second consideration is that for the same reasons (which we’ll discuss in a second), we had to go down from 1,000 runs to just 100.</p>

  <p class="body"><a id="pgfId-1003356"></a>Both changes are due to the fact that computing the cost function and the transitions (by cloning a graph embedding) were particularly computationally heavy. Of course, this is partly due to using a general-purpose version of simulated annealing; it could be optimized by writing an ad hoc version that doesn’t need to clone the whole embedding, but just remember what was changed, and what the cost of the current solution was (and maybe also computing the delta based on the changes, not the whole solutions: for instance, just compute the number of intersections for the vertex moved before and after the change).</p>

  <p class="body"><a id="pgfId-1003375"></a>I would like to avoid focusing on these optimizations here. Don’t get me wrong; optimization is crucial, especially for production-ready code, but early optimization can also get in the way of improving your algorithm or learning process. Premature optimization is still the source of all evil (well, maybe not all of it, but a good chunk!).</p>

  <p class="body"><a id="pgfId-1003392"></a>In this case, I preferred providing simple, clean, non-optimized code rather than more obscure (though better-performing) routines.</p>

  <p class="body"><a id="pgfId-1003401"></a>The next step was adding a larger-range search: swapping two vertices’ positions in 10% of the iterations, so that 90% of the time we apply the short-range transition. How do you think it went? Well, just poorly; the average number of intersections grew to <code class="fm-code-in-text">22.89</code>. This, however, wasn’t unexpected. If you think about it, a complete graph is <i class="calibre17">completely symmetrical</i><a id="marker-1004637"></a><a id="marker-1004641"></a>, so that swapping two vertices is totally useless! Even worse, it was detrimental because we were wasting 10% of the iterations, hence the poorer result.</p>

  <p class="body"><a id="pgfId-1003426"></a>Nevertheless, this transition can be useful for other types of graphs that aren’t symmetrical, so we’ll leave it in. (While we are using complete graphs for our examples, the algorithm can and will be applied to any graph. We’ll see in the next section some examples where swapping vertices becomes crucial to getting a good result.)</p>

  <p class="body"><a id="pgfId-1003439"></a>Yet, we still need to do something different to improve our algorithm. What about choosing a single vertex and moving it randomly anywhere in the drawing area?</p>

  <p class="body"><a id="pgfId-1003448"></a>We applied this transition 10% of the time, and the average crossing number went down to 19.17, meaning that the algorithm was almost always finding the best solution. Speaking of which, figure 17.14 compares two solutions for the embedding of <code class="fm-code-in-text">K<sub class="subscript1">8</sub></code>. The one on the left was found by the random sampling algorithm provided in chapter 16, and the other one is the result of the simulated annealing-based method.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F14.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1020944"></a>Figure 17.14 Two embeddings of complete graph <code class="fm-code-in-text">K<sub class="subscript1">8</sub></code>. On the left, one was computed by the random sampling algorithm; on the right is the result of simulated annealing. Both algorithms were run for 500 iterations: random sampling could only get as low as 27 intersections, but simulated annealing produced an optimal embedding with 19 crossing points.</p>

  <p class="body"><a id="pgfId-1003487"></a>It goes without saying that there could be a lot more work to do to improve the algorithm by fine-tuning the parameters and perhaps coming up with better operators to tweak the solutions.</p>

  <p class="body"><a id="pgfId-1003496"></a>Last but not least, the algorithm should be tried and optimized on a diverse set of graphs to be sure to avoid overfitting it to complete graphs. (Alternatively, when faced with a specific problem, you can tune parameters on small instances of the graphs you expect to see, and once ready, apply the tuned version to your real instances.)</p>

  <p class="body"><a id="pgfId-1003512"></a>From what we could see, scaling up to <code class="fm-code-in-text">K<sub class="subscript1">10</sub></code> (figure 17.15), for instance, the configuration we used seems to work well with larger complete <a id="marker-1004645"></a><a id="marker-1004649"></a>graphs.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F15.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1020986"></a>Figure 17.15 Two embeddings of complete graph <code class="fm-code-in-text">K<sub class="subscript1">10</sub></code>. Simulated annealing (right) produced an optimal embedding with 62 intersections, while the one output by random sampling has 81 intersections.</p>

  <h3 class="fm-head2" id="heading_id_19"><a id="pgfId-1003549"></a>17.3.2 Force-directed drawing</h3>

  <p class="body"><a id="pgfId-1003563"></a>In <a id="marker-1004653"></a><a id="marker-1004657"></a>section 16.5 we described a class of graph drawing algorithms called force-directed drawing<a id="marker-1004661"></a> that use a physics-based approach to compute aesthetically-pleasing embeddings of graphs. Figure 17.16 reminds us why a nice embedding is important when graphs need to be visualized.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F16.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1021028"></a>Figure 17.16 A nice embedding can make the difference in understanding a graph: complete bipartite graphs <code class="fm-code-in-text">K<sub class="subscript1">10,10</sub></code> is drawn randomly (left) and symmetrically (right). Which one better shows the structure of this graph?</p>

  <p class="body"><a id="pgfId-1003604"></a>The spring embedder model for drawing undirected graphs was first introduced<a href="#pgfId-1005478"><sup class="footnotenumber">15</sup></a> by Peter Eades in the late 1980s, then refined<a href="#pgfId-1005498"><sup class="footnotenumber">16</sup></a> by Kamada and Kawai, who introduced the idea of optimal edge length and sequential vertex update (by moving only one vertex at each step).</p>

  <p class="body"><a id="pgfId-1003622"></a>The algorithm is evolved into a state of minimum energy by using gradient-descent. Often, though, by using a deterministic learning technique like gradient descent, we are bound to remain stuck in local minima, reaching an equilibrium for the system, but not the state with the minimum possible energy.</p>

  <p class="body"><a id="pgfId-1003635"></a>It’s no surprise that simulated annealing can help in this case as well. Davidson and Harel first used this technique<a href="#pgfId-1005523"><sup class="footnotenumber">17</sup></a> to converge to optimal embeddings while staying out of local pitfalls.</p>

  <p class="body"><a id="pgfId-1003651"></a>The problem with using standard simulated annealing is that random search makes convergence too slow. To work around this limitation, several authors suggested using hybrid solutions that leverage the strengths of both approaches. GEM<a href="#pgfId-1005539"><sup class="footnotenumber">18</sup></a> algorithm stands out among them for its innovative approach and impressive results.</p>

  <p class="body"><a id="pgfId-1003666"></a>GEM doesn’t use simulated annealing, but it borrows the concept of temperature from it. There is no cooling cycle; rather the temperature (which is still expressing the degree of “chaos” in the system) is used to control the range of movement for vertices on update, is computed for each vertex after an update, and is scaled down to smooth vertices’ oscillation.</p>

  <p class="body"><a id="pgfId-1003681"></a>Since GEM algorithm is not directly appliable as an instance of simulated annealing, we’ll stick with the algorithm developed by Davidson and Harel, which produces results of comparable quality.</p>

  <p class="body"><a id="pgfId-1003692"></a>As we mentioned in the previous chapters, the first step with graph drawing algorithms is stating the criteria that will be used to judge the quality of an embedding; the crossing number is not the only key to drawing graphs nicely. Davidson and Harel’s approach uses five criteria:</p>

  <ul class="calibre19">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre14" id="pgfId-1003709"></a>Distributing nodes evenly to spread the vertices uniformly in the canvas</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1003724"></a>Keeping vertices away from borders</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1003737"></a>Making edge-lengths uniform</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1003750"></a>Minimizing edge-crossings</p>
    </li>

    <li class="fm-list-numbered-last">
      <p class="list"><a class="calibre14" id="pgfId-1003763"></a>Avoiding vertex-edges overlap by keeping vertices from coming too close to edges</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1003775"></a>We must also clarify that the algorithm assumes the edges will be drawn as straight-line segments. Now, let’s see how these five criteria translate to formulas by writing five components of the cost function.</p>

  <p class="body"><a id="pgfId-1003786"></a>For the first component, the algorithm uses a formula derived from electric potential energy; given two vertices <code class="fm-code-in-text">v<sub class="subscript1">i</sub></code> and <code class="fm-code-in-text">v<sub class="subscript1">j</sub></code>, we compute</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F_EQ2.png"/></p>

  <p class="body"><a id="pgfId-1003811"></a>where <code class="fm-code-in-text">d<sub class="subscript1">ij</sub></code> is the distance between the two vertices, and <span class="cambria">λ</span><code class="fm-code-in-text"><sub class="subscript1">1</sub></code> is a parameter we pass to the algorithm to control the weight of this component, a normalizing factor that defines the relative importance of this criterion compared to the others. This term behaves like a repulsive force, so higher values push the algorithm to prefer embeddings with larger distances between the vertices.</p>

  <p class="body"><a id="pgfId-1003830"></a>To keep vertices away from borders, we add another component. For each vertex <code class="fm-code-in-text">v<sub class="subscript1">i</sub></code> we compute</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F_EQ3.png"/></p>

  <p class="body"><a id="pgfId-1003856"></a>Here the values <code class="fm-code-in-text">r<sub class="subscript1">i</sub></code>, <code class="fm-code-in-text">l<sub class="subscript1">i</sub></code>, <code class="fm-code-in-text">t<sub class="subscript1">i</sub></code>, and <code class="fm-code-in-text">b<sub class="subscript1">i</sub></code>, are the distances between <code class="fm-code-in-text">v<sub class="subscript1">i</sub></code> and the margins of the rectangular canvas where the graph is embedded; <span class="cambria">λ</span><code class="fm-code-in-text"><sub class="subscript1">2</sub></code> is another normalization factor, to weight this term. Higher values of <span class="cambria">λ</span><code class="fm-code-in-text"><sub class="subscript1">2</sub></code> will cause embeddings with vertices close to the borders to be penalized more.</p>

  <p class="body"><a id="pgfId-1003892"></a>Now let’s talk about edges. For each edge <code class="fm-code-in-text">e<sub class="subscript1">k</sub> = u</code> <span class="cambria">→</span> <code class="fm-code-in-text">v</code>, we compute</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F_EQ4.png"/></p>

  <p class="body"><a id="pgfId-1003919"></a>where <code class="fm-code-in-text">d<sub class="subscript1">k</sub> = distance(u,v)</code> is the length of the edge and <span class="cambria">λ</span><sub class="subscript">3</sub> is the usual normalization parameter. This behaves like an attractive force, so larger values for <span class="cambria">λ</span><sub class="subscript">3</sub> favor smaller distances between adjacent vertices.</p>

  <p class="body"><a id="pgfId-1003945"></a>For edge intersections, we can just count them and multiply the numbers of intersections by a normalization factor <span class="cambria">λ</span><code class="fm-code-in-text"><sub class="subscript1">4</sub></code>.</p>

  <p class="body"><a id="pgfId-1003957"></a>Finally, to keep vertices away from edges (if you remember, this was one of the key criteria we gave in chapter 16 to validate embeddings) we can add this term for each vertex-edge pair:</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F_EQ5.png"/></p>

  <p class="body"><a id="pgfId-1003975"></a>where <code class="fm-code-in-text">g<sub class="subscript1">kl</sub> = distance(e<sub class="subscript1">k</sub>,v<sub class="subscript1">l</sub>)</code> and <span class="cambria">λ</span><code class="fm-code-in-text"><sub class="subscript1">5</sub></code> is another normalization factor.</p>

  <p class="body"><a id="pgfId-1003998"></a>This term (another repulsive force) is quite expensive to compute (the edge-vertex distance is computationally heavy, as we saw in chapter 15), and even in the original paper is not used in the default settings of the algorithm. We’ll leave it out for now, while encouraging the reader to implement it as an exercise and then experiment with it on the examples we will show.</p>

  <p class="body"><a id="pgfId-1004013"></a>Listing 17.4 shows an implementation of the full cost function (with all five components), although the examples shown were run without the edge-vertex distance term.</p>

  <p class="body"><a id="pgfId-1004024"></a>As the next step, we would need the methods to compute transitions to new solutions; luckily, though, we can reuse the same methods we defined in the last section. After all, the problem space is the same; the only thing that we need to change is the cost function because we are changing our criteria to decide what a good embedding is.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1012006"></a>Listing 17.4 Cost function for Davidson and Harel’s algorithm</p>
  <pre class="programlisting"><b class="strong">function</b> cost(P, w, h, lambda1, lambda2, lambda3, lambda4, lambda5)        <span class="fm-combinumeral">❶</span>
  total ← 0
  <b class="strong">for</b> v <b class="strong">in</b> P.vertices <b class="strong">do</b>                                                   <span class="fm-combinumeral">❷</span>
    total ← total + lambda2 * ((1/x2) + (1/y2) + (1/(w-x)2) + (1/(h-y)2))  <span class="fm-combinumeral">❸</span>
    <b class="strong">for</b> u <b class="strong">in</b> P.vertices-{v} <b class="strong">do</b>                                             <span class="fm-combinumeral">❹</span>
      total ← total + lambda1 / distance(u, v)2                            <span class="fm-combinumeral">❺</span>
  <b class="strong">for</b> e=(u,v) <b class="strong">in</b> P.edges <b class="strong">do</b>                                                <span class="fm-combinumeral">❻</span>
    total ← total + lambda3 * distance(u, v)2                              <span class="fm-combinumeral">❼</span>
    <b class="strong">for</b> z <b class="strong">in</b> P.vertices-{u,v} <b class="strong">do</b>                                           <span class="fm-combinumeral">❽</span>
      total ← total + lambda5 / distance(e, z)2                            <span class="fm-combinumeral">❾</span>
  total ← total + lambda4 * P.intersections()                              <span class="fm-combinumeral">❿</span>
  <b class="strong">return</b> total                  </pre>

  <p class="fm-code-annotation"><a id="pgfId-1017841"></a><span class="fm-combinumeral">❶</span> Method <code class="fm-code-in-text2">cost</code><a id="marker-1017871"></a> takes the current solution, <code class="fm-code-in-text2">P</code> (a graph embedding, and also a point in the problem space), width (<code class="fm-code-in-text2">w</code>) and height (<code class="fm-code-in-text2">h</code>) of the canvas, and the normalizing factors for the components.</p>

  <p class="fm-code-annotation"><a id="pgfId-1014025"></a><span class="fm-combinumeral">❷</span> Cycles through the vertices in the graph.</p>

  <p class="fm-code-annotation"><a id="pgfId-1014056"></a><span class="fm-combinumeral">❸</span> Adds the second component of the objective function, corresponding to the repulsive force between vertices and the canvas’ borders</p>

  <p class="fm-code-annotation"><a id="pgfId-1014087"></a><span class="fm-combinumeral">❹</span> Cycles through all vertices except <code class="fm-code-in-text2">v</code></p>

  <p class="fm-code-annotation"><a id="pgfId-1014128"></a><span class="fm-combinumeral">❺</span> Adds the first component of the cost function, the repulsive force between vertices</p>

  <p class="fm-code-annotation"><a id="pgfId-1014167"></a><span class="fm-combinumeral">❻</span> Cycles through edges, for each edge <code class="fm-code-in-text2">e</code> between vertices <code class="fm-code-in-text2">u</code> and <code class="fm-code-in-text2">v</code> ...</p>

  <p class="fm-code-annotation"><a id="pgfId-1014198"></a><span class="fm-combinumeral">❼</span> Adds the third component, the attractive force between adjacent vertices</p>

  <p class="fm-code-annotation"><a id="pgfId-1014231"></a><span class="fm-combinumeral">❽</span> Cycles through all vertices except the edge’s endpoints</p>

  <p class="fm-code-annotation"><a id="pgfId-1014264"></a><span class="fm-combinumeral">❾</span> Adds the fifth component, the repulsive force between edges and vertices (except for each edge’s endpoints, of course)</p>

  <p class="fm-code-annotation"><a id="pgfId-1014296"></a><span class="fm-combinumeral">❿</span> At last, adds the fourth component, proportional to the number of edge intersections in the embedding</p>

  <p class="body"><a id="pgfId-1004363"></a>The algorithm in the paper was only using the vertex local update heuristic, but not with a constant range, rather making the neighborhood in which a vertex can be moved smaller with the progressing of the algorithm.</p>

  <p class="body"><a id="pgfId-1004372"></a>You can also find the working code implemented for the JsGraphs library<a id="marker-1004669"></a> on GitHub.<a href="#pgfId-1005557"><sup class="footnotenumber">19</sup></a></p>

  <p class="body"><a id="pgfId-1004388"></a>Speaking of good embeddings, in this case it makes little sense to check the quality of the results by looking at the average numbers over many repetitions: we want graphs to be drawn nicely, and there is no magic formula to measure “niceness.” The only way to judge the results is by presenting them to the human eye.</p>

  <p class="body"><a id="pgfId-1004401"></a>Figure 17.17 is, I think, the perfect summary to explain what we have been building in these last couple of sections. We are trying to come up with a nice embedding for the square grid graph with side 4, a graph with 16 vertices arranged like a square mesh.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F17.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1021070"></a>Figure 17.17 The square mesh graph with 16 vertices embedded by random sampling (left), minimum crossing simulated annealing (center), and Davidson and Harel’s algorithm (right)</p>

  <p class="body"><a id="pgfId-1004435"></a>Random sampling struggles to even find an embedding without intersections, a goal that is reached by the algorithm presented in section 17.3.1, which, however, doesn’t do a particularly good job of making the structure of the graph clear to us.</p>

  <p class="body"><a id="pgfId-1004446"></a>The drawing on the right, instead, looks almost perfectly symmetrical. Would you have been able to understand the shape of this graph from the other two embeddings?</p>

  <p class="body"><a id="pgfId-1004459"></a>For the record, this embedding was obtained by using the values summarized in table 17.4.</p>

  <p class="fm-table-caption"><a id="pgfId-1012097"></a>Table 17.4 Parameter values for Davidson and Harel’s drawing algorithm, used to draw the square grid in figure 17.17</p>

  <table border="1" class="contenttable" width="100%">
    <tr class="calibre8">
      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1012103"></a>Parameter</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1012105"></a>Meaning</p>
      </th>

      <th class="fm-contenttable" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1012107"></a>Value</p>
      </th>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012109"></a><code class="fm-code-in-text2">T<sub class="subscript">0</sub></code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012111"></a>Initial temperature</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012113"></a>1000</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012115"></a><code class="fm-code-in-text2">k</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012117"></a>(pseudo) Boltzmann constant</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012119"></a>1e+8</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012121"></a><span class="cambria">α</span></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012123"></a>Temperature decay</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012125"></a>0.95</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012127"></a><code class="fm-code-in-text2">Max Steps</code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012129"></a> </p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012131"></a>10000</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012133"></a><span class="cambria">λ</span><code class="fm-code-in-text2"><sub class="subscript">1</sub></code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012135"></a>Distance to border</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012137"></a>10</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012139"></a><span class="cambria">λ</span><code class="fm-code-in-text2"><sub class="subscript">2</sub></code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012141"></a>Distance between vertices</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012143"></a>0.01</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012145"></a><span class="cambria">λ</span><sub class="calibre25">3</sub></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012147"></a>Edge length</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012149"></a>2e-8</p>
      </td>
    </tr>

    <tr class="calibre8">
      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012151"></a><span class="cambria">λ</span><code class="fm-code-in-text2"><sub class="subscript">4</sub></code></p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012153"></a>Edge intersections</p>
      </td>

      <td class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1012155"></a>100</p>
      </td>
    </tr>
  </table>

  <p class="body"><a id="pgfId-1004672"></a>Figure 17.18 shows a couple more examples, with a larger grid and a different kind of graph, the triangular grid. They both look pretty nicely drawn, after some parameter tuning.</p>

  <p class="body"><a id="pgfId-1004966"></a>Before getting too excited and assuming this is the perfect algorithm for all graphs, we obviously need to try it on other kinds of graphs.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F18.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1021112"></a>Figure 17.18 The triangular graph with 4 vertices per side and the square mesh graph with 36 vertices embedded by Davidson and Harel’s algorithm.</p>

  <p class="body"><a id="pgfId-1004998"></a>Figure 17.19 shows the results for <code class="fm-code-in-text">K<sub class="subscript1">5</sub></code> and <code class="fm-code-in-text">K<sub class="subscript1">7</sub></code>. For both graphs, the embedding found has the minimum possible number of intersections, and vertices look well-spread but, as you can see, these embeddings are not perfect, because some vertices are too close to non-adjacent edges, and thus some edges overlap.</p>

  <p class="fm-figure"><img alt="" class="calibre23" src="../Images/ch17_F19.png"/></p>

  <p class="fm-figure-caption">Figure 17.19 <a id="pgfId-1021154"></a>Complete graphs <code class="fm-code-in-text">K<sub class="subscript1">5</sub></code> and <code class="fm-code-in-text">K<sub class="subscript1">7</sub></code> embedded by Davidson and Harel’s algorithm. Here we could use that fifth component of the cost function keeping vertices away from edges.</p>

  <p class="body"><a id="pgfId-1005050"></a>These situations can be corrected by adding the fifth component of the cost function, the one discouraging short distances between vertices and edges.</p>

  <p class="body"><a id="pgfId-1005059"></a>So, to close this chapter, here is a bit of homework for you: extend the cost function and find better embeddings for <a id="marker-1005224"></a><a id="marker-1005228"></a>these <a id="marker-1005232"></a><a id="marker-1005236"></a><a id="marker-1005240"></a>graphs.</p>

  <h2 class="fm-head" id="heading_id_20"><a id="pgfId-1005077"></a>Summary</h2>

  <ul class="calibre19">
    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1005089"></a>Simulated annealing is a stochastic alternative to gradient descent that uses concepts from physics to provide a dynamic technique, focusing on large-range search in the initial phases, and fine-tuning towards the end.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1005104"></a>Pros: it should be preferred when the domain of the cost function is discrete, the cost function is not differentiable or step-shaped, and there are many local minima.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1005119"></a>Cons: it should be avoided when local minima are in narrow “valleys,” because it becomes unlikely that the algorithm will find them.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1005134"></a>Compared to gradient descent, which takes the steepest path to minima, simulated annealing will need many more iterations to get to the same point.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1005146"></a>Simulated annealing can’t be guaranteed to return the optimal solution. If sub-optimal solutions are not admissible, then a different algorithm should be used.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1005162"></a>Finding the right configuration requires time and needs to be done for each problem.</p>
    </li>

    <li class="fm-list-numbered1">
      <p class="list"><a class="calibre14" id="pgfId-1005174"></a>The traveling salesman problem is a difficult problem to solve, but it is ubiquitous in logistics, planning, and electronics (for circuit <a class="calibre14" id="marker-1005244"></a><a class="calibre14" id="marker-1005248"></a>boards).</p>
    </li>
  </ul>
  <hr class="calibre22"/>

  <p class="fm-footnote"><sup class="footnotenumber">1.</sup> <a id="pgfId-1005253"></a>Technically, even gradient descent can move past a local minimum, depending on the shape of the function around it and on the learning rate; see figure 16.16.</p>

  <p class="fm-footnote"><sup class="footnotenumber">2.</sup> <a id="pgfId-1005265"></a>The original name would be <i class="calibre17">Hill climbing</i><a id="marker-1005283"></a>, when the goal of the optimization is maximization; since we aim to reduce cost, though, that would be confusing.</p>

  <p class="fm-footnote"><sup class="footnotenumber">3.</sup> <a id="pgfId-1005288"></a>Not having a static formula can be helpful with dynamic cost functions like externally computed metrics or simulations (for instance, in reinforcement learning, the cost is determined by running a simulation).</p>

  <p class="fm-footnote"><sup class="footnotenumber">4.</sup> <a id="pgfId-1005304"></a>Greedy algorithms make locally-optimal choices (for instance, they only go to positions with a lower cost). Unfortunately, this doesn’t always translate into getting to optimal results.</p>

  <p class="fm-footnote"><sup class="footnotenumber">5.</sup> <a id="pgfId-1005318"></a>We can consider simulated annealing a category of heuristics, aka <i class="calibre17">meta-heuristic</i><a id="marker-1005336"></a>: each algorithm using simulated annealing to solve a specific problem is going to be a heuristic.</p>

  <p class="fm-footnote"><sup class="footnotenumber">6.</sup> <a id="pgfId-1005341"></a>With the right configuration, and, as we’ll see, with a small-range transition function.</p>

  <p class="fm-footnote"><sup class="footnotenumber">7.</sup> <a id="pgfId-1005353"></a><span class="fm-hyperlink"><a href="https://github.com/mlarocca/AlgorithmsAndDataStructuresInAction#simulated-annealing">https://github.com/mlarocca/AlgorithmsAndDataStructuresInAction#simulated-annealing</a></span>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">8.</sup> <a id="pgfId-1005366"></a>Accordingly, the constant k is called <a id="id_Hlk56842265"></a>Boltzmann’s constant.</p>

  <p class="fm-footnote"><sup class="footnotenumber">9.</sup> <a id="pgfId-1005387"></a>We described hill descent in chapter 16.</p>

  <p class="fm-footnote"><sup class="footnotenumber">10.</sup> <a id="pgfId-1005401"></a>We would have to constrain the first and last delivery in each city to be the closest to the connections to the previous and the next city in the tour. Even so, it’s possible that the solution found in two steps is not the best possible. If the connections to other cities starts in different areas of town, this could influence the choice of the best sequence for the cities; the influence, however, could be small enough to be considered acceptable.</p>

  <p class="fm-footnote"><sup class="footnotenumber">11.</sup> <a id="pgfId-1005417"></a>As we discussed in chapter 16, NP-hardness is based on the worst-case scenarios; many problems, however, are difficult only for a minority of edge cases, while many real-world scenarios can be tackled more efficiently.</p>

  <p class="fm-footnote"><sup class="footnotenumber">12.</sup> <a id="pgfId-1005432"></a>See <span class="fm-hyperlink"><a href="http://mng.bz/PPWv">http://mng.bz/PPWv</a></span>.</p>

  <p class="fm-footnote"><sup class="footnotenumber">13.</sup> <a id="pgfId-1005448"></a>Or, equivalently, we could say that we were dealing with precisely one embedding on the graph.</p>

  <p class="fm-footnote"><sup class="footnotenumber">14.</sup> <a id="pgfId-1005463"></a>Because we are restricted to using straight-line segments, that are a sub-set of all possible curves for edges.</p>

  <p class="fm-footnote"><sup class="footnotenumber">15.</sup> <a id="pgfId-1005478"></a>P. Eades. “A Heuristic for Graph Drawing.” <i class="calibre17">Congressus Numerantium</i>, 42:149-160, 1984.</p>

  <p class="fm-footnote"><sup class="footnotenumber">16.</sup> <a id="pgfId-1005498"></a>T. Kamada and S. Kawai. “An algorithm for drawing general undirected graphs.” <i class="calibre17">Information Processing Letters,</i> 31, 1989.</p>

  <p class="fm-footnote"><sup class="footnotenumber">17.</sup> <a id="pgfId-1005523"></a>Ron Davidson and David Harel. “Drawing graphs nicely using simulated annealing.” <i class="calibre17">ACM Transactions on Graphics</i>, 15(4):301–331, 1996.</p>

  <p class="fm-footnote"><sup class="footnotenumber">18.</sup> <a id="pgfId-1005539"></a>Frick, Arne, Andreas Ludwig, and Heiko Mehldau. “A fast adaptive layout algorithm for undirected graphs (extended abstract and system demonstration).” <i class="calibre17">International Symposium on Graph Drawing</i>. Springer, Berlin, Heidelberg, 1994.</p>

  <p class="fm-footnote"><sup class="footnotenumber">19.</sup> <a id="pgfId-1005557"></a>See <span class="fm-hyperlink"><a href="http://mng.bz/JD1a">http://mng.bz/JD1a</a></span>.</p>
</body>
</html>
