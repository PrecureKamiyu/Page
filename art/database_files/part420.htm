<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>23.3  Concurrency Control in Distributed Databases</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part419.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part421.htm">下一个 &gt;</a></p><p class="s65" style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: left;"><a name="bookmark487">23.3  </a><span style=" color: #00AEEF;">Concurrency Control in Distributed Databases</span><a name="bookmark532">&zwnj;</a></p><p style="padding-top: 11pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">We now consider how the concurrency-control schemes discussed in Chapter 18 can be modiﬁed so that they can be used in a distributed environment. We assume that each node participates in the execution of a commit protocol to ensure global transaction atomicity.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">In this section, we assume that data items are not replicated, and we do not consider multiversion techniques. We discuss how to handle replicas later, in Section 23.4, and we discuss distributed multiversion concurrency control techniques in Section 23.5.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 8pt;padding-left: 119pt;text-indent: 0pt;text-align: left;">23.3.1 Locking Protocols</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">The various locking protocols described in Chapter 18 can be used in a distributed environment. We discuss implementation issues in this section.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">23.3.1.1 Single Lock-Manager Approach</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">In the <span class="s63">single lock-manager </span>approach, the system maintains a <i>single </i>lock manager that resides in a <i>single </i>chosen node— say <i>N</i><span class="s97">i</span>. All lock and unlock requests are made at node <i>N</i><span class="s97">i</span>. When a transaction needs to lock a data item, it sends a lock request to <i>N</i><span class="s97">i</span>. The lock manager determines whether the lock can be granted immediately. If the lock can be granted, the lock manager sends a message to that eﬀect to the node at which the lock request was initiated. Otherwise, the request is delayed until it can be granted, at</p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">which time a message is sent to the node at which the lock request was initiated. The transaction can read the data item from <i>any </i>one of the nodes at which a replica of the data item resides. In the case of a write, all the nodes where a replica of the data item resides must be involved in the writing.</p><p style="padding-left: 137pt;text-indent: 0pt;text-align: justify;">The scheme has these advantages:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s39" style="padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s46">Simple implementation</span><span class="p">. This scheme requires two messages for handling lock re- quests and one message for handling unlock requests.</span></p><p class="s39" style="padding-top: 4pt;padding-left: 139pt;text-indent: -16pt;text-align: justify;">• <span class="s46">Simple deadlock handling</span><span class="p">. Since all lock and unlock requests are made at one node, the deadlock-handling algorithms discussed in Chapter 18 can be applied directly.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 137pt;text-indent: 0pt;text-align: justify;">The disadvantages of the scheme are:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 139pt;text-indent: -16pt;line-height: 87%;text-align: justify;"><span class="s39">• </span><b>Bottleneck</b>. The node <i>N</i><span class="s97">i </span>becomes a bottleneck, since all requests must be processed there.</p><p style="padding-top: 4pt;padding-left: 139pt;text-indent: -16pt;line-height: 94%;text-align: justify;"><span class="s39">• </span><b>Vulnerability</b>. If the node <i>N</i><span class="s97">i </span>fails, the concurrency controller is lost. Either pro- cessing must stop, or a recovery scheme must be used so that a backup node can take over lock management from <i>N</i><span class="s97">i</span>, as described in Section 23.7.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s183" style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">23.3.1.2 Distributed Lock Manager</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">A compromise between the advantages and disadvantages can be achieved through the <span class="s63">distributed-lock-manager </span>approach, in which the lock-manager function is distributed over several nodes.</p><p style="padding-left: 88pt;text-indent: 17pt;line-height: 13pt;text-align: justify;">Each node maintains a local lock manager whose function is to administer the lock and unlock requests for those data items that are stored in that node. When a transaction wishes to lock a data item <i>Q </i>that resides at node <i>N</i><span class="s97">i </span>, a message is sent to the lock manager at node <i>N</i><span class="s97">i </span>requesting a lock (in a particular lock mode). If data item <i>Q </i>is locked in an incompatible mode, then the request is delayed until it can be granted. Once it has determined that the lock request can be granted, the lock manager sends a message back to the initiator indicating that it has granted the lock request.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The distributed-lock-manager scheme has the advantage of simple implementation, and it reduces the degree to which the coordinator is a bottleneck. It has a reasonably low overhead, requiring two message transfers for handling lock requests, and one mes- sage transfer for handling unlock requests. However, deadlock handling is more com- plex, since the lock and unlock requests are no longer made at a single node: There may be internode deadlocks even when there is no deadlock within a single node. The deadlock-handling algorithms discussed in Chapter 18 must be modiﬁed, as we shall discuss in Section 23.3.2, to detect global deadlocks.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">23.3.2 Deadlock Handling</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">The deadlock-prevention and deadlock-detection algorithms in Chapter 18 can be used in a distributed system, with some modiﬁcations.</p><p style="padding-left: 106pt;text-indent: 0pt;text-align: left;">Consider ﬁrst the deadlock-prevention techniques, which we saw in Section 18.2.1.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s39" style="padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Techniques for deadlock prevention based on lock ordering can be used in a dis- tributed system, with no changes at all. These techniques prevent cyclic lock waits; the fact that locks may be obtained at diﬀerent nodes has no eﬀect on prevention of cyclic lock waits.</span></p><p class="s39" style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;">• <span class="s40">Techniques based on preemption and transaction rollback can also be used un- changed in a distributed system. In particular, the </span><span class="s13">wait-die </span><span class="p">technique is used in several distributed systems. Recall that this technique allows older transactions to wait for locks held by younger transactions, but if a younger transaction needs to wait for a lock held by an older transaction, the younger transaction is rolled back. The transaction that is rolled back may subsequently be executed again; recall that it retains its original start time; if it is treated as a new transaction, it could be rolled back repeatedly, and starve, even as other transactions make progress and complete.</span></p><p class="s39" style="padding-top: 3pt;padding-left: 91pt;text-indent: 0pt;text-align: justify;">• <span class="s40">Timeout-based schemes, too, work without any changes in a distributed system.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s465" style="padding-left: 182pt;text-indent: 0pt;text-align: left;">	</p><p style="text-indent: 0pt;text-align: left;"><span><img width="149" height="108" alt="image" src="Image_3277.png"/></span></p><p class="s69" style="text-indent: 0pt;line-height: 13pt;text-align: left;">T<span class="s93">2</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s69" style="text-indent: 0pt;line-height: 13pt;text-align: left;">T<span class="s93">4</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s69" style="text-indent: 0pt;line-height: 13pt;text-align: left;">T<span class="s93">3</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="149" height="108" alt="image" src="Image_3278.png"/></span></p><p class="s69" style="text-indent: 0pt;line-height: 13pt;text-align: left;">T<span class="s93">1</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s69" style="text-indent: 0pt;line-height: 13pt;text-align: left;">T<span class="s93">2</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s69" style="text-indent: 0pt;line-height: 13pt;text-align: left;">T<span class="s93">5</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s69" style="text-indent: 0pt;line-height: 13pt;text-align: left;">T<span class="s93">3</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s42" style="padding-top: 4pt;padding-left: 224pt;text-indent: 0pt;text-align: left;">site <i>S</i><span class="s175">1</span>             site <i>S</i><span class="s175">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-left: 228pt;text-indent: 0pt;text-align: left;">Figure 23.4 <span class="s74">Local wait-for graphs.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Deadlock-prevention techniques may result in unnecessary waiting and rollback when used in a distributed system, just as in a centralized system,</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">We now consider deadlock-detection techniques that allow deadlocks to occur and detect them if they do. The main problem in a distributed system is deciding how to maintain the wait-for graph. Common techniques for dealing with this issue require that each node keep a <span class="s63">local wait-for graph</span>. The nodes of the graph correspond to all the transactions (local as well as nonlocal) that are currently either holding or requesting any of the items local to that node. For example, Figure 23.4 depicts a system consisting of two nodes, each maintaining its local wait-for graph. Note that transactions <i>T</i><span class="s98">2</span> and <i>T</i><span class="s98">3</span> appear in both graphs, indicating that the transactions have requested items at both nodes.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">These local wait-for graphs are constructed in the usual manner for local transac- tions and data items. When a transaction <i>T</i><span class="s97">i </span>on node <i>N</i><span class="s98">1</span> needs a resource in node <i>N</i><span class="s98">2</span>,</p><p class="s13" style="padding-left: 119pt;text-indent: 0pt;line-height: 80%;text-align: justify;"><span class="p">it sends a request message to node </span>N<span class="s98">2</span><span class="p">. If the resource is held by transaction </span>T<span class="s145">j </span><span class="p">, the system inserts an edge </span>T<span class="s145">i </span><span class="s86">→ </span>T<span class="s145">j </span><span class="p">in the local wait-for graph of node </span>N<span class="s93">2</span><span class="s94">.</span></p><p style="padding-left: 137pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">If any local wait-for graph has a cycle, a deadlock has occurred. On the other hand,</p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">the fact that there are no cycles in any of the local wait-for graphs does not mean that there are no deadlocks. To illustrate this problem, we consider the local wait-for graphs of Figure 23.4. Each wait-for graph is acyclic; nevertheless, a deadlock exists in the system because the <i>union </i>of the local wait-for graphs contains a cycle. This graph appears in Figure 23.5.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="149" height="108" alt="image" src="Image_3279.png"/></span></p><p class="s69" style="text-indent: 0pt;line-height: 13pt;text-align: left;">T<span class="s93">1</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s69" style="text-indent: 0pt;line-height: 13pt;text-align: left;">T<span class="s93">2</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s69" style="text-indent: 0pt;line-height: 13pt;text-align: left;">T<span class="s93">4</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s69" style="text-indent: 0pt;line-height: 13pt;text-align: left;">T<span class="s93">5</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s69" style="text-indent: 0pt;line-height: 13pt;text-align: left;">T<span class="s93">3</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s73" style="padding-top: 4pt;padding-left: 195pt;text-indent: 0pt;text-align: left;">Figure 23.5 <span class="s74">Global wait-for graph for Figure 23.4.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 17pt;text-align: justify;">In the <span class="s63">centralized deadlock detection </span>approach, the system constructs and main- tains a <span class="s63">global wait-for graph </span>(the union of all the local graphs) in a <i>single </i>node: the deadlock-detection coordinator. Since there is communication delay in the system, we must distinguish between two types of wait-for graphs. The <i>real </i>graph describes the real but unknown state of the system at any instance in time, as would be seen by an omni- scient observer. The <i>constructed </i>graph is an approximation generated by the controller during the execution of the controller’s algorithm. Obviously, the controller must gen- erate the constructed graph in such a way that, whenever the detection algorithm is invoked, the reported results are correct. <i>Correct </i>means in this case that, if a deadlock exists, it is reported promptly, and if the system reports a deadlock, it is indeed in a deadlock state.</p><p style="padding-left: 106pt;text-indent: 0pt;text-align: justify;">The global wait-for graph can be reconstructed or updated under these conditions:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s39" style="padding-left: 107pt;text-indent: -16pt;text-align: left;">• <span class="s40">Whenever a new edge is inserted in or removed from one of the local wait-for graphs.</span></p><p class="s39" style="padding-top: 4pt;padding-left: 91pt;text-indent: 0pt;text-align: left;">• <span class="s40">Periodically, when a number of changes have occurred in a local wait-for graph.</span></p><p class="s39" style="padding-top: 3pt;padding-left: 91pt;text-indent: 0pt;text-align: left;">• <span class="s40">Whenever the coordinator needs to invoke the cycle-detection algorithm.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">When the coordinator invokes the deadlock-detection algorithm, it searches its global graph. If it ﬁnds a cycle, it selects a victim to be rolled back. The coordinator must notify all the nodes that a particular transaction has been selected as the victim. The nodes, in turn, roll back the victim transaction.</p><p style="padding-left: 106pt;text-indent: 0pt;text-align: justify;">This scheme may produce unnecessary rollbacks if:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 107pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><span class="s63">False cycles </span><span class="p">exist in the global wait-for graph. As an illustration, consider a snap- shot of the system represented by the local wait-for graphs of Figure 23.6. Suppose that </span>T<span class="s98">2</span><span class="p"> releases the resource that it is holding in node </span>N<span class="s98">1</span><span class="p">, resulting in the deletion of the edge </span>T<span class="s130">1 </span><span class="s86">→ </span>T<span class="s130">2 </span><span class="s94">in </span>N<span class="s130">1</span><span class="s94">. Transaction </span>T<span class="s130">2 </span><span class="s94">then requests a resource held by </span>T<span class="s130">3</span></p><p class="s13" style="padding-left: 107pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><span class="p">at node </span>N<span class="s130">2</span><span class="s94">, resulting in the addition of the edge </span>T<span class="s130">2  </span><span class="s86">→ </span>T<span class="s130">3 </span><span class="s94">in </span>N<span class="s130">2</span><span class="s94">. If the </span><span class="s49">insert</span></p><p class="s13" style="padding-left: 107pt;text-indent: 0pt;line-height: 81%;text-align: justify;">T<span class="s130">2 </span><span class="s86">→ </span>T<span class="s130">3 </span><span class="s94">message from </span>N<span class="s130">2 </span><span class="s94">arrives before the </span><span class="s49">remove </span>T<span class="s130">1 </span><span class="s86">→ </span>T<span class="s130">2 </span><span class="s94">message from </span>N<span class="s130">1</span><span class="s94">, the coordinator may discover the false cycle </span>T<span class="s98">1</span><span class="p"> </span><span class="s86">→ </span>T<span class="s98">2</span><span class="p"> </span><span class="s86">→ </span>T<span class="s98">3</span><span class="p"> after the </span><span class="s49">insert </span><span class="p">(but before the </span><span class="s49">remove</span><span class="p">). Deadlock recovery may be initiated, although no deadlock</span></p><p style="padding-left: 107pt;text-indent: 0pt;text-align: justify;">has occurred.</p><p style="padding-left: 107pt;text-indent: 15pt;text-align: justify;">Note that the false-cycle situation could not occur under two-phase locking. The likelihood of false cycles is usually suﬃciently low that they do not cause a serious performance problem.</p><p class="s13" style="padding-top: 3pt;padding-left: 107pt;text-indent: -16pt;text-align: justify;"><span class="s39">• </span><span class="s40">A </span>deadlock <span class="p">has indeed occurred and a victim has been picked, while one of the transactions was aborted for reasons unrelated to the deadlock. For example, sup- pose that node </span>N<span class="s98">1</span><span class="p"> in Figure 23.4 decides to abort </span>T<span class="s98">2</span><span class="p">. At the same time, the coor- dinator has discovered a cycle and has picked </span>T<span class="s98">3</span><span class="p"> as a victim. Both </span>T<span class="s98">2</span><span class="p"> and </span>T<span class="s98">3</span><span class="p"> are now rolled back, although only </span>T<span class="s98">2</span><span class="p"> needed to be rolled back.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s465" style="padding-left: 213pt;text-indent: 0pt;text-align: left;">	</p><p style="text-indent: 0pt;text-align: left;"><span><img width="31" height="85" alt="image" src="Image_3280.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s69" style="padding-left: 35pt;text-indent: 0pt;text-align: center;">T<span class="s93">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s69" style="padding-left: 35pt;text-indent: 0pt;text-align: center;">T<span class="s93">2</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="108" height="108" alt="image" src="Image_3281.png"/></span></p><p class="s69" style="text-indent: 0pt;line-height: 13pt;text-align: left;">T<span class="s93">1</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s69" style="text-indent: 0pt;line-height: 13pt;text-align: left;">T<span class="s93">2</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s69" style="text-indent: 0pt;line-height: 13pt;text-align: left;">T<span class="s93">3</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="80" height="85" alt="image" src="Image_3282.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s69" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">T<span class="s93">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s69" style="text-indent: 0pt;text-align: right;">T<span class="s93">3</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s317" style="padding-top: 4pt;padding-left: 248pt;text-indent: 0pt;text-align: left;">S<span class="s119">1             </span>S<span class="s119">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s42" style="padding-top: 4pt;padding-left: 84pt;text-indent: 0pt;text-align: center;">coordinator</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-top: 4pt;padding-left: 187pt;text-indent: 0pt;text-align: left;">Figure 23.6 <span class="s74">False cycles in the global wait-for graph.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">Deadlock detection can be done in a distributed manner, with several nodes taking on parts of the task, instead of it being done at a single node. However, such algorithms are more complicated and more expensive. See the bibliographical notes for references to such algorithms.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">23.3.3 Leases</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">One of the issues with using locking in a distributed system is that a node holding a lock may fail, and not release the lock. The locked data item could thus become (logically) inaccessible, until the failed node recovers and releases the lock, or the lock is released by another node on behalf of the failed node.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">If an exclusive lock has been obtained on a data item, and the transaction is in the prepared state, the lock cannot be released until a commit/abort decision is made for the transaction. However, in many other cases it is acceptable for a lock that has been granted earlier to be revoked subsequently. In such cases, the concept of a lease can be very useful.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">A <span class="s63">lease </span>is a lock that is granted for a speciﬁc period of time. If the process that acquires a lease needs to continue holding the lock beyond the speciﬁed period, it can <span class="s63">renew </span>the lease. A lease renewal request is sent to the lock manager, which extends the lease and responds with an acknowledgment as long as the renewal request comes in time. However, if the time expires, and the process does not renew the lease, the lease is said to <span class="s63">expire</span>, and the lock is released. Thus, any lease acquired by a node that either fails, or gets disconnected from the lock manager, is automatically released when the</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">lease expires. The node that holds a lease regularly compares the current lease expiry time with its local clock to determine if it still has the lease or the lease has expired.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">One of the uses of leases is to ensure that there is only one coordinator for a proto- col in a distributed system. A node that wants to act as coordinator requests an exclusive lease on a data item associated with the protocol. If it gets the lease, it can act as co- ordinator until the lease expires; as long as it is active, it requests lease renewal before the lease expires, and it continues to be the coordinator as long as the lock manager permits the lease renewal.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">If a node <i>N</i><span class="s98">1</span> acting as coordinator dies after the expiry of the lease period, the lease automatically expires, and another node <i>N</i><span class="s98">2</span> that requests the lease can acquire it and become the coordinator. In most protocols it is important that there should be only one coordinator at a given time. The lease mechanism guarantees this, as long as clocks are synchronized. However, if the coordinator’s clock runs slower than the lock manager’s clock, a situation can arise where the coordinator thinks it still has the lease, while the lock manager thinks the lease has expired. While clocks cannot be exactly synchronized, in practice the inaccuracy is not very high. The lock manager waits for some extra wait time after the lease expiry time to account for clock inaccuracies before it actually treats the lease as expired.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">A node that checks the local clock and decides it still has a lease may then take a subsequent action as coordinator. It is possible that the lease may have expired between when the clock was checked and when the subsequent action took place, which could result in the action taking place after the node is no longer the coordinator. Further, even if the action took place while the node had a valid lease, a message sent by the node may be delivered after a delay, by which time the node may have lost its lease. While it is possible for the network to deliver a message arbitrarily late, the system can decide on a maximum message delay time, and any message that is older is ignored by the recipient; messages have timestamps set by the sender, which are used to detect if a message needs to be ignored.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The time gaps due to the above two issues can be taken into account by checking that the lease expiry is at least some time <i>t</i><span class="s181">′</span><span class="s15"> </span>into the future before initiating an action, where <i>t</i><span class="s181">′</span><span class="s15"> </span>is a bound on how long the action will take after the lease time check, including the maximum message delay.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">We have assumed here that while coordinators may fail, the lock manager that issues leases is able to tolerate faults. We study in Section 23.8.4 how to build a fault- tolerant lock manager; we note that the techniques described in that section are general purpose and can be used to implement fault-tolerant versions of any deterministic pro- cess, modeled as a “state machine.”</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 7pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">23.3.4 Distributed Timestamp-Based Protocols</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">The principal idea behind the timestamp-based concurrency control protocols in Sec- tion 18.5 is that each transaction is given a <i>unique </i>timestamp that the system uses in</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="110" height="65" alt="image" src="Image_3283.png"/></span></p><p class="s42" style="padding-left: 152pt;text-indent: -6pt;text-align: left;">local unique timestamp</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="108" height="65" alt="image" src="Image_3284.png"/></span></p><p class="s42" style="padding-left: 143pt;text-indent: 0pt;text-align: left;">site identifier</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse" cellspacing="0"><tr style="height:22pt"><td style="width:66pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#E9F7FE"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:65pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20;border-left-style:solid;border-left-width:1pt;border-left-color:#231F20;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#231F20;border-right-style:solid;border-right-width:1pt;border-right-color:#231F20" bgcolor="#E9F7FE"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p class="s42" style="padding-left: 380pt;text-indent: 0pt;text-align: left;">global unique identifier</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s73" style="padding-left: 202pt;text-indent: 0pt;text-align: left;">Figure 23.7 <span class="s74">Generation of unique timestamps.</span></p><p style="padding-top: 8pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">deciding the serialization order. Our ﬁrst task, then, in generalizing the centralized scheme to a distributed scheme is to develop a scheme for generating unique times- tamps. We then discuss how the timestamp-based protocols can be used in a distributed setting.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">23.3.5 Generation of Timestamps</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">There are two primary methods for generating unique timestamps, one centralized and one distributed. In the centralized scheme, a single node distributes the timestamps. The node can use a logical counter or its own local clock for this purpose. While this scheme is easy to implement, failure of the node would potentially block all transaction processing in the system.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">In the distributed scheme, each node generates a unique local timestamp by using either a logical counter or the local clock. We obtain the unique global timestamp by concatenating the unique local timestamp with the node identiﬁer, which also must be unique (Figure 23.7). If a node has multiple threads running on it (as is almost always the case today), a thread identiﬁer is concatenated with the node identiﬁer, to make the timestamp unique. Further, we assume that consecutive calls to get the local timestamp within a node/thread will return diﬀerent timestamps; if this is not guaranteed by the local clock, the returned local timestamp value may need to be incremented, to ensure two calls do not get the same local timestamp.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">The order of concatenation is important! We use the node identiﬁer in the least signiﬁcant position to ensure that the global timestamps generated in one node are not always greater than those generated in another node.</p><p style="padding-left: 119pt;text-indent: 17pt;text-align: justify;">We may still have a problem if one node generates local timestamps at a rate faster than that of the other nodes. In such a case, the fast node’s logical counter will be larger than that of other nodes. Therefore, all timestamps generated by the fast node will be larger than those generated by other nodes. What we need is a mechanism to ensure that local timestamps are generated fairly across the system. There are two solution approaches for this problem.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 145pt;text-indent: -16pt;text-align: left;">1. <span class="p">Keep the clocks synchronized by using a network time protocol, which is a stan- dard feature in computers today. The protocol periodically communicates with a</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 113pt;text-indent: 0pt;text-align: justify;">server to ﬁnd the current time. If the local time is ahead of the time returned by the server, the local clock is slowed down, whereas if the local time is behind the time returned by the server it is speeded up, to bring it back in synchronization with the time at the server. Since all nodes are approximately synchronized with the server, they are also approximately synchronized with each other.</p><p style="padding-top: 6pt;padding-left: 113pt;text-indent: -17pt;line-height: 94%;text-align: justify;"><span class="s63">2. </span>We deﬁne within each node <i>N</i><span class="s97">i </span>a <span class="s63">logical clock </span>(<i>LC</i><span class="s97">i</span>), which generates the unique local timestamp. The logical clock can be implemented as a counter that is in- cremented after a new local timestamp is generated. To ensure that the various</p><p style="padding-top: 2pt;padding-left: 113pt;text-indent: 0pt;line-height: 72%;text-align: justify;">logical clocks are synchronized, we require that a node <i>N</i><span class="s97">i </span>advance its logical clock whenever a transaction <i>T</i><span class="s97">i </span>with timestamp <span class="s83">&lt; </span><i>x</i>, <i>y </i><span class="s83">&gt; </span>visits that node and <i>x </i>is greater than the current value of <i>LC</i><span class="s145">i</span>. In this case, node <i>N</i><span class="s145">i </span>advances its logical</p><p style="padding-left: 113pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">clock to the value <i>x </i>+ 1. As long as messages are exchanged regularly, the logical</p><p style="padding-left: 113pt;text-indent: 0pt;text-align: justify;">clocks will be approximately synchronized.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">23.3.6 Distributed Timestamp Ordering</p><p style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;text-align: justify;">The timestamp ordering protocol can be easily extended to a parallel or distributed database setting. Each transaction is assigned a globally unique timestamp at the node where it originates. Requests sent to other nodes include the transaction timestamp. Each node keeps track of the read and write timestamps of the data items at that node. Whenever an operation is received by a node, it does the timestamp checks that we saw in Section 18.5.2, locally, without any need to communicate with other nodes.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Timestamps must be reasonably synchronized across nodes; otherwise, the follow- ing problem can occur. Suppose one node has a time signiﬁcantly lagging the others, and a transaction <i>T</i><span class="s98">1</span> gets its timestamp at that node <i>n</i><span class="s98">1</span>. Suppose the transaction <i>T</i><span class="s98">1</span> fails a timestamp test on a data item <i>d</i><span class="s97">i </span>because <i>d</i><span class="s97">i </span>has been updated by a transaction <i>T</i><span class="s98">2</span></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">with a higher timestamp; <i>T</i><span class="s98">1</span> would be restarted with a new timestamp, but if the time</p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">at node <i>n</i><span class="s98">1</span> is not synchronized, the new timestamp may still be old enough to cause the timestamp test to fail, and <i>T</i><span class="s98">1</span> would be restarted repeatedly until the time at <i>n</i><span class="s98">1</span> advances ahead of the timestamp of <i>T</i><span class="s98">2</span>.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Note that as in the centralized case, if a transaction <i>T</i><span class="s145">i </span>reads an uncommitted value written by another transaction <i>T</i><span class="s97">j </span>, <i>T</i><span class="s97">i </span>cannot commit until <i>T</i><span class="s97">j </span>commits. This can be en- sured either by making reads wait for uncommitted writes to be committed, which can be implemented using locking, or by introducing commit dependencies, as discussed in Section 18.5. The waiting time can be exacerbated by the time required to perform</p><p class="s13" style="padding-left: 88pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><span class="s42">2PC</span><span class="s43">, if the transaction performs updates at more than one node. While a transaction </span>T<span class="s97">i </span><span class="p">is in the prepared state, its writes are not committed, so any transaction with a higher timestamp that reads an item written by </span>T<span class="s145">i </span><span class="p">would be forced to wait.</span></p><p style="padding-left: 106pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">We also note that the <i>multiversion timestamp ordering protocol </i>can be used locally</p><p style="padding-left: 88pt;text-indent: 0pt;text-align: justify;">at each node, without any need to communicate with other nodes, similar to the case of the timestamp ordering protocol.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 4pt;padding-left: 119pt;text-indent: 0pt;text-align: left;">23.3.7 Distributed Validation</p><p style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: justify;">We now consider the validation-based protocol (also called the optimistic concurrency control protocol) that we saw in Section 18.6. The protocol is based on three times- tamps:</p><p class="s39" style="padding-top: 10pt;padding-left: 123pt;text-indent: 0pt;text-align: left;">• <span class="s40">The start timestamp StartTS(</span><span class="s13">T</span><span class="s97">i</span><span class="p">).</span></p><p class="s39" style="padding-top: 1pt;padding-left: 123pt;text-indent: 0pt;text-align: left;">• <span class="s40">The validation timestamp, TS(</span><span class="s13">T</span><span class="s97">i</span><span class="p">), which is used as the serialization order.</span></p><p class="s39" style="padding-top: 2pt;padding-left: 139pt;text-indent: -16pt;line-height: 90%;text-align: left;">• <span class="s40">The ﬁnish timestamp FinishTS(</span><span class="s13">T</span><span class="s145">i</span><span class="p">) which identiﬁes when the writes of a transac- tion have completed.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;text-align: justify;">While we saw a serial version of the validation protocol in Section 18.6, where only one transaction can perform validation at a time, there are extensions to the protocol to allow validations of multiple transactions to occur concurrently, within a single system.</p><p style="padding-left: 137pt;text-indent: 0pt;text-align: justify;">We now consider how to adapt the protocol to a distributed setting.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 145pt;text-indent: -16pt;text-align: left;">1. <span class="p">Validation is done locally at each node, with timestamps assigned as described below.</span></p><p style="padding-top: 6pt;padding-left: 145pt;text-indent: -17pt;line-height: 92%;text-align: justify;"><span class="s63">2. </span>In a distributed setting, the validation timestamp TS(<i>T</i><span class="s97">i</span>) can be assigned at any of the nodes, but the same timestamp TS(<i>T</i><span class="s97">i</span>) must be used at all nodes where validation is to be performed. Transactions must be serializable based on their timestamps TS(<i>T</i><span class="s145">i</span>).</p><p style="padding-top: 5pt;padding-left: 145pt;text-indent: -17pt;line-height: 70%;text-align: justify;"><span class="s63">3. </span>The validation test for a transaction <i>T</i><span class="s97">i </span>looks at all transactions <i>T</i><span class="s97">j </span>with TS(<i>T</i><span class="s97">j </span>) <span class="s83">&lt; </span>TS(<i>T</i><span class="s97">i</span>), to check if <i>T</i><span class="s97">j </span>either ﬁnished before <i>T</i><span class="s97">i </span>started, or has no conﬂicts with</p><p class="s13" style="padding-left: 145pt;text-indent: 0pt;line-height: 87%;text-align: justify;">T<span class="s97">i</span><span class="p">. The assumption is that once a particular transaction enters the validation phase, no transaction with a lower timestamp can enter the validation phase. The</span></p><p style="padding-left: 145pt;text-indent: 0pt;text-align: justify;">assumption can be ensured in a centralized system by assigning the timestamps in a critical section, but cannot be ensured in a distributed setting.</p><p style="padding-top: 2pt;padding-left: 145pt;text-indent: 14pt;line-height: 72%;text-align: justify;">A key problem in the distributed setting is that a transaction <i>T</i><span class="s97">j </span>may enter the validation phase after a transaction <i>T</i><span class="s97">i</span>, but with TS(<i>T</i><span class="s97">j </span>) <span class="s83">&lt; </span>TS(<i>T</i><span class="s97">i</span>). It is too late for <i>T</i><span class="s97">i </span>to be validated against <i>T</i><span class="s97">j </span>. However, this problem can be easily ﬁxed by rolling</p><p style="padding-left: 145pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">back any transaction if, when it starts validation at a node, a transaction with a</p><p style="padding-left: 145pt;text-indent: 0pt;text-align: justify;">later timestamp had already started validation at that node.</p><p style="padding-top: 7pt;padding-left: 145pt;text-indent: -17pt;line-height: 83%;text-align: justify;"><span class="s63">4. </span>The start and ﬁnish timestamps are used to identify transactions <i>T</i><span class="s97">j </span>whose writes would deﬁnitely have been seen by a transaction <i>T</i><span class="s97">i</span>. These timestamps must be assigned locally at each node, and must satisfy StartTS(<i>T</i><span class="s97">i</span>) <span class="s86">≤ </span>TS(<i>T</i><span class="s97">i</span>) <span class="s86">≤ </span>FinishTS(<i>T</i><span class="s97">i</span>). Each node uses these timestamps to perform validation locally.</p><p style="padding-top: 4pt;padding-left: 145pt;text-indent: -17pt;text-align: justify;"><span class="s63">5. </span>When used in conjunction with <span class="s44">2PC</span>, a transaction must ﬁrst be validated and then enter the prepared state. Writes cannot be committed at the database until</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 113pt;text-indent: 0pt;line-height: 88%;text-align: justify;">the transaction enters the committed state in <span class="s44">2PC</span>. Suppose a transaction <i>T</i><span class="s145">j </span>reads an item updated by a transaction <i>T</i><span class="s97">i </span>that is in the prepared state and is allowed to proceed using the old value of the data item (since the value generated by <i>T</i><span class="s97">i </span>has not yet been written to the database). Then, when transaction <i>T</i><span class="s97">j </span>attempts to validate, it will be serialized after <i>T</i><span class="s97">i </span>and will surely fail validation if <i>T</i><span class="s97">i </span>commits. Thus, the read by <i>T</i><span class="s145">j </span>may as well be held until <i>T</i><span class="s145">i </span>commits and ﬁnishes its writes. The above behavior is the same as what would happen with locking, with write</p><p style="padding-left: 113pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">locks acquired at the time of validation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Although full implementations of validation-based protocols are not widely used in distributed settings, optimistic concurrency control without read validation, which we saw in Section 18.9.3, is widely used in distributed settings. Recall that the scheme depends on storing a version number with each data item, a feature that is supported by many key-value stores.<span class="s76">1</span> Version numbers are incremented each time the data item is updated.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">Validation is performed at the time of writing the data item, which can be done using a test-and-set function based on version numbers, that is supported by some key- value stores. This function allows an update to a data item to be conditional on the current version of the data item being the same as a speciﬁed version number. If the current version number of the data item is more recent than the speciﬁed version num- ber, the update is not performed. For example, a transaction that read version 7 of a data item can perform a write, conditional on the version still being at 7. If the item has been updated meanwhile, the current version would not match, and the write would fail; however, if the version number is still 7, the write would be performed successfully, and the version number incremented to 8.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">The test-and-set function can thus be used by applications to implement the lim- ited form of validation-based concurrency control, discussed in Section 18.9.3, at the level of individual data items. Thereby, a transaction could read a value from a data item, perform computation locally, and update the data item at the end, as long as the value it read has not changed subsequently. This approach does not guarantee overall serializability, but it does prevent the lost-update anomaly.</p><p style="padding-left: 88pt;text-indent: 17pt;text-align: justify;">HBase supports the test-and-set operation based on comparing values (similar to the hardware test-and-set operation), which is called <span class="s49">checkAndPut()</span>. Instead of com- paring to a system-generated version number, the <span class="s49">checkAndPut() </span>invocation can pass in a column and a value; the update is performed only if the row has the speciﬁed value for the speciﬁed column. The check and the update are performed atomically. A variant, <span class="s49">checkAndMutate()</span>, allows multiple modiﬁcations to a row, such as adding or updating a column, deleting a column, or incrementing a column, after checking a condition, as a single atomic operation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="95" height="1" alt="image" src="Image_3285.png"/></span></p><p class="s77" style="padding-top: 3pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">1<span class="s78">Note that this is not the same as multiversioning, since only one version needs to be stored.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part419.htm">&lt; 上一个</a><span> | </span><a href="../database.html">内容</a><span> | </span><a href="part421.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
