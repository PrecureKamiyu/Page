<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Linux Kernel Development, Third Edition</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<div id="filepos223287" style="height:0pt"></div><h2 class="calibre_4" id="calibre_pb_35"><span class="bold">4. Process Scheduling</span></h2><div class="calibre_5"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos223450"> </div>
<p class="calibre_2">The prior chapter discussed <em class="calibre4">processes</em>, the operating system abstraction of active program code. This chapter discusses the <em class="calibre4">process scheduler</em>, the kernel subsystem that puts those processes to work.</p><div class="calibre_3"> </div>
<p class="calibre_2">The process scheduler decides which process runs, when, and for how long. The process scheduler (or simply the <em class="calibre4">scheduler</em>, to which it is often shortened) divides the finite resource of processor time between the runnable processes on a system. The scheduler is the basis of a multitasking operating system such as Linux. By deciding which process runs next, the scheduler is responsible for best utilizing the system and giving users the impression that multiple processes are executing simultaneously.</p><div class="calibre_3"> </div>
<p class="calibre_2">The idea behind the scheduler is simple. To best utilize processor time, assuming there are <em class="calibre4">runnable</em> processes, a process should always be running. If there are more runnable processes than processors in a system, some processes will not be running at a given moment. These processes are <em class="calibre4">waiting to run</em>. Deciding which process runs next, given a set of runnable processes, is the fundamental decision that the scheduler must make.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos224828"> </div>
<h3 class="calibre_21"><span class="bold">Multitasking</span></h3><div class="calibre_22"> </div>
<p class="calibre_2">A <em class="calibre4">multitasking</em> operating system is one that can simultaneously interleave execution of more than one process. On single processor machines, this gives the illusion of multiple processes running concurrently. On multiprocessor machines, such functionality enables processes to actually run concurrently, in parallel, on different processors. On either type of machine, it also enables many processes to <em class="calibre4">block</em> or <em class="calibre4">sleep</em>, not actually executing until work is available. These processes, although in memory, are not <em class="calibre4">runnable</em>. Instead, such processes utilize the kernel to wait until some event (keyboard input, network data, passage of time, and so on) occurs. Consequently, a modern Linux system can have many processes in memory but, say, only one in a runnable state.</p><div class="calibre_3"> </div>
<p class="calibre_2">Multitasking operating systems come in two flavors: <em class="calibre4">cooperative multitasking</em> and <em class="calibre4">preemptive multitasking</em>. Linux, like all Unix variants and most modern operating systems, implements preemptive multitasking. In preemptive multitasking, the scheduler decides when a process is to cease running and a new process is to begin running. The act of <a id="filepos226155"></a>involuntarily suspending a running process is called <em class="calibre4">preemption</em>. The time a process runs before it is preempted is usually predetermined, and it is called the <em class="calibre4">timeslice</em> of the process. The timeslice, in effect, gives each runnable process a <em class="calibre4">slice</em> of the processor’s time. Managing the timeslice enables the scheduler to make global scheduling decisions for the system. It also prevents any one process from monopolizing the processor. On many modern operating systems, the timeslice is dynamically calculated as a function of process behavior and configurable system policy. As we shall see, Linux’s unique “fair” scheduler does not employ timeslices <em class="calibre4">per se</em>, to interesting effect.</p><div class="calibre_3"> </div>
<p class="calibre_2">Conversely, in <em class="calibre4">cooperative multitasking</em>, a process does not stop running until it voluntary decides to do so. The act of a process voluntarily suspending itself is called <em class="calibre4">yielding</em>. Ideally, processes yield often, giving each runnable process a decent chunk of the processor, but the operating system cannot enforce this. The shortcomings of this approach are manifest: The scheduler cannot make global decisions regarding how long processes run; processes can monopolize the processor for longer than the user desires; and a hung process that never yields can potentially bring down the entire system. Thankfully, most operating systems designed in the last two decades employ preemptive multitasking, with Mac OS 9 (and earlier) and Windows 3.1 (and earlier) being the most notable (and embarrassing) exceptions. Of course, Unix has sported preemptive multitasking since its inception.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos227902"> </div>
<h3 class="calibre_21"><span class="bold">Linux’s Process Scheduler</span></h3><div class="calibre_22"> </div>
<p class="calibre_2">From Linux’s first version in 1991 through the 2.4 kernel series, the Linux scheduler was simple, almost pedestrian, in design. It was easy to understand, but scaled poorly in light of many runnable processes or many processors.</p><div class="calibre_3"> </div>
<p class="calibre_2">In response, during the 2.5 kernel development series, the Linux kernel received a scheduler overhaul. A new scheduler, commonly called the <em class="calibre4"><code class="calibre6"><span class="calibre7">O(1)</span></code> scheduler</em> because of its algorithmic behavior,<sup class="calibre8"><a id="filepos228561" href="#filepos228949">1</a></sup> solved the shortcomings of the previous Linux scheduler and introduced powerful new features and performance characteristics. By introducing a constant-time algorithm for timeslice calculation and per-processor runqueues, it rectified the design limitations of the earlier scheduler.</p><div class="calibre_3"> </div>
<p class="calibre_2"><sup class="calibre8"><a id="filepos228949" href="#filepos228561">1</a></sup>
<em class="calibre4"><code class="calibre6"><span class="calibre7">O(1)</span></code> is an example of big-o notation. In short, it means the scheduler can perform its work in constant time, regardless of the size of any inputs. A full explanation of big-o notation is in <a href="index_split_015.html#filepos343716">Chapter 6</a>, “Kernel Data Structures.”</em></p><div class="calibre_3"> </div>
<p class="calibre_2">The <code class="calibre6"><span class="calibre7">O(1)</span></code> scheduler performed admirably and scaled effortlessly as Linux supported large “iron” with tens if not hundreds of processors. Over time, however, it became evident that the <code class="calibre6"><span class="calibre7">O(1)</span></code> scheduler had several pathological failures related to scheduling latency-sensitive applications. These applications, called <em class="calibre4">interactive processes</em>, include any application with which the user interacts. Thus, although the <code class="calibre6"><span class="calibre7">O(1)</span></code> scheduler was ideal for large server workloads—which lack interactive processes—it performed below par on desktop systems, where interactive applications are the <em class="calibre4">raison d’être</em>. Beginning early in the <a id="filepos230136"></a>2.6 kernel series, developers introduced new process schedulers aimed at improving the interactive performance of the <code class="calibre6"><span class="calibre7">O(1)</span></code> scheduler. The most notable of these was the <em class="calibre4">Rotating Staircase Deadline</em> scheduler, which introduced the concept of <em class="calibre4">fair scheduling</em>, borrowed from queuing theory, to Linux’s process scheduler. This concept was the inspiration for the <code class="calibre6"><span class="calibre7">O(1)</span></code> scheduler’s eventual replacement in kernel version 2.6.23, the <em class="calibre4">Completely Fair Scheduler</em>, or <em class="calibre4">CFS</em>.</p><div class="calibre_3"> </div>
<p class="calibre_2">This chapter discusses the fundamentals of scheduler design and how they apply to the Completely Fair Scheduler and its goals, design, implementation, algorithms, and related system calls. We also discuss the <code class="calibre6"><span class="calibre7">O(1)</span></code> scheduler because its implementation is a more “classic” Unix process scheduler model.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos231162"> </div>
<h3 class="calibre_21"><span class="bold">Policy</span></h3><div class="calibre_22"> </div>
<p class="calibre_2">Policy is the behavior of the scheduler that determines what runs when. A scheduler’s policy often determines the overall feel of a system and is responsible for optimally utilizing processor time. Therefore, it is very important.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos231558"> </div>
<h4 class="calibre_27"><span class="calibre3">I/O-Bound Versus Processor-Bound Processes</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">Processes can be classified as either <em class="calibre4">I/O-bound</em> or <em class="calibre4">processor-bound</em>. The former is characterized as a process that spends much of its time submitting and waiting on I/O requests. Consequently, such a process is runnable for only short durations, because it eventually blocks waiting on more I/O. (Here, by <em class="calibre4">I/O</em>, we mean any type of blockable resource, such as keyboard input or network I/O, and not just disk I/O.) Most graphical user interface (GUI) applications, for example, are I/O-bound, even if they never read from or write to the disk, because they spend most of their time waiting on user interaction via the keyboard and mouse.</p><div class="calibre_3"> </div>
<p class="calibre_2">Conversely, processor-bound processes spend much of their time executing code. They tend to run until they are preempted because they do not block on I/O requests very often. Because they are not I/O-driven, however, system response does not dictate that the scheduler run them often. A scheduler policy for processor-bound processes, therefore, tends to run such processes less frequently but for longer durations. The ultimate example of a processor-bound process is one executing an infinite loop. More palatable examples include programs that perform a lot of mathematical calculations, such as <em class="calibre4">ssh-keygen</em> or <em class="calibre4">MATLAB</em>.</p><div class="calibre_3"> </div>
<p class="calibre_2">Of course, these classifications are not mutually exclusive. Processes can exhibit both behaviors simultaneously: The X Window server, for example, is both processor and I/O-intense. Other processes can be I/O-bound but dive into periods of intense processor action. A good example of this is a word processor, which normally sits waiting for key presses but at any moment might peg the processor in a rabid fit of spell checking or macro calculation.</p><div class="calibre_3"> </div>
<p class="calibre_2">The scheduling policy in a system must attempt to satisfy two conflicting goals: fast process response time (low latency) and maximal system utilization (high throughput). To <a id="filepos233781"></a>satisfy these at-odds requirements, schedulers often employ complex algorithms to determine the most worthwhile process to run while not compromising fairness to other, lower priority, processes. The scheduler policy in Unix systems tends to explicitly favor I/O-bound processes, thus providing good process response time. Linux, aiming to provide good interactive response and desktop performance, optimizes for process response (low latency), thus favoring I/O-bound processes over processor-bound processors. As we will see, this is done in a creative manner that does not neglect processor-bound processes.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos234442"> </div>
<h4 class="calibre_27"><span class="calibre3">Process Priority</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">A common type of scheduling algorithm is <em class="calibre4">priority-based</em> scheduling. The goal is to rank processes based on their worth and need for processor time. The general idea, which isn’t exactly implemented on Linux, is that processes with a higher priority run before those with a lower priority, whereas processes with the same priority are scheduled <em class="calibre4">round-robin</em> (one after the next, repeating). On some systems, processes with a higher priority also receive a longer timeslice. The runnable process with timeslice remaining and the highest priority always runs. Both the user and the system can set a process’s priority to influence the scheduling behavior of the system.</p><div class="calibre_3"> </div>
<p class="calibre_2">The Linux kernel implements two separate priority ranges. The first is the <em class="calibre4">nice</em> value, a number from –20 to +19 with a default of 0. Larger nice values correspond to a lower priority—you are being “nice” to the other processes on the system. Processes with a lower nice value (higher priority) receive a larger proportion of the system’s processor compared to processes with a higher nice value (lower priority). Nice values are the standard priority range used in all Unix systems, although different Unix systems apply them in different ways, reflective of their individual scheduling algorithms. In other Unix-based systems, such as Mac OS X, the nice value is a control over the <em class="calibre4">absolute</em> timeslice allotted to a process; in Linux, it is a control over the <em class="calibre4">proportion</em> of timeslice. You can see a list of the processes on your system and their respective nice values (under the column marked <em class="calibre4">NI</em>) with the command <code class="calibre6"><span class="calibre7">ps -el</span></code>.</p><div class="calibre_3"> </div>
<p class="calibre_2">The second range is the <em class="calibre4">real-time priority</em>. The values are configurable, but by default range from 0 to 99, inclusive. Opposite from nice values, higher real-time priority values correspond to a greater priority. All real-time processes are at a higher priority than normal processes; that is, the real-time priority and nice value are in disjoint value spaces. Linux implements real-time priorities in accordance with the relevant Unix standards, specifically POSIX.1b. All modern Unix systems implement a similar scheme. You can see a list of the processes on your system and their respective real-time priority (under the column marked <em class="calibre4">RTPRIO</em>) with the command</p><div class="calibre_3"> </div>
<p class="calibre_28"><tt class="calibre6"><span class="calibre12">ps -eo state,uid,pid,ppid,rtprio,time,comm.</span></tt></p><div class="calibre_22"> </div>
<p class="calibre_2">A value of “-” means the process is not real-time.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos237371"> </div>
<h4 class="calibre_27"><span class="calibre3">Timeslice</span></h4><div class="calibre_24"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos237482"> </div>
<p class="calibre_2">The timeslice<sup class="calibre8"><a id="filepos237551" href="#filepos238498">2</a></sup> is the numeric value that represents how long a task can run until it is preempted. The scheduler policy must dictate a default timeslice, which is not a trivial exercise. Too long a timeslice causes the system to have poor interactive performance; the system will no longer feel as if applications are concurrently executed. Too short a timeslice causes significant amounts of processor time to be wasted on the overhead of switching processes because a significant percentage of the system’s time is spent switching from one process with a short timeslice to the next. Furthermore, the conflicting goals of I/O-bound versus processor-bound processes again arise: I/O-bound processes do not need longer timeslices (although they do like to run often), whereas processor-bound processes crave long timeslices (to keep their caches hot).</p><div class="calibre_3"> </div>
<p class="calibre_2"><sup class="calibre8"><a id="filepos238498" href="#filepos237551">2</a></sup>
<em class="calibre4">Timeslice is sometimes called</em> quantum <em class="calibre4">or</em> processor slice <em class="calibre4">in other systems. Linux calls it timeslice, thus so should you.</em></p><div class="calibre_3"> </div>
<p class="calibre_2">With this argument, it would seem that <em class="calibre4">any</em> long timeslice would result in poor interactive performance. In many operating systems, this observation is taken to heart, and the default timeslice is rather low—for example, 10 milliseconds. Linux’s CFS scheduler, however, does not directly assign timeslices to processes. Instead, in a novel approach, CFS assigns processes a <em class="calibre4">proportion</em> of the processor. On Linux, therefore, the amount of processor time that a process receives is a function of the load of the system. This assigned proportion is further affected by each process’s nice value. The nice value acts as a weight, changing the proportion of the processor time each process receives. Processes with higher nice values (a lower priority) receive a deflationary weight, yielding them a smaller proportion of the processor; processes with smaller nice values (a higher priority) receive an inflationary weight, netting them a larger proportion of the processor.</p><div class="calibre_3"> </div>
<p class="calibre_2">As mentioned, the Linux operating system is <em class="calibre4">preemptive</em>. When a process enters the runnable state, it becomes eligible to run. In most operating systems, whether the process runs immediately, preempting the currently running process, is a function of the process’s priority and available timeslice. In Linux, under the new CFS scheduler, the decision is a function of how much of a proportion of the processor the newly runnable processor has consumed. If it has consumed a smaller proportion of the processor than the currently executing process, it runs immediately, preempting the current process. If not, it is scheduled to run at a later time.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos240517"> </div>
<h4 class="calibre_27"><span class="calibre3">The Scheduling Policy in Action</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">Consider a system with two runnable tasks: a text editor and a video encoder. The text editor is I/O-bound because it spends nearly all its time waiting for user key presses. (No matter how fast the user types, it is not <em class="calibre4">that</em> fast.) Despite this, when the text editor does receive a key press, the user expects the editor to respond immediately. Conversely, the video encoder is processor-bound. Aside from reading the raw data stream from the disk <a id="filepos241108"></a>and later writing the resulting video, the encoder spends all its time applying the video codec to the raw data, easily consuming 100% of the processor. The video encoder does not have any strong time constraints on when it runs—if it started running now or in half a second, the user could not tell and would not care. Of course, the sooner it finishes the better, but latency is not a primary concern.</p><div class="calibre_3"> </div>
<p class="calibre_2">In this scenario, ideally the scheduler gives the text editor a larger proportion of the available processor than the video encoder, because the text editor is interactive. We have two goals for the text editor. First, we want it to have a large amount of processor time available to it; not because it needs a lot of processor (it does not) but because we want it to always have processor time available the moment it needs it. Second, we want the text editor to preempt the video encoder the moment it wakes up (say, when the user presses a key). This can ensure the text editor has good <em class="calibre4">interactive performance</em> and is responsive to user input. On most operating systems, these goals are accomplished (if at all) by giving the text editor a higher priority and larger timeslice than the video encoder. Advanced operating systems do this automatically, by detecting that the text editor is interactive. Linux achieves these goals too, but by different means. Instead of assigning the text editor a specific priority and timeslice, it guarantees the text editor a specific proportion of the processor. If the video encoder and text editor are the only running processes and both are at the same nice level, this proportion would be 50%—each would be guaranteed half of the processor’s time. Because the text editor spends most of its time blocked, waiting for user key presses, it does not use anywhere near 50% of the processor. Conversely, the video encoder is free to use <em class="calibre4">more</em> than its allotted 50%, enabling it to finish the encoding quickly.</p><div class="calibre_3"> </div>
<p class="calibre_2">The crucial concept is what happens when the text editor wakes up. Our primary goal is to ensure it runs immediately upon user input. In this case, when the editor wakes up, CFS notes that it is allotted 50% of the processor but has used considerably less. Specifically, CFS determines that the text editor has run for <em class="calibre4">less time</em> than the video encoder. Attempting to give all processes a <em class="calibre4">fair share</em> of the processor, it then preempts the video encoder and enables the text editor to run. The text editor runs, quickly processes the user’s key press, and again sleeps, waiting for more input. As the text editor has not consumed its allotted 50%, we continue in this manner, with CFS always enabling the text editor to run when it wants and the video encoder to run the rest of the time.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos244056"> </div>
<h3 class="calibre_21"><span class="bold">The Linux Scheduling Algorithm</span></h3><div class="calibre_22"> </div>
<p class="calibre_2">In the previous sections, we discussed process scheduling in the abstract, with only occasional mention of how Linux applies a given concept to reality. With the foundation of scheduling now built, we can dive into Linux’s process scheduler.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos244487"> </div>
<h4 class="calibre_27"><span class="calibre3">Scheduler Classes</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">The Linux scheduler is modular, enabling different algorithms to schedule different types of processes. This modularity is called <em class="calibre4">scheduler classes</em>. Scheduler classes enable different, pluggable algorithms to coexist, scheduling their own types of processes. Each scheduler <a id="filepos244889"></a>class has a priority. The base scheduler code, which is defined in <code class="calibre6"><span class="calibre7">kernel/sched.c</span></code>, iterates over each scheduler class in order of priority. The highest priority scheduler class that has a runnable process wins, selecting who runs next.</p><div class="calibre_3"> </div>
<p class="calibre_2">The Completely Fair Scheduler (CFS) is the registered scheduler class for normal processes, called <code class="calibre6"><span class="calibre7">SCHED_NORMAL</span></code> in Linux (and <code class="calibre6"><span class="calibre7">SCHED_OTHER</span></code> in POSIX). CFS is defined in <code class="calibre6"><span class="calibre7">kernel/sched_fair.c</span></code>. The rest of this section discusses the CFS algorithm and is germane to any Linux kernel since 2.6.23. We discuss the scheduler class for real-time processes in a later section.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos245730"> </div>
<h4 class="calibre_27"><span class="calibre3">Process Scheduling in Unix Systems</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">To discuss fair scheduling, we must first describe how traditional Unix systems schedule processes. As mentioned in the previous section, modern process schedulers have two common concepts: process priority and timeslice. Timeslice is how long a process runs; processes start with some default timeslice. Processes with a higher priority run more frequently and (on many systems) receive a higher timeslice. On Unix, the priority is exported to user-space in the form of nice values. This sounds simple, but in practice it leads to several pathological problems, which we now discuss.</p><div class="calibre_3"> </div>
<p class="calibre_2">First, mapping nice values onto timeslices requires a decision about what absolute timeslice to allot each nice value. This leads to suboptimal switching behavior. For example, let’s assume we assign processes of the default nice value (zero) a timeslice of 100 milliseconds and processes at the highest nice value (+20, the lowest priority) a timeslice of 5 milliseconds. Further, let’s assume one of each of these processes is runnable. Our default-priority process thus receives 20/21 (100 out of 105 milliseconds) of the processor, whereas our low priority process receives <sup class="calibre8">1</sup>/<sub class="calibre16">21</sub> (5 out of 105 milliseconds) of the processor. We could have used any numbers for this example, but we assume this allotment is optimal since we chose it. Now, what happens if we run exactly two low priority processes? We’d expect they each receive 50% of the processor, which they do. But they each enjoy the processor for only 5 milliseconds at a time (5 out of 10 milliseconds each)! That is, instead of context switching twice every 105 milliseconds, we now context switch twice every 10 milliseconds. Conversely, if we have two normal priority processes, each again receives the correct 50% of the processor, but in 100 millisecond increments. Neither of these timeslice allotments are necessarily ideal; each is simply a byproduct of a given nice value to timeslice mapping coupled with a specific runnable process priority mix. Indeed, given that high nice value (low priority) processes tend to be background, processor-intensive tasks, while normal priority processes tend to be foreground user tasks, this timeslice allotment is exactly <em class="calibre4">backward</em> from ideal!</p><div class="calibre_3"> </div>
<p class="calibre_2">A second problem concerns relative nice values and again the nice value to timeslice mapping. Say we have two processes, each a single nice value apart. First, let’s assume they are at nice values 0 and 1. This might map (and indeed did in the <code class="calibre6"><span class="calibre7">O(1)</span></code> scheduler) to timeslices of 100 and 95 milliseconds, respectively. These two values are nearly identical, and thus the difference here between a single nice value is small. Now, instead, let’s assume our two processes are at nice values of 18 and 19. This now maps to timeslices of <a id="filepos248828"></a>10 and 5 milliseconds, respectively—the former receiving twice the processor time as the latter! Because nice values are most commonly used in relative terms (as the system call accepts an increment, not an absolute value), this behavior means that “nicing down a process by one” has wildly different effects depending on the starting nice value.</p><div class="calibre_3"> </div>
<p class="calibre_2">Third, if performing a nice value to timeslice mapping, we need the ability to assign an absolute timeslice. This absolute value must be measured in terms the kernel can measure. In most operating systems, this means the timeslice must be some integer multiple of the timer tick. (See <a href="index_split_020.html#filepos701833">Chapter 11</a>, “Timers and Time Management,” for a discussion on time.) This introduces several problems. First, the minimum timeslice has a floor of the period of the timer tick, which might be as high as 10 milliseconds or as low as 1 millisecond. Second, the system timer limits the difference between two timeslices; successive nice values might map to timeslices as much as 10 milliseconds or as little as 1 millisecond apart. Finally, timeslices change with different timer ticks. (If this paragraph’s discussion of timer ticks is foreign, reread it after reading <a href="index_split_020.html#filepos701833">Chapter 11</a>. This is only one motivation behind CFS.)</p><div class="calibre_3"> </div>
<p class="calibre_2">The fourth and final problem concerns handling process wake up in a priority-based scheduler that wants to optimize for interactive tasks. In such a system, you might want to give freshly woken-up tasks a priority boost by allowing them to run immediately, even if their timeslice was expired. Although this improves interactive performance in many, if not most, situations, it also opens the door to pathological cases where certain sleep/wake up use cases can game the scheduler into providing one process an unfair amount of processor time, at the expense of the rest of the system.</p><div class="calibre_3"> </div>
<p class="calibre_2">Most of these problems are solvable by making substantial but not paradigm-shifting changes to the old-school Unix scheduler. For example, making nice values geometric instead of additive solves the second problem. And mapping nice values to timeslices using a measurement decoupled from the timer tick solves the third problem. But such solutions belie the true problem, which is that assigning absolute timeslices yields a constant switching rate but variable fairness. The approach taken by CFS is a radical (for process schedulers) rethinking of timeslice allotment: Do away with timeslices completely and assign each process a proportion of the processor. CFS thus yields constant fairness but a variable switching rate.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos251682"> </div>
<h4 class="calibre_27"><span class="calibre3">Fair Scheduling</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">CFS is based on a simple concept: Model process scheduling as if the system had an ideal, perfectly multitasking processor. In such a system, each process would receive 1/n of the processor’s time, where <em class="calibre4">n</em> is the number of runnable processes, and we’d schedule them for infinitely small durations, so that in any measurable period we’d have run all <em class="calibre4">n</em> processes for the same amount of time. As an example, assume we have two processes. In the standard Unix model, we might run one process for 5 milliseconds and then another process for 5 milliseconds. While running, each process would receive 100% of the processor. In an ideal, perfectly multitasking processor, we would run both processes <em class="calibre4">simultaneously</em> for 10 milliseconds, each at 50% power. This latter model is called <em class="calibre4">perfect multitasking</em>.</p><div class="calibre_3"> </div>
<p class="calibre_2">Of course, such a model is also impractical, because it is not possible on a single processor to <em class="calibre4">literally</em> run multiple processes simultaneously. Moreover, it is not efficient to run processes for infinitely small durations. That is, there is a <em class="calibre4">switching cost</em> to preempting one process for another: the overhead of swapping one process for another and the effects on caches, for example. Thus, although we’d like to run processes for very small durations, CFS is mindful of the overhead and performance hit in doing so. Instead, CFS will run each process for some amount of time, round-robin, selecting next the process that has run the least. Rather than assign each process a timeslice, CFS calculates how long a process should run as a function of the total number of runnable processes. Instead of using the nice value to calculate a timeslice, CFS uses the nice value to <em class="calibre4">weight</em> the proportion of processor a process is to receive: Higher valued (lower priority) processes receive a fractional weight relative to the default nice value, whereas lower valued (higher priority) processes receive a larger weight.</p><div class="calibre_3"> </div>
<p class="calibre_2">Each process then runs for a “timeslice” proportional to its weight divided by the total weight of all runnable threads. To calculate the actual timeslice, CFS sets a target for its approximation of the “infinitely small” scheduling duration in perfect multitasking. This target is called the <em class="calibre4">targeted latency</em>. Smaller targets yield better interactivity and a closer approximation to perfect multitasking, at the expense of higher switching costs and thus worse overall throughput. Let’s assume the targeted latency is 20 milliseconds and we have two runnable tasks at the same priority. <em class="calibre4">Regardless of those task’s priority</em>, each will run for 10 milliseconds before preempting in favor of the other. If we have four tasks at the same priority, each will run for 5 milliseconds. If there are 20 tasks, each will run for 1 millisecond.</p><div class="calibre_3"> </div>
<p class="calibre_2">Note as the number of runnable tasks approaches infinity, the proportion of allotted processor and the assigned timeslice approaches zero. As this will eventually result in unacceptable switching costs, CFS imposes a floor on the timeslice assigned to each process. This floor is called the <em class="calibre4">minimum granularity</em>. By default it is 1 millisecond. Thus, even as the number of runnable processes approaches infinity, each will run for at least 1 millisecond, to ensure there is a ceiling on the incurred switching costs. (Astute readers will note that CFS is thus not perfectly fair when the number of processes grows so large that the calculated proportion is floored by the minimum granularity. This is true. Although modifications to fair queuing exist to improve upon this fairness, CFS was explicitly designed to make this trade-off. In the common case of only a handful of runnable processes, CFS is perfectly fair.)</p><div class="calibre_3"> </div>
<p class="calibre_2">Now, let’s again consider the case of two runnable processes, except with dissimilar nice values—say, one with the default nice value (zero) and one with a nice value of 5. These nice values have dissimilar weights and thus our two processes receive different proportions of the processor’s time. In this case, the weights work out to about a 1/3 penalty for the nice-5 process. If our target latency is again 20 milliseconds, our two processes will receive 15 milliseconds and 5 milliseconds each of processor time, respectively. Say our two runnable processes instead had nice values of 10 and 15. What would be the allotted timeslices? Again 15 and 5 milliseconds each! Absolute nice values no <a id="filepos256528"></a>longer affect scheduling decisions; only relative values affect the proportion of processor time allotted.</p><div class="calibre_3"> </div>
<p class="calibre_2">Put generally, the proportion of processor time that any process receives is determined only by the relative difference in niceness between it and the other runnable processes. The nice values, instead of yielding additive increases to timeslices, yield geometric differences. The absolute timeslice allotted any nice value is not an absolute number, but a given proportion of the processor. CFS is called a <em class="calibre4">fair scheduler</em> because it gives each process a fair share—a proportion—of the processor’s time. As mentioned, note that CFS isn’t perfectly fair, because it only approximates perfect multitasking, but it <em class="calibre4">can</em> place a lower bound on latency of <em class="calibre4">n</em> for <em class="calibre4">n</em> runnable processes on the unfairness.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos257491"> </div>
<h3 class="calibre_21"><span class="bold">The Linux Scheduling Implementation</span></h3><div class="calibre_22"> </div>
<p class="calibre_2">With the discussion of the motivation for and logic of CFS behind us, we can now explore CFS’s actual implementation, which lives in <code class="calibre6"><span class="calibre7">kernel/sched_fair.c</span></code>. Specifically, we discuss four components of CFS:</p><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• <a href="#filepos258454">Time Accounting</a></p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• <a href="#filepos262523">Process Selection</a></p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• <a href="#filepos270318">The Scheduler Entry Point</a></p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• <a href="#filepos272900">Sleeping and Waking Up</a></p></blockquote><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos258454"> </div>
<h4 class="calibre_27"><span class="calibre3">Time Accounting</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">All process schedulers must account for the time that a process runs. Most Unix systems do so, as discussed earlier, by assigning each process a timeslice. On each tick of the system clock, the timeslice is decremented by the tick period. When the timeslice reaches zero, the process is preempted in favor of another runnable process with a nonzero timeslice.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos258981"> </div>
<h5 class="calibre_29"><span class="calibre3">The Scheduler Entity Structure</span></h5><div class="calibre_24"> </div>
<p class="calibre_2">CFS does not have the notion of a timeslice, but it must still keep account for the time that each process runs, because it needs to ensure that each process runs only for its fair share of the processor. CFS uses the <em class="calibre4">scheduler entity structure</em>, <code class="calibre6"><span class="calibre7">struct sched_entity</span></code>, defined in <code class="calibre6"><span class="calibre7">&lt;linux/sched.h&gt;</span></code>, to keep track of process accounting:</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00020.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2"><a id="filepos259668"></a>The scheduler entity structure is embedded in the <em class="calibre4">process descriptor</em>, <code class="calibre6"><span class="calibre7">struct task_stuct</span></code>, as a member variable named <code class="calibre6"><span class="calibre7">se</span></code>. We discussed the process descriptor in <a href="index_split_012.html#filepos167044">Chapter 3</a>, “Process Management.”</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos260028"> </div>
<h5 class="calibre_29"><span class="calibre3">The Virtual Runtime</span></h5><div class="calibre_24"> </div>
<p class="calibre_2">The <code class="calibre6"><span class="calibre7">vruntime</span></code> variable stores the <em class="calibre4">virtual runtime</em> of a process, which is the actual runtime (the amount of time spent running) normalized (or weighted) by the number of runnable processes. The virtual runtime’s units are nanoseconds and therefore <code class="calibre6"><span class="calibre7">vruntime</span></code> is decoupled from the timer tick. The virtual runtime is used to help us approximate the “ideal multitasking processor” that CFS is modeling. With such an ideal processor, we wouldn’t need <code class="calibre6"><span class="calibre7">vruntime</span></code>, because all runnable processes would perfectly multitask. That is, on an ideal processor, the virtual runtime of all processes of the same priority would be identical—all tasks would have received an equal, fair share of the processor. Because processors are not capable of perfect multitasking and we must run each process in succession, CFS uses <code class="calibre6"><span class="calibre7">vruntime</span></code> to account for how long a process has run and thus how much longer it ought to run.</p><div class="calibre_3"> </div>
<p class="calibre_2">The function <code class="calibre6"><span class="calibre7">update_curr()</span></code>, defined in <code class="calibre6"><span class="calibre7">kernel/sched_fair.c</span></code>, manages this accounting:</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00021.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2"><code id="filepos261566" class="calibre6"><span class="calibre7">update_curr()</span></code> calculates the execution time of the current process and stores that value in <code class="calibre6"><span class="calibre7">delta_exec</span></code>. It then passes that runtime to <code class="calibre6"><span class="calibre7">__update_curr()</span></code>, which weights the time by the number of runnable processes. The current process’s <code class="calibre6"><span class="calibre7">vruntime</span></code> is then incremented by the weighted value:</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00022.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2"><code class="calibre6"><span class="calibre7">update_curr()</span></code> is invoked periodically by the system timer and also whenever a process becomes runnable or blocks, becoming unrunnable. In this manner, <code class="calibre6"><span class="calibre7">vruntime</span></code> is an accurate measure of the runtime of a given process and an indicator of what process should run next.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos262523"> </div>
<h4 class="calibre_27"><span class="calibre3">Process Selection</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">In the last section, we discussed how <code class="calibre6"><span class="calibre7">vruntime</span></code> on an ideal, perfectly multitasking processor would be identical among all runnable processes. In reality, we cannot perfectly multitask, so CFS attempts to balance a process’s virtual runtime with a simple rule: When CFS is deciding what process to run next, it picks the process with the smallest <code class="calibre6"><span class="calibre7">vruntime</span></code>. This is, in fact, the core of CFS’s scheduling algorithm: Pick the task with the smallest <code class="calibre6"><span class="calibre7">vruntime</span></code>. That’s it! The rest of this subsection describes how the selection of the process with the smallest <code class="calibre6"><span class="calibre7">vruntime</span></code> is implemented.</p><div class="calibre_3"> </div>
<p class="calibre_2">CFS uses a <em class="calibre4">red-black tree</em> to manage the list of runnable processes and efficiently find the process with the smallest <code class="calibre6"><span class="calibre7">vruntime</span></code>. A red-black tree, called an <em class="calibre4">rbtree</em> in Linux, is a type of <em class="calibre4">self-balancing binary search tree</em>. We discuss self-balancing binary search trees in general and red-black trees in particular in <a href="index_split_015.html#filepos343716">Chapter 6</a>. For now, if you are unfamiliar, you need to know only that red-black trees are a data structure that store <em class="calibre4">nodes</em> of arbitrary data, identified by a specific <em class="calibre4">key</em>, and that they enable efficient search for a given <em class="calibre4">key</em>. (Specifically, obtaining a node identified by a given key is logarithmic in time as a function of total nodes in the tree.)</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos264264"> </div>
<h5 class="calibre_29"><span class="calibre3">Picking the Next Task</span></h5><div class="calibre_24"> </div>
<p class="calibre_2">Let’s start with the assumption that we have a red-black tree populated with every runnable process in the system where the key for each node is the runnable process’s virtual runtime. We’ll look at how we build that tree in a moment, but for now let’s assume we have it. Given this tree, the process that CFS wants to run next, which is the process with the smallest <code class="calibre6"><span class="calibre7">vruntime</span></code>, is the leftmost node in the tree. That is, if you follow the tree from the root down through the left child, and continue moving to the left until you reach a leaf node, you find the process with the smallest <code class="calibre6"><span class="calibre7">vruntime</span></code>. (Again, if you are unfamiliar with binary search trees, don’t worry. Just know that this process is efficient.) CFS’s process selection algorithm is thus summed up as “run the process represented by the leftmost node in the rbtree.” The function that performs this selection is <code class="calibre6"><span class="calibre7">__pick_next_entity()</span></code>, defined in <code class="calibre6"><span class="calibre7">kernel/sched_fair.c</span></code>:</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00023.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2">Note that <code class="calibre6"><span class="calibre7">__pick_next_entity()</span></code> does not actually traverse the tree to find the leftmost node, because the value is cached by <code class="calibre6"><span class="calibre7">rb_leftmost</span></code>. Although it is efficient to walk the tree to find the leftmost node—<code class="calibre6"><span class="calibre7">O(height of tree)</span></code>, which is <code class="calibre6"><span class="calibre7">O(log N)</span></code> for <code class="calibre6"><span class="calibre7">N</span></code> nodes if the tree is balanced—it is even easier to cache the leftmost node. The return value from this function is the process that CFS next runs. If the function returns NULL, there is no leftmost node, and thus no nodes in the tree. In that case, there are no runnable processes, and CFS schedules the idle task.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos266443"> </div>
<h5 class="calibre_29"><span class="calibre3">Adding Processes to the Tree</span></h5><div class="calibre_24"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos266573"> </div>
<p class="calibre_2">Now let’s look at how CFS adds processes to the rbtree and caches the leftmost node. This would occur when a process becomes runnable (wakes up) or is first created via <code class="calibre6"><span class="calibre7">fork()</span></code>, as discussed in <a href="index_split_012.html#filepos167044">Chapter 3</a>. Adding processes to the tree is performed by <code class="calibre6"><span class="calibre7">enqueue_entity()</span></code>:</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00024.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2">This function updates the runtime and other statistics and then invokes <code class="calibre6"><span class="calibre7">__enqueue_entity()</span></code> to perform the actual heavy lifting of inserting the entry into the red-black tree:</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00025.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_31"><img alt="image" src="images/00026.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2">Let’s look at this function. The body of the <code class="calibre6"><span class="calibre7">while()</span></code> loop traverses the tree in search of a matching key, which is the inserted process’s <code class="calibre6"><span class="calibre7">vruntime</span></code>. Per the rules of the balanced tree, it moves to the left child if the key is smaller than the current node’s key and to the right child if the key is larger. If it ever moves to the right, even once, it knows the inserted process cannot be the new leftmost node, and it sets <code class="calibre6"><span class="calibre7">leftmost</span></code> to zero. If it moves only to the left, <code class="calibre6"><span class="calibre7">leftmost</span></code> remains one, and we have a new leftmost node and can update the cache by setting <code class="calibre6"><span class="calibre7">rb_leftmost</span></code> to the inserted process. The loop terminates when we compare ourselves to a node that has no child in the direction we move; <code class="calibre6"><span class="calibre7">link</span></code> is then <code class="calibre6"><span class="calibre7">NULL</span></code> and the loop terminates. When out of the loop, the function calls <code class="calibre6"><span class="calibre7">rb_link_node()</span></code> on the parent node, making the inserted process the new child. The function <code class="calibre6"><span class="calibre7">rb_insert_color()</span></code> updates the self-balancing properties of the tree; we discuss the coloring in <a href="index_split_015.html#filepos343716">Chapter 6</a>.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos268958"> </div>
<h5 class="calibre_29"><span class="calibre3">Removing Processes from the Tree</span></h5><div class="calibre_24"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos269092"> </div>
<p class="calibre_2">Finally, let’s look at how CFS removes processes from the red-black tree. This happens when a process blocks (becomes unrunnable) or terminates (ceases to exist):</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00027.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2">As with adding a process to the red-black tree, the real work is performed by a helper function, <code class="calibre6"><span class="calibre7">__dequeue_entity()</span></code>:</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00028.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2">Removing a process from the tree is much simpler because the rbtree implementation provides the <code class="calibre6"><span class="calibre7">rb_erase()</span></code> function that performs all the work. The rest of this function updates the <code class="calibre6"><span class="calibre7">rb_leftmost</span></code> cache. If the process-to-remove is the leftmost node, the <a id="filepos270062"></a>function invokes <code class="calibre6"><span class="calibre7">rb_next()</span></code> to find what would be the next node in an in-order traversal. This is what will be the leftmost node when the current leftmost node is removed.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos270318"> </div>
<h4 class="calibre_27"><span class="calibre3">The Scheduler Entry Point</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">The main entry point into the process schedule is the function <code class="calibre6"><span class="calibre7">schedule()</span></code>, defined in <code class="calibre6"><span class="calibre7">kernel/sched.c</span></code>. This is the function that the rest of the kernel uses to invoke the process scheduler, deciding which process to run and then running it. <code class="calibre6"><span class="calibre7">schedule()</span></code> is generic with respect to scheduler classes. That is, it finds the highest priority scheduler class with a runnable process and asks it what to run next. Given that, it should be no surprise that <code class="calibre6"><span class="calibre7">schedule()</span></code> is simple. The only important part of the function—which is otherwise too uninteresting to reproduce here—is its invocation of <code class="calibre6"><span class="calibre7">pick_next_task()</span></code>, also defined in <code class="calibre6"><span class="calibre7">kernel/sched.c</span></code>. The <code class="calibre6"><span class="calibre7">pick_next_task()</span></code> function goes through each scheduler class, starting with the highest priority, and selects the highest priority process in the highest priority class:</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00029.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2"><a id="filepos271649"></a>Note the optimization at the beginning of the function. Because CFS is the scheduler class for normal processes, and most systems run mostly normal processes, there is a small hack to quickly select the next CFS-provided process if the number of runnable processes is equal to the number of CFS runnable processes (which suggests that all runnable processes are provided by CFS).</p><div class="calibre_3"> </div>
<p class="calibre_2">The core of the function is the <code class="calibre6"><span class="calibre7">for()</span></code> loop, which iterates over each class in priority order, starting with the highest priority class. Each class implements the <code class="calibre6"><span class="calibre7">pick_next_task()</span></code> function, which returns a pointer to its next runnable process or, if there is not one, <code class="calibre6"><span class="calibre7">NULL</span></code>. The first class to return a non-<code class="calibre6"><span class="calibre7">NULL</span></code> value has selected the next runnable process. CFS’s implementation of <code class="calibre6"><span class="calibre7">pick_next_task()</span></code> calls <code class="calibre6"><span class="calibre7">pick_next_entity()</span></code>, which in turn calls the <code class="calibre6"><span class="calibre7">__pick_next_entity()</span></code> function that we discussed in the previous section.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos272900"> </div>
<h4 class="calibre_27"><span class="calibre3">Sleeping and Waking Up</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">Tasks that are sleeping (blocked) are in a special nonrunnable state. This is important because without this special state, the scheduler would select tasks that did not want to run or, worse, sleeping would have to be implemented as busy looping. A task sleeps for a number of reasons, but always while it is waiting for some event. The event can be a specified amount of time, more data from a file I/O, or another hardware event. A task can also involuntarily go to sleep when it tries to obtain a contended semaphore in the kernel (this is covered in <a href="index_split_018.html#filepos575425">Chapter 9</a>, “An Introduction to Kernel Synchronization”). A common reason to sleep is file I/O—for example, the task issued a <code class="calibre6"><span class="calibre7">read()</span></code> request on a file, which needs to be read in from disk. As another example, the task could be waiting for keyboard input. Whatever the case, the kernel behavior is the same: The task marks itself as sleeping, puts itself on a wait queue, removes itself from the red-black tree of runnable, and calls <code class="calibre6"><span class="calibre7">schedule()</span></code> to select a new process to execute. Waking back up is the inverse: The task is set as runnable, removed from the wait queue, and added back to the red-black tree.</p><div class="calibre_3"> </div>
<p class="calibre_2">As discussed in the previous chapter, two states are associated with sleeping, <code class="calibre6"><span class="calibre7">TASK_INTERRUPTIBLE</span></code> and <code class="calibre6"><span class="calibre7">TASK_UNINTERRUPTIBLE</span></code>. They differ only in that tasks in the <code class="calibre6"><span class="calibre7">TASK_UNINTERRUPTIBLE</span></code> state ignore signals, whereas tasks in the <code class="calibre6"><span class="calibre7">TASK_INTERRUPTIBLE</span></code> state wake up prematurely and respond to a signal if one is issued. Both types of sleeping tasks sit on a wait queue, waiting for an event to occur, and are not runnable.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos274956"> </div>
<h5 class="calibre_29"><span class="calibre3">Wait Queues</span></h5><div class="calibre_24"> </div>
<p class="calibre_2">Sleeping is handled via wait queues. A wait queue is a simple list of processes waiting for an event to occur. Wait queues are represented in the kernel by <code class="calibre6"><span class="calibre7">wake_queue_head_t</span></code>. Wait queues are created statically via <code class="calibre6"><span class="calibre7">DECLARE_WAITQUEUE()</span></code> or dynamically via <code class="calibre6"><span class="calibre7">init_waitqueue_head()</span></code>. Processes put themselves on a wait queue and mark themselves not runnable. When the event associated with the wait queue occurs, the processes on the queue are awakened. It is important to implement sleeping and waking correctly, to avoid race conditions.</p><div class="calibre_3"> </div>
<p class="calibre_2">Some simple interfaces for sleeping used to be in wide use. These interfaces, however, have races: It is possible to go to sleep <em class="calibre4">after</em> the condition becomes true. In that case, the task might sleep indefinitely. Therefore, the recommended method for sleeping in the kernel is a bit more complicated:</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00030.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2">The task performs the following steps to add itself to a wait queue:</p><div class="calibre_3"> </div>
<ol class="calibre13">
<li value="1" class="calibre14">Creates a wait queue entry via the macro <code class="calibre6"><span class="calibre7">DEFINE_WAIT()</span></code>.</li>
<li value="2" class="calibre14">Adds itself to a wait queue via <code class="calibre6"><span class="calibre7">add_wait_queue()</span></code>. This wait queue awakens the process when the condition for which it is waiting occurs. Of course, there needs to be code elsewhere that calls <code class="calibre6"><span class="calibre7">wake_up()</span></code> on the queue when the event actually does occur.</li>
<li value="3" class="calibre14">Calls <code class="calibre6"><span class="calibre7">prepare_to_wait()</span></code> to change the process state to either <code class="calibre6"><span class="calibre7">TASK_INTERRUPTIBLE</span></code> or <code class="calibre6"><span class="calibre7">TASK_UNINTERRUPTIBLE</span></code>. This function also adds the task back to the wait queue if necessary, which is needed on subsequent iterations of the loop.</li>
<li value="4" class="calibre14">If the state is set to <code class="calibre6"><span class="calibre7">TASK_INTERRUPTIBLE</span></code>, a signal wakes the process up. This is called a <em class="calibre4">spurious wake up</em> (a wake-up not caused by the occurrence of the event). So check and handle signals.</li>
<li value="5" class="calibre14">When the task awakens, it again checks whether the condition is true. If it is, it exits the loop. Otherwise, it again calls <code class="calibre6"><span class="calibre7">schedule()</span></code> and repeats.</li>
<li value="6" class="calibre14">Now that the condition is true, the task sets itself to <code class="calibre6"><span class="calibre7">TASK_RUNNING</span></code> and removes itself from the wait queue via <code class="calibre6"><span class="calibre7">finish_wait()</span></code>.</li>
</ol>
<p class="calibre_2">If the condition occurs before the task goes to sleep, the loop terminates, and the task does not erroneously go to sleep. Note that kernel code often has to perform various other tasks in the body of the loop. For example, it might need to release locks before calling <code class="calibre6"><span class="calibre7">schedule()</span></code> and reacquire them after or react to other events.</p><div class="calibre_3"> </div>
<p class="calibre_2">The function <code class="calibre6"><span class="calibre7">inotify_read()</span></code> in <code class="calibre6"><span class="calibre7">fs/notify/inotify/inotify_user.c</span></code>, which handles reading from the inotify file descriptor, is a straightforward example of using wait queues:</p><div class="calibre_3"> </div>
<p class="calibre_31"><img alt="image" src="images/00031.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_31"><img alt="image" src="images/00032.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2"><a id="filepos278696"></a>This function follows the pattern laid out in our example. The main difference is that it checks for the condition in the body of the <code class="calibre6"><span class="calibre7">while()</span></code> loop, instead of in the <code class="calibre6"><span class="calibre7">while()</span></code> statement itself. This is because checking the condition is complicated and requires grabbing locks. The loop is terminated via <code class="calibre6"><span class="calibre7">break</span></code>.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos279160"> </div>
<h5 class="calibre_29"><span class="calibre3">Waking Up</span></h5><div class="calibre_24"> </div>
<p class="calibre_2">Waking is handled via <code class="calibre6"><span class="calibre7">wake_up()</span></code>, which wakes up all the tasks waiting on the given wait queue. It calls <code class="calibre6"><span class="calibre7">try_to_wake_up()</span></code>, which sets the task’s state to <code class="calibre6"><span class="calibre7">TASK_RUNNING</span></code>, calls <code class="calibre6"><span class="calibre7">enqueue_task()</span></code> to add the task to the red-black tree, and sets <code class="calibre6"><span class="calibre7">need_resched</span></code> if the awakened task’s priority is higher than the priority of the current task. The code that causes the event to occur typically calls <code class="calibre6"><span class="calibre7">wake_up()</span></code> itself. For example, when data arrives from the hard disk, the VFS calls <code class="calibre6"><span class="calibre7">wake_up()</span></code> on the wait queue that holds the processes waiting for the data.</p><div class="calibre_3"> </div>
<p class="calibre_2">An important note about sleeping is that there are spurious wake-ups. Just because a task is awakened does not mean that the event for which the task is waiting has occurred; sleeping should always be handled in a loop that ensures that the condition for which the task is waiting has indeed occurred. <a href="#filepos280567">Figure 4.1</a> depicts the relationship between each scheduler state.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos280567"> </div>
<p class="calibre_23"><span class="calibre9"><span class="calibre3">Figure 4.1. Sleeping and waking up.</span></span></p><div class="calibre_24"> </div>
<p class="calibre_23"><img alt="image" src="images/00033.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos280852"> </div>
<h3 class="calibre_21"><span class="bold">Preemption and Context Switching</span></h3><div class="calibre_22"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos280987"> </div>
<p class="calibre_2">Context switching, the switching from one runnable task to another, is handled by the <code class="calibre6"><span class="calibre7">context_switch()</span></code>function defined in <code class="calibre6"><span class="calibre7">kernel/sched.c</span></code>. It is called by <code class="calibre6"><span class="calibre7">schedule()</span></code> when a new process has been selected to run. It does two basic jobs:</p><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• Calls <code class="calibre6"><span class="calibre7">switch_mm()</span></code>, which is declared in <code class="calibre6"><span class="calibre7">&lt;asm/mmu_context.h&gt;</span></code>, to switch the virtual memory mapping from the previous process’s to that of the new process.</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• Calls <code class="calibre6"><span class="calibre7">switch_to()</span></code>, declared in <code class="calibre6"><span class="calibre7">&lt;asm/system.h&gt;</span></code>, to switch the processor state from the previous process’s to the current’s. This involves saving and restoring stack information and the processor registers and any other architecture-specific state that must be managed and restored on a per-process basis.</p></blockquote><div class="calibre_3"> </div>
<p class="calibre_2">The kernel, however, must know when to call <code class="calibre6"><span class="calibre7">schedule()</span></code>. If it called <code class="calibre6"><span class="calibre7">schedule()</span></code> only when code explicitly did so, user-space programs could run indefinitely. Instead, the kernel provides the <code class="calibre6"><span class="calibre7">need_resched</span></code> flag to signify whether a reschedule should be performed (see <a href="#filepos283209">Table 4.1</a>). This flag is set by <code class="calibre6"><span class="calibre7">scheduler_tick()</span></code> when a process should be preempted, and by <code class="calibre6"><span class="calibre7">try_to_wake_up()</span></code> when a process that has a higher priority than the currently running process is awakened. The kernel checks the flag, sees that it is set, and calls <code class="calibre6"><span class="calibre7">schedule()</span></code> to switch to a new process. The flag is a message to the kernel that the scheduler should be invoked as soon as possible because another process deserves to run.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos283209"> </div>
<p class="calibre_23"><span class="calibre9"><span class="calibre3">Table 4.1. Functions for Accessing and Manipulating <code class="calibre6"><span class="calibre15">need_resched</span></code></span></span></p><div class="calibre_24"> </div>
<p class="calibre_23"><img alt="image" src="images/00034.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2">Upon returning to user-space or returning from an interrupt, the <code class="calibre6"><span class="calibre7">need_resched</span></code> flag is checked. If it is set, the kernel invokes the scheduler before continuing.</p><div class="calibre_3"> </div>
<p class="calibre_2">The flag is per-process, and not simply global, because it is faster to access a value in the process descriptor (because of the speed of <code class="calibre6"><span class="calibre7">current</span></code> and high probability of it being cache hot) than a global variable. Historically, the flag was global before the 2.2 kernel. In 2.2 and 2.4, the flag was an <code class="calibre6"><span class="calibre7">int</span></code> inside the <code class="calibre6"><span class="calibre7">task_struct</span></code>. In 2.6, it was moved into a single bit of a special flag variable inside the <code class="calibre6"><span class="calibre7">thread_info</span></code> structure.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos284425"> </div>
<h4 class="calibre_27"><span class="calibre3">User Preemption</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">User preemption occurs when the kernel is about to return to user-space, <code class="calibre6"><span class="calibre7">need_resched</span></code> is set, and therefore, the scheduler is invoked. If the kernel is returning to user-space, it <a id="filepos284757"></a>knows it is in a safe quiescent state. In other words, if it is safe to continue executing the current task, it is also safe to pick a new task to execute. Consequently, whenever the kernel is preparing to return to user-space either on return from an interrupt or after a system call, the value of <code class="calibre6"><span class="calibre7">need_resched</span></code> is checked. If it is set, the scheduler is invoked to select a new (more fit) process to execute. Both the return paths for return from interrupt and return from system call are architecture-dependent and typically implemented in assembly in <code class="calibre6"><span class="calibre7">entry.S</span></code> (which, aside from kernel entry code, also contains kernel exit code).</p><div class="calibre_3"> </div>
<p class="calibre_2">In short, user preemption can occur</p><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• When returning to user-space from a system call</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• When returning to user-space from an interrupt handler</p></blockquote><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos285869"> </div>
<h4 class="calibre_27"><span class="calibre3">Kernel Preemption</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">The Linux kernel, unlike most other Unix variants and many other operating systems, is a fully preemptive kernel. In nonpreemptive kernels, kernel code runs until completion. That is, the scheduler cannot reschedule a task while it is in the kernel—kernel code is scheduled cooperatively, not preemptively. Kernel code runs until it finishes (returns to user-space) or explicitly blocks. In the 2.6 kernel, however, the Linux kernel became preemptive: It is now possible to preempt a task at any point, so long as the kernel is in a state in which it is safe to reschedule.</p><div class="calibre_3"> </div>
<p class="calibre_2">So when is it safe to reschedule? The kernel can preempt a task running in the kernel so long as it does not hold a lock. That is, locks are used as markers of regions of nonpreemptibility. Because the kernel is SMP-safe, if a lock is not held, the current code is reentrant and capable of being preempted.</p><div class="calibre_3"> </div>
<p class="calibre_2">The first change in supporting kernel preemption was the addition of a preemption counter, <code class="calibre6"><span class="calibre7">preempt_count</span></code>, to each process’s <code class="calibre6"><span class="calibre7">thread_info</span></code>. This counter begins at zero and increments once for each lock that is acquired and decrements once for each lock that is released. When the counter is zero, the kernel is preemptible. Upon return from interrupt, if returning to kernel-space, the kernel checks the values of <code class="calibre6"><span class="calibre7">need_resched</span></code> and <code class="calibre6"><span class="calibre7">preempt_count</span></code>. If <code class="calibre6"><span class="calibre7">need_resched</span></code> is set and <code class="calibre6"><span class="calibre7">preempt_count</span></code> is zero, then a more important task is runnable, and it is safe to preempt. Thus, the scheduler is invoked. If <code class="calibre6"><span class="calibre7">preempt_count</span></code> is nonzero, a lock is held, and it is unsafe to reschedule. In that case, the interrupt returns as usual to the currently executing task. When all the locks that the current task is holding are released, <code class="calibre6"><span class="calibre7">preempt_count</span></code> returns to zero. At that time, the unlock code checks whether <code class="calibre6"><span class="calibre7">need_resched</span></code> is set. If so, the scheduler is invoked. Enabling and disabling kernel preemption is sometimes required in kernel code and is discussed in <a href="index_split_018.html#filepos575425">Chapter 9</a>.</p><div class="calibre_3"> </div>
<p class="calibre_2">Kernel preemption can also occur explicitly, when a task in the kernel blocks or explicitly calls <code class="calibre6"><span class="calibre7">schedule()</span></code>. This form of kernel preemption has always been supported because no additional logic is required to ensure that the kernel is in a state that is safe to <a id="filepos288723"></a>preempt. It is assumed that the code that explicitly calls <code class="calibre6"><span class="calibre7">schedule()</span></code> knows it is safe to reschedule.</p><div class="calibre_3"> </div>
<p class="calibre_2">Kernel preemption can occur</p><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• When an interrupt handler exits, before returning to kernel-space</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• When kernel code becomes preemptible again</p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• If a task in the kernel explicitly calls <code class="calibre6"><span class="calibre7">schedule()</span></code></p></blockquote><div class="calibre_3"> </div>
<blockquote class="calibre10"><p class="calibre_25">• If a task in the kernel blocks (which results in a call to <code class="calibre6"><span class="calibre7">schedule()</span></code>)</p></blockquote><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos289630"> </div>
<h3 class="calibre_21"><span class="bold">Real-Time Scheduling Policies</span></h3><div class="calibre_22"> </div>
<p class="calibre_2">Linux provides two real-time scheduling policies, <code class="calibre6"><span class="calibre7">SCHED_FIFO</span></code> and <code class="calibre6"><span class="calibre7">SCHED_RR</span></code>. The normal, not real-time scheduling policy is <code class="calibre6"><span class="calibre7">SCHED_NORMAL</span></code>. Via the <em class="calibre4">scheduling classes</em> framework, these real-time policies are managed not by the Completely Fair Scheduler, but by a special real-time scheduler, defined in <code class="calibre6"><span class="calibre7">kernel/sched_rt.c</span></code>. The rest of this section discusses the real-time scheduling policies and algorithm.</p><div class="calibre_3"> </div>
<p class="calibre_2"><code class="calibre6"><span class="calibre7">SCHED_FIFO</span></code> implements a simple first-in, first-out scheduling algorithm without timeslices. A runnable <code class="calibre6"><span class="calibre7">SCHED_FIFO</span></code> task is always scheduled over any <code class="calibre6"><span class="calibre7">SCHED_NORMAL</span></code> tasks. When a <code class="calibre6"><span class="calibre7">SCHED_FIFO</span></code> task becomes runnable, it continues to run until it blocks or explicitly yields the processor; it has no timeslice and can run indefinitely. Only a higher priority <code class="calibre6"><span class="calibre7">SCHED_FIFO</span></code> or <code class="calibre6"><span class="calibre7">SCHED_RR</span></code> task can preempt a <code class="calibre6"><span class="calibre7">SCHED_FIFO</span></code> task. Two or more <code class="calibre6"><span class="calibre7">SCHED_FIFO</span></code> tasks at the same priority run round-robin, but again only yielding the processor when they explicitly choose to do so. If a <code class="calibre6"><span class="calibre7">SCHED_FIFO</span></code> task is runnable, all tasks at a lower priority cannot run until it becomes unrunnable.</p><div class="calibre_3"> </div>
<p class="calibre_2"><code class="calibre6"><span class="calibre7">SCHED_RR</span></code> is identical to <code class="calibre6"><span class="calibre7">SCHED_FIFO</span></code> except that each process can run only until it exhausts a predetermined timeslice. That is, <code class="calibre6"><span class="calibre7">SCHED_RR</span></code> is <code class="calibre6"><span class="calibre7">SCHED_FIFO</span></code> with timeslices—it is a real-time, round-robin scheduling algorithm. When a <code class="calibre6"><span class="calibre7">SCHED_RR</span></code> task exhausts its timeslice, any other real-time processes at its priority are scheduled round-robin. The timeslice is used to allow only rescheduling of same-priority processes. As with <code class="calibre6"><span class="calibre7">SCHED_FIFO</span></code>, a higher-priority process always immediately preempts a lower-priority one, and a lower-priority process can never preempt a <code class="calibre6"><span class="calibre7">SCHED_RR</span></code> task, even if its timeslice is exhausted.</p><div class="calibre_3"> </div>
<p class="calibre_2">Both real-time scheduling policies implement static priorities. The kernel does not calculate dynamic priority values for real-time tasks. This ensures that a real-time process at a given priority <em class="calibre4">always</em> preempts a process at a lower priority.</p><div class="calibre_3"> </div>
<p class="calibre_2">The real-time scheduling policies in Linux provide soft real-time behavior. <em class="calibre4">Soft real-time</em> refers to the notion that the kernel tries to schedule applications within timing deadlines, but the kernel does not promise to always achieve these goals. Conversely, <em class="calibre4">hard real-time</em> systems are guaranteed to meet any scheduling requirements within certain limits. Linux makes no guarantees on the capability to schedule real-time tasks. Despite not having a design that guarantees hard real-time behavior, the real-time scheduling performance in Linux is quite good. The 2.6 Linux kernel is capable of meeting stringent timing requirements.</p><div class="calibre_3"> </div>
<p class="calibre_2"><a id="filepos293299"></a>Real-time priorities range inclusively from zero to <code class="calibre6"><span class="calibre7">MAX_RT_PRIO</span></code> minus 1. By default, <code class="calibre6"><span class="calibre7">MAX_RT_PRIO</span></code> is 100—therefore, the default real-time priority range is zero to 99. This priority space is shared with the nice values of <code class="calibre6"><span class="calibre7">SCHED_NORMAL</span></code> tasks: They use the space from <code class="calibre6"><span class="calibre7">MAX_RT_PRIO</span></code> to (<code class="calibre6"><span class="calibre7">MAX_RT_PRIO + 40)</span></code>. By default, this means the –20 to +19 nice range maps directly onto the priority space from 100 to 139.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos293940"> </div>
<h3 class="calibre_21"><span class="bold">Scheduler-Related System Calls</span></h3><div class="calibre_22"> </div>
<p class="calibre_2">Linux provides a family of system calls for the management of scheduler parameters. These system calls allow manipulation of process priority, scheduling policy, and processor affinity, as well as provide an explicit mechanism to <em class="calibre4">yield</em> the processor to other tasks.</p><div class="calibre_3"> </div>
<p class="calibre_2">Various books—and your friendly system man pages—provide reference to these system calls (which are all implemented in the C library without much wrapper—they just invoke the system call). <a href="#filepos294878">Table 4.2</a> lists the system calls and provides a brief description. How system calls are implemented in the kernel is discussed in <a href="index_split_014.html#filepos301917">Chapter 5</a>, “System Calls.”</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos294878"> </div>
<p class="calibre_23"><span class="calibre9"><span class="calibre3">Table 4.2. Scheduler-Related System Calls</span></span></p><div class="calibre_24"> </div>
<p class="calibre_23"><img alt="image" src="images/00035.jpg" class="calibre2"/></p><div class="calibre_7"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos295169"> </div>
<h4 class="calibre_27"><span class="calibre3">Scheduling Policy and Priority-Related System Calls</span></h4><div class="calibre_24"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos295322"> </div>
<p class="calibre_2">The <code class="calibre6"><span class="calibre7">sched_setscheduler()</span></code> and <code class="calibre6"><span class="calibre7">sched_getscheduler()</span></code> system calls set and get a given process’s scheduling policy and real-time priority, respectively. Their implementation, like most system calls, involves a lot of argument checking, setup, and cleanup. The important work, however, is merely to read or write the <code class="calibre6"><span class="calibre7">policy</span></code> and <code class="calibre6"><span class="calibre7">rt_priority</span></code> values in the process’s <code class="calibre6"><span class="calibre7">task_struct</span></code>.</p><div class="calibre_3"> </div>
<p class="calibre_2">The <code class="calibre6"><span class="calibre7">sched_setparam()</span></code> and <code class="calibre6"><span class="calibre7">sched_getparam()</span></code> system calls set and get a process’s real-time priority. These calls merely encode <code class="calibre6"><span class="calibre7">rt_priority</span></code> in a special <code class="calibre6"><span class="calibre7">sched_param</span></code> structure. The calls <code class="calibre6"><span class="calibre7">sched_get_priority_max()</span></code> and <code class="calibre6"><span class="calibre7">sched_get_priority_min()</span></code> return the maximum and minimum priorities, respectively, for a given scheduling policy. The maximum priority for the real-time policies is <code class="calibre6"><span class="calibre7">MAX_USER_RT_PRIO</span></code> minus one; the minimum is one.</p><div class="calibre_3"> </div>
<p class="calibre_2">For normal tasks, the <code class="calibre6"><span class="calibre7">nice()</span></code>function increments the given process’s static priority by the given amount. Only root can provide a negative value, thereby lowering the nice value and increasing the priority. The <code class="calibre6"><span class="calibre7">nice()</span></code> function calls the kernel’s <code class="calibre6"><span class="calibre7">set_user_nice()</span></code> function, which sets the <code class="calibre6"><span class="calibre7">static_prio</span></code> and <code class="calibre6"><span class="calibre7">prio</span></code> values in the task’s <code class="calibre6"><span class="calibre7">task_struct</span></code> as appropriate.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos297341"> </div>
<h4 class="calibre_27"><span class="calibre3">Processor Affinity System Calls</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">The Linux scheduler enforces hard processor affinity. That is, although it tries to provide soft or natural affinity by attempting to keep processes on the same processor, the scheduler also enables a user to say, “This task must remain on this subset of the available processors no matter what.” This hard affinity is stored as a bitmask in the task’s <code class="calibre6"><span class="calibre7">task_struct</span></code> as <code class="calibre6"><span class="calibre7">cpus_allowed</span></code>. The bitmask contains one bit per possible processor on the system. By default, all bits are set and, therefore, a process is potentially runnable on any processor. The user, however, via <code class="calibre6"><span class="calibre7">sched_setaffinity()</span></code>, can provide a different bitmask of any combination of one or more bits. Likewise, the call <code class="calibre6"><span class="calibre7">sched_getaffinity()</span></code> returns the current <code class="calibre6"><span class="calibre7">cpus_allowed</span></code> bitmask.</p><div class="calibre_3"> </div>
<p class="calibre_2">The kernel enforces hard affinity in a simple manner. First, when a process is initially created, it inherits its parent’s affinity mask. Because the parent is running on an allowed processor, the child thus runs on an allowed processor. Second, when a processor’s affinity is changed, the kernel uses the <em class="calibre4">migration threads</em> to push the task onto a legal processor. Finally, the load balancer pulls tasks to only an allowed processor. Therefore, a process only ever runs on a processor whose bit is set in the <code class="calibre6"><span class="calibre7">cpus_allowed</span></code> field of its process descriptor.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos299121"> </div>
<h4 class="calibre_27"><span class="calibre3">Yielding Processor Time</span></h4><div class="calibre_24"> </div>
<p class="calibre_2">Linux provides the <code class="calibre6"><span class="calibre7">sched_yield()</span></code> system call as a mechanism for a process to explicitly yield the processor to other waiting processes. It works by removing the process from the active array (where it currently is, because it is running) and inserting it into the expired array. This has the effect of not only preempting the process and putting it at the end of its priority list, but also putting it on the expired list—guaranteeing it will not run for a <a id="filepos299744"></a>while. Because real-time tasks never expire, they are a special case. Therefore, they are merely moved to the end of their priority list (and not inserted into the expired array). In earlier versions of Linux, the semantics of the <code class="calibre6"><span class="calibre7">sched_yield()</span></code>call were quite different; at best, the task was moved only to the end of its priority list. The yielding was often not for a long time. Nowadays, applications and even kernel code should be certain they truly want to give up the processor before calling <code class="calibre6"><span class="calibre7">sched_yield()</span></code>.</p><div class="calibre_3"> </div>
<p class="calibre_2">Kernel code, as a convenience, can call <code class="calibre6"><span class="calibre7">yield()</span></code>, which ensures that the task’s state is <code class="calibre6"><span class="calibre7">TASK_RUNNING</span></code> and then call <code class="calibre6"><span class="calibre7">sched_yield()</span></code>. User-space applications use the <code class="calibre6"><span class="calibre7">sched_yield()</span></code>system call.</p><div class="calibre_3"> </div>
<p class="calibre_2"></p><div class="calibre_3" id="filepos300762"> </div>
<h3 class="calibre_21"><span class="bold">Conclusion</span></h3><div class="calibre_22"> </div>
<p class="calibre_2">The process scheduler is an important part of any kernel because running processes is (for most of us, at least) the point of using the computer in the first place. Juggling the demands of process scheduling is nontrivial, however: A large number of runnable processes, scalability concerns, trade-offs between latency and throughput, and the demands of various workloads make a one-size-fits-all algorithm hard to achieve. The Linux kernel’s new CFS process scheduler, however, comes close to appeasing all parties and providing an optimal solution for most use cases with good scalability through a novel, interesting approach.</p><div class="calibre_3"> </div>
<p class="calibre_2">The previous chapter covered process management. This chapter ruminated on the theory behind process scheduling and the specific implementation, algorithms, and interfaces used by the current Linux kernel. The next chapter covers the primary interface that the kernel provides to running processes: system calls.</p><div class="calibre_3"> </div>  <div class="mbp_pagebreak" id="calibre_pb_36"></div>
</body></html>
